\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\def\labelitemii{$\circ$}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Linear Algebra -- Đại Số Tuyến Tính}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Linear Algebra -- Đại Số Tuyến Tính}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/linear_algebra/NQBH_linear_algebra.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/linear_algebra/NQBH_linear_algebra.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basic}
Tôi được giải Nhì Đại số Olympic Toán Sinh viên 2014 (VMC2014) khi còn học năm nhất Đại học \& được giải Nhất Đại số Olympic Toán Sinh viên 2015 (VMC2015) khi học năm 2 Đại học. Nhưng điều đó không có nghĩa là tôi giỏi Đại số. Bằng chứng là 10 năm sau khi nhận các giải đó, tôi đang tự học lại Đại số tuyến tính với hy vọng có 1 hay nhiều cách nhìn mới mẻ hơn \& mang tính ứng dụng hơn cho các đề tài cá nhân của tôi.

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{itemize}
	\item \cite{Hung_linear_algebra}. {\sc Nguyễn Hữu Việt Hưng}. {\it Đại Số Tuyến Tính}.
	\item \cite{Tiep_ML_co_ban}. {\sc Vũ Hữu Tiệp}. {\it Machine Learning Cơ Bản}.
	
	Mã nguồn cuốn ebook ``Machine Learning Cơ Bản'': \url{https://github.com/tiepvupsu/ebookMLCB}.
	
	Phép nhân từng phần{\tt/}tích Hadamard (Hadamard product) thường xuyên được sử dụng trong ML. Tích Hadamard của 2 ma trận cùng kích thước $A,B\in\mathbb{R}^{m\times n}$, được ký hiệu là $A\odot B = (a_{ij}b_{ij})_{i,j=1}^{m,n}\in\mathbb{R}^{m\times n}$.
	
	Việc chuyển đổi hệ cơ sở sử dụng ma trận trực giao có thể được coi như 1 phép xoay trục tọa độ. Nhìn theo 1 cách khác, đây cũng chính là 1 phép xoay vector dữ liệu theo chiều ngược lại, nếu ta coi các trục tọa độ là cố định.
	
	Việc phân tích 1 đại lượng toán học ra thành các đại lượng nhỏ hơn mang lại nhiều hiệu quả. Phân tích 1 số thành tích các thừa số nguyên tố giúp kiểm tra 1 số có bao nhiêu ước số. Phân tích đa thức thành nhân tử giúp tìm nghiệm của đa thức. Việc phân tích 1 ma trận thành tích của các ma trận đặc biệt cũng mang lại nhiều lợi ích trong việc giải hệ phương trình tuyến tính, tính lũy thừa của ma trận, xấp xỉ ma trận, $\ldots$
	
	{\bf Phép phân tích trị riêng.} Cách biểu diễn 1 ma trận vuông $A$ với ${\bf x}_i\ne{\bf 0}$ là các vector riêng của 1 ma trận vuông $A$ ứng với các giá trị riêng lặp hoặc phức $\lambda_i$: $A{\bf x}_i = \lambda_i{\bf x}_i$, $\forall i = 1,\ldots,n$: $A = X\Lambda X^{-1}$ với $\Lambda = {\rm diag}(\lambda_1,\ldots,\lambda_n)$, $X = [{\bf x}_1,\ldots,{\bf x}_n]$.
	
	{\bf Norm -- Chuẩn.} Khoảng cách Euclid chính là độ dài đoạn thẳng nối 2 điểm trong mặt phẳng. Đôi khi, để đi từ 1 điểm này tới 1 điểm kia, không thể đi bằng đường thẳng vì còn phụ thuộc vào hình dạng đường đi nối giữa 2 điểm. Cf. đường trắc địa trong Hình học Vi phân -- geodesics in Differential Geometry. Việc đo khoảng cách giữa 2 điểm dữ liệu nhiều chiều rất cần thiết trong ML -- chính là lý do khái niệm {\it chuẩn} (norm) ra đời.
	
	{\bf Trace -- Vết.} {\it Vết} (trace) của 1 ma trận vuông $A$ được ký hiệu là $\operatorname{trace}A$ là tổng tất cả các phần tử trên đường chéo chính của nó. Hàm vết xác định trên tập các ma trận vuông được sử dụng nhiều trong tối ưu vì nó có các tính chất đẹp.
	
	{\bf Kiểm tra gradient.} Việc tính gradient của hàm nhiều biến thông thường khá phức tạp \& rất dễ mắc lỗi. Trong thực nghiệm, có 1 cách để kiểm tra liệu gradient tính được có chính xác không. Cách này dựa trên định nghĩa của đạo hàm cho hàm 1 biến.
	\item \cite{Trefethen_Bau1997,Trefethen_Bau2022}. {\sc Lloyd N. Trefethen, David Bau III}. {\it Numerical Linear Algebra}.
\end{itemize}

%------------------------------------------------------------------------------%

\section{Wikipedia}

\subsection{Wikipedia{\tt/}abstract structure}
``An {\it abstract structure} is an \href{https://en.wikipedia.org/wiki/Abstraction}{abstraction} that might be of the \href{https://en.wikipedia.org/wiki/Euclidean_space}{geometric spaces} or a set structure, or a \href{https://en.wikipedia.org/wiki/Hypostatic_abstraction}{hypostatic abstraction} that is defined by a set of mathematical theorems \& laws, properties, \& relationships in a way that is logically if not always historically independent\footnote{However historical dependencies are partially considered in event theory as part of the \href{https://en.wikipedia.org/wiki/Combinatorics}{combinatorics} theory in \href{https://en.wikipedia.org/wiki/Kolmogorov_complexity}{Kolmogorov complexity} \& \href{https://en.wikipedia.org/wiki/Wiener-Khinchin_theorem}{Kolmogorov-Khinchin} equations.} of the structure of contingent experiences, e.g., those involving physical objects. Abstract structures are studied not only in \href{https://en.wikipedia.org/wiki/Logic}{local} \& \href{https://en.wikipedia.org/wiki/Mathematics}{mathematics} but in the fields that apply them, as \href{https://en.wikipedia.org/wiki/Computer_science}{computer science} \& \href{https://en.wikipedia.org/wiki/Computer_graphics}{computer graphics}, \& in the studies that reflect on them, such as \href{https://en.wikipedia.org/wiki/Philosophy}{philosophy} (especially the \href{https://en.wikipedia.org/wiki/Philosophy_of_mathematics}{philosophy of mathematics}). Indeed, modern mathematics has been defined in a very general sense as the study of abstract structures (by the \href{https://en.wikipedia.org/wiki/Nicolas_Bourbaki}{Bourbaki} group: see discussion there, at \href{https://en.wikipedia.org/wiki/Algebraic_structure}{algebraic structure} \& also structure).

An abstract structure may be represented (perhaps with some degree of approximation) by 1 or more physical objects -- this is called an implementation or \href{https://en.wikipedia.org/wiki/Instantiation_principle}{instantiation} of the abstract structure. But the abstract structure itself is defined in a way that is not dependent on the properties of any particular implementation.

An abstract structure has a richer structure than a \href{https://en.wikipedia.org/wiki/Concept}{concept} or an \href{https://en.wikipedia.org/wiki/Idea}{idea}. An abstract structure must include precise rules of behavior which can be used to determine whether a candidate implementation actually matches the abstract structure in question, \& it must be free from \href{https://en.wikipedia.org/wiki/Contradiction}{contradictions}. Thus we may debate how well a particular government fits the concept of \href{https://en.wikipedia.org/wiki/Democracy}{democracy}, but there is no room for debate over whether a given sequence of moves is or is not a valid game of chess (e.g. \href{https://en.wikipedia.org/wiki/Kasparov}{Kasparovian} approaches).

\subsubsection{Examples}

\begin{itemize}
	\item A \href{https://en.wikipedia.org/wiki/Sorting_algorithm}{sorting algorithm} is an abstract structure, but a \href{https://en.wikipedia.org/wiki/Recipe}{recipe} is not, because it depends on the properties \& quantities of its ingredients.
	\item A simple \href{https://en.wikipedia.org/wiki/Melody}{melody} is an abstract structure, but an \href{https://en.wikipedia.org/wiki/Orchestration}{orchestration} is not, because it depends on the properties of particular instruments.
	\item \href{https://en.wikipedia.org/wiki/Euclidean_geometry}{Euclidean geometry} is an abstract structure, but the theory of \href{https://en.wikipedia.org/wiki/Continental_drift}{continent drift} is not, because it depends on the geology of the \href{https://en.wikipedia.org/wiki/Earth}{Earth}.
	\item A \href{https://en.wikipedia.org/wiki/Formal_language}{formal language} is an abstract structure, but a \href{https://en.wikipedia.org/wiki/Natural_language}{natural language} is not, because its rules of grammar \& syntax are open to debate \& interpretation.
\end{itemize}

\subsection{Wikipedia{\tt/}direct sum}
``The {\it direct sum} is an \href{https://en.wikipedia.org/wiki/Operation_(mathematics)}{operation} between \href{https://en.wikipedia.org/wiki/Mathematical_structure}{structures} in \href{https://en.wikipedia.org/wiki/Abstract_algebra}{abstract algebra}, a branch of mathematics. It is defined differently, but analogously, for different kinds of structures. E.g., the direct sum of 2 abelian groups $A,B$ is another abelian group $A\oplus B$ consisting of the ordered pairs $(a,b)$ where $a\in A,b\in B$. To add ordered pairs, we define the sum $(a,b) + (c,d)\coloneqq(a + c,b + d)$, i.e., addition is defined coordinate-wise. E.g., the direct sum $\mathbb{R}\oplus\mathbb{R}$ where $\mathbb{R}$ is \href{https://en.wikipedia.org/wiki/Real_coordinate_space}{real coordinate space}, is the \href{https://en.wikipedia.org/wiki/Cartesian_plane}{Cartesian plane} $\mathbb{R}^2$. A similar process can be used to form the direct sum of 2 \href{https://en.wikipedia.org/wiki/Vector_space}{vector spaces} or 2 \href{https://en.wikipedia.org/wiki/Module_(mathematics)}{modules}.

We can also form direct sums with any finite number of summands, e.g., $A\oplus B\oplus C$, provided $A,B,C$ are the same kinds of algebraic structures (e.g., all abelian groups, or all vector spaces). This relies on the fact that the direct sum is \href{https://en.wikipedia.org/wiki/Associative}{associative} \href{https://en.wikipedia.org/wiki/Up_to}{up to} \href{https://en.wikipedia.org/wiki/Isomorphism}{isomorphism}. I.e., $(A\oplus B)\oplus C\cong A\oplus(B\oplus C)$ for any algebraic structures $A,B,C$ of the same kind. The direct sum is also \href{https://en.wikipedia.org/wiki/Commutative}{commutative} up to isomorphism, i.e., $A\oplus B\cong B\oplus A$ for any algebraic structures $A,B$ of the same kind.

The direct sum of finitely many abelian groups, vector spaces, or modules is \href{https://en.wikipedia.org/wiki/Isomorphism}{canonically isomorphic} to the corresponding \href{https://en.wikipedia.org/wiki/Direct_product}{direct product}. This is false, however, for some algebraic object, like nonabelian groups.

In the case where infinitely many objects are combined, the direct sum \& direct product are not isomorphic, even for abelian groups, vector spaces, or modules. E.g., consider the direct sum \& direct product of (countably) infinitely many copies of the integers. All element in the direct product is an infinite sequence, e.g., $(1,2,3,\ldots)$ but in the direct sum, there is a requirement that all but finitely many coordinates be zero, so the sequence $(1,2,3,\ldots)$ would be an element of the direct product but not of the direct sum, while $(1,2,0,0,\ldots)$ would be an element of both. Often, if a $+$ sign is used, all but finitely many coordinates must be zero, while if some form of multiplication is used, all but finitely many coordinates must be 1. In more technical language, if the summands are $(A_i) _{i\in I}$, the direct sum $\bigoplus_{i\in I} A_i$ is defined to be the set of tuples $(a_i)_{i\in I}$ with $a_i\in A_i$ s.t. $a_i = 0$ for all but finitely many $i$. The direct sum $\bigoplus_{i\in I} A_i$ is contained in the \href{https://en.wikipedia.org/wiki/Direct_product}{direct product} $\prod_{i\in I} A_i$, but is strictly smaller when the \href{https://en.wikipedia.org/wiki/Index_set}{index set} $I$ is infinite, because an element of the direct product can have infinitely many nonzero coordinates.

\subsubsection{Examples}
The $xy$-plane, a 2D \href{https://en.wikipedia.org/wiki/Vector_space}{vector space}, can be thought of as the direct sum of 2 1D vector spaces, namely the $x$ \& $y$ axes. In this direct sum, the $x,y$ axes intersect only at the origin (the zero vector). Addition is defined coordinate-wise, i.e., $(x_1,y_1) + (x_2,y_2)\coloneqq(x_1 + x_2,y_1 + y_2)$, which is the same as vector addition.

Given 2 structures $A,B$, their direct sum is written as $A\oplus B$. Given an \href{https://en.wikipedia.org/wiki/Indexed_family}{indexed family} of structures $A_i$, indexed with $i\in I$, the direct sum may be written $A = \bigoplus_{i\in I} A_i$. Each $A_i$ is called a {\it direct summand} of $A$. If the index set is finite, the direct sum is the same as the direct product. In the case of groups, if the group operation is written as $+$ the phrase ``direct sum'' is used, while if the group operation is written $*$ the phrase ``direct product'' is used. When the index set is infinite, the direct sum is not the same as the direct product since the direct sum has the extra requirement that all but finitely many coordinates must be 0.

\paragraph{Internal \& external direct sums.} A distinction is made between internal \& external direct sums, though the 2 are isomorphic. If the summands are defined 1st, \& then the direct sum is defined in terms of the summands, we have an external direct sum. E.g., if we define the real numbers $\mathbb{R}$ \& then define $\mathbb{R}\oplus\mathbb{R}$ the direct sum is said to be {\it external}.

If, on the other hand, 1st define some algebraic structure $S$ \& then write $S$ as a direct sum of 2 substructures $V,W$, then the direct sum is said to be internal. In this case, each element of $S$ is expressible uniquely as an algebraic combination of an element of $V$ \& an element of $W$. For an example of an internal direct sum, consider $\mathbb{Z}_6$ (the integers modulo 6), whose elements are $\{0,1,2,3,4,5\}$. This is expressible as an internal direct sum $\mathbb{Z}_6 = \{0,2,4\}\oplus\{0,3\}$.

\subsubsection{Types of direct sum}
[$\ldots$]

\subsubsection{Homomorphisms}
The direct sum $\bigoplus_{i\in I} A_i$ comes equipped with a \href{https://en.wikipedia.org/wiki/Projection_(mathematics)}{\it projection} \href{https://en.wikipedia.org/wiki/Homomorphism}{homomorphism} $\pi_j:\bigoplus_{i\in I} A_i\to A_j$ for each $j\in I$ \& a {\it coprojection} $\alpha_j:A_j\to\bigoplus_{i\in I} A_i$ for each $j\in I$. Given another algebraic structure $B$ (with the same additional structure) \& homomorphisms $g_j:A_j\to B$, $\forall j\in I$, there is a unique homomorphism $g:\bigoplus_{i\in I} A_i\to B$, called the sum of the $g_j$, s.t. $g\alpha_j = g_j$, $\forall j$. Thus the direct sum is the \href{https://en.wikipedia.org/wiki/Coproduct}{coproduct} in the appropriate \href{https://en.wikipedia.org/wiki/Category_(mathematics)}{category}.'' -- \href{https://en.wikipedia.org/wiki/Direct_sum}{Wikipedia{\tt/}direct sum}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}mathematical structure}
``In mathematics, a structure on a set (or on some sets) refers to providing it (or them) with certain additional features (e.g. an \href{https://en.wikipedia.org/wiki/Operation_(mathematics)}{operation}, \href{https://en.wikipedia.org/wiki/Relation_(mathematics)}{relation}, \href{https://en.wikipedia.org/wiki/Metric_(mathematics)}{metric}, or \href{https://en.wikipedia.org/wiki/Topological_space}{topology}). The additional features are attached or related to the set (or to the sets), so as to provide it (or them) with some additional meaning or significance.

A partial list of possible structures are \href{https://en.wikipedia.org/wiki/Measure_theory}{measures}, \href{https://en.wikipedia.org/wiki/Algebraic_structure}{algebraic structures} (\href{https://en.wikipedia.org/wiki/Group_(mathematics)}{groups}, \href{https://en.wikipedia.org/wiki/Field_(mathematics)}{fields}, etc.), \href{https://en.wikipedia.org/wiki/Topology}{topologies}, \href{https://en.wikipedia.org/wiki/Metric_space}{metric structures} (\href{https://en.wikipedia.org/wiki/Geometry}{geometries}), \href{https://en.wikipedia.org/wiki/Order_theory}{orders}, \href{https://en.wikipedia.org/wiki/Graph_theory}{graphs}, \href{https://en.wikipedia.org/wiki/Event_structure}{events}, \href{https://en.wikipedia.org/wiki/Equivalence_relation}{equivalence relations}, \href{https://en.wikipedia.org/wiki/Differential_structure}{differential structures}, \& \href{https://en.wikipedia.org/wiki/Category_(mathematics)}{categories}.

Sometimes, a set is endowed with $> 1$ feature simultaneously, which allows mathematicians to study the interaction between the different structures more richly. E.g., an ordering imposes a rigid form, shape, or topology on the set, \& if a set has both a topology feature \& a group feature, s.t. these 2 features are related in a certain way, then the structure becomes a \href{https://en.wikipedia.org/wiki/Topological_group}{topological group}.

\href{https://en.wikipedia.org/wiki/Map_(mathematics)}{Map} between 2 sets with the same type of structure, which preserve this structure [\href{https://en.wikipedia.org/wiki/Morphism}{morphism}: structure in the \href{https://en.wikipedia.org/wiki/Domain_of_a_function}{domain} is mapped properly to the (same type) structure in the \href{https://en.wikipedia.org/wiki/Codomain}{codomain}] is of special interest in many fields of mathematics. E.g.: \href{https://en.wikipedia.org/wiki/Homomorphisms}{homomorphisms}, which preserve algebraic structures; \href{https://en.wikipedia.org/wiki/Continuous_functions}{continuous functions}, which preserve topological structures; \& \href{https://en.wikipedia.org/wiki/Differentiable_functions}{differential functions}, which preserve differential structures.

\subsubsection{History}
In 1939, the French group with the pseudonym \href{https://en.wikipedia.org/wiki/Nicolas_Bourbaki}{\sc Nicolas Bourbaki} saw structures as the root of mathematics. They 1st mentioned them in their ``Fascicule'' of {\it Theory of Sets} \& expanded it into Chap. IV of the 1957 edition. They identified 3 {\it mother structures}: algebraic, topological, \& order.

\subsubsection{Example: the real numbers}
``The set of \href{https://en.wikipedia.org/wiki/Real_number}{real numbers} has several standard structures:
\begin{itemize}
	\item An order: each number is either less than or greater than any other number.
	\item Algebraic structure: there are operations of addition \& multiplication, the 1st of which makes it into a \href{https://en.wikipedia.org/wiki/Group_theory}{group} \& the pair of which together make it into a \href{https://en.wikipedia.org/wiki/Field_(mathematics)}{field}.
	\item A measure: \href{https://en.wikipedia.org/wiki/Interval_(mathematics)}{intervals} of the real line have a specific \href{https://en.wikipedia.org/wiki/Length}{length}, which can be extended to the \href{https://en.wikipedia.org/wiki/Lebesgue_measure}{Lebesgue measure} on many of its \href{https://en.wikipedia.org/wiki/Subset}{subsets}.
	\item A metric: there is a notion of \href{https://en.wikipedia.org/wiki/Metric_(mathematics)}{distance} between points.
	\item A geometry: it is equipped with a \href{https://en.wikipedia.org/wiki/Metric_(mathematics)}{metric} \& is \href{https://en.wikipedia.org/wiki/Flat_space}{flat}.
	\item A topology: there is a notion of \href{https://en.wikipedia.org/wiki/Open_set}{open sets}.
\end{itemize}
There are interfaces among these:
\begin{itemize}
	\item Its order \&, independently, its metric structure induce its topology.
	\item Its order \& algebraic structure make it into an \href{https://en.wikipedia.org/wiki/Ordered_field}{ordered field}.
	\item Its algebraic structure \& topology make it into a \href{https://en.wikipedia.org/wiki/Lie_group}{Lie group}, a type of \href{https://en.wikipedia.org/wiki/Topological_group}{topological group}.'' -- \href{https://en.wikipedia.org/wiki/Mathematical_structure}{Wikipedia{\tt/}mathematical structure}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}numerical linear algebra}
``{\it Numerical linear algebra}, sometimes called {\it applied linear algebra}, is the study of how \href{https://en.wikipedia.org/wiki/Matrix_operation}{matrix operations} can be used to create \href{https://en.wikipedia.org/wiki/Algorithms}{compute algorithms} which \href{https://en.wikipedia.org/wiki/Algorithmic_efficiency}{efficiently} \& accurately provide approximate answers to questions in continuous mathematics. It is a subfield of numerical analysis, \& a type of linear algebra. Computers use \href{https://en.wikipedia.org/wiki/Floating-point_arithmetic}{floating-point arithmetic} \& cannot exactly represent \href{https://en.wikipedia.org/wiki/Irrational_number}{irrational} data, so when a computer is applied to a matrix of data, it can sometimes \href{https://en.wikipedia.org/wiki/Propagation_of_uncertainty}{increase the difference} between a number stored in the computer \& the true number that it is an approximation of. Numerical linear algebra uses properties of vectors \& matrices to develop computer algorithms that minimize the error introduced by the computer, \& is also concerned with ensuring that the algorithm is as efficient as possible.

Numerical linear algebra aims to solve problems of continuous mathematics using finite precise computers, so its applications to the \href{https://en.wikipedia.org/wiki/Natural_science}{natural science} \& \href{https://en.wikipedia.org/wiki/Social_science}{social sciences} are as vast as the applications of continuous mathematics. It is often a fundamental part of \href{https://en.wikipedia.org/wiki/Engineering}{engineering} \& \href{https://en.wikipedia.org/wiki/Computational_science}{computational science} problems, e.g., \href{https://en.wikipedia.org/wiki/Image_processing}{image processing} \& \href{https://en.wikipedia.org/wiki/Signal_processing}{signal processing}, \href{https://en.wikipedia.org/wiki/Telecommunication}{telecommunication}, \href{https://en.wikipedia.org/wiki/Computational_finance}{computational finance}, \href{https://en.wikipedia.org/wiki/Materials_science}{materials science} simulations, \href{https://en.wikipedia.org/wiki/Structural_biology}{structural biology}, \href{https://en.wikipedia.org/wiki/Data_mining}{data mining}, \href{https://en.wikipedia.org/wiki/Bioinformatics}{bioinformatics}, \& \href{https://en.wikipedia.org/wiki/Fluid_dynamics}{fluid dynamics}. Matrix methods are particularly used in FDMs, FEMs, \& the modeling of differential equations. Noting the broad applications of numerical linear algebra, \href{https://en.wikipedia.org/wiki/Lloyd_N._Trefethen}{\sc Lloyd N. Trefethen} \& {\sc David Bau III} argue that it is ``as fundamental to the mathematical sciences as calculus \& differential equations'', even though it is a comparatively small field. Because many properties of matrices \& vectors also apply to functions \& operators, numerical linear algebra can also be viewed as a type of functional analysis which has a particular emphasis on practical algorithms.

Common problems in numerical linear algebra include obtaining matrix decompositions like \href{https://en.wikipedia.org/wiki/Singular_value_decomposition}{singular value decomposition}, \href{https://en.wikipedia.org/wiki/QR_factorization}{QR factorization}, \href{https://en.wikipedia.org/wiki/LU_factorization}{LU factorization}, or \href{https://en.wikipedia.org/wiki/Eigendecomposition}{eigendecomposition}, which can then be used to answer common linear algebraic problems like solving linear systems of equations, locating eigenvalues, or least squares optimization. Numerical linear algebra's central concern with developing algorithms that do not introduce errors when applied to real data on a finite precision computer is often achieved by \href{https://en.wikipedia.org/wiki/Iterative}{iterative} methods rather than direct ones.

\subsubsection{History}
Numerical linear algebra was developed by computer pioneers like \href{https://en.wikipedia.org/wiki/John_von_Neumann}{\sc John von Neumann}, \href{https://en.wikipedia.org/wiki/Alan_Turing}{\sc Alan Turing}, \href{https://en.wikipedia.org/wiki/James_H._Wilkinson}{\sc James H. Wilkinson}, \href{https://en.wikipedia.org/wiki/Alston_Scott_Householder}{\sc Alston Scott Householder}, \href{https://en.wikipedia.org/wiki/George_Forsythe}{\sc George Forsythe}, \& \href{https://en.wikipedia.org/wiki/Heinz_Rutishauser}{Heinz Rutishauser}, in order to apply the earliest computers to problems in continuous mathematics, e.g. ballistics problems \& the solutions to systems of PDEs. The 1st serious attempt to minimize computer error in the application of algorithms to real data is {\sc John von Neumann} \& \href{https://en.wikipedia.org/wiki/Herman_Goldstine}{\sc Herman Goldstine}'s work in 1947. The field has grown as technology has increasingly enabled researchers to solve complex problems on extremely large high-precision matrices, \& some numerical algorithms have grown in prominence as technologies like \href{https://en.wikipedia.org/wiki/Parallel_computing}{parallel computing} have made them practical approaches to scientific problems.

\subsubsection{Matrix decompositions}
Main article: \href{https://en.wikipedia.org/wiki/Matrix_decomposition}{Wikipedia{\tt/}matrix decomposition}.
\begin{itemize}
	\item {\bf Partitioned matrices.} Main article: \href{https://en.wikipedia.org/wiki/Block_matrix}{Wikipedia{\tt/}block matrix}. For many problems in applied linear algebra, it is useful to adopt the perspectives of a matrix as being a concatenation of column vectors. E.g., when solving the linear system ${\bf x} = A^{-1}{\bf b}$, rather than understanding ${\bf x}$ as the product $A^{-1}$ with ${\bf b}$, it is helpful to think of ${\bf x}$ as the vector of \href{https://en.wikipedia.org/wiki/Coefficient}{coefficients} in the linear expansion of ${\bf b}$ in the \href{https://en.wikipedia.org/wiki/Basis_(linear_algebra)}{basis} formed by the columns of $A$. Thinking of matrices as a concatenation of columns is also a practical approach for the purposes of matrix algorithms. This is because matrix algorithms frequently contain 2 nested loops: one over the columns of a matrix $A$, \& another over the rows of $A$. E.g., for matrices $A^{m\times n}$ \& vectors $x^{n\times1},y^{m\times1}$, we could use the column partitioning perspective to compute $y\coloneqq Ax + y$ as
	\begin{verbatim}
for q = 1:n
    for p = 1:m
        y(p) = A(p,q)*x(q) + y(p)
    end
end
	\end{verbatim}
	\item {\bf Singular value decomposition.} Main article: \href{https://en.wikipedia.org/wiki/Singular_value_decomposition}{Wikipedia{\tt/}singular value decompostion}. The singular value decomposition of a matrix $A^{m\times n}$ is $A = U\Sigma V^\star$ where $U,V$ are \href{https://en.wikipedia.org/wiki/Unitary_matrix}{unitary}, \& $\Sigma$ is \href{https://en.wikipedia.org/wiki/Diagonal_matrix}{diagonal}. The diagonal entries of $\Sigma$ are called the \href{https://en.wikipedia.org/wiki/Singular_values}{singular values} of $A$. Because singular values are the square roots of the \href{https://en.wikipedia.org/wiki/Eigenvalues}{eigenvalues} of $AA^\star$, there is a tight connection between the singular value decomposition \& eigenvalue decompositions, i.e., most methods for computing the singular value decomposition are similar to eigenvalue methods; perhaps the most common method involves \href{https://en.wikipedia.org/wiki/Householder_transformation}{Householder procedures}.
	\item {\bf QR factorization.} Main article: \href{https://en.wikipedia.org/wiki/QR_decomposition}{Wikipedia{\tt/}QR decompostion}. The QR factorization of a matrix $A^{m\times n}$ is a matrix $Q^{m\times m}$ \& a matrix $R^{m\times n}$ so that $A = QR$, where $Q$ is \href{https://en.wikipedia.org/wiki/Orthogonal_matrix}{orthogonal} \& $R$ is \href{https://en.wikipedia.org/wiki/Triangular_matrix}{upper triangular}. The 2 main algorithms for computing QR factorizations are the \href{https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process}{Gram--Schmidt process} \& the \href{https://en.wikipedia.org/wiki/Householder_transformation}{Householder transformation}. The QR factorization is often used to solve \href{https://en.wikipedia.org/wiki/Linear_least-squares}{linear least-squares} problems, \& eigenvalue problems (by way of the iterative \href{https://en.wikipedia.org/wiki/QR_algorithm}{QR algorithm}).
	\item {\bf LU factorization.} Main article: \href{https://en.wikipedia.org/wiki/LU_decomposition}{Wikipedia{\tt/}LU decomposition}. An LU factorization of a matrix $A$ consists of a lower triangular matrix $L$ \& an upper triangular matrix $U$ so that $A = LU$. The matrix $U$ is found by an upper triangularization procedure which involves left-multiplying $A$ by a series of matrices $M_1,\ldots,M_{n-1}$ to form the product $M_{n-1}\cdots M_1A = U$, so that equivalently $L = M_1^{-1}\cdots M_{n-1}^{-1}$.
	\item {\bf Eigenvalue decomposition.} Main article: \href{https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix}{Wikipedia{\tt/}eigendecomposition of a matrix}. The eigenvalue decomposition of a matrix $A^{m\times m}$ is $A = X\Lambda X^{-1}$, where the columns of $X$ are the eigenvectors of $A$, \& $\Lambda$ is a diagonal matrix the diagonal entries of which are the corresponding eigenvalues of $A$. There is no direct method for finding the eigenvalue decomposition of an arbitrary matrix. Because it is not possible to write a program that finds the exact roots of an arbitrary polynomial in finite time, any general eigenvalue solver must necessarily be iterative.
\end{itemize}

\subsubsection{Algorithms}

\begin{enumerate}
	\item {\bf Gaussian elimination.}
	\item {\bf Solutions of linear systems.}
	\item {\bf Least squares optimization.}
\end{enumerate}

\subsubsection{Conditioning \& stability}

\subsubsection{Iterative methods}

\subsubsection{Software}
Main article: \href{https://en.wikipedia.org/wiki/List_of_numerical_analysis_software}{Wikipedia{\tt/}list of numerical analysis software}. Several programming languages use numerical linear algebra optimization techniques \& are designed to implement numerical linear algebra algorithms. These languages include \href{https://en.wikipedia.org/wiki/MATLAB}{MATLAB}, \href{https://en.wikipedia.org/wiki/Analytica_(software)}{Analytica}, \href{https://en.wikipedia.org/wiki/Maple_(software)}{Maple}, \& \href{https://en.wikipedia.org/wiki/Mathematica}{Mathematica}. Other programming languages which are not explicitly designed for numerical linear algebra have libraries that provide numerical linear algebra routines \& optimization; C \& Fortran have packages like \href{https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms}{Basic Linear Algebra Subprograms} \& \href{https://en.wikipedia.org/wiki/LAPACK}{LAPACK}, Python has the library \href{https://en.wikipedia.org/wiki/NumPy}{NumPy}, \& \href{https://en.wikipedia.org/wiki/Perl}{Perl} has the \href{https://en.wikipedia.org/wiki/Perl_Data_Language}{Perl Data Language}. Many numerical linear algebra commands in R rely on these more fundamental libraries like LAPACK. More libraries can be found on the \href{https://en.wikipedia.org/wiki/List_of_numerical_libraries}{list of numerical libraries}.'' -- \href{https://en.wikipedia.org/wiki/Numerical_linear_algebra}{Wikipedia{\tt/}numerical linear algebra}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}