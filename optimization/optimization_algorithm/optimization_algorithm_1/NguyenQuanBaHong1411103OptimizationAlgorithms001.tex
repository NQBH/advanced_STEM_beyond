\documentclass[a4paper]{article}
\usepackage{longtable,float,hyperref,color,amsmath,amsxtra,amssymb,latexsym,amscd,amsthm,amsfonts,graphicx}
\numberwithin{equation}{section}
\allowdisplaybreaks
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[RE,LO]{\footnotesize \textsc \leftmark}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\usepackage{imakeidx}
\makeindex[columns=2, title=Alphabetical Index, 
           options= -s index.ist]
\title{\huge Optimization Algorithms Assignment 001}
\author{\textsc{Nguyen Quan Ba Hong}\footnote{Student ID: 1411103}\\
{\small Students at Faculty of Math and Computer Science,}\\ 
{\small Ho Chi Minh University of Science, Vietnam} \\
{\small \texttt{email. nguyenquanbahong@gmail.com}}\\
{\small \texttt{blog. \url{www.nguyenquanbahong.com}} 
\footnote{Copyright \copyright\ 2016-2018 by Nguyen Quan Ba Hong, Student at Ho Chi Minh University of Science, Vietnam. This document may be copied freely for the purposes of education and non-commercial research. Visit my site \texttt{\url{www.nguyenquanbahong.com}} to get more.}}}
\begin{document}
\maketitle
\begin{abstract}
This assignment aims at solving some selected problems for the mid-term exam of the course \textit{Optimization Algorithms}.
\end{abstract}
\newpage
\tableofcontents
\newpage
\section{Problems}
\textbf{Problem 1.1.} \textit{Let $\Omega  \subseteq {\mathbb{R}^n}$ be a convex set, $k\in \mathbb{N}^\star$, $x_i \in \Omega$, $\lambda _i \ge  0$ for $i=1,\ldots,k$ and $\sum\nolimits_{i = 1}^k {{\lambda _i}}  = 1$. Prove that $\sum\nolimits_{i = 1}^k {{\lambda _i}{x_i}}  \in \Omega $.}\\
\\
\textsc{Proof.} The case $k=2$ can be deduced directly from the definition of convex sets. Given $x_i \in \Omega$ for $i=1,\ldots,k$, we suppose, for some $k>2$, that 
\begin{align}
\label{1.1}
\left( {{x_i} \in \Omega , \hspace{0.1cm}{\lambda _i} \ge 0,\hspace{0.1cm}i = 1, \ldots ,k - 1,\mbox{ and } \sum\limits_{i = 1}^{k - 1} {{\lambda _i}}  = 1} \right) \Rightarrow \sum\limits_{i = 1}^{k - 1} {{\lambda _i}{x_i}}  \in \Omega .
\end{align}
Then for any $k$-tuple $\left(\lambda _1,\ldots,\lambda _k \right)$ satisfying $\lambda _i\ge 0$ for $i=1,\ldots,k$ and $\sum\nolimits_{i = 1}^k {{\lambda _i}}  = 1$. If $\lambda _k =1$, then $\lambda _i=0$, for $i=1,\ldots,k-1$. Thus, $\sum\nolimits_{i = 1}^k {{\lambda _i}{x_i}}  = {x_k} \in \Omega $. If $\lambda _k <1$, we have $\sum\nolimits_{i = 1}^{k - 1} {\frac{{{\lambda _i}}}{{1 - {\lambda _k}}}}  = 1$. Then \eqref{1.1} implies that $\sum\nolimits_{i = 1}^{k - 1} {\frac{{{\lambda _i}{x_i}}}{{1 - {\lambda _k}}}}  \in \Omega $. Hence, 
\begin{align}
\sum\limits_{i = 1}^k {{\lambda _i}{x_i}}  &= \sum\limits_{i = 1}^{k - 1} {{\lambda _i}{x_i}}  + {\lambda _k}{x_k}\\
 &= \left( {1 - {\lambda _k}} \right)\sum\limits_{i = 1}^{k - 1} {\frac{{{\lambda _i}}}{{1 - {\lambda _k}}}{x_i}}  + {\lambda _k}{x_k} \in \Omega .
\end{align}
By the principle of mathematical induction, we deduce that \eqref{1.1} holds for all $k \in \mathbb{N}^\star$. \hfill $\square$\\
\\
\textbf{Problem 1.2.} \textit{Use characterizations of convex functions, check whether the following functions is convex or not.}\footnote{Notation: ${\mathbb{R}_ + } = \left\{ {x \in \mathbb{R};x \ge 0} \right\}$, ${\mathbb{R}_{ +  + }} = \left\{ {x \in \mathbb{R};x > 0} \right\}$.}
\begin{enumerate}
\item \textit{$f\left( x \right) = {e^{\alpha x}} - x$ in the domain $\mathbb{R}$.}
\item \textit{$f\left( x \right) = {x^q}$ for $q>1$, in the domain $\mathbb{R}_+$.}
\item \textit{$f\left( x \right) =  - \ln x$ in the domain $\mathbb{R}_{++}$.}
\item \textit{$f\left( x \right) =  x\ln x$ in the domain $\mathbb{R}_{++}$.}
\item \textit{$f\left( {{x_1},{x_2}} \right) = x_1^2 + x_2^2 - {x_1}{x_2} + {x_1} - 2{x_2}$ in the domain $\mathbb{R}^2$.}
\item \textit{$f\left( {{x_1},{x_2}} \right) = {x_1}{x_2}$ in the domain $\mathbb{R}_{++}^2$.}
\item \textit{$f\left( {{x_1},{x_2}} \right) = \frac{{x_1^2}}{{{x_2}}}$ in the domain $\mathbb{R}\times \mathbb{R}_{++}$.}
\item \textit{$f\left( {{x_1},{x_2}} \right) = x_1^\alpha x_2^{1 - \alpha }$ for $0\le \alpha \le 1$, in the domain $\mathbb{R}_{++}^2$.}
\item \textit{$f\left( {{x_1},{x_2},{x_3}} \right) = x_1^2 + x_2^2 + x_3^2 - {x_1}{x_2} - {x_2}{x_3} - {x_3}{x_1}$ in the domain $\mathbb{R}^3$.}
\end{enumerate}
\textsc{Solution.} We mainly use the result ``$f$ is convex (resp. concave) on a convex set $C$ if and only if the Hessian matrix ${\nabla ^2}f\left( x \right)$ is positive (resp. negative) semi-definite for all $x\in C$'' to check the convexity of the given functions.
\begin{enumerate}
\item The second derivative of $f$ is given by $f''\left( x \right) = {\alpha ^2}{e^{\alpha x}} \ge 0$. Thus, $f$ is convex.
\item The second derivative of $f$ is given by $f''\left( x \right) = q\left( {q - 1} \right){x^{q - 2}} \ge 0$ for all $x\in \mathbb{R}_+$. Thus, $f$ is convex.
\item The second derivative of $f$ is given by $f''\left( x \right) = \frac{1}{{{x^2}}} >0$ for all $x\in \mathbb{R}_{++}$. Thus, $f$ is convex.
\item The second derivative of $f$ is given by $f''\left( x \right) = \frac{1}{x} > 0$ for all $x\in \mathbb{R}_{++}$. Thus, $f$ is convex.
\item The Hessian matrix of $f$ is given by
\begin{align}
{\nabla ^2}f\left( {{x_1},{x_2}} \right) = \left[ {\begin{array}{*{20}{c}}
2&{ - 1}\\
{ - 1}&2
\end{array}} \right], \mbox{ for all }\left(x_1,x_2\right)\in \mathbb{R}^2,
\end{align}
whose eigenvalues are $\lambda _1=1$, $\lambda _2 = 3$. Thus, ${\nabla ^2}f$ is positive definite and $f$ is convex.
\item Similarly, the Hessian matrix of $f$ is given by
\begin{align}
{\nabla ^2}f\left( {{x_1},{x_2}} \right) = \left[ {\begin{array}{*{20}{c}}
0&1\\
1&0
\end{array}} \right], \mbox{ for all }\left(x_1,x_2\right)\in \mathbb{R}_{++}^2,
\end{align}
whose eigenvalues are $\lambda _1=-1$, $\lambda _2 = 1$. Thus, $f$ is non-convex.
\item The Hessian matrix of $f$ is given by
\begin{align}
{\nabla ^2}f\left( {{x_1},{x_2}} \right) = \left[ {\begin{array}{*{20}{c}}
{\frac{2}{{{x_2}}}}&{ - \frac{{2{x_1}}}{{x_2^2}}}\\
{ - \frac{{2{x_1}}}{{x_2^2}}}&{\frac{{2x_1^2}}{{x_2^3}}}
\end{array}} \right], \mbox{ for all } \left( {{x_1},{x_2}} \right) \in \mathbb{R} \times {\mathbb{R}_{ +  + }},
\end{align}
whose eigenvalues are $\lambda _1=0$, ${\lambda _2} = \frac{{2\left( {x_1^2 + x_2^2} \right)}}{{x_2^3}} >0$. Thus, ${\nabla ^2}f$ is semi-positive definite and $f$ is convex.
\item The Hessian matrix of $f$ is given by
\begin{align}
{\nabla ^2}f\left( {{x_1},{x_2}} \right) = \alpha \left( {\alpha  - 1} \right)\left[ {\begin{array}{*{20}{c}}
{x_1^{\alpha  - 2}x_2^{1 - \alpha }}&{ - \frac{{x_1^{\alpha  - 1}}}{{x_2^\alpha }}}\\
{ - \frac{{x_1^{\alpha  - 1}}}{{x_2^\alpha }}}&{\frac{{x_1^\alpha }}{{x_2^{\alpha  + 1}}}}
\end{array}} \right],
\end{align}
for all $\left( {{x_1},{x_2}} \right) \in \mathbb{R}_{ +  + }^2$, whose eigenvalues are $\lambda _1=0$, ${\lambda _2} = \frac{{\alpha \left( {\alpha  - 1} \right)x_1^{\alpha  - 2}\left( {x_1^2 + x_2^2} \right)}}{{x_2^{\alpha  + 1}}}$. Since $0\le \alpha \le 1$, we have $\lambda _2\le 0$. Thus, $f$ is non-convex. 
\item The Hessian matrix of $f$ is given by
\begin{align}
{\nabla ^2}f\left( {{x_1},{x_2},{x_3}} \right) = \left[ {\begin{array}{*{20}{c}}
2&{ - 1}&{ - 1}\\
{ - 1}&2&{ - 1}\\
{ - 1}&{ - 1}&2
\end{array}} \right], \mbox{ for all } \left(x_1,x_2,x_3\right) \in \mathbb{R}^3,
\end{align}
whose eigenvalues are $\lambda _1=0$, $\lambda _2=\lambda _3 =3$. Thus, ${\nabla ^2}f$ is semi-positive definite, and $f$ is convex. \hfill $\square$
\end{enumerate}
\textbf{Problem 1.3.} \textit{Let ${f_1},{f_2}:{\mathbb{R}^n} \to \mathbb{R} \cup \left\{ { + \infty } \right\}$ be convex mappings. Define}
\begin{align}
g\left( x \right) = \min \left\{ {{f_1}\left( x \right),{f_2}\left( x \right)} \right\},\hspace{0.2cm} h\left( x \right) = \max \left\{ {{f_1}\left( x \right),{f_2}\left( x \right)} \right\}.
\end{align}
\textit{Which of these functions is convex?}\\
\\
\textsc{Solution.} Take ${f_1}\left( x \right) = {x^2}$, ${f_2}\left( x \right) = {\left( {x - 2} \right)^2}$ for all $x\in\mathbb{R}$. Since ${f_1}''\left( x \right) = {f_2}''\left( x \right) = 2$, both $f_1$ and $f_2$ are convex. We have $g\left( x \right) = \min \left\{ {{x^2},{{\left( {x - 2} \right)}^2}} \right\}$. Then $g\left( 0 \right) = \min \left\{ {0,4} \right\} = 0$, $g\left( 1 \right) = \min \left\{ {1,1} \right\} = 1$, $g\left( 2 \right) = \min \left\{ {4,0} \right\} =0$. Thus
\begin{align}
g\left( {\frac{1}{2} \cdot 0 + \frac{1}{2} \cdot 2} \right) = 1 > 0 = \frac{{g\left( 0 \right) + g\left( 2 \right)}}{2}.
\end{align}
This inequality implies that $g$ is non-convex. 

Next, we will prove that $h$ is convex. Indeed, for $i=1,2$, $x\in \mathbb{R}^n$, $y\in \mathbb{R}^n$, $t\in \left[0,1\right]$,
\begin{align}
{f_i}\left( {tx + \left( {1 - t} \right)y} \right) &\le t{f_i}\left( x \right) + \left( {1 - t} \right){f_i}\left( y \right)\\
& \le t\max \left\{ {{f_1}\left( x \right),{f_2}\left( x \right)} \right\} + \left( {1 - t} \right)\max \left\{ {{f_1}\left( y \right),{f_2}\left( y \right)} \right\}\\
& = th\left( x \right) + \left( {1 - t} \right)h\left( y \right).
\end{align}
Thus, for all $x\in \mathbb{R}^n$, $y\in \mathbb{R}^n$, $t\in \left[0,1\right]$,
\begin{align}
h\left( {tx + \left( {1 - t} \right)y} \right) &= \max \left\{ {{f_1}\left( {tx + \left( {1 - t} \right)y} \right),{f_2}\left( {tx + \left( {1 - t} \right)y} \right)} \right\}\\
& \le th\left( x \right) + \left( {1 - t} \right)h\left( y \right).
\end{align}
This completes our proof. \hfill $\square$\\
\\
\textbf{Problem 1.4.} \textit{Let $f:{\mathbb{R}^n} \to \mathbb{R} \cup \left\{ { + \infty } \right\}$ and $\alpha$ be an arbitrary real number. The level set $\alpha$ is defined as follows,}
\begin{align}
{L_\alpha }: = \left\{ {x \in {\mathbb{R}^n};f\left( x \right) \le \alpha } \right\}.
\end{align}
\begin{enumerate}
\item \textit{Prove that if $f$ is convex then for all $\alpha \in \mathbb{R}$, the  level set $L_\alpha$ is convex.}
\item \textit{The converse of the above statement is true or false? Why?}
\end{enumerate}
\textsc{Solution.} 
\begin{enumerate}
\item For $x\in L_\alpha$, $y\in L_\alpha$, we have $f\left(x\right)\le \alpha$, $f\left(y\right)\le \alpha$. Since $f$ is convex, for all $t\in \left[0,1\right]$,
\begin{align}
f\left( {tx + \left( {1 - t} \right)y} \right) &\le tf\left( x \right) + \left( {1 - t} \right)f\left( y \right)\\
& \le t\alpha  + \left( {1 - t} \right)\alpha \\
& = \alpha .
\end{align}
Hence, $tx + \left( {1 - t} \right)y \in {L_\alpha }$ for all $t\in \left[0,1\right]$, which implies that $L_\alpha$ is a convex set.
\item (Counter-example) Consider the function $f: \mathbb{R}\to \mathbb{R}_+$ defined as $f\left( x \right) = {\left| x \right|^{\frac{1}{2}}}$ for all $x\in \mathbb{R}$, we have $L_\alpha =\emptyset$ for all $\alpha <0$ and 
\begin{align}
{L_\alpha }: &= \left\{ {x \in {\mathbb{R}^n};f\left( x \right) \le \alpha } \right\}\\
 &= \left\{ {x \in {\mathbb{R}^n};{{\left| x \right|}^{\frac{1}{2}}} \le \alpha } \right\}\\
& = \left[ { - {\alpha ^2},{\alpha ^2}} \right] \mbox{ for all } \alpha \ge 0.
\end{align}
Combining both cases, $L_\alpha$ is convex for all $\alpha \in \mathbb{R}$. Now we check whether $f$ is convex or not. Since 
\begin{align}
2f\left( {\frac{1}{2}} \right) = \sqrt 2  > 1 = f\left( 0 \right) + f\left( 1 \right),
\end{align}
the function $f$ chosen is non-convex. Thus, the converse of the first statement fails in general. \hfill $\square$
\end{enumerate}
\textbf{Problem 1.5 (Jensen inequality).)} \textit{Let $f:\mbox{dom} f \subset {\mathbb{R}^n} \to \mathbb{R} \cup \left\{ { + \infty } \right\}$ be a convex function, $\mbox{dom}f$ is a convex set, $k\in \mathbb{N}^\star$, $x_1,\ldots,x_k\in \mbox{dom} f$ and $\lambda _i \ge 0$, for $i=1,\ldots,k$ satisfying $\sum\nolimits_{i = 1}^k {{\lambda _i}}  = 1$. Prove that}
\begin{align}
\label{1.24}
f\left( {\sum\limits_{i = 1}^k {{\lambda _i}{x_i}} } \right) \le \sum\limits_{i = 1}^k {{\lambda _i}f\left( {{x_i}} \right)} .
\end{align}
\textsc{Applications.} \textit{Use the convexity of the function $f\left(x\right) =-\ln x$.}
\begin{enumerate}
\item \textit{(Cauchy inequality) For $a_i\in \mathbb{R}_+$, $i=1,\ldots,n$,}
\begin{align}
\label{1.25}
\frac{{{a_1} +  \cdots  + {a_n}}}{n} \ge \sqrt[n]{{{a_1} \cdots {a_n}}} .
\end{align}
\item \textit{(H\"{o}lder inequality) Let $x,y \in \mathbb{R}^n$, $p>1$ and $\frac{1}{p}+\frac{1}{q}=1$. Prove that}
\begin{align}
\label{1.26}
\sum\limits_{i = 1}^n {{x_i}{y_i}}  \le {\left( {\sum\limits_{i = 1}^n {{{\left| {{x_i}} \right|}^p}} } \right)^{\frac{1}{p}}}{\left( {\sum\limits_{i = 1}^n {{{\left| {{y_i}} \right|}^q}} } \right)^{\frac{1}{q}}}.
\end{align}
\end{enumerate}
\textsc{Proof.} The case $k=2$ is deduced directly from the definition of convex functions. For some $k >2$, we suppose that
\begin{align}
\label{1.27}
\left( {{x_i} \in \mbox{dom}f,\hspace{0.1cm}{\lambda _i} \ge 0,\hspace{0.1cm}i = 1, \ldots ,k - 1,\hspace{0.1cm} \sum\limits_{i = 1}^{k - 1} {{\lambda _i}}  = 1} \right) \Rightarrow f\left( {\sum\limits_{i = 1}^{k - 1} {{\lambda _i}{x_i}} } \right) \le \sum\limits_{i = 1}^{k - 1} {{\lambda _i}f\left( {{x_i}} \right)} .
\end{align}
Similar to the proof of Problem 1.1, for any $k$-tuple $\left(\lambda _1,\ldots,\lambda _k\right)$ satisfying $\lambda _i\ge 0$ for $i=1,\ldots,k$ and $\sum\nolimits_{i = 1}^k {{\lambda _i}}  = 1$. If $\lambda _k=1$, then $\lambda _i=0$, for $i=1,\ldots,k-1$. Thus, 
\begin{align}
f\left( {\sum\limits_{i = 1}^k {{\lambda _i}{x_i}} } \right) = f\left( {{x_k}} \right) = \sum\limits_{i = 1}^k {{\lambda _i}f\left( {{x_i}} \right)} .
\end{align}
If $\lambda _k<1$, we have $\sum\limits_{i = 1}^{k - 1} {\frac{{{\lambda _i}}}{{1 - {\lambda _k}}}}  = 1$. Then \eqref{1.27} implies that
\begin{align}
f\left( {\sum\limits_{i = 1}^{k - 1} {\frac{{{\lambda _i}{x_i}}}{{1 - {\lambda _k}}}} } \right) \le \sum\limits_{i = 1}^{k - 1} {\frac{{{\lambda _i}}}{{1 - {\lambda _k}}}f\left( {{x_i}} \right)} .
\end{align}
Hence, 
\begin{align}
f\left( {\sum\limits_{i = 1}^k {{\lambda _i}{x_i}} } \right) &= f\left( {\left( {1 - {\lambda _k}} \right)\sum\limits_{i = 1}^{k - 1} {\frac{{{\lambda _i}{x_i}}}{{1 - {\lambda _k}}}}  + {\lambda _k}{x_k}} \right)\\
& \le \left( {1 - {\lambda _k}} \right)f\left( {\sum\limits_{i = 1}^{k - 1} {\frac{{{\lambda _i}{x_i}}}{{1 - {\lambda _k}}}} } \right) + {\lambda _k}f\left( {{x_k}} \right)\\
& \le \left( {1 - {\lambda _k}} \right)\sum\limits_{i = 1}^{k - 1} {\frac{{{\lambda _i}}}{{1 - {\lambda _k}}}f\left( {{x_i}} \right)}  + {\lambda _k}f\left( {{x_k}} \right)\\
& = \sum\limits_{i = 1}^k {{\lambda _i}f\left( {{x_i}} \right)} .
\end{align}
By the principle of mathematical induction, we deduce that \eqref{1.24} holds for all $k\in \mathbb{N}^\star$. 

Now we consider some applications of Jensen inequality.
\begin{enumerate}
\item Applying Jensen inequality for the convex function $f\left(x\right) =-\ln x$ for $x\in \mathbb{R}_+$, $a_i \in \mathbb{R}_+$, $\lambda _i=\frac{1}{n}$ for $i=1,\ldots,n$, (the convexity of $f$ has been proved in Problem 1.2-3) yields
\begin{align}
- \ln \left( {\frac{1}{n}\sum\limits_{i = 1}^n {{a_i}} } \right) \le  - \frac{1}{n}\sum\limits_{i = 1}^n {\ln {a_i}} ,
\end{align}
which is equivalent to \eqref{1.25}. 
\item Since $\frac{1}{p}+\frac{1}{q}=1$ and $p>1$, we also have $q>1$. It suffices to prove \eqref{1.26} for $x_i >0$, $i=1,\ldots,n$ since the zero terms (if exist) can be removed without affecting the inequality. Since $f\left(x\right)=x^q$, $q>1$ is convex in $\mathbb{R}_+$ (this has been proved in Problem 1.2-2), applying Jensen inequality to $f$ yields
\begin{align}
\label{1.35}
\left( {{x_i} > 0,\hspace{0.1cm} {\lambda _i} > 0,\hspace{0.1cm} i = 1, \ldots ,n,\hspace{0.1cm}\sum\limits_{i = 1}^n {{\lambda _i}}  = 1} \right) \Rightarrow {\left( {\sum\limits_{i = 1}^n {{\lambda _i}{x_i}} } \right)^q} \le \sum\limits_{i = 1}^n {{\lambda _i}x_i^q} .
\end{align}
Plugging ${\lambda _i} = \frac{{{{\left| {{x_i}} \right|}^p}}}{{\sum\nolimits_{i = 1}^n {{{\left| {{x_i}} \right|}^p}} }} >0$, $ {x_i} = \frac{{\left| {{x_i}} \right|\left| {{y_i}} \right|}}{{{\lambda _i}}} \ge 0$, for $i=1,\ldots,n$ in \eqref{1.35} yields
\begin{align}
{\left( {\sum\limits_{i = 1}^n {\frac{{{{\left| {{x_i}} \right|}^p}}}{{\sum\limits_{i = 1}^n {{{\left| {{x_i}} \right|}^p}} }} \cdot \frac{{\left| {{x_i}} \right|\left| {{y_i}} \right|}}{{\frac{{{{\left| {{x_i}} \right|}^p}}}{{\sum\limits_{i = 1}^n {{{\left| {{x_i}} \right|}^p}} }}}}} } \right)^q} \le \sum\limits_{i = 1}^n {\frac{{{{\left| {{x_i}} \right|}^p}}}{{\sum\nolimits_{i = 1}^n {{{\left| {{x_i}} \right|}^p}} }} \cdot \frac{{{{\left| {{x_i}} \right|}^q}{{\left| {{y_i}} \right|}^q}}}{{\frac{{{{\left| {{x_i}} \right|}^{pq}}}}{{{{\left( {\sum\limits_{i = 1}^n {{{\left| {{x_i}} \right|}^p}} } \right)}^q}}}}}} ,
\end{align}
which is equivalent to
\begin{align}
{\left( {\sum\limits_{i = 1}^n {\left| {{x_i}} \right|\left| {{y_i}} \right|} } \right)^q} \le {\left( {\sum\nolimits_{i = 1}^n {{{\left| {{x_i}} \right|}^p}} } \right)^{q - 1}}\sum\limits_{i = 1}^n {{{\left| {{y_i}} \right|}^q}} .
\end{align}
Thus,
\begin{align}
\sum\limits_{i = 1}^n {{x_i}{y_i}} &\le \sum\limits_{i = 1}^n {\left| {{x_i}} \right|\left| {{y_i}} \right|} \\
& \le {\left( {\sum\limits_{i = 1}^n {{{\left| {{x_i}} \right|}^p}} } \right)^{\frac{{q - 1}}{q}}}{\left( {\sum\limits_{i = 1}^n {{{\left| {{y_i}} \right|}^q}} } \right)^{\frac{1}{q}}}\\
& = {\left( {\sum\limits_{i = 1}^n {{{\left| {{x_i}} \right|}^p}} } \right)^{\frac{1}{p}}}{\left( {\sum\limits_{i = 1}^n {{{\left| {{y_i}} \right|}^q}} } \right)^{\frac{1}{q}}} ,
\end{align}
for all $x,y\in \mathbb{R}^n$. \hfill $\square$
\end{enumerate}
\textbf{Problem 1.6.} \textit{Use the first-order necessary condition to find stationary points\footnote{``stationary point'', see \cite{2}, or ``critical points'', see \cite{1}.} of the following functions.}
\begin{enumerate}
\item $f\left( {{x_1},{x_2}} \right) = x_1^2 + 3x_2^2 - 4{x_1} + 8{x_2}$.
\item $f\left( {{x_1},{x_2},{x_3}} \right) = 2x_1^2 + {x_1}{x_2} + x_2^2 + {x_2}{x_3} + x_3^2 - 6{x_1} - 7{x_2} - 8{x_3} + 9$.
\item $f\left( {{x_1},{x_2}} \right) = {\left( {{x_1}{x_2} - {x_1} - 1} \right)^2} + {\left( {x_2^2 - 1} \right)^2}$.
\item $f\left( {{x_1},{x_2},{x_3}} \right) = {x_1}{x_2}{x_3}{e^{ - {x_1} - {x_2} - {x_3}}}$.
\item $f\left( {{x_1},{x_2}} \right) = \frac{1}{{{x_1}{x_2}}} + {x_1} + {x_2}$ in the domain $\mathbb{R}_{++}^2$.
\end{enumerate}
\textsc{Solution.} 
\begin{enumerate}
\item The gradient of $f$ is given by 
\begin{align}
\nabla f\left( {{x_1},{x_2}} \right) = \left[ {\begin{array}{*{20}{c}}
{2{x_1} - 4}\\
{6{x_2} + 8}
\end{array}} \right],
\end{align}
for all $\left(x_1,x_2\right) \in \mathbb{R}^2$. Solving the equation $\nabla f\left( {{x_1},{x_2}} \right) = \mathbf{0}$ yields that $\left(2,-\frac{4}{3}\right)$ is the unique stationary point of $f$.
\item The gradient of $f$ is given by 
\begin{align}
\nabla f\left( {{x_1},{x_2},{x_3}} \right) = \left[ {\begin{array}{*{20}{c}}
{4{x_1} + {x_2} - 6}\\
{{x_1} + 2{x_2} + {x_3} - 7}\\
{{x_2} + 2{x_3} - 8}
\end{array}} \right] ,
\end{align}
for all $\left(x_1,x_2,x_3\right) \in \mathbb{R}^3$. Solving the equation $\nabla f\left( {{x_1},{x_2},{x_3}} \right) = \mathbf{0}$ yields that $\left(\frac{6}{5},\frac{6}{5},\frac{17}{5}\right)$ is the unique stationary point of $f$.
\item The gradient of $f$ is given by
\begin{align}
\nabla f\left( {{x_1},{x_2}} \right) = \left[ {\begin{array}{*{20}{c}}
{ - 2\left( {{x_2} - 1} \right)\left( {{x_1} - {x_1}{x_2} + 1} \right)}\\
{4{x_2}\left( {x_2^2 - 1} \right) - 2{x_1}\left( {{x_1} - {x_1}{x_2} + 1} \right)}
\end{array}} \right],
\end{align}
for all $\left(x_1,x_2\right) \in \mathbb{R}^2$. Solving the equation $\nabla f\left( {{x_1},{x_2}} \right) = \mathbf{0}$ yields that $\left(-1,0\right)$, $\left(0,1\right)$, and $\left(-\frac{1}{2},-1\right)$ are the only stationary points of $f$.
\item The gradient of $f$ is given by
\begin{align}
\nabla f\left( {{x_1},{x_2},{x_3}} \right) = \left[ {\begin{array}{*{20}{c}}
{\left( {{x_2}{x_3} - {x_1}{x_2}{x_3}} \right){e^{ - {x_1} - {x_2} - {x_3}}}}\\
{\left( {{x_3}{x_1} - {x_1}{x_2}{x_3}} \right){e^{ - {x_1} - {x_2} - {x_3}}}}\\
{\left( {{x_1}{x_2} - {x_1}{x_2}{x_3}} \right){e^{ - {x_1} - {x_2} - {x_3}}}}
\end{array}} \right], 
\end{align}
for all $\left(x_1,x_2,x_3\right) \in \mathbb{R}^3$. Solving the equation $\nabla f\left( {{x_1},{x_2},{x_3}} \right) = \mathbf{0}$ yields that $\left(1,1,1\right)$, $\left(a,0,0\right)$, for arbitrary $a\in \mathbb{R}$ and its permutations are the only stationary points of $f$. 
\item The gradient of $f$ is given by
\begin{align}
\nabla f\left( {{x_1},{x_2}} \right) = \left[ {\begin{array}{*{20}{c}}
{1 - \frac{1}{{x_1^2{x_2}}}}\\
{1 - \frac{1}{{{x_1}x_2^2}}}
\end{array}} \right],
\end{align}
for all $\left(x_1,x_2\right) \in \mathbb{R}_{++}^2$. Solving the equation $\nabla f\left( {{x_1},{x_2}} \right) = 0$ yields that $\left(1,1\right)$ is the unique stationary point of $f$. \hfill $\square$
\end{enumerate}
\textbf{Problem 1.7.} \textit{Let $f: \mathbb{R}^n\to \mathbb{R}$ be a G\^{a}teaux differentiable and convex function. Consider the following unconstrained minimization problem}
\begin{align}
\mbox{Min }f\left( x \right) \mbox{ s.t. }x \in {\mathbb{R}^n}.
\end{align}
\textit{Prove the following properties.}
\begin{enumerate}
\item \textit{$\bar x$ is a local minimizer of $\left( P \right) \Leftrightarrow \nabla f\left( {\bar x} \right) = 0$.}
\item \textit{$\bar x$ is a local minimizer of $\left( P \right) \Leftrightarrow \bar x$ is a global minimizer of $\left(P\right)$.}
\item \textit{The set of minimizers of $\left(P\right)$ is a convex set.}
\item \textit{If $f$ is strictly convex, then $\left(P\right)$ has the unique minimizer (if there exists at least one).}
\end{enumerate}
\textsc{Proof.}
\begin{enumerate}
\item $\left(  \Rightarrow  \right)$ \textit{First-order necessary condition for a local optimizer.}\footnote{See \cite{1}, Theorem 2.7, p. 35.} We have the following theorem, which is more general than our task.\\
\\
\textbf{Theorem 1.7.1 (First-order necessary condition for a local optimizer).} \textit{Let $f:U\to \mathbb{R}$ be a G\^{a}teaux differentiable function on an open subset $U \subseteq \mathbb{R}^n$. A local optimizer is a critical point, that is,}
\begin{align}
\bar x \mbox{ a local optimizer } \Rightarrow \nabla f\left( \bar x \right) = 0.
\end{align}
\textit{Proof of Theorem 1.7.1.} Let $\bar x$ be a local minimizer of $f$. Then there exists $\varepsilon >0$ such that
\begin{align}
x \in B\left( {\bar x,\varepsilon } \right) \Rightarrow f\left( {\bar x} \right) \le f\left( x \right).
\end{align}
If $d\in \mathbb{R}^n$, then
\begin{align}
\label{1.49}
f'\left( {\bar x;d} \right) = \mathop {\lim }\limits_{t \to 0} \frac{{f\left( {\bar x + td} \right) - f\left( {\bar x} \right)}}{t} = \left\langle {\nabla f\left( {\bar x} \right),d} \right\rangle .
\end{align}
If $\left| t \right|$ is small, then the numerator in the above limit is nonnegative, since $\bar x$ is a local minimizer. If $t>0$, then the difference quotient is nonnegative, so in the limit as $t \downarrow 0$, we have $f'\left( {\bar x;d} \right) \ge 0$. However, if $t<0$, the difference quotient  is nonpositive, and we have $f'\left( {\bar x;d} \right) \le 0$. Thus, we conclude that $f'\left( {\bar x;d} \right) = \left\langle {\nabla f\left(\bar x \right),d} \right\rangle  = 0$. If $\bar x$ is a local maximizer of $f$, then $\left\langle {\nabla f\left( \bar x \right),d} \right\rangle  = 0$, since $\bar x$ is a local minimizer of $-f$. Picking $d = \nabla f\left( \bar x \right)$ gives 
\begin{align}
f'\left( {\bar x;\nabla f\left( {\bar x} \right)} \right) = {\left\| {\nabla f\left( {\bar x} \right)} \right\|^2} = 0,
\end{align}
that is, $\nabla f\left( {\bar x} \right) = 0$. \hfill $\square$

$\left(  \Leftarrow  \right)$ Suppose $\nabla f\left( {\bar x} \right) = 0$, we will prove $\bar x$ is a local minimizer of $\left(P\right)$. Indeed, plugging $d =x-\bar x$ for arbitrary $x\in \mathbb{R}^n$ into \eqref{1.49} yields
\begin{align}
\label{1.51}
f'\left( {\bar x;x - \bar x} \right) = \mathop {\lim }\limits_{t \to 0} \frac{{f\left( {\bar x + t\left( {x - \bar x} \right)} \right) - f\left( {\bar x} \right)}}{t} = \left\langle {\nabla f\left( {\bar x} \right),x - \bar x} \right\rangle  = 0,
\end{align}
for all $x\in \mathbb{R}^n$. Since $f$ is convex, we also have
\begin{align}
\label{1.52}
f\left( {\bar x + t\left( {x - \bar x} \right)} \right) &= f\left( {tx + \left( {1 - t} \right)\bar x} \right) \le tf\left( x \right) + \left( {1 - t} \right)f\left( {\bar x} \right),
\end{align}
for all $t\in \left[0,1\right]$. 

Combining \eqref{1.51} and \eqref{1.52} yields
\begin{align}
0 &= \mathop {\lim }\limits_{t \to {0^ + }} \frac{{f\left( {\bar x + t\left( {x - \bar x} \right)} \right) - f\left( {\bar x} \right)}}{t}\\
 &\le \mathop {\lim }\limits_{t \to {0^ + }} \frac{{tf\left( x \right) + \left( {1 - t} \right)f\left( {\bar x} \right) - f\left( {\bar x} \right)}}{t}\\
 &= f\left( x \right) - f\left( {\bar x} \right),
\end{align}
or equivalently, $f\left(\bar x\right) \le f\left(x\right)$ for all $x\in \mathbb{R}^n$. That means $x$ is a global (thus local) minimizer of $\left(P\right)$.
\item We have just proved that for any convex function $f$, $\bar x$ is a local minimizer  of $\left(P\right) \Rightarrow \nabla f\left( {\bar x} \right) = 0 \Rightarrow \bar x$ is a global minimizer  of $\left(P\right) \Rightarrow \bar x$ is a local minimizer  of $\left(P\right)$, where the last implication is obvious. Thus, $\bar x$ is a local minimizer $\Leftrightarrow \bar x$ is a global minimizer.

\textit{Alternative proof.} In this proof, we will prove that $\bar x$ is a local minimizer of $\left(P\right) \Rightarrow \bar x$ is a global minimizer of $\left(P\right)$. Indeed, let $\bar x$ be a local minimizer of $\left(P\right)$. Then there exists $\varepsilon >0$ such that 
\begin{align}
\label{1.56}
x \in B\left( {\bar x,\varepsilon } \right) \Rightarrow f\left( {\bar x} \right) \le f\left( x \right).
\end{align}
So it remains to prove that $f\left(\bar x\right) \le f\left(x\right)$ for all $x \in {\mathbb{R}^n}\backslash B\left( {\bar x,\varepsilon } \right)$. Suppose, to get a contradiction, that there exists $x \in {\mathbb{R}^n}\backslash B\left( {\bar x,\varepsilon } \right)$ such that $f\left(x\right) < f\left(\bar x\right)$ and consider $z \in \left\{ {tx + \left( {1 - t} \right)\bar x;t \in \left[ {0,1} \right]} \right\} \cap B\left( {\bar x,\varepsilon } \right) $. Then $z$ can be expressed as $z = tx + \left( {1 - t} \right)\bar x$ for some $0 < t < \frac{\varepsilon }{{\left\| {x - \bar x} \right\|}} \le 1$. Since $f$ is convex, we have successively
\begin{align}
f\left( z \right) &= f\left( {tx + \left( {1 - t} \right)\bar x} \right)\\
 &\le tf\left( x \right) + \left( {1 - t} \right)f\left( {\bar x} \right)\\
& < tf\left( {\bar x} \right) + \left( {1 - t} \right)f\left( {\bar x} \right)\\
 &= f\left( {\bar x} \right).
\end{align}
But this contradicts \eqref{1.56}. So $\bar x$ is a global minimizer of $\left(P\right)$.
\item Suppose that $x \in \mathbb{R}^n$, $y\in \mathbb{R}^n$ are two (global) minimizers of $f$ ($x=y$ is a possibility), we denote $\alpha  = f\left( x \right) = f\left( y \right)$ the minimizer value of $f$. Since $f$ is convex, we have
\begin{align}
f\left( {tx + \left( {1 - t} \right)y} \right) &\le tf\left( x \right) + \left( {1 - t} \right)f\left( y \right)\\
& = t\alpha  + \left( {1 - t} \right)\alpha \\
& = \alpha ,
\end{align}
for all $t\in \left[0,1\right]$. Since $\alpha$ is the minimizer value of $f$, this implies that $f\left( {tx + \left( {1 - t} \right)y} \right) = \alpha $ for all $t\in \left[0,1\right]$. Thus, the set of minimizers of $f$ is convex.
\item Suppose that there exist two (global) minima $\bar x$ and $\bar y$ of $\left(P\right)$. Since $f$ is strictly convex, we have
\begin{align}
f\left( {\frac{{\bar x + \bar y}}{2}} \right) < \frac{{f\left( {\bar x} \right) + f\left( {\bar y} \right)}}{2} = f\left( {\bar x} \right),
\end{align}
which contradicts the fact that $\bar x$  is a global minimizer of $\left(P\right)$. Thus, if a strictly convex function has a (global) minimizer, then it is unique. \hfill $\square$
\end{enumerate}
\textbf{Problem 1.8.} \textit{Find the number of minimizers with respect to $m$ of the following problem}
\begin{align}
\mbox{Min }\frac{3}{2}\left( {{x^2} + {y^2}} \right) + \left( {1 + m} \right)xy - x - y + 4 \mbox{ s.t. }\left( {x,y} \right) \in {\mathbb{R}^2}.
\end{align}
\textsc{Solution.} Consider the mapping $f:\mathbb{R}^2\to \mathbb{R}$ defined by
\begin{align}
f\left( {x,y} \right) = \frac{3}{2}\left( {{x^2} + {y^2}} \right) + \left( {1 + m} \right)xy - x - y + 4, \mbox{ for }\left( {x,y} \right) \in {\mathbb{R}^2},
\end{align}
this is a quadratic function since it can be expressed as
\begin{align}
f\left( {x,y} \right) = \frac{1}{2}\left[ {\begin{array}{*{20}{c}}
x&y
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
3&{1 + m}\\
{1 + m}&3
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
x\\
y
\end{array}} \right] + \left[ {\begin{array}{*{20}{c}}
{ - 1}&{ - 1}
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
x\\
y
\end{array}} \right] + 4,
\end{align}
for all $\left(x,y\right)\in \mathbb{R}^2$. Its gradient is given by
\begin{align}
\nabla f\left( {x,y} \right) = \left[ {\begin{array}{*{20}{c}}
{3x + \left( {1 + m} \right)y - 1}\\
{3y + \left( {1 + m} \right)x - 1}
\end{array}} \right], \mbox{ for all } \left( {x,y} \right) \in {\mathbb{R}^2},
\end{align}
and its Hessian matrix is 
\begin{align}
{\nabla ^2}f\left( {x,y} \right) = \left[ {\begin{array}{*{20}{c}}
3&{1 + m}\\
{1 + m}&3
\end{array}} \right], \mbox{ for all } \left( {x,y} \right) \in {\mathbb{R}^2},
\end{align}
whose eigenvalues are $\lambda _1 =2-m$ and $\lambda _2=m+4$. The equation $\nabla f\left( {x,y} \right) = 0$ has the unique root $\left(\frac{1}{m+4},\frac{1}{m+4}\right)$ when $m\ne -4$ and $m\ne 2$, no root when $m=-4$, and infinite roots with the form $\left(a,\frac{1}{3}-a\right)$ for arbitrary $a\in \mathbb{R}$ when $m=2$. 

We consider the following cases depending on $m$.
\begin{itemize}
\item \textit{Case $m\le -4$.} Similarly, $f$ can be expressed as
\begin{align}
f\left( {x,y} \right) = \frac{3}{2}{\left( {x - y} \right)^2} + \left( {4 + m} \right)xy - x - y + 4,
\end{align}
for all $\left( {x,y} \right) \in {\mathbb{R}^2}$. In particular, $f\left( {x,x} \right) = \left( {4 + m} \right){x^2} - 2x + 4 \to  - \infty$ as $x \to  + \infty $. Thus, $f$ has no minimizers in this case.

\item \textit{Case $m\in \left(-4,2\right)$.} In this case, we have $\lambda _1>0$, and $\lambda _2 >0$, which implies that ${\nabla ^2}f\left( {x,y} \right)$ is positive definite, and thus $f$ is strictly convex. Applying Problem 1.7 to $f$, we deduce that $\left(\frac{1}{m+4},\frac{1}{m+4}\right)$, which is the unique stationary point of $f$, is the unique minimizer of $f$.
\item \textit{Case $m=2$.} In this case, we have $\lambda _1 =0$, $\lambda _2 =6$, which implies that ${\nabla ^2}f\left( {x,y} \right)$ is semi-positive definite, and thus $f$ is convex. Applying Problem 1.7 to $f$, we deduce that $\left(a,\frac{1}{3}-a\right)$ for arbitrary $a\in \mathbb{R}$ are minimizers of $f$. Thus, there are infinite minimizers of $f$ in this case.
\item \textit{Case $m>2$.} We have $f\left( {x, - x} \right) = \left( {2 - m} \right){x^2} + 4 \to  - \infty$ as $x \to  + \infty $. Thus, $f$ has no minimizers in this case. \hfill $\square$
\end{itemize}
\textbf{Remark 1.8.1.} We also give elementary proofs for some cases in the proof above.\\
\\
\textit{Elementary proof for the case $m=2$.} When $m=2$, 
\begin{align}
f\left( {x,y} \right) &= \frac{3}{2}\left( {{x^2} + {y^2}} \right) + 3xy - x - y + 4\\
 &= \frac{3}{2}{\left( {x + y} \right)^2} - \left( {x + y} \right) + 4,\mbox{ for all } \left( {x,y} \right) \in {\mathbb{R}^2}.
\end{align}
Put $t=x+y$, we have
\begin{align}
g\left( t \right): &= \frac{3}{2}{t^2} - t + 4\\
 &= \frac{3}{2}{\left( {t - \frac{1}{3}} \right)^2} + \frac{{23}}{6} \ge \frac{{23}}{6}, \mbox{ for all } t \in \mathbb{R}.
\end{align}
Hence, $f$ attains its minimum value $\frac{23}{6}$ at the points $\left(x,y\right)$ satisfying $x+y=\frac{1}{3}$, i.e., $\left(a,\frac{1}{3}-a\right)$ for arbitrary $a\in \mathbb{R}$. \hfill $\square$\\

In fact, the cases $m=2$ and $m\in \left(-4,2\right)$ can be merged as in the following elementary proof.\\
\\
\textit{Elementary proof for the case $m\in \left(-4,2\right]$.} We rewrite $f$ as
\begin{align}
f\left( {x,y} \right) =&  - \frac{{1 + m}}{2}{\left( {x - y} \right)^2} + \frac{{m + 4}}{2}\left( {{x^2} + {y^2}} \right) - x - y + 4\\
 =&  - \frac{{1 + m}}{2}{\left( {x - y} \right)^2} + \frac{{m + 4}}{2}{\left( {x - \frac{1}{{m + 4}}} \right)^2}\\
 &+ \frac{{m + 4}}{2}{\left( {y - \frac{1}{{m + 4}}} \right)^2} + \frac{{4m + 15}}{{m + 4}},
\end{align}
for all $\left(x,y\right)\in \mathbb{R}^2$. Applying the inequality $2\left(a^2+b^2\right) \ge \left(a+b\right)^2$, whose the equality holds if and only if $a=b$ to $a={x - \frac{1}{{m + 4}}}$ and $b={ \frac{1}{{m + 4}}-y}$ yields
\begin{align}
{\left( {x - \frac{1}{{m + 4}}} \right)^2} + {\left( {y - \frac{1}{{m + 4}}} \right)^2} \ge \frac{1}{2}{\left( {x - y} \right)^2},
\end{align}
for all $\left( {x,y} \right) \in {\mathbb{R}^2}$. Thus,
\begin{align}
f\left( {x,y} \right) &\ge  - \frac{{1 + m}}{2}{\left( {x - y} \right)^2} + \frac{{m + 4}}{4}{\left( {x - y} \right)^2} + \frac{{4m + 15}}{{m + 4}}\\
 &= \frac{{2 - m}}{4}{\left( {x - y} \right)^2} + \frac{{4m + 15}}{{m + 4}}\\
& \ge \frac{{4m + 15}}{{m + 4}},
\end{align}
for all $\left( {x,y} \right) \in {\mathbb{R}^2}$. For $m\in \left(-4,2\right]$, the equality $f\left( {x,y} \right) = \frac{{4m + 15}}{{m + 4}}$ holds if and only if 
\begin{align}
\left\{ {\begin{array}{*{20}{c}}
{x - \frac{1}{{m + 4}} = \frac{1}{{m + 4}} - y}\\
{\left( {2 - m} \right)\left( {x - y} \right) = 0}
\end{array}} \right. ,
\end{align}
which is equivalent to
\begin{align}
\left\{ {\begin{array}{*{20}{c}}
{x + y = \frac{2}{{m + 4}}}\\
{\left[ {\begin{array}{*{20}{c}}
{m = 2}\\
{x = y}
\end{array}} \right.}
\end{array}} \right. ,
\end{align}
or,
\begin{align}
\left[ {\begin{array}{*{20}{c}}
{m = 2\mbox{ and } x + y = \frac{1}{3}}\\
{m \in \left( { - 4,2} \right)\mbox{ and } x = y = \frac{1}{{m + 4}}}
\end{array}} \right. .
\end{align}
Thus $\left(\frac{1}{m+4},\frac{1}{m+4}\right)$ is the unique minimizer of $f$ when $m\in \left(-4,2\right)$, and $\left(a,\frac{1}{3}-a\right)$ for arbitrary $a\in \mathbb{R}$ are minimizers of $f$ when $m=2$. \hfill $\square$\\
\\
\textbf{Problem 1.9.} \textit{Let $f:\mathbb{R}^n \to \mathbb{R}$ be a G\^{a}teaux differentiable function and $\bar x \in \mathbb{R}^n$, $d\in \mathbb{R}^n$. Prove the following properties.}
\begin{enumerate}
\item \textit{If $\nabla f{\left( {\bar x} \right)^T}d < 0$ then $d$ is a descent direction at $\bar x$ for $f$.}
\item \textit{If $f$ is convex then: $d$ is a descent direction at $\bar x$ of $f$ $ \Leftrightarrow \nabla f{\left( {\bar x} \right)^T}d < 0$.}
\end{enumerate}
\textsc{Proof.}
\begin{enumerate}
\item Assume $\nabla f{\left( {\bar x} \right)^T}d<0$ for some $d\in \mathbb{R}^n$, we have
\begin{align}
\nabla f{\left( {\bar x} \right)^T}d = \mathop {\lim }\limits_{t \to 0} \frac{{f\left( {\bar x + td} \right) - f\left( {\bar x} \right)}}{t} < 0.
\end{align}
Thus, for $\varepsilon>0$ small enough, 
\begin{align}
\label{1.79}
t \in \left( {0,\varepsilon } \right] \Rightarrow f\left( {\bar x + td} \right) < f\left( {\bar x} \right),
\end{align}
that means $d$ is a descent direction at $\bar x$ of $f$.
\item For a convex function $f$, it suffices to prove that $d$ is a descent direction at $\bar x$ of $f$ $\Rightarrow \nabla f{\left( {\bar x} \right)^T}d < 0$. Assume that $d \in \mathbb{R}^n$ is a descent direction at $\bar x$ of $f$, there exists $\varepsilon >0$ such that \eqref{1.79} holds. Since $f$ is convex, we have
\begin{align}
\label{1.80}
f\left( {\bar x + td} \right) \ge f\left( {\bar x} \right) + t\nabla f{\left( {\bar x} \right)^T}d.
\end{align}
Combining \eqref{1.79} and \eqref{1.80} yields
\begin{align}
t \in \left( {0,\varepsilon } \right] \Rightarrow \nabla f{\left( {\bar x} \right)^T}d \le \frac{{f\left( {\bar x + td} \right) - f\left( {\bar x} \right)}}{t} < 0 .
\end{align}
This ends our proof. \hfill $\square$
\end{enumerate}
\textbf{Problem 1.10.} \textit{Let $f: \mathbb{R}^n \to \mathbb{R}$ and $\bar x\in \mathbb{R}^n$, $d\in \mathbb{R}^n$. Prove that  if $d\ne \mathbf{0}$ and ${\left\| {\nabla f\left( {\bar x} \right) + d} \right\|^2} \le {\left\| {\nabla f\left( {\bar x} \right)} \right\|^2}$ then $d$ is a descent direction at $\bar x$ of $f$.}\\
\\
\textsc{Proof.} We have
\begin{align}
{\left\| {\nabla f\left( {\bar x} \right)} \right\|^2} &\ge {\left\| {\nabla f\left( {\bar x} \right) + d} \right\|^2}\\
& = {\left\| {\nabla f\left( {\bar x} \right)} \right\|^2} + 2\nabla f{\left( {\bar x} \right)^T}d + {\left\| d \right\|^2},
\end{align}
which implies that $\nabla f{\left( {\bar x} \right)^T}d \le  - \frac{{{{\left\| d \right\|}^2}}}{2} < 0$ (since $d\ne \mathbf{0}$). Due to Problem 1.9, $d$ is a descent direction at $\bar x$ of $f$. \hfill $\square$\\
\\
\textbf{Problem 1.11.} \textit{Let $f: \mathbb{R}^n \to \mathbb{R}$ and $\bar x\in \mathbb{R}^n$, $y\in \mathbb{R}^n$ satisfying $f\left(y\right) <f\left(\bar x\right)$. Prove that $d=y-\bar x$ is a descent direction at $\bar x$ of $f$.}\\
\\
\textsc{Proof.} Since $f\left(y\right) <f\left(\bar x\right)$, we have $\bar x\ne y$. The convexity of $f$ gives us 
\begin{align}
\nabla f{\left( {\bar x} \right)^T}\left( {y - \bar x} \right) \le f\left( y \right) - f\left( {\bar x} \right) < 0.
\end{align}
Thus, $d=y-\bar x$ is a descent direction at $\bar x$ of $f$. \hfill $\square$\\
\\
\textbf{Problem 1.12.} \textit{Given $f: \mathbb{R}^n \to \mathbb{R}$ and $\bar x \in \mathbb{R}^n$, find a descent direction $d$ at $\bar x$ of $f$ in the following cases.}
\begin{enumerate}
\item $f\left( {x,y} \right) = {x^2} + {y^2} - xy - x + 2y - 3$ and $\bar x = \left( {0,0} \right)$.
\item $f\left( {x,y} \right) = 2{x^2} + {y^2} - 2xy + 2{x^3} + {x^4}$ and $\bar x = \left( { - 1,0} \right)$.
\item $f\left( {x,y} \right) = \frac{1}{2}{\left( {x - 2y} \right)^2} + {x^4}$ and $\bar x = \left( {2,1} \right)$.
\item $f\left( {x,y,z} \right) = {x^2} + {y^2} + {z^2} - xy - zx + 2x - 4y - 2z$ and $\bar x = \left( {0,0,1} \right)$
\end{enumerate}
\textsc{Solution.} We mainly use the result ``$d =  - \nabla f\left( {\bar x} \right)$ is a descent direction at $\bar x$ for $f$ if $\nabla f\left( {\bar x} \right) \ne \mathbf{0}$''.
\begin{enumerate}
\item The gradient of $f$ is given by
\begin{align}
\nabla f\left( {x,y} \right) = \left[ {\begin{array}{*{20}{c}}
{2x - y - 1}\\
{2y - x + 2}
\end{array}} \right], \mbox{ for all } \left( {x,y} \right) \in {\mathbb{R}^2}.
\end{align}
In particular, $\nabla f\left( {0,0} \right) = \left[ {\begin{array}{*{20}{c}}
{ - 1}\\
2
\end{array}} \right]$. Thus, $d = \left[ {\begin{array}{*{20}{c}}
1\\
{ - 2}
\end{array}} \right]$ is a descent direction at $\bar x$ for $f$.
\item The gradient of $f$ is given by
\begin{align}
\nabla f\left( {x,y} \right) = \left[ {\begin{array}{*{20}{c}}
{4{x^3} + 6{x^2} + 4x - 2y}\\
{2y - 2x}
\end{array}} \right], \mbox{ for all } \left( {x,y} \right) \in {\mathbb{R}^2}.
\end{align}
In particular, $\nabla f\left( { - 1,0} \right) = \left[ {\begin{array}{*{20}{c}}
{ - 2}\\
2
\end{array}} \right]$. Thus, $d = \left[ {\begin{array}{*{20}{c}}
2\\
{ - 2}
\end{array}} \right]$ is a descent direction at $\bar x$ for $f$.
\item The gradient of $f$ is given by
\begin{align}
\nabla f\left( {x,y} \right) = \left[ {\begin{array}{*{20}{c}}
{4{x^3} + x - 2y}\\
{4y - 2x}
\end{array}} \right],\mbox{ for all } \left( {x,y} \right) \in {\mathbb{R}^2}.
\end{align}
In particular, $\nabla f\left( {2,1} \right) = \left[ {\begin{array}{*{20}{c}}
{32}\\
0
\end{array}} \right]$. Thus, $d = \left[ {\begin{array}{*{20}{c}}
{ - 32}\\
0
\end{array}} \right]$ is a descent direction at $\bar x$ for $f$.
\item The gradient of $f$ is given by
\begin{align}
\nabla f\left( {x,y,z} \right) = \left[ {\begin{array}{*{20}{c}}
{2x - y - z + 2}\\
{2y - x - 4}\\
{2z - x - 2}
\end{array}} \right],\mbox{ for all } \left( {x,y,z} \right) \in {\mathbb{R}^3}.
\end{align}
In particular, $\nabla f\left( {0,0,1} \right) = \left[ {\begin{array}{*{20}{c}}
1\\
{ - 4}\\
0
\end{array}} \right]$. Thus, $d = \left[ {\begin{array}{*{20}{c}}
{ - 1}\\
4\\
0
\end{array}} \right]$ is a descent direction at $\bar x$ for $f$. \hfill $\square$
\end{enumerate}
\vspace{1cm}
\begin{center}
\textsc{The End}
\end{center}
\newpage
\begin{thebibliography}{999}
\bibitem {1} O. G\"{u}ler.  \textit{Foundations of Optimization}. Graduate Texts in Mathematics 258, Springer.
\bibitem {2} Strodiot, J-J. \textit{Numerical Methods in Optimization}. Natural Sciences University, Ho Chi Minh City, Viet Nam, April 2007.
\end{thebibliography}
\end{document}