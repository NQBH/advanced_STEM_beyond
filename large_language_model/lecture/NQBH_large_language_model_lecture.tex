\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Lecture Note: Large Language Models -- Bài Giảng: Mô Hình Ngôn Ngữ Lớn}
\author{Nguyễn Quản Bá Hồng\footnote{A scientist- {\it\&} creative artist wannabe, a mathematics {\it\&} computer science lecturer of Department of Artificial Intelligence {\it\&} Data Science (AIDS), School of Technology (SOT), UMT Trường Đại học Quản lý {\it\&} Công nghệ TP.HCM, Hồ Chí Minh City, Việt Nam.\\E-mail: {\sf nguyenquanbahong@gmail.com} {\it\&} {\sf hong.nguyenquanba@umt.edu.vn}. Website: \url{https://nqbh.github.io/}. GitHub: \url{https://github.com/NQBH}.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Lecture Note: Large Language Models -- Bài Giảng: Mô Hình Ngôn Ngữ Lớn}.
		
		PDF: {\sc url}: \url{.pdf}.
		
		\TeX: {\sc url}: \url{.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basic LLMs}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item {\it Attention Is All You Need}.
	
	\item \cite{Raschka2024}. {\sc Sebastian Raschka}. {\it Build A Large Language Model (From Scratch)}.
\end{enumerate}

\begin{question}[LLMs vs. Music Generation]
	Draw similarities between LLMs \& Music Generation in Computer Music.
	
	-- Vẽ ra điểm tương đồng giữa LLM và ngành Tạo nhạc trong Âm nhạc máy tính.
\end{question}

\begin{proof}[Grok3's DeepSearch]
	Large Language Models (LLMs) \& music generation in computer music share several conceptual and technical similarities, as both involve generative processes that create structured outputs from complex data. Below, I outline key parallels:
	\begin{enumerate}
		\item {\sf Sequential Data Processing.}
		\begin{itemize}
			\item {\it LLMs}: Operate on sequences of tokens (words, subwords, or characters) to predict the next token based on context, generating coherent text.
			\item {\it Music Generation}: Processes sequences of musical elements (notes, chords, or time steps) to predict subsequent elements, creating melodies, harmonies, or rhythms. Models like MIDI-based systems or audio spectrogram processors treat music as a time-ordered sequence, akin to text.
		\end{itemize}
		\item {\it Pattern Learning and Generalization}:
		\begin{itemize}
			\item {\it LLMs.} Learn linguistic patterns, grammar, and semantic relationships from vast text corpora, enabling them to generate contextually relevant sentences.
			\item {\it Music Generation}: Learn musical patterns, such as chord progressions, scales, or stylistic motifs, from datasets of compositions or audio. Models like MuseNet or Magenta capture genre-specific structures to generate music that aligns with learned styles.
		\end{itemize}
		\item {\it Transformer Architecture.}
		\begin{itemize}
			\item {\it LLMs}: Rely heavily on transformer models (e.g., GPT architectures) to model long-range dependencies in text, using attention mechanisms to weigh the importance of prior tokens.
			\item {\it Music Generation}: Many modern music generation systems, such as Jukebox or Music Transformer, also use transformers to model long-range dependencies in musical sequences, ensuring coherence over extended passages.
		\end{itemize}
		\item {\it Training Objective (Next-Token Prediction)}:
		\begin{itemize}
			\item {\it LLMs}: Trained to minimize the error in predicting the next token in a sequence, optimizing for fluency and coherence.
			\item {\it Music Generation}: Similarly trained to predict the next note, chord, or audio frame, ensuring the output adheres to musical logic. For example, symbolic music models predict note sequences, while audio-based models like WaveNet predict waveform samples.
		\end{itemize}
		\item {\it Creative Output with Variability}:
		\begin{itemize}
			\item {\it LLMs}: Generate diverse text outputs by sampling from probability distributions (e.g., top-k or nucleus sampling), allowing for creative variations in responses.
			\item {\it Music Generation}: Use similar sampling techniques to introduce variability in generated music, enabling models to produce novel melodies or improvisations while adhering to learned structures.
		\end{itemize}
		\item {\it Contextual Awareness}:
		\begin{itemize}
			\item {\it LLMs}: Use context windows to maintain coherence, drawing on prior tokens to shape the narrative or argument.
			\item {\it Music Generation}: Maintain musical context, ensuring that generated notes or phrases align with the preceding musical structure, such as staying in key or following a rhythmic pattern.
		\end{itemize}
		\item {\it Fine-Tuning for Style}:
		\begin{itemize}
			\item {\it LLMs}: Can be fine-tuned on specific datasets to adopt particular writing styles, tones, or domains (e.g., legal or poetic text).
			\item {\it Music Generation}: Fine-tuned to emulate specific genres, composers, or instruments (e.g., Bach-style chorales or jazz improvisations), tailoring outputs to desired musical aesthetics.
		\end{itemize}
		\item {\it Evaluation Challenges}:
		\begin{itemize}
			\item {\it LLMs}: Difficult to evaluate due to subjective quality of text; metrics like BLEU are limited, and human judgment is often required.
			\item {\it Music Generation}: Similarly hard to assess, as musical quality is subjective. Metrics like pitch entropy or harmonic consistency are used, but human listening tests remain critical.
		\end{itemize}
		\item {\it Ethical and Bias Concerns}:
		\begin{itemize}
			\item {\it LLMs}: Can perpetuate biases present in training data, generating biased or harmful text if not carefully managed.
			\item {\it Music Generation}: May replicate biases in musical datasets, such as over-representing certain genres or cultural styles, potentially marginalizing others.
		\end{itemize}
		\item {\it Interdisciplinary Applications}:
		\begin{itemize}
			\item {\it LLMs}: Applied beyond text generation to tasks like code generation, translation, or dialogue systems.
			\item {\it Music Generation}: Used in interactive composition tools, film scoring, or therapeutic applications, extending beyond standalone music creation.
		\end{itemize}
	\end{enumerate}
	In summary, both LLMs and music generation leverage probabilistic, sequence-based modeling to create structured, creative outputs. Their reliance on similar architectures (e.g., transformers) and training paradigms highlights a deep connection between natural language and music as forms of human expression processed by AI.
\end{proof}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}