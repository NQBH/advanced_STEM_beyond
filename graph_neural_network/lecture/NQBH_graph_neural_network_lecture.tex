\documentclass[oneside]{book}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,fancyvrb,float,graphicx,mathtools,minitoc,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
%  \addtolength{\headheight}{0pt}% obsolete
\lhead{\scshape\chaptername~\thechapter}
\rhead{\nouppercase{\leftmark}} %\nouppercase !
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\cfoot{\thepage}

\usepackage{textcase}

\makeatletter
\def\@makechapterhead#1{%
    \vspace*{50\p@}%
    {\parindent \z@ \centering\normalfont
        \ifnum \c@secnumdepth >\m@ne
        \if@mainmatter
        \huge\bfseries \MakeTextUppercase{\@chapapp}\space \thechapter
        \par\nobreak
        \vskip 20\p@
        \fi
        \fi
        \interlinepenalty\@M
        \huge \bfseries \MakeTextUppercase{#1}\par\nobreak
        \vskip 40\p@
}}
\def\@makeschapterhead#1{%
    \vspace*{50\p@}%
    {\parindent \z@ \centering
        \normalfont
        \interlinepenalty\@M
        \huge \bfseries  \MakeTextUppercase{#1}\par\nobreak
        \vskip 40\p@
}}
\makeatother


\DeclareMathSymbol{\mathinvertedexclamationmark}{\mathclose}{operators}{'074}
\DeclareMathSymbol{\mathexclamationmark}{\mathclose}{operators}{'041}

\makeatletter
\newcommand{\raisedmathinvertedexclamationmark}{%
    \mathclose{\mathpalette\raised@mathinvertedexclamationmark\relax}%
}
\newcommand{\raised@mathinvertedexclamationmark}[2]{%
    \raisebox{\depth}{$\m@th#1\mathinvertedexclamationmark$}%
}
\begingroup\lccode`~=`! \lowercase{\endgroup
    \def~}{\@ifnextchar`{\raisedmathinvertedexclamationmark\@gobble}{\mathexclamationmark}}
\mathcode`!="8000
\makeatother

\usepackage{sectsty}
\allsectionsfont{\sffamily}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{Bài toán}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{goal}{Goal}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{intuition}{Intuition}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=1.5cm,bottom=1.5cm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
    \mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}
\newcommand{\genstirlingI}[3]{%
    \genfrac{[}{]}{0pt}{#1}{#2}{#3}%
}
\newcommand{\genstirlingII}[3]{%
    \genfrac{\{}{\}}{0pt}{#1}{#2}{#3}%
}
\newcommand{\stirlingI}[2]{\genstirlingI{}{#1}{#2}}
\newcommand{\dstirlingI}[2]{\genstirlingI{0}{#1}{#2}}
\newcommand{\tstirlingI}[2]{\genstirlingI{1}{#1}{#2}}
\newcommand{\stirlingII}[2]{\genstirlingII{}{#1}{#2}}
\newcommand{\dstirlingII}[2]{\genstirlingII{0}{#1}{#2}}
\newcommand{\tstirlingII}[2]{\genstirlingII{1}{#1}{#2}}

\title{Lecture Note: Graph Neural Networks\\Bài Giảng: Mạng Nơron Đồ Thị}
\author{Nguyễn Quản Bá Hồng\footnote{A scientist- {\it\&} creative artist wannabe, a mathematics {\it\&} computer science lecturer of Department of Artificial Intelligence {\it\&} Data Science (AIDS), School of Technology (SOT), UMT Trường Đại học Quản lý {\it\&} Công nghệ TP.HCM, Hồ Chí Minh City, Việt Nam.\\E-mail: {\sf nguyenquanbahong@gmail.com} {\it\&} {\sf hong.nguyenquanba@umt.edu.vn}. Website: \url{https://nqbh.github.io/}. GitHub: \url{https://github.com/NQBH}.}}
\date{\today}

\begin{document}
\maketitle
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\dominitoc % Initialization
\tableofcontents

%------------------------------------------------------------------------------%

\chapter*{Preface}

\section*{Abstract}
This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:

{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.

Latest version:
\begin{itemize}
    \item {\it Lecture Note: Graph Neural Networks\\Bài Giảng: Mạng Nơron Đồ Thị}.

    PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/lecture/NQBH_combinatorics_graph_theory_lecture.pdf}.

    \TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/lecture/NQBH_combinatorics_graph_theory_lecture.tex}.
    \item {\it Slide: Combinatorics \& Graph Theory -- Slide Bài Giảng: Tổ Hợp \& Lý Thuyết Đồ Thị}.

    PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/slide/NQBH_combinatorics_graph_theory_slide.pdf}.

    \TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/slide/NQBH_combinatorics_graph_theory_slide.tex}.
    \item {\it Survey: Combinatorics \& Graph Theory -- Khảo Sát: Tổ Hợp \& Lý Thuyết Đồ Thị}.

    PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/NQBH_combinatorics.pdf}.

    \TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/NQBH_combinatorics.tex}.
    \item Codes:
    \begin{itemize}
        \item C{\tt/}C++: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/C++}.
        \item Pascal: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/Pascal}.
        \item Python: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/Python}.
    \end{itemize}
\end{itemize}
Tài liệu này là bài giảng tôi dạy cho sinh viên Khoa Công Nghệ (undegraduate Computer Science students) chuyên ngành Kỹ Thuật Phần Mềm (Software Engineering, abbr., SE) \& Trí Tuệ Nhân Tạo--Khoa Học Dữ Liệu (Artificial Intelligence--Data Science, abbr., AIDS) nên sẽ tập trung vào phương diện lập trình cho các khái niệm Tổ hợp \& Lý thuyết đồ thị được nghiên cứu. Bài giảng này gồm 2 phần chính:
\begin{itemize}
    \item {\bf Part I: Combinatorics -- Tổ Hợp}.
    \item {\bf Part II: Graph Theory -- Lý Thuyết Đồ Thị}. Tập trung vào các thuật toán trên cây (algorithms on trees) \& thuật toán trên đồ thị (algorithms on graphs)
\end{itemize}

%------------------------------------------------------------------------------%

\chapter{Introduction to Graph Neural Networks -- Nhập Môn Mạng Nơron Đồ Thị}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
    \item \href{https://en.wikipedia.org/wiki/Graph_neural_network}{Wikipedia{\tt}graph neural network}.
\end{enumerate}
{\it Graph neural networks (GNN)} are specialized artificial neural networks (ANNs) that are designed for tasks whose inputs are graphs.

-- Mạng nơ-ron đồ thị (GNN) là mạng nơ-ron nhân tạo chuyên biệt (ANN) được thiết kế cho các tác vụ có đầu vào là đồ thị.

1 prominent example is molecular drug design. Each input sample is a graph representation of a molecule, where atoms form the nodes \& chemical bonds between atoms form the edges. In addition to the graph representation, the input also includes known chemical properties for each of the atoms. Dataset samples may thus differ in length, reflecting the varying numbers of atoms in molecules, \& the varying number of bonds between them. The task is to predict the efficacy of a given molecule for a specific medical application, like eliminating E. coli bacteria.

-- 1 ví dụ nổi bật là thiết kế thuốc phân tử. Mỗi mẫu đầu vào là một biểu diễn đồ thị của một phân tử, trong đó các nguyên tử tạo thành các nút \& các liên kết hóa học giữa các nguyên tử tạo thành các cạnh. Ngoài biểu diễn đồ thị, dữ liệu đầu vào còn bao gồm các đặc tính hóa học đã biết của từng nguyên tử. Do đó, các mẫu tập dữ liệu có thể khác nhau về độ dài, phản ánh số lượng nguyên tử khác nhau trong phân tử, \& số lượng liên kết khác nhau giữa chúng. Nhiệm vụ là dự đoán hiệu quả của một phân tử nhất định cho một ứng dụng y tế cụ thể, chẳng hạn như loại bỏ vi khuẩn E. coli.

The key design element of GNNs is the use of {\it pairwise message passing}, s.t. graph nodes iteratively update their representations by exchanging information with their neighbors. Several GNN architectures have been proposed, which implement different flavors of message passing, started by recursive or convolutional constructive approaches. As of 2022, it is an open question whether it is possible to define GNN architectures ``going beyond'' message passing, or instead every GNN can be built on message passing over suitably defined graphs.

-- Yếu tố thiết kế chính của GNN là việc sử dụng {\it pairwise message passing}, tức là các nút đồ thị cập nhật biểu diễn của chúng một cách lặp đi lặp lại bằng cách trao đổi thông tin với các nút lân cận. Một số kiến trúc GNN đã được đề xuất, triển khai các phương pháp truyền thông điệp khác nhau, bắt đầu bằng các phương pháp xây dựng đệ quy hoặc tích chập. Tính đến năm 2022, vẫn còn là một câu hỏi bỏ ngỏ liệu có thể định nghĩa kiến trúc GNN ``vượt ra ngoài'' việc truyền thông điệp hay không, hay thay vào đó, mọi GNN đều có thể được xây dựng dựa trên việc truyền thông điệp trên các đồ thị được xác định phù hợp.

In the more general subject of ``geometric DL'', certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A \href{https://en.wikipedia.org/wiki/Convolutional_neural_network}{convolutional neural network} layer, in the context of computer vision, can be considered a GNN applied to graphs whose nodes are \href{https://en.wikipedia.org/wiki/Pixel}{pixels} \& only adjacent pixels are connected by edges in the graph. A \href{https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)}{transformer} layer, in \href{https://en.wikipedia.org/wiki/Natural_language_processing}{natural language processing}, can be considered a GNN applied to \href{https://en.wikipedia.org/wiki/Complete_graph}{complete graphs} whose nodes are words or tokens in a passing of natural language text.

-- Trong lĩnh vực tổng quát hơn của ``học máy hình học'', một số kiến trúc mạng nơ-ron hiện có có thể được hiểu là mạng nơ-ron tích chập (GNN) hoạt động trên các đồ thị được định nghĩa phù hợp. Trong bối cảnh thị giác máy tính, một lớp mạng nơ-ron tích chập có thể được coi là một GNN áp dụng cho các đồ thị có các nút là \href{https://en.wikipedia.org/wiki/Pixel}{pixels} \& chỉ các điểm ảnh liền kề được kết nối bằng các cạnh trong đồ thị. Trong xử lý ngôn ngữ tự nhiên, một lớp biến đổi có thể được coi là một GNN áp dụng cho các đồ thị hoàn chỉnh có các nút là các từ hoặc token trong quá trình truyền văn bản ngôn ngữ tự nhiên.

Relevant application domains for GNNs include natural language processing, social networks, citation networks, molecular biology, chemistry, physics, \& NP-hard combinatorial optimization problem.

-- Các lĩnh vực ứng dụng có liên quan cho GNN bao gồm xử lý ngôn ngữ tự nhiên, mạng xã hội, mạng trích dẫn, sinh học phân tử, hóa học, vật lý và bài toán tối ưu hóa tổ hợp NP-hard.

Open source libraries implementing GNNs include PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), Deep Graph Library (framework agnostic), jraph (Google JAX), \& {\tt GraphNeuralNetworks.jl/GeometricFlux.jl} (Julia, Flux).

%------------------------------------------------------------------------------%

\section{Architecture of GNNs -- Kiến Trúc của GNNs}
The architecture of a generic GNN implements the following fundamental layers:
\begin{enumerate}
    \item {\it Permutation equivariant}: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature, permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively, in a message passing layer, nodes {\it update} their representations by {\it aggregating} the {\it messages} received from their immediate neighbors. As such, each message passing layer increases the receptive field of the GNN by 1 hop.
    \item {\it Local pooling}: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN, in a similar fashion to pooling layers in CNNs. Examples include k-nearest neighbors pooling, top-k pooling, \& self-attention pooling.
    \item {\it Global pooling}: a global pooling layer, also known as {\it readout} layer, provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant, s.t. permutations in the ordering of graph nodes \& edges do not alter the final output. Examples include element-wise sum, mean or maximum.
\end{enumerate}
-- Kiến trúc của một GNN chung triển khai các lớp cơ bản sau:
\begin{enumerate}
    \item {\it Permutation equivariant}: một lớp permutation equivariant ánh xạ một biểu diễn của một đồ thị thành một biểu diễn được cập nhật của cùng một đồ thị. Trong các tài liệu, các lớp permutation equivariant được triển khai thông qua việc truyền thông điệp từng cặp giữa các nút đồ thị. Theo trực giác, trong một lớp truyền thông điệp, các nút {\it cập nhật} biểu diễn của chúng bằng cách {\it tổng hợp} các {\it thông điệp} nhận được từ các nút lân cận trực tiếp của chúng. Như vậy, mỗi lớp truyền thông điệp sẽ tăng trường tiếp nhận của GNN thêm 1 bước nhảy.

    \item {\it Local pooling}: một lớp local pooling làm đồ thị thô hơn thông qua việc giảm mẫu. Local pooling được sử dụng để tăng trường tiếp nhận của GNN, tương tự như các lớp pooling trong CNN. Các ví dụ bao gồm pooling k-nearest neighbors, pooling top-k, và pooling \& self-attention.

    \item {\it Tổng hợp toàn cục}: một lớp tổng hợp toàn cục, còn được gọi là lớp {\it readout}, cung cấp biểu diễn kích thước cố định cho toàn bộ đồ thị. Lớp tổng hợp toàn cục phải bất biến hoán vị, tức là các hoán vị trong thứ tự các nút \& cạnh của đồ thị không làm thay đổi kết quả đầu ra cuối cùng. Ví dụ bao gồm tổng từng phần tử, trung bình hoặc cực đại.
\end{enumerate}
It has been demonstrated that GNNs cannot be more expensive than the \href{https://en.wikipedia.org/wiki/Weisfeiler_Leman_graph_isomorphism_test}{Weisfeiler--Leman Graph Isomorphism Test}. In practice, this means that there exist different graph structures (e.g., molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries e.g. simplicial complexes can be designed. As of 2022, whether or not future architectures will overcome the message passing primitive is an open research question.

-- Người ta đã chứng minh rằng GNN không thể đắt hơn Phép thử Đồng cấu Đồ thị Weisfeiler-Leman. Trên thực tế, điều này có nghĩa là tồn tại các cấu trúc đồ thị khác nhau (ví dụ: các phân tử có cùng nguyên tử nhưng liên kết khác nhau) mà GNN không thể phân biệt được. Có thể thiết kế các GNN mạnh hơn hoạt động trên các hình học chiều cao hơn, ví dụ như các phức hợp đơn giản. Tính đến năm 2022, liệu các kiến trúc trong tương lai có vượt qua được nguyên thủy truyền tin hay không vẫn là một câu hỏi nghiên cứu chưa có lời giải.

%------------------------------------------------------------------------------%

\section{Message Passing Layers -- Các Lớp Truyền Tin Nhắn}
Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally, they can be expressed as message passing neural networks (MPNNs).

-- Các lớp truyền thông điệp là các lớp hoán vị tương đương biến thể, ánh xạ một đồ thị thành một biểu diễn được cập nhật của cùng đồ thị đó. Về mặt hình thức, chúng có thể được biểu diễn dưới dạng mạng nơ-ron truyền thông điệp (MPNN).

\begin{definition}[MPNN layer]
    Let $G = (V,E)$ be a graph, where $V$ is the node set \& $E$ is the edge set. Let $N_u$ be the neighborhood of some node $u\in V$. Additionally, let ${\bf x}_u$ be the features of node $u\in V$, \& ${\bf e}_{uv}$ be the features of edge $(u,v)\in E$. An MPNN layer can be expressed as follows:
    \begin{equation*}
        {\bf h}_u = \phi\left({\bf x}_u,\bigoplus_{v\in N_u} \psi({\bf x}_u,{\bf x}_v,{\bf e}_{uv})\right),
    \end{equation*}
    where $\phi,\psi$ are differentiable functions (e.g., artificial neural networks), \& $\bigoplus$ is a permutation invariant \href{https://en.wikipedia.org/wiki/Aggregation_operator}{aggregation operator} that can accept an arbitrary number of inputs (e.g., element-wise sum, mean, or max). In particular, $\phi,\psi$ are referred to as {\rm update} \& {\rm message} functions, respectively.
\end{definition}

\begin{dinhnghia}[Lớp mạng nơ-ron truyền tin]
    Cho $G = (V,E)$ là một đồ thị, trong đó $V$ là tập nút \& $E$ là tập cạnh. Cho $N_u$ là lân cận của một nút $u\in V$. Ngoài ra, cho ${\bf x}_u$ là các đặc trưng của nút $u\in V$, \& ${\bf e}_{uv}$ là các đặc trưng của cạnh $(u,v)\in E$. Một lớp MPNN có thể được biểu diễn như sau:
    \begin{equation*}
        {\bf h}_u = \phi\left({\bf x}_u,\bigoplus_{v\in N_u} \psi({\bf x}_u,{\bf x}_v,{\bf e}_{uv})\right),
    \end{equation*}
    trong đó $\phi,\psi$ là các hàm khả vi (ví dụ: mạng nơ-ron nhân tạo), \& $\bigoplus$ là một toán tử tổng hợp bất biến hoán vị có thể chấp nhận một số lượng đầu vào tùy ý (ví dụ: tổng từng phần tử, trung bình hoặc cực đại). Cụ thể, $\phi,\psi$ được gọi tương ứng là các hàm {\rm cập nhật} \& {\rm tin nhắn}.
\end{dinhnghia}
Intuitively, in an MPNN computational block, graph nodes {\it update} their representations by {\it aggregating} the {\it messages} received from their neighbors.

-- Theo trực giác, trong khối tính toán MPNN, các nút đồ thị {\it cập nhật} biểu diễn của chúng bằng cách {\it tổng hợp} các {\it thông điệp} nhận được từ các nút lân cận của chúng.

The outputs of 1 or more MPNN layers are node representations ${\bf h}_u$ for each node $u\in V$ in the graph. Node representations can be employed for any downstream task, e.g. node{\tt/}graph classification or edge prediction.

-- Đầu ra của 1 hoặc nhiều lớp MPNN là biểu diễn nút ${\bf h}_u$ cho mỗi nút $u\in V$ trong đồ thị. Biểu diễn nút có thể được sử dụng cho bất kỳ tác vụ hạ nguồn nào, ví dụ: phân loại nút{\tt/}đồ thị hoặc dự đoán cạnh.

Graph nodes in an MPNN update their representation aggregation information from their immediate neighbors. As such, stacking $n$ MPNN layers means that 1 node will be able to communicate with nodes that are at most $n$ ``hops'' away. In principle, to ensure that every node receives information from every other node, one would need to stack a number of MPNN layers equal to the graph diameter. However, stacking many MPNN layers may cause issues e.g. oversmoothing \& oversquashing. Oversmoothing refers to issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures e.g. skip connections (as in residual neural networks), gated updated rules \& jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer, i.e., by considering the graph as a complete graph, can mitigate oversquashing in problems where long-range dependencies are required.

-- Các nút đồ thị trong MPNN cập nhật thông tin tổng hợp biểu diễn của chúng từ các nút lân cận trực tiếp. Do đó, việc xếp chồng $n$ lớp MPNN có nghĩa là 1 nút sẽ có thể giao tiếp với các nút cách xa tối đa $n$ ``bước nhảy''. Về nguyên tắc, để đảm bảo rằng mọi nút đều nhận được thông tin từ mọi nút khác, người ta cần xếp chồng một số lớp MPNN bằng với đường kính đồ thị. Tuy nhiên, việc xếp chồng nhiều lớp MPNN có thể gây ra các vấn đề, ví dụ như làm mịn quá mức \& đè bẹp quá mức. Làm mịn quá mức đề cập đến vấn đề biểu diễn nút trở nên không thể phân biệt được. Đè bẹp quá mức đề cập đến nút thắt cổ chai được tạo ra bằng cách nén các phụ thuộc tầm xa vào các biểu diễn có kích thước cố định. Các biện pháp đối phó, ví dụ như kết nối bỏ qua (như trong mạng nơ-ron dư), các quy tắc cập nhật có cổng \& kiến thức nhảy có thể giảm thiểu việc làm mịn quá mức. Việc sửa đổi lớp cuối cùng thành một lớp hoàn toàn liền kề, tức là bằng cách coi đồ thị là một đồ thị hoàn chỉnh, có thể giảm thiểu việc đè bẹp quá mức trong các vấn đề yêu cầu các phụ thuộc tầm xa.

Other ``flavors'' of MPNN have been developed in the literature, e.g. graph convolutional networks \& graph attention networks, whose definitions can be expressed in terms of the MPNN formalism.

-- Các ``hương vị'' khác của MPNN đã được phát triển trong tài liệu, ví dụ như mạng tích chập đồ thị \& mạng chú ý đồ thị, định nghĩa của chúng có thể được thể hiện theo hình thức MPNN.

%------------------------------------------------------------------------------%

\subsection{Graph convolutional network (GCN) -- Mạng lưới tích chập đồ thị}
The graph convolutional network (GCN) was 1st introduced by {\sc Thomas Kipf \& Max Welling} in 2017. A GCN layer defines a 1st-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data.

-- Mạng tích chập đồ thị (GCN) được giới thiệu lần đầu tiên bởi {\sc Thomas Kipf \& Max Welling} vào năm 2017. Một lớp GCN định nghĩa phép xấp xỉ bậc 1 của bộ lọc phổ cục bộ trên đồ thị. GCN có thể được hiểu là sự tổng quát hóa của mạng nơ-ron tích chập lên dữ liệu có cấu trúc đồ thị.

The formal expression of a GCN layer reads as follows:
\begin{equation*}
    {\bf H} = \sigma\left(\widetilde{\bf D}^{-\frac{1}{2}}\widetilde{\bf A}\widetilde{\bf D}^{-\frac{1}{2}}{\bf X}\Theta\right),
\end{equation*}
where ${\bf H}$ is the matrix of node representations ${\bf h}_u$, ${\bf X}$ is the matrix of node features ${\bf x}_u$, $\sigma(\cdot)$ is an activation function (e.g., ReLU), $\widetilde{\bf A}$ is the graph adjacency matrix with the addition of self-loops, $\widetilde{\bf D}$ is the graph degree matrix with the addition of self-loops, \& $\Theta$ is a matrix of trainable parameters.

-- Biểu thức chính thức của một lớp GCN được đọc như sau:
\begin{equation*}
    {\bf H} = \sigma\left(\widetilde{\bf D}^{-\frac{1}{2}}\widetilde{\bf A}\widetilde{\bf D}^{-\frac{1}{2}}{\bf X}\Theta\right),
\end{equation*}
trong đó ${\bf H}$ là ma trận biểu diễn nút ${\bf h}_u$, ${\bf X}$ là ma trận đặc trưng nút ${\bf x}_u$, $\sigma(\cdot)$ là một hàm kích hoạt (ví dụ: ReLU), $\widetilde{\bf A}$ là ma trận kề đồ thị với việc bổ sung các vòng lặp tự thân, $\widetilde{\bf D}$ là ma trận bậc đồ thị với việc bổ sung các vòng lặp tự thân, \& $\Theta$ là ma trận các tham số có thể đào tạo được.

%------------------------------------------------------------------------------%

\chapter{Miscellaneous}
\minitoc

%------------------------------------------------------------------------------%

\section{Contributors}

\begin{enumerate}
    \item {\sc Võ Ngọc Trâm Anh [VNTA].}
    \item {\sc Đặng Phúc An Khang [DPAK]}
    \item {\sc Nguyễn Ngọc Thạch [NNT].}
    \item {\sc Phan Vĩnh Tiến [PVT].}
\end{enumerate}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]

\end{document}