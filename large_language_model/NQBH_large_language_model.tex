\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Large Language Models (LLMs) -- Mô Hình Ngôn Ngữ Lớn}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Large Language Models -- Mô Hình Ngôn Ngữ Lớn}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/large_language_model/NQBH_large_language_model.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/large_language_model/NQBH_large_language_model.tex}.
		\item {\it }.
		
		PDF: {\sc url}: \url{.pdf}.
		
		\TeX: {\sc url}: \url{.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basics LLMs}

\subsection{\cite{Raschka2024}. {\sc Sebastian Raschka}. Build A Large Language Model (From Scratch)}
{\sf[125 Amazon ratings]}
\begin{itemize}
	\item {\sf Amazon review.} Learn how to create, train, \& tweak LLMs by building 1 from ground up! In {\it Build a Large Language Model (from Scratch)} bestselling author {\sc Sebastian Raschka} guides you step by step through creating your own LLM. Each stage is explained with clear text, diagrams, \& examples. Go from initial design \& creation, to pretraining on a general corpus, \& on to fine-tuning for specific tasks.
	
	{\it Build a Large Language Model (from Scratch)} teaches how to:
	\begin{itemize}
		\item Plan \& code all parts of an LLM
		\item Prepare a dataset suitable for LLM training
		\item Fine-tune LLMs for text classification \& with your own data
		\item Use human feedback to ensure your LLM follows instructions
		\item Load pretrained weights into an LLM
	\end{itemize}
	{\it Build a Large Language Model (from Scratch)} takes you inside AI black box to tinker with internal systems that power generative AI. As work through each key stage of LLM creation, develop an in-depth understanding of how LLMs work, their limitations, \& their customization methods. Your LLM can be developed on an ordinary laptop, \& used as your own personal assistant.
	
	{\bf About technology.} Physicist {\sc Richard P. Feynman} reportedly said, ``I don't understand anything I can't build.'' Based on this same powerful principle, bestselling author {\sc Sebastian Raschka} guides you step by step as build a GPT-style LLM that can run on laptop. This is an engaging book that covers each stage of process, from planning \& coding to training \& fine-tuning.
	
	{\bf About book.} {\it Build a Large Language Model (From Scratch)} is a practical \& eminently-satisfying hands-on journey into foundations of generative AI. Without relying on any existing LLM libraries, code a base model, evolve it into a text classifier, \& ultimately create a chatbot that can follow your conversational instructions. \& really understand it because built it yourself!
	
	{\bf What's inside.}
	\begin{itemize}
		\item Plan \& code an LLM comparable to GPT-2
		\item Load pretrained weights
		\item Construct a complete training pipeline
		\item Fine-tune your LLM for text classification
		\item Develop LLMs that follow human instructions
	\end{itemize}
	\item {\sf About reader.} Readers need intermediate Python skills \& some knowledge of ML. LLM you create will run on any modern laptop \& can optionally utilize GPUs.
	\item {\sf About author.} {\sc Sebastian Raschka} is a Staff Research Engineer at Lighting AI, where he works on LLM research \& develops an open-source software. The technical editor on this book was  {\sc David Caswell}.
	\item {\sf Editorial Reviews.}
	\begin{itemize}
		\item ``The most comprehensive book I've seen on building LLMs. Highly recommended!'' -- {\sc Raul Ciotescu}, CTO, Netzinkubator Software
		\item ``A clear, hands-on guide that empowers readers to build their own models \& explore cutting edge of AI.'' -- {\sc Guillermo Alcántara}, Project manager, PepsiCo Global
		\item ``Must-have resource for quickly getting up to speed on LLMs. Whether you're new to field or looking to deepen your knowledge, it's perfect guide.'' -- {\sc Walter Reade},  Staff Developer Relations Engineer, Kaggle{\tt/}Google
		\item ``A fantastic resource for diving into LLMs -- a must-read for anyone eager to get hands-on!'' -- Dr. {\sc Vahid Mirjalili}, Senior Data Scientist, FM Global
	\end{itemize}
	{\sf Pipeline: 3 main stages of coding a LLM are implementing LLM architecture \& data preparation process (stage 1), pretraining an LLM to create a foundation model (stage 2), \& fine-tuning foundation model to become a personal assistant or text classifier (stage 3). Each of these stages is explored \& implemented in this book.}
	\item {\sf Preface.} Always been fascinated with language models. $> 1$ decade ago, journey into AI began with a statistical pattern classification class, which led to 1st independent project: developing a model \& web application to detect mood of a song based on its lyrics.
	
	Fast forward to 2022, with release of ChatGPT, LLMs have taken world by storm \& have revolutionized how many of us work. These models are incredibly versatile, aiding in tasks e.g. checking grammar, composing emails, summarizing lengthy documents, \& much more. This is owed to their ability to parse \& generate human-like text, which is important in various fields, from customer service to content creation, \& even in more technical domains like coding \& data analysis.
	
	As their name implies, a hallmark of LLMs: they are ``large'' -- very large -- encompassing millions to billions of parameters. (For comparison, using more traditional ML or statistical methods, Iris flower dataset can be classified with $>$ 90\% accuracy using a small model with only 2 parameters.) However, despite large size of LLMs compared to more traditional methods, LLMs don't have to be a black box.
	
	In this book, will learn how to build an LLM 1 step at a time. By end, will have a solid understanding of how an LLM, like ones used in ChatGPT, works on a fundamental level. Believe: developing confidence with each part of fundamental concepts \& underlying code is crucial for success. This not only helps in fixing bugs \& improving performance but also enables experimentation with new ideas.
	
	Several years ago, when I started working with LLMs, had to learn how to implement them hard way, sifting through many research papers \& incomplete code repositories to develop a general understanding. With this book, hope to make LLMs more accessible by developing \& sharing a step-by-step implementation tutorial detailing all major components \& developments phases of an LLM.
	
	Strongly believe: best way to understand LLMs: code one from scratch -- \& will see that this can be fun too! Happy reading \& coding!
	\item {\sf Acknowledgments.} Writing a book is a significant undertaking -- Viết 1 cuốn sách là 1  công việc quan trọng.
	\item {\sf About this Book.} {\it Build a Large Language Model (From Scratch)} was written to help you understand \& create your own GPT-like LLMs from ground up. It begins by focusing on fundamentals of working with text data \& coding attention mechanisms \& then guides through implementing a complete GPT model from scratch. Book then covers pretraining mechanism as well as fine-tuning for specific tasks e.g. text classification \& following instructions. By end of this book, have a deep understanding of how LLMs work \& skills to build your own models. While models you'll create are smaller in scale compared to large foundational models, they use same concepts \& serve as powerful educational tools to grasp core mechanisms \& techniques used in building state-of-art LLMs.
	\begin{itemize}
		\item {\sf Who should read this book.} {\it Build a Large Language Model (From Scratch)} is for ML enthusiasts, engineers, researchers, students, \& practitioners who want to gain a deep understanding of how LLMs work \& learn to build their own models from scratch. Both beginners \& experienced developers will be able to use their existing skills \& knowledge to grasp concepts \& techniques used in creating LLMs.
		
		What sets this book apart is its comprehensive coverage of entire process of building LLMs, from working with datasets to implementing model architecture, pretraining on unlabeled data, \& fine-tuning for specific tasks. As of this writing, no other resource provides such a complete \& hands-on approach to building LLMs from ground up.
		
		To understand code examples in this book, should have a solid grasp of Python programming. While some familiarity with ML, DL, \& AI can be beneficial, an extensive background in these areas is not required. LLMs are a unique subset of AI, so even if relatively new to field, will be able to follow along.
		
		If have some experience with deep neural networks, may find certain concepts more familiar, as LLMs are built upon these architectures. However, proficiency in PyTorch is not a prerequisite. Appendix A provides a concise introduction to PyTorch, equipping with necessary skills to comprehend code examples throughout book.
		
		A high school-level understanding of mathematics, particularly working with vectors \& matrices, can be helpful as explore inner workings of LLMs. However, advanced mathematical knowledge is not necessary to grasp key concepts \& ideas presented in this book.
		
		Most important prerequisite is a strong foundation in Python programming. With this knowledge, will be well prepared to explore fascinating world of LLMs \& understand concepts \& code examples presented in this book.
		\item {\sf How this book is organized: A roadmap.} This book is designed to be read sequentially, as each chap builds upon concepts \& techniques introduced in prev ones. Book is divided into 7 chaps that cover essential aspects of LLMs \& their implementation.
		\begin{itemize}
			\item Chap. 1 provides a high-level introduction to fundamental concepts behind LLMs. It explores transformer architecture, which forms basis for LLMs e.g. those used on ChatGPT platform.
			\item Chap. 2 lays out a plan for building an LLM from scratch. It covers process of preparing text for LLM training, including splitting text into word \& subword tokens, using byte pair encoding for advanced tokenization, sampling training examples with a sliding window approach, \& converting tokens into vectors that feed into LLM.
			\item Chap. 3 focuses on attention mechanisms used in LLMs. It introduces a basic self-attention framework \& progresses to an enhanced self-attention mechanism. Chap also covers implementation of a causal attention module that enables LLMs to generate 1 token at a time, masking randomly selected attention weights with dropout to reduce overfitting \& stacking multiple causal attention modules into a multihead attention module.
			\item Chap. 4 focuses on coding a GPT-like LLM that can be trained to generate human-like text. It covers techniques e.g. normalizing layer activations to stabilize neural network training, adding shortcut connections in deep neural networks to train models more effectively, implementing transformer blocks to create GPT models of various sizes, \& computing number of parameters \& storage requirements of GPT models.
			\item Chap. 5 implements pretraining process of LLMs. It covers computing training \& validation set losses to assess quality of LLM-generated text, implementing a training function \& pretraining LLM, saving \& loading model weights to continue training an LLM, \& loading pretrained weights form OpenAI.
			\item Chap. 6 introduces different LLM fine-tuning approaches. It covers preparing a dataset for text classification, modifying a pretrained LLM for fine-tuning, fine-tuning an LLM to identify spam messages, \& evaluating accuracy of a fine-tuned LLM classifier.
			\item Chap. 7 explores instruction fine-tuning process of LLMs. It covers preparing a dataset for supervised instruction fine-tuning, organizing instruction data in training batches, loading a pretrained LLM \& fine-tuning it to follow human instructions, extracting LLM-generated instruction responses for evaluation, \& evaluating an instruction-fine-tuned LLM.
		\end{itemize}
		\item {\sf About code.} To make it as easy as possible to follow along, all code examples in this book are conveniently available on Manning website at \url{https://www.manning.com/books/build-a-large-language-model-from-scratch}, as well as in Jupyter notebook format on GitHub at \url{https://github.com/rasbt/LLMs-from-scratch}. Solutions to all code exercises can be found in appendix C.
		
		This book contains many examples of source code both in numbered listings \& in line with normal text.
		
		In many cases, original source code has been reformatted; added line breaks \& reworked indentation to accommodate available page space in book. In rare cases, even this was not enough, \& listings include line-continuation markers. Additionally, comments in source code have often been removed from listings when code is described in text. Code annotations accompany many of listings, highlighting important concepts.
		
		1 of key goals of this book is accessibility, so code examples have been carefully designed to run efficiently on a regular laptop, without need for any special hardware. But if do have access to a GPU, certain sects provide helpful tips on scaling up datasets \& models to take advantage of that extra power.
		
		Throughout book, use PyTorch as our go-to tensor \& a DL library to implement LLMs from ground up. If PyTorch is new to you, I recommend you start with appendix A, which provides an in-depth introduction, complete with setup recommendations.
		\item {\sf Other online resources.} Interested in latest AI \& LLM research trends? $\to$ Check out blog at \url{https://magazine.sebastianraschka.com/}, where regularly discuss latest AI research with a focus on LLMs.
		
		Need help getting up to speed with DL \& PyTorch? $\to$ Offer several free courses on website at \url{https://sebastianraschka.com/teaching/}. These resources can help you quickly get up to speed with latest techniques.
		
		Looking for bonus materials related to book? $\to$ Visit book's GitHub repo at \url{https://github.com/rasbt/LLMs-from-scratch} to find additional resources \& examples to supplement your learning.		
	\end{itemize}
	\item {\sf1. Understanding LLMs.} Cover:
	\begin{itemize}
		\item High-level explanations of fundamental concepts behind LLMs
		\item Insights into transformer architecture from which LLMs are derived
		\item A plan for building an LLM from scratch
	\end{itemize}
	LLMs, e.g. those offered in OpenAI's ChatGPT, are deep neural network models that have been developed over the past few years. They ushered in a new era for NLP. Before advent of LLMs, traditional methods excelled at categorization tasks e.g. email spam classification \& straightforward pattern recognition that could be captured with handcrafted rules or simpler models. However, they typically underperformed in language tasks that demanded complex understanding \& generation abilities, e.g. parsing detailed instructions, conducting contextual analysis, \& creating coherent \& contextually appropriate original text. E.g., previous generations of language models could not write an email from a list of keywords -- a task that is trivial for contemporary LLMs.
	
	-- LLM, ví dụ như những LLM được cung cấp trong ChatGPT của OpenAI, là các mô hình mạng nơ-ron sâu đã được phát triển trong vài năm qua. Chúng mở ra 1  kỷ nguyên mới cho NLP. Trước khi LLM ra đời, các phương pháp truyền thống đã xuất sắc trong các nhiệm vụ phân loại, ví dụ như phân loại thư rác email \& nhận dạng mẫu đơn giản có thể được nắm bắt bằng các quy tắc thủ công hoặc các mô hình đơn giản hơn. Tuy nhiên, chúng thường hoạt động kém trong các nhiệm vụ ngôn ngữ đòi hỏi khả năng hiểu phức tạp \& tạo ra, ví dụ như phân tích các hướng dẫn chi tiết, tiến hành phân tích theo ngữ cảnh, \& tạo ra văn bản gốc mạch lạc \& phù hợp với ngữ cảnh. Ví dụ, các thế hệ mô hình ngôn ngữ trước đây không thể viết email từ danh sách các từ khóa -- 1  nhiệm vụ tầm thường đối với LLM đương đại.
	
	LLMs have remarkable capabilities to understand, generate, \& interpret human language. However, important to clarify that when we say language models ``understand,'' i.e., they can process \& generate text in ways that appear coherent \& contextually relevant, not that they possess human-like consciousness or comprehension.
	
	Enabled by advancements in DL, which is a subset of ML \& AI focused on neural networks, LLMs are trained on vast quantities of text data. This large-scale training allows LLMs to capture deeper contextual information \& subtleties of human language compared to previous approaches. As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, \& many more.
	
	Another important distinction between contemporary LLMs \& earlier NLP models is that earlier NLP models were typically designed for specific tasks, e.g. text categorization, language translation, etc. While those earlier NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency across a wide range of NLP tasks.
	
	Success behind LLMs can be attribute to transformer architecture that underpins many LLMs \& vast amounts of data on which LLMs are trained, allowing them to capture a wide variety of linguistic nuances, contexts, \& patterns that would be challenging to encode manually.
	
	This shift toward implementing models based on transformer architecture \& using large training datasets to train LLMs has fundamentally transformed NLP, providing more capable tools for understanding \& interacting with human language.
	
	Following discussion sets a foundation to accomplish primary objective of this book: understanding LLMs by implementing a ChatGPT-like LLM based on transformer architecture step by step in code.
	\begin{itemize}
		\item {\sf1.1. What is an LLM?} An LLM is a neural network designed to understand, generate, \& respond to human-like text. These models are deep neural networks trained on massive amounts of text data, sometimes encompassing large portions of entire publicly available text on internet.
		
		``Large'' in ``LLM'' refers to both model's size in terms of parameters \& immense dataset on which it's trained. Models like this often have 10s or even hundreds of billions of parameters, which are adjustable weights in network that are optimized during training to predict next word in a sequence. Next-word prediction is sensible because it harnesses inherent sequential nature of language to train models on understanding context, structure, \& relationships within text. Yet, it is a very simple task, \& so it is surprising to many researchers that it can produce such capable models. In later chaps, discuss \& implement next-word training procedure step by step.
		
		LLMs utilize an architecture called {\it transformers}, which allows them to pay selective attention to different parts of input when making predictions, making them especially adept at handling nuances \& complexities of human language.
		
		Since LLMs are capable of {\it generating} text, LLMs are also often referred to as a form of generative AI, often abbreviated as {\it generative AI} or {\it GenAI}. As illustrated in {\sf Fig. 1.1: As this hierarchical depiction of relationship between different fields suggests, LLMs represent a specific application of DL techniques, using their ability to process \& generate human-like text. DL is a specialized branch of ML that focuses on using multilayer neural networks. ML \& DL are fields aimed at implementing algorithms that enable computers to learn from data \& perform tasks that typically require human intelligence.}, AI encompasses broader field of creating machines that can perform tasks requiring human-like intelligence, including understanding language, recognizing patterns, \& making decisions, \& includes subfields like ML \& DL.
		\begin{itemize}
			\item AI: Systems with human-like intelligence
			\item ML: Algorithms that learn rules automatically from data
			\item DL: ML with neural networks consisting of many layers
			\item LLM: Deep neural network for parsing \& generating human-like text
			\item GenAI: GenAI involves use of deep neural networks to create new content, e.g., text, images, or various forms of media
		\end{itemize}
		Algorithms used to implement AI are focus of field of ML. Specifically, ML involves development of algorithms that can learn from \& make predictions or decisions based on data without being explicitly programmed. To illustrate this, imagine a spam filter as a practical application of ML. Instead of manually writing rules to identify spam emails, a ML algorithm is fed examples of emails labeled as spam \& legitimate emails. By minimizing error in its predictions on a training dataset, model then learns to recognize patterns \& characteristics indicative of spam, enabling it to classify new emails as either spam or not spam.
		
		DL is a subset of ML that focuses on utilizing neural networks with 3 or more layers (also called deep neural networks) to model complex patterns \& abstractions in data. In contrast to DL, traditional ML requires manual feature extraction. I.e., human experts need to identify \& select most relevant features for model.
		
		While field of AI is now dominated by ML \& DL, it also includes other approaches -- e.g., using rule-based systems, genetic algorithms, expert systems, fuzzy logic, or symbolic reasoning.
		
		Returning to spam classification example, in traditional ML, human experts might manually extract features from email text e.g. frequency of certain trigger words (e.g., ``prize'', ``win'', ``free''), number of exclamation marks, use of all uppercase words, or presence of suspicious links. This dataset, created based on these expert-defined features, would then be used to train model. In contrast to traditional ML, \fbox{DL does not require manual feature extraction}. I.e., human experts do not need to identify \& select most relevant features for a DL model. (However, both traditional ML \& DL for spam classification still require collection of labels, e.g. spam or non-spam, which need to be gathered either by an expert or users.)
		
		Look at some of problems LLMs can solve today, challenges that LLMs address, \& general LLM architecture we will implement later.		
		\item {\sf1.2. Applications of LLMs.} Owing to their advanced capabilities to parse \& understand unstructured text data, LLMs have a broad range of applications across various domains. Today, LLms are employed for machine translation, generation of novel texts (see {\sf Fig. 1.2: LLM interfaces enable natural language communication between users \& AI systems. This screenshot shows ChatGPT writing a poem according to a user's specifications.}), sentiment analysis, text summarization, \& many other tasks. LLMs have recently been used for content creation, e.g. writing fiction, articles, \& even computer code.
		
		LLMs can also power sophisticated chatbots \& virtual assistants, e.g. OpenAI's ChatGPT or Google's Gemini (formerly called Bard), which can answer user queries \& augment traditional search engines e.g. Google Search or Microsoft Bing.
		
		Moreover, LLMs may be used for effective knowledge retrieval from vast volumes of text in specialized areas e.g. medicine or law. This includes sifting through document, summarizing lengthy passages, \& answering technical questions.
		
		In short, LLMs are invaluable for automating almost any task that involves parsing \& generating text. Their applications are virtually endless, \& as we continue to innovate \& explore new ways to use these models, clear: LLMs have potential to redefine our relationship with technology, making it more conversational, intuitive, \& accessible.
		
		Will focus on understanding how LLMs work from ground up, coding an LLM that can generate texts. Will also learn about techniques that allow LLMs to carry out queries, ranging from answering questions to summarizing text, translating text into different languages, \& more. I.e., will learn how complex LLM assistants e.g. ChatGPT work by building 1 step by step.
		\item {\sf1.3. Stages of building \& using LLMs.} Why should we build our own LLMs? Coding an LLM from ground up is an excellent exercise to understand its mechanics \& limitations. Also, it equips us with required knowledge for pretraining or fine-tuning existing open source LLM architectures to our own domain-specific datasets or tasks.
		
		-- Tại sao chúng ta nên xây dựng LLM của riêng mình? Việc mã hóa LLM từ đầu là 1  bài tập tuyệt vời để hiểu cơ chế \& hạn chế của nó. Ngoài ra, nó trang bị cho chúng ta kiến thức cần thiết để đào tạo trước hoặc tinh chỉnh các kiến trúc LLM nguồn mở hiện có cho các tập dữ liệu hoặc tác vụ cụ thể theo miền của riêng chúng ta.
		\begin{note}
			Most LLMs today are implemented using PyTorch DL library, which is what we will use. Readers can find a comprehensive introduction to PyTorch in appendix A.
		\end{note}
		Research has shown: when it comes to modeling performance, customs-built LLMs -- those tailored for specific tasks or domains -- can outperform general-purpose LLMs, e.g. those provided by ChatGPT, which are designed for a wide array of applications. Examples of these include BloombergGPT (specialized for finance) \& LLMs tailored for medical question answering (see appendix B for more details).
		
		Using custom-built LLMs offers several advantages, particularly regarding data privacy. E.g., companies may prefer not to share sensitive data with 3rd-party LLM providers like OpenAI due to confidentiality concerns. Additionally, developing smaller custom LLMs enables deployment directly on customer devices, e.g. laptops \& smartphones, which is something companies like Apple are currently exploring. This local implementation can significantly decrease latency \& reduce server-related costs. Furthermore, custom LLMs grant developers complete autonomy, allowing them to control updates \& modifications to model as needed.
		
		-- Sử dụng LLM tùy chỉnh mang lại 1  số lợi thế, đặc biệt là về quyền riêng tư dữ liệu. Ví dụ, các công ty có thể không muốn chia sẻ dữ liệu nhạy cảm với các nhà cung cấp LLM bên thứ 3 như OpenAI do lo ngại về tính bảo mật. Ngoài ra, việc phát triển các LLM tùy chỉnh nhỏ hơn cho phép triển khai trực tiếp trên các thiết bị của khách hàng, ví dụ như máy tính xách tay \& điện thoại thông minh, đây là điều mà các công ty như Apple hiện đang khám phá. Việc triển khai cục bộ này có thể giảm đáng kể độ trễ \& giảm chi phí liên quan đến máy chủ. Hơn nữa, các LLM tùy chỉnh cấp cho các nhà phát triển quyền tự chủ hoàn toàn, cho phép họ kiểm soát các bản cập nhật \& sửa đổi mô hình khi cần.
		
		General process of creating an LLM includes pretraining \& fine-tuning. ``Pre'' in ``preparing'' refers to initial phase where a model like an LLM is trained on a large, diverse dataset to develop a broad understanding of language. This pre-trained model then serves as a foundational resource that can be further refined through fine-tuning, a process where model is specifically trained on a narrower dataset that is more specific to particular tasks or domains. This 2-stage training approach consisting of pretraining \& fine-tuning is depicted in {\sf Fig. 1.3: Pretraining an LLM involves next-word prediction on large text datasets. A pretrained LLM can then be fine-tuned using a smaller labeled dataset.}
		
		1st step in creating an LLM: train it on a large corpus of text data, sometimes referred to as {\it raw} text. Here, ``raw'' refers to fact that this data is just regular text without any labeling information. (Filtering may be applied, e.g. removing formatting characters or documents in unknown languages.)
		\begin{note}
			Readers with a background in ML may note: labeling information is typically required for traditional ML models \& deep neural networks trained via conventional supervised learning paradigm. However, this is not case for pretraining stage of LLMs. In this phase, LLMs use self-supervised learning, where model generates its own labels from input data.
		\end{note}
		This 1st training stage of an LLM is also known as {\it pretraining}, creating an initial pretrained LLM, often called a {\it base} or {\it foundation model}. A typical example of such a model is GPT-3 model (precursor of original model offered in ChatGPT). This model is capable of text completion -- i.e., finishing a half-written sentence provided by a user. It also has limited few-shot capabilities, i.e., it can learn to perform new tasks based on only a few examples instead of needing extensive training data.
		
		After obtaining a pretrained LLM from training on large text datasets, where LLM is trained to predict next word in text, we can further train LLM on labeled data, also known as {\it fine-tuning}.
		
		2 most popular categories of fine-tuning LLMs are {\it instruction fine-tuning} \& {\it classification fine-tuning}. In instruction fine-tuning, labeled dataset consists of instruction \& answer pairs, e.g. a query to translate a text accompanied by correctly translated text. In classification fine-tuning, labeled dataset consists of texts \& associated class labels -- e.g., emails associated with ``spam'' \& ``hot spam'' labels.
		
		-- 2 loại LLM tinh chỉnh phổ biến nhất là {\it tinh chỉnh hướng dẫn} \& {\it tinh chỉnh phân loại}. Trong tinh chỉnh hướng dẫn, tập dữ liệu được gắn nhãn bao gồm các cặp hướng dẫn \& câu trả lời, ví dụ: truy vấn để dịch văn bản đi kèm với văn bản được dịch chính xác. Trong tinh chỉnh phân loại, tập dữ liệu được gắn nhãn bao gồm các văn bản \& nhãn lớp liên quan -- ví dụ: email liên quan đến nhãn ``spam'' \& ``hot spam''.
		
		Cover code implementations for pretraining \& fine-tuning an LLM, \& delve deeper into specifics of both instruction \& classification fine-tuning after pretraining a base LLM.
		\item {\sf1.4. Introducing transformer architecture.} Most modern LLMs rely on {\it transformer} architecture, which is a deep neural network architecture introduced in 2017 paper ``Attention Is All You Need''. To understand LLMs, must understand original transformer, which was developed for machine translation, translating English texts to German \& French. A simplified version of transformer architecture is depicted in {\sf Fig. 1.4: A simplified depiction of original transformer architecture, which is a DL model for language translation. Transformer consists of 2 parts: (a) an encoder that processes input text \& produces an embedding representation (a numerical representation that captures many different factors in different dimensions) of text that (b) decoder can use to generate translated text 1 word at a time. This figure shows final stage of translation process where decoder has to generate only final word (``Beispiel''), given original input text (``This is an example'') \& a partially translated sentence (``Das ist ein''), to complete translation.}
		
		Transformer architecture consists of 2 submodules: an encoder \& a decoder. Encoder module processes input text \& encodes it into a series of numerical representations or vectors that capture contextual information of input. Then, decoder module takes these encoded vectors \& generates output text. In a translation task, e.g., encoder would encode text from source language into vectors, \& decoder would decode these vectors to generate text in target language. Both encoder \& decoder consist of many layers connected by a so-called self-attention mechanism. You may have many questions regarding how inputs are preprocessed \& encoded. These will be addressed in a step-by-step implementation in subsequent chaps.
		
		-- Kiến trúc máy biến áp bao gồm 2 mô-đun con: 1  bộ mã hóa \& 1  bộ giải mã. Mô-đun mã hóa xử lý văn bản đầu vào \& mã hóa nó thành 1  chuỗi các biểu diễn số hoặc vectơ nắm bắt thông tin ngữ cảnh của đầu vào. Sau đó, mô-đun giải mã lấy các vectơ được mã hóa này \& tạo văn bản đầu ra. Trong 1  tác vụ dịch, ví dụ, bộ mã hóa sẽ mã hóa văn bản từ ngôn ngữ nguồn thành các vectơ, \& bộ giải mã sẽ giải mã các vectơ này để tạo văn bản ở ngôn ngữ đích. Cả bộ mã hóa \& bộ giải mã đều bao gồm nhiều lớp được kết nối bởi cái gọi là cơ chế tự chú ý. Bạn có thể có nhiều câu hỏi liên quan đến cách đầu vào được xử lý trước \& mã hóa. Những câu hỏi này sẽ được giải quyết trong phần triển khai từng bước trong các chương tiếp theo.
		
		A key component of transformers \& LLMs is self-attention mechanism (not shown), which allows model to weigh importance of different words or tokens in a sequence relative to each other. This mechanism enables model to capture long-range dependencies \& contextual relationships within input data, enhancing its ability to generate coherent \& contextually relevant output. However, due to its complexity, will defer further explanation to Chap. 3, where discuss \& implement it step \& step.
		
		Later variants of transformer architecture, e.g. BERT (short for {\it bidirectional encoder representations from transformers}) \& various GPT models (short for {\it generative pretrained transformers}), built on this concept to adapt this architecture for different tasks. If interested, refer to Appendix B for further reading suggestions.
		
		BERT, which is built upon original transformer's encoder submodule, differs in its training approach from GPT. While GPT is designed for generative tasks, BERT \& its variants specialize in masked word prediction, where model predicts masked or hidden words in a given sentence, as shown in {\sf Fig. 1.5: A visual representation of transformer's encoder \& decoder submodules. On left, encoder segment exemplifies BERT-like LLMs, which focus on masked word prediction \& are primarily used for tasks like text classification. On right, decoder segment showcases GPT-like LLMs, designed for generative tasks \& producing coherent text sequences.} This unique training strategy equips BERT with strengths in text classification tasks, including sentiment prediction \& document categorization. As an application of its capabilities, as of this writing, X (formerly Twitter) uses BERT to detect toxic content.
		
		GPT, on other hand, focuses on decoder portion of original transformer architecture \& is designed for tasks that require generating texts. This includes machine translation, text summarization, fiction writing, writing computer code, \& more.
		
		GPT models, primarily designed \& trained to perform text completion tasks, also show remarkable versatility in their capabilities. These models are adept at executing both zero-shot \& few-shot learning tasks. Zero-shot learning refers to ability to generalize to completely unseen tasks without any prior specific examples. On other hand, few-shot learning involves learning from a minimal number of examples user provides as input, as shown in {\sf Fig. 1.6: In addition to text completion, GPT-like LLMs can solve various tasks based on their inputs without needing retraining, fine-tuning, or task-specific model architecture changes. Sometimes it is helpful to provide examples of target within input, which is known as a few-shot setting. However, GPT-like LLMs are also capable of carrying out tasks without a specific example, which is called zero-shot setting.}
		
		\begin{remark}[Transformers vs. LLMs]
			Today's LLMs are based on transformer architecture. Hence, transformers \& LLMs are terms that are often used synonymously in literature. However, note: not all transformers are LLMs since transformers can also be used for computer vision. Also, not all LLMs are transformers, as there are LLMs based on recurrent \& convolutional architectures. Main motivation behind these alternative approaches: improve computational efficiency of LLMs. Whether these alternative LLM architectures can compete with capabilities of transformer-based LLMs \& whether they are going to be adopted in practice remains to be seen. For simplicity, use term ``LLM'' to refer to transformer-based LLMs similar to GPT. (Interested readers can find literature references describing these architectures in Appendix B.)
			
			-- LLM ngày nay dựa trên kiến trúc máy biến áp. Do đó, máy biến áp \& LLM là những thuật ngữ thường được sử dụng đồng nghĩa trong tài liệu. Tuy nhiên, lưu ý: không phải tất cả máy biến áp đều là LLM vì máy biến áp cũng có thể được sử dụng cho thị giác máy tính. Ngoài ra, không phải tất cả LLM đều là máy biến áp, vì có những LLM dựa trên kiến trúc hồi quy \& tích chập. Động lực chính đằng sau các phương pháp tiếp cận thay thế này: cải thiện hiệu quả tính toán của LLM. Liệu các kiến trúc LLM thay thế này có thể cạnh tranh với khả năng của LLM dựa trên máy biến áp \& liệu chúng có được áp dụng trong thực tế hay không vẫn còn phải chờ xem. Để đơn giản, hãy sử dụng thuật ngữ ``LLM'' để chỉ các LLM dựa trên máy biến áp tương tự như GPT. (Những độc giả quan tâm có thể tìm thấy tài liệu tham khảo mô tả các kiến trúc này trong Phụ lục B.)
		\end{remark}		
		\item {\sf1.5. utilizing large datasets.} Large training datasets for popular GPT- \& BERT-like models represent diverse \& comprehensive text corpora encompassing billions of words, which include a vast array of topics \& natural \& computer languages. To provide a concrete example, {\sf Table 1.1: Pretraining dataset of popular GPT-3 LLM.} summarizes dataset used for pretraining GPT-3, which served as base model for 1st version of ChatGPT. Table 1.1 reports number of tokens, where a token is a unit of text that a model reads \& number of tokens in a dataset is roughly equivalent to number of words \& punctuation characters in text. Chap. 2 addresses tokenization, process of converting text into tokens.
		
		Main takeaways: scale \& diversity of this training dataset allow these models to perform well on diverse tasks, including language syntax, semantics, \& context -- even some requiring general knowledge.
		\begin{remark}[GPT-3 dataset details]
			Table 1.1 displays dataset used for GPT-3. Proportions column in table sums up to 100\% of sampled data, adjusted for rounding errors. Although subsets in Number of Tokens column total 499 billion, model was trained on only 300 billion tokens. Authors of GPT-3 paper did not specify why model was not trained on all 499 billion tokens.
			
			For context, consider size of CommonCrawl dataset, which alone consists of 410 billion tokens \& requires about 570 GB of storage. In comparison, later iterations of models like GPT-3, e.g. Meta's LLaMA, have expanded their training scope to include additional data source like Arxiv research papers (92 GB) \& StackExchange's code-related Q\&As (78 GB).
			
			Authors of GPT-3 paper did not share training dataset, but a comparable dataset that is publicly available is {\it Dolma: An Open Corpus of 3 Trillion Tokens for LLM Pretraining Research} by Soldaini et al. 2024 (\url{https://arxiv.org/abs/2402.00159}). However, collection may contain copyrighted works, \& exact usage terms may depend on intended use case \& country.
		\end{remark}
		Pretrained nature of these models makes them incredibly versatile for further fine-tuning on downstream tasks, which is why they are also known as base or foundation models. Pretraining LLMs requires access to significant resources \& is very expensive. E.g., GPT-3 pretraining cost is estimated to be \$4.6 million in terms of cloud computing credits \url{https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/}.
		
		Good news: many pretrained LLMs, available as open source models, can be used as general-purpose tools to write, extract, \& edit texts that were not part of training data. Also, LLMs can be fine-tuned on specific tasks with relatively smaller datasets, reducing computational resources needed \& improving performance.
		
		Will implement code for pretraining \& use it to pretrain an LLM for educational purposes. All computations are executable on consumer hardware. After implementing pretraining code, will learn how to reuse openly available model weights \& load them into architecture we will implement, allowing us to skip expensive pretraining stage when fine-tune our LLM.
		\item {\sf1.6. A closer look at GPT architecture.} GPT was originally introduced in paper ``Improving Language Understanding by Generative Pre-Training'' \url{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf} by by Radford et al. from OpenAI. GPT-3 is a scaled-up version of this model that has more parameters \& was trained on a larger dataset. In addition, original model offered in ChatGPT was created by fine-tuning GPT-3 on a large instruction dataset using a method from OpenAI's InstructGPT paper \url{https://arxiv.org/pdf/2203.02155}. As Fig. 1.6 shows, these models are competent text completion models \& can carry out other tasks e.g. spelling correction, classification, or language translation. This is actually very remarkable given that GPT models are pretrained on a relatively simple next-word prediction task, as depicted in {\sf Fig. 1.7: In next-word prediction pretraining task for GPT models, system learns to predict upcoming word in a sentence by looking at words that have come before it. This approach helps model understand how words \& phrases typically fit together in language, forming a foundation that can be applied to various other tasks.}
		
		Next-word prediction task is a form of self-supervised learning, which is a form of self-labeling. I.e. we don't need to collect labels for training data explicitly but can use structure of data itself: can use next word in a sentence or document as label that model is supposed to predict. Since this next-word prediction task allows us to create labels ``on the fly'', possible to use massive unlabeled text datasets to train LLMs.
		
		-- Nhiệm vụ dự đoán từ tiếp theo là 1  dạng học tự giám sát, là 1  dạng tự dán nhãn. Tức là chúng ta không cần phải thu thập nhãn cho dữ liệu đào tạo 1  cách rõ ràng nhưng có thể sử dụng cấu trúc của chính dữ liệu: có thể sử dụng từ tiếp theo trong 1  câu hoặc tài liệu làm nhãn mà mô hình được cho là dự đoán. Vì nhiệm vụ dự đoán từ tiếp theo này cho phép chúng ta tạo nhãn ``ngay lập tức'', có thể sử dụng các tập dữ liệu văn bản không có nhãn lớn để đào tạo LLM.
		
		Compared to original transformer architecture covered in Sect. 1.4, general GPT architecture is relatively simple. Essentially, just decoder part without encoder ({\sf Fig. 1.8: GPT architecture employs only decoder portion of original transformer. It is designed for unidirectional, left-to-right processing, making it well suited for text generation \& next-word prediction tasks to generate text in an iterative fashion, 1 word at a time.}). Since decoder-style models like GPT generate text by predicting text 1 word at a time, they are considered a type of {\it autoregressive} model. Autoregressive models incorporate their previous outputs as inputs for future predictions. Consequently, in GPT, each new word is chosen based on sequence that precedes it, which improves coherence of resulting text.
		
		-- So với kiến trúc biến áp ban đầu được đề cập trong Phần 1.4, kiến trúc GPT nói chung tương đối đơn giản. Về cơ bản, chỉ có phần giải mã mà không có phần mã hóa ({\sf Hình 1.8: Kiến trúc GPT chỉ sử dụng phần giải mã của biến áp ban đầu. Nó được thiết kế để xử lý một chiều, từ trái sang phải, khiến nó rất phù hợp cho các tác vụ tạo văn bản \& dự đoán từ tiếp theo để tạo văn bản theo cách lặp lại, từng từ một.}). Vì các mô hình kiểu giải mã như GPT tạo văn bản bằng cách dự đoán văn bản từng từ một, nên chúng được coi là một loại mô hình {\it tự hồi quy}. Các mô hình tự hồi quy kết hợp các đầu ra trước đó của chúng làm đầu vào cho các dự đoán trong tương lai. Do đó, trong GPT, mỗi từ mới được chọn dựa trên trình tự trước đó, giúp cải thiện tính mạch lạc của văn bản kết quả.
		
		Architectures e.g. GPT-3 are also significantly larger than original transformer model. E.g., original transformer repeated encoder \& decoder blocks 6 times. GPT-3 has 96 transformer layers \& 175 billion parameters in total.
		
		GPT-3 was introduced in 2020, which, by standards of DL \& LLM development, is considered a long time ago. However, more recent architectures, e.g. Meta's Llama models, are still based on same underlying concepts, introducing only minor modifications. Hence, understanding GPT remains as relevant as ever, so I focus on implementing prominent architecture behind GPT while providing pointers to specific tweaks employed by alternative LLMs.
		
		Although original transformer model, consisting of encoder \& decoder blocks, was explicitly designed for language translation, GPT models -- despite their larger yet simpler decoder-only architecture aimed at next-word prediction -- are also capable of performing translation tasks. This capability was initially unexpected to researchers, as it emerged from a model primarily trained on a next-word prediction task, which is a task that did not specifically target translation.
		
		Ability to perform tasks that model wasn't explicitly trained to perform is called an {\it emergent behavior}. This capability isn't explicitly taught during training but emerges as a natural consequence of model's exposure to vast quantities of multilingual data in diverse contexts. Fact GPT models can ``learn'' translation patterns between languages \& perform translation tasks even though they weren't specifically trained for it demonstrates benefits \& capabilities of these large-scale, generative language models. Can perform diverse tasks without using diverse models for each.
		\item {\sf1.7. Building a LLM.} Now we've laid groundwork for understanding LLMs, code 1 from scratch. Take fundamental idea behind GPT as a blueprint \& tackle this in 3 stages, as outlined in {\sf Fig. 1.9: 3 main stages of coding an LLM are implementing LLM architecture \& data preparation process (stage 1), pretraining an LLM to create a foundation model (stage 2), \& fine-tuning foundation model to become a personal assistant or text classifier (stage 3).} In stage 1, learn about fundamental data preprocessing steps \& code attention mechanism at heart of every LLM. Next, in stage 2, learn how to code \& pretrain a GPT-like LLM capable of generating new texts. Also go over fundamentals of evaluating LLMs, which is essential for developing capable NLP systems.
		
		Pretraining an LLM from scratch is a significant endeavor, demanding thousands to millions of dollars in computing costs for GPT-like models. Therefore, focus of stage 2 is on implementing training for educational purposes using a small dataset. In addition, also provide code examples for loading openly available model weights.
		
		Finally, in stage 3, take a pretrained LLM \& fine-tune it to follow instructions e.g. answering queries or classifying texts -- most common tasks in many real-world applications \& research.
		\item {\sf Summary.}
		\begin{itemize}
			\item LLMs have transformed field of NLP, which previously mostly relied on explicit rule-based systems \& simpler statistical methods. Advent of LLMs introduced new DL-driven approaches that led to advancements in understanding, generating, \& translating human language.
			\item Modern LLMs are trained in 2 main steps:
			\begin{itemize}
				\item 1st, they are pretrained on a large corpus of unlabeled text by using prediction of next word in a sentence as a label.
				\item Then, they are fine-tuned on a smaller, labeled target dataset to follow instructions or perform classification tasks.
			\end{itemize}
			\item LLMs are based on transformer architecture. Key idea of transformer architecture is an attention mechanism that gives LLM selective access to whole input sentence when generating output 1 word at a time.
			\item Original transformer architecture consists of an encoder for parsing text \& a decoder for generating text.
			\item LLms for generating text \& following instructions, e.g. GPT-3 \& ChatGPT, only implement decoder modules, simplifying architecture.
			\item Large datasets consisting of billions of words are essential for pretraining LLMs.
			\item While general pretraining task for GPT-like models is to predict next word in a sentence, these LLMs exhibit emergent properties, e.g. capabilities to classify, translate, or summarize texts.
			\item Once an LLM is pretrained, resulting foundation model can be fine-tuned more efficiently for various downstream tasks.
			\item LLMs fine-tuned on custom datasets can outperform general LLMs on specific tasks.
		\end{itemize}
	\end{itemize}
	\item {\sf2. Working with text data.}
	\begin{itemize}
		\item {\sf2.1. Understanding word embeddings.}
		\item {\sf2.2. Tokenizing text.}
		\item {\sf2.3. Converting tokens into token IDs.}
		\item {\sf2.4. Adding special context tokens.}
		\item {\sf2.5. Byte pair encoding.}
		\item {\sf2.6. Data sampling with a sliding window.}
		\item {\sf2.7. Creating token embeddings.}
		\item {\sf2.8. Encoding word positions.}
	\end{itemize}
	\item {\sf3. Coding attention mechanisms.}
	\begin{itemize}
		\item {\sf3.1. Problem with modeling long sequences.}
		\item {\sf3.2. Capturing data dependencies with attention mechanisms.}
		\item {\sf3.3. Attending to different parts of input with self-attention.}
		\begin{itemize}
			\item {\sf A simple self-attention mechanism without trainable weights.}
			\item {\sf Computing attention weights for all input tokens.}
		\end{itemize}
		\item {\sf3.4. Implementing self-attention with trainable weights.}
		\begin{itemize}
			\item {\sf Computing attention weights step by step.}
			\item {\sf Implementing a compact self-attention Python class.}
		\end{itemize}
		\item {\sf3.5. Hiding future words with causal attention.}
		\begin{itemize}
			\item {\sf Applying a causal attention mask.}
			\item {\sf Masking additional attention weights with dropout.}
			\item {\sf Implementing a compact causal attention class.}
		\end{itemize}
		\item {\sf3.6. Extending single-head attention to multi-head.}
		\begin{itemize}
			\item {\sf Stacking multiple single-head attention layers.}
			\item {\sf Implementing multi-head attention with weight splits.}
		\end{itemize}
	\end{itemize}
	\item {\sf4. Implementing a GPT model from scratch to generate text.}
	\begin{itemize}
		\item {\sf4.1. Coding an LLM architecture.}
		\item {\sf4.2. Normalizing activations with layer normalization.}
		\item {\sf4.3. Implementing a feed forward network with GELU activations.}
		\item {\sf4.4. Adding shortcut connections.}
		\item {\sf4.5. Connecting attention \& linear layers in a transformer block.}
		\item {\sf4.6. Coding GPT model.}
		\item {\sf4.7. Generating text.}
	\end{itemize}
	\item {\sf5. Pretraining on unlabeled data.}
	\begin{itemize}
		\item {\sf5.1. Evaluating generative text models.}
		\begin{itemize}
			\item {\sf Using GPT to generate text.}
			\item {\sf Calculating text generation loss.}
			\item {\sf Calculating training \& validation set losses.}
		\end{itemize}
		\item {\sf5.2. Training an LLM.}
		\item {\sf5.3. Decoding strategies to control randomness.}
		\begin{itemize}
			\item {\sf Temperature scaling.}
			\item {\sf Top-$k$ sampling.}
			\item {\sf Modifying text generation function.}
		\end{itemize}
		\item {\sf5.4. Loading \& saving model weights in PyTorch.}
		\item {\sf5.5. Loading pretrained weights from OpenAI.}
	\end{itemize}
	\item {\sf6. Fine-tuning for classification.}
	\begin{itemize}
		\item {\sf6.1. Different categories of fine-tuning.}
		\item {\sf6.2. Preparing dataset.}
		\item {\sf6.3. Creating data loaders.}
		\item {\sf6.4. Initializing a model with pretrained weights.}
		\item {\sf6.5. Adding a classification head.}
		\item {\sf6.6. Calculating classification loss \& accuracy.}
		\item {\sf6.7. Fine-tuning model on supervised data.}
		\item {\sf6.8. Using LLM as a spam classifier.}
	\end{itemize}
	\item {\sf7. Fine-tuning to follow instructions.}
	\begin{itemize}
		\item {\sf7.1. Introduction to instruction fine-tuning.}
		\item {\sf7.2. Preparing a dataset for supervised instruction fine-tuning.}
		\item {\sf7.3. Organizing data into training batches.}
		\item {\sf7.4. Creating data loaders for an instruction dataset.}
		\item {\sf7.5. Loading a pretrained LLM.}
		\item {\sf7.6. Fine-tuning LLM on instruction data.}
		\item {\sf7.7. Extracting \& saving responses.}
		\item {\sf7.8. Evaluating fine-tuned LLM.}
		\item {\sf7.9. Conclusions.}
		\begin{itemize}
			\item {\sf What's next?}
			\item {\sf Staying up to data in a fast-moving field.}
			\item {\sf Final words.}
		\end{itemize}
	\end{itemize}
	\item {\sf A. Introduction to PyTorch.}
	\item {\sf B. References \& further reading.}
	\item {\sf C. Exercise solutions.}
	\item {\sf D. Adding bells \& whistles to training loop.}
	\item {\sf E. Parameter-efficient fine-tuning with LoRA.}
\end{itemize}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}