\documentclass[oneside]{book}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,fancyvrb,float,graphicx,mathtools,minitoc,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\addtolength{\headheight}{0pt}% obsolete
\lhead{\scshape\small\chaptername~\thechapter}
\rhead{\small\nouppercase{\leftmark}}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}
\fancyheadoffset[RE,LO]{-0.0\textwidth}

\usepackage{textcase}

\makeatletter
\def\@makechapterhead#1{%
    \vspace*{50\p@}%
    {\parindent \z@ \centering\normalfont
        \ifnum \c@secnumdepth >\m@ne
        \if@mainmatter
        \huge\bfseries \MakeTextUppercase{\@chapapp}\space \thechapter
        \par\nobreak
        \vskip 20\p@
        \fi
        \fi
        \interlinepenalty\@M
        \huge \bfseries \MakeTextUppercase{#1}\par\nobreak
        \vskip 40\p@
}}
\def\@makeschapterhead#1{%
    \vspace*{50\p@}%
    {\parindent \z@ \centering
        \normalfont
        \interlinepenalty\@M
        \huge \bfseries  \MakeTextUppercase{#1}\par\nobreak
        \vskip 40\p@
}}
\makeatother


\DeclareMathSymbol{\mathinvertedexclamationmark}{\mathclose}{operators}{'074}
\DeclareMathSymbol{\mathexclamationmark}{\mathclose}{operators}{'041}

\makeatletter
\newcommand{\raisedmathinvertedexclamationmark}{%
    \mathclose{\mathpalette\raised@mathinvertedexclamationmark\relax}%
}
\newcommand{\raised@mathinvertedexclamationmark}[2]{%
    \raisebox{\depth}{$\m@th#1\mathinvertedexclamationmark$}%
}
\begingroup\lccode`~=`! \lowercase{\endgroup
    \def~}{\@ifnextchar`{\raisedmathinvertedexclamationmark\@gobble}{\mathexclamationmark}}
\mathcode`!="8000
\makeatother

\usepackage{sectsty}
\allsectionsfont{\sffamily}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{Bài toán}
\newtheorem{bode}{Bổ đề}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{goal}{Goal}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{intuition}{Intuition}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=1.5cm,bottom=1.5cm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
    \mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}
\newcommand{\genstirlingI}[3]{%
    \genfrac{[}{]}{0pt}{#1}{#2}{#3}%
}
\newcommand{\genstirlingII}[3]{%
    \genfrac{\{}{\}}{0pt}{#1}{#2}{#3}%
}
\newcommand{\stirlingI}[2]{\genstirlingI{}{#1}{#2}}
\newcommand{\dstirlingI}[2]{\genstirlingI{0}{#1}{#2}}
\newcommand{\tstirlingI}[2]{\genstirlingI{1}{#1}{#2}}
\newcommand{\stirlingII}[2]{\genstirlingII{}{#1}{#2}}
\newcommand{\dstirlingII}[2]{\genstirlingII{0}{#1}{#2}}
\newcommand{\tstirlingII}[2]{\genstirlingII{1}{#1}{#2}}
\def\multiset#1#2{\ensuremath{\left(\kern-.3em\left(\genfrac{}{}{0pt}{}{#1}{#2}\right)\kern-.3em\right)}}

\title{Lecture Note: Graph Neural Networks\\Bài Giảng: Mạng Nơron Đồ Thị}
\author{Nguyễn Quản Bá Hồng\footnote{A scientist- {\it\&} creative artist wannabe, a mathematics {\it\&} computer science lecturer of Department of Artificial Intelligence {\it\&} Data Science (AIDS), School of Technology (SOT), UMT Trường Đại học Quản lý {\it\&} Công nghệ TP.HCM, Hồ Chí Minh City, Việt Nam.\\E-mail: {\sf nguyenquanbahong@gmail.com} {\it\&} {\sf hong.nguyenquanba@umt.edu.vn}. Website: \url{https://nqbh.github.io/}. GitHub: \url{https://github.com/NQBH}.}}
\date{\today}

\begin{document}
\maketitle
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\dominitoc % Initialization
\tableofcontents

%------------------------------------------------------------------------------%

\chapter*{Preface}

\section*{Abstract}
This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:

{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.

Latest version:
\begin{itemize}
    \item {\it Lecture Note: Graph Neural Networks\\Bài Giảng: Mạng Nơron Đồ Thị}.

    PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/lecture/NQBH_combinatorics_graph_theory_lecture.pdf}.

    \TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/lecture/NQBH_combinatorics_graph_theory_lecture.tex}.
    \item {\it Slide: Combinatorics \& Graph Theory -- Slide Bài Giảng: Tổ Hợp \& Lý Thuyết Đồ Thị}.

    PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/slide/NQBH_combinatorics_graph_theory_slide.pdf}.

    \TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/slide/NQBH_combinatorics_graph_theory_slide.tex}.
    \item {\it Survey: Combinatorics \& Graph Theory -- Khảo Sát: Tổ Hợp \& Lý Thuyết Đồ Thị}.

    PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/NQBH_combinatorics.pdf}.

    \TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/NQBH_combinatorics.tex}.
    \item Codes:
    \begin{itemize}
        \item C{\tt/}C++: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/C++}.
        \item Pascal: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/Pascal}.
        \item Python: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/combinatorics/Python}.
    \end{itemize}
\end{itemize}
Tài liệu này là bài giảng tôi dạy cho sinh viên Khoa Công Nghệ (undegraduate Computer Science students) chuyên ngành Kỹ Thuật Phần Mềm (Software Engineering, abbr., SE) \& Trí Tuệ Nhân Tạo--Khoa Học Dữ Liệu (Artificial Intelligence--Data Science, abbr., AIDS) nên sẽ tập trung vào phương diện lập trình cho các khái niệm Tổ hợp \& Lý thuyết đồ thị được nghiên cứu. Bài giảng này gồm 2 phần chính:
\begin{itemize}
    \item {\bf Part I: Combinatorics -- Tổ Hợp}.
    \item {\bf Part II: Graph Theory -- Lý Thuyết Đồ Thị}. Tập trung vào các thuật toán trên cây (algorithms on trees) \& thuật toán trên đồ thị (algorithms on graphs)
\end{itemize}

%------------------------------------------------------------------------------%

\chapter{Introduction to Graph Neural Networks -- Nhập Môn Mạng Nơron Đồ Thị}
\minitoc

%------------------------------------------------------------------------------%

\section{Some Basic Concepts -- Vài Khái Niệm Cơ Bản}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
    \item \href{https://www.geeksforgeeks.org/deep-learning/what-are-graph-neural-networks/}{Geeks4Geeks{\tt/}what are graph neural networks?}.

    \item \href{https://en.wikipedia.org/wiki/Graph_neural_network}{Wikipedia{\tt}graph neural network}.
\end{enumerate}
We explore what GNNs are, how they work, their types, \& their wide range of applications.

Graph Neural Networks (GNNs) are a neural network specifically designed to work with data represented as graphs. Unlike traditional neural networks, which operate on grid-like data structures like images (2D grids) or text (sequential), GNNs can model complex, non-Euclidean relationships in data, e.g. social networks, molecular structures, \& knowledge graphs.

-- Mạng nơ-ron đồ thị (GNN) là một mạng nơ-ron được thiết kế đặc biệt để hoạt động với dữ liệu được biểu diễn dưới dạng đồ thị. Không giống như các mạng nơ-ron truyền thống, hoạt động trên các cấu trúc dữ liệu dạng lưới như hình ảnh (lưới 2D) hoặc văn bản (tuần tự), GNN có thể mô hình hóa các mối quan hệ phức tạp, phi Euclid trong dữ liệu, ví dụ như mạng xã hội, cấu trúc phân tử, \& đồ thị tri thức.

%------------------------------------------------------------------------------%

\subsection{Understanding Graph Structures -- Hiểu Về Cấu Trúc Đồ Thị}
A graph is a data structure that represents relationships between pairs of objects. Each object is known as a {\it node} (or {\it vertex}), \& each connection between nodes is an {\it edge.} Graphs can vary based on the type of edges:
\begin{enumerate}
    \item \href{https://www.geeksforgeeks.org/dsa/what-is-directed-graph-directed-graph-meaning/}{Directed graphs}: In these graphs, edges have a direction, indicated by an arrow, showing a 1-way relationship from 1 node to another.
    \item \href{https://www.geeksforgeeks.org/dsa/what-is-unidrected-graph-undirected-graph-meaning/}{Undirected graphs}: In these graphs, edges are bidirectical, representing 2-way relationships without any direction.
\end{enumerate}
-- Đồ thị là một cấu trúc dữ liệu biểu diễn mối quan hệ giữa các cặp đối tượng. Mỗi đối tượng được gọi là một {\it node} (hoặc {\it vertex}), và mỗi kết nối giữa các nút là một {\it edge}. Đồ thị có thể thay đổi tùy theo loại cạnh:
\begin{enumerate}
    \item \href{https://www.geeksforgeeks.org/dsa/what-is-directed-graph-directed-graph-meaning/}{Đồ thị có hướng}: Trong các đồ thị này, các cạnh có hướng, được biểu thị bằng mũi tên, thể hiện mối quan hệ một chiều từ nút này đến nút khác.
    \item \href{https://www.geeksforgeeks.org/dsa/what-is-unidrected-graph-undirected-graph-meaning/}{Đồ thị vô hướng}: Trong các đồ thị này, các cạnh là song hướng, biểu diễn mối quan hệ hai chiều không có hướng.
\end{enumerate}
Graphs can be classified based on their structural properties: {\tt Type of graph | Description | Example{\tt/}Use case}
\begin{enumerate}
    \item {\it Homogeneous graph} | contains only 1 type of node \& edge, simplifying representation \& analysis. | Social network with people as nodes \& friendships as edges.
    \item {\it Heterogeneous graph} | consists of multiple types of nodes \& edges, suitable for complex relationships | Knowledge graph with nodes for entities (people, places) \& varied edges.
    \item {\it Static graph} | remains fixed once started, with no changes to nodes or edges | Scenarios where relationships are constant over time.
    \item {\it Dynamic graph} | allows the addition or deletion of nodes \& edges, ideal for situations with frequently changing relationships. | Real-time communication tracking or evolving social networks.
\end{enumerate}



%------------------------------------------------------------------------------%

{\it Graph neural networks (GNN)} are specialized artificial neural networks (ANNs) that are designed for tasks whose inputs are graphs.

-- Mạng nơ-ron đồ thị (GNN) là mạng nơ-ron nhân tạo chuyên biệt (ANN) được thiết kế cho các tác vụ có đầu vào là đồ thị.

1 prominent example is molecular drug design. Each input sample is a graph representation of a molecule, where atoms form the nodes \& chemical bonds between atoms form the edges. In addition to the graph representation, the input also includes known chemical properties for each of the atoms. Dataset samples may thus differ in length, reflecting the varying numbers of atoms in molecules, \& the varying number of bonds between them. The task is to predict the efficacy of a given molecule for a specific medical application, like eliminating E. coli bacteria.

-- 1 ví dụ nổi bật là thiết kế thuốc phân tử. Mỗi mẫu đầu vào là một biểu diễn đồ thị của một phân tử, trong đó các nguyên tử tạo thành các nút \& các liên kết hóa học giữa các nguyên tử tạo thành các cạnh. Ngoài biểu diễn đồ thị, dữ liệu đầu vào còn bao gồm các đặc tính hóa học đã biết của từng nguyên tử. Do đó, các mẫu tập dữ liệu có thể khác nhau về độ dài, phản ánh số lượng nguyên tử khác nhau trong phân tử, \& số lượng liên kết khác nhau giữa chúng. Nhiệm vụ là dự đoán hiệu quả của một phân tử nhất định cho một ứng dụng y tế cụ thể, chẳng hạn như loại bỏ vi khuẩn E. coli.

The key design element of GNNs is the use of {\it pairwise message passing}, s.t. graph nodes iteratively update their representations by exchanging information with their neighbors. Several GNN architectures have been proposed, which implement different flavors of message passing, started by recursive or convolutional constructive approaches. As of 2022, it is an open question whether it is possible to define GNN architectures ``going beyond'' message passing, or instead every GNN can be built on message passing over suitably defined graphs.

-- Yếu tố thiết kế chính của GNN là việc sử dụng {\it pairwise message passing}, tức là các nút đồ thị cập nhật biểu diễn của chúng một cách lặp đi lặp lại bằng cách trao đổi thông tin với các nút lân cận. Một số kiến trúc GNN đã được đề xuất, triển khai các phương pháp truyền thông điệp khác nhau, bắt đầu bằng các phương pháp xây dựng đệ quy hoặc tích chập. Tính đến năm 2022, vẫn còn là một câu hỏi bỏ ngỏ liệu có thể định nghĩa kiến trúc GNN ``vượt ra ngoài'' việc truyền thông điệp hay không, hay thay vào đó, mọi GNN đều có thể được xây dựng dựa trên việc truyền thông điệp trên các đồ thị được xác định phù hợp.

In the more general subject of ``geometric DL'', certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A \href{https://en.wikipedia.org/wiki/Convolutional_neural_network}{convolutional neural network} layer, in the context of computer vision, can be considered a GNN applied to graphs whose nodes are \href{https://en.wikipedia.org/wiki/Pixel}{pixels} \& only adjacent pixels are connected by edges in the graph. A \href{https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)}{transformer} layer, in \href{https://en.wikipedia.org/wiki/Natural_language_processing}{natural language processing}, can be considered a GNN applied to \href{https://en.wikipedia.org/wiki/Complete_graph}{complete graphs} whose nodes are words or tokens in a passing of natural language text.

-- Trong lĩnh vực tổng quát hơn của ``học máy hình học'', một số kiến trúc mạng nơ-ron hiện có có thể được hiểu là mạng nơ-ron tích chập (GNN) hoạt động trên các đồ thị được định nghĩa phù hợp. Trong bối cảnh thị giác máy tính, một lớp mạng nơ-ron tích chập có thể được coi là một GNN áp dụng cho các đồ thị có các nút là \href{https://en.wikipedia.org/wiki/Pixel}{pixels} \& chỉ các điểm ảnh liền kề được kết nối bằng các cạnh trong đồ thị. Trong xử lý ngôn ngữ tự nhiên, một lớp biến đổi có thể được coi là một GNN áp dụng cho các đồ thị hoàn chỉnh có các nút là các từ hoặc token trong quá trình truyền văn bản ngôn ngữ tự nhiên.

Relevant application domains for GNNs include natural language processing, social networks, citation networks, molecular biology, chemistry, physics, \& NP-hard combinatorial optimization problem.

-- Các lĩnh vực ứng dụng có liên quan cho GNN bao gồm xử lý ngôn ngữ tự nhiên, mạng xã hội, mạng trích dẫn, sinh học phân tử, hóa học, vật lý và bài toán tối ưu hóa tổ hợp NP-hard.

Open source libraries implementing GNNs include PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), Deep Graph Library (framework agnostic), jraph (Google JAX), \& {\tt GraphNeuralNetworks.jl/GeometricFlux.jl} (Julia, Flux).

%------------------------------------------------------------------------------%

\section{Architecture of GNNs -- Kiến Trúc của GNNs}
The architecture of a generic GNN implements the following fundamental layers:
\begin{enumerate}
    \item {\it Permutation equivariant}: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature, permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively, in a message passing layer, nodes {\it update} their representations by {\it aggregating} the {\it messages} received from their immediate neighbors. As such, each message passing layer increases the receptive field of the GNN by 1 hop.
    \item {\it Local pooling}: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN, in a similar fashion to pooling layers in CNNs. Examples include k-nearest neighbors pooling, top-k pooling, \& self-attention pooling.
    \item {\it Global pooling}: a global pooling layer, also known as {\it readout} layer, provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant, s.t. permutations in the ordering of graph nodes \& edges do not alter the final output. Examples include element-wise sum, mean or maximum.
\end{enumerate}
-- Kiến trúc của một GNN chung triển khai các lớp cơ bản sau:
\begin{enumerate}
    \item {\it Permutation equivariant}: một lớp permutation equivariant ánh xạ một biểu diễn của một đồ thị thành một biểu diễn được cập nhật của cùng một đồ thị. Trong các tài liệu, các lớp permutation equivariant được triển khai thông qua việc truyền thông điệp từng cặp giữa các nút đồ thị. Theo trực giác, trong một lớp truyền thông điệp, các nút {\it cập nhật} biểu diễn của chúng bằng cách {\it tổng hợp} các {\it thông điệp} nhận được từ các nút lân cận trực tiếp của chúng. Như vậy, mỗi lớp truyền thông điệp sẽ tăng trường tiếp nhận của GNN thêm 1 bước nhảy.

    \item {\it Local pooling}: một lớp local pooling làm đồ thị thô hơn thông qua việc giảm mẫu. Local pooling được sử dụng để tăng trường tiếp nhận của GNN, tương tự như các lớp pooling trong CNN. Các ví dụ bao gồm pooling k-nearest neighbors, pooling top-k, và pooling \& self-attention.

    \item {\it Tổng hợp toàn cục}: một lớp tổng hợp toàn cục, còn được gọi là lớp {\it readout}, cung cấp biểu diễn kích thước cố định cho toàn bộ đồ thị. Lớp tổng hợp toàn cục phải bất biến hoán vị, tức là các hoán vị trong thứ tự các nút \& cạnh của đồ thị không làm thay đổi kết quả đầu ra cuối cùng. Ví dụ bao gồm tổng từng phần tử, trung bình hoặc cực đại.
\end{enumerate}
It has been demonstrated that GNNs cannot be more expensive than the \href{https://en.wikipedia.org/wiki/Weisfeiler_Leman_graph_isomorphism_test}{Weisfeiler--Leman Graph Isomorphism Test}. In practice, this means that there exist different graph structures (e.g., molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries e.g. simplicial complexes can be designed. As of 2022, whether or not future architectures will overcome the message passing primitive is an open research question.

-- Người ta đã chứng minh rằng GNN không thể đắt hơn Phép thử Đồng cấu Đồ thị Weisfeiler-Leman. Trên thực tế, điều này có nghĩa là tồn tại các cấu trúc đồ thị khác nhau (ví dụ: các phân tử có cùng nguyên tử nhưng liên kết khác nhau) mà GNN không thể phân biệt được. Có thể thiết kế các GNN mạnh hơn hoạt động trên các hình học chiều cao hơn, ví dụ như các phức hợp đơn giản. Tính đến năm 2022, liệu các kiến trúc trong tương lai có vượt qua được nguyên thủy truyền tin hay không vẫn là một câu hỏi nghiên cứu chưa có lời giải.

%------------------------------------------------------------------------------%

\section{Message Passing Layers -- Các Lớp Truyền Tin Nhắn}
Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally, they can be expressed as message passing neural networks (MPNNs).

-- Các lớp truyền thông điệp là các lớp hoán vị tương đương biến thể, ánh xạ một đồ thị thành một biểu diễn được cập nhật của cùng đồ thị đó. Về mặt hình thức, chúng có thể được biểu diễn dưới dạng mạng nơ-ron truyền thông điệp (MPNN).

\begin{definition}[MPNN layer]
    Let $G = (V,E)$ be a graph, where $V$ is the node set \& $E$ is the edge set. Let $N_u$ be the neighborhood of some node $u\in V$. Additionally, let ${\bf x}_u$ be the features of node $u\in V$, \& ${\bf e}_{uv}$ be the features of edge $(u,v)\in E$. An MPNN layer can be expressed as follows:
    \begin{equation*}
        {\bf h}_u = \phi\left({\bf x}_u,\bigoplus_{v\in N_u} \psi({\bf x}_u,{\bf x}_v,{\bf e}_{uv})\right),
    \end{equation*}
    where $\phi,\psi$ are differentiable functions (e.g., artificial neural networks), \& $\bigoplus$ is a permutation invariant \href{https://en.wikipedia.org/wiki/Aggregation_operator}{aggregation operator} that can accept an arbitrary number of inputs (e.g., element-wise sum, mean, or max). In particular, $\phi,\psi$ are referred to as {\rm update} \& {\rm message} functions, respectively.
\end{definition}

\begin{dinhnghia}[Lớp mạng nơ-ron truyền tin]
    Cho $G = (V,E)$ là một đồ thị, trong đó $V$ là tập nút \& $E$ là tập cạnh. Cho $N_u$ là lân cận của một nút $u\in V$. Ngoài ra, cho ${\bf x}_u$ là các đặc trưng của nút $u\in V$, \& ${\bf e}_{uv}$ là các đặc trưng của cạnh $(u,v)\in E$. Một lớp MPNN có thể được biểu diễn như sau:
    \begin{equation*}
        {\bf h}_u = \phi\left({\bf x}_u,\bigoplus_{v\in N_u} \psi({\bf x}_u,{\bf x}_v,{\bf e}_{uv})\right),
    \end{equation*}
    trong đó $\phi,\psi$ là các hàm khả vi (ví dụ: mạng nơ-ron nhân tạo), \& $\bigoplus$ là một toán tử tổng hợp bất biến hoán vị có thể chấp nhận một số lượng đầu vào tùy ý (ví dụ: tổng từng phần tử, trung bình hoặc cực đại). Cụ thể, $\phi,\psi$ được gọi tương ứng là các hàm {\rm cập nhật} \& {\rm tin nhắn}.
\end{dinhnghia}
Intuitively, in an MPNN computational block, graph nodes {\it update} their representations by {\it aggregating} the {\it messages} received from their neighbors.

-- Theo trực giác, trong khối tính toán MPNN, các nút đồ thị {\it cập nhật} biểu diễn của chúng bằng cách {\it tổng hợp} các {\it thông điệp} nhận được từ các nút lân cận của chúng.

The outputs of 1 or more MPNN layers are node representations ${\bf h}_u$ for each node $u\in V$ in the graph. Node representations can be employed for any downstream task, e.g. node{\tt/}graph classification or edge prediction.

-- Đầu ra của 1 hoặc nhiều lớp MPNN là biểu diễn nút ${\bf h}_u$ cho mỗi nút $u\in V$ trong đồ thị. Biểu diễn nút có thể được sử dụng cho bất kỳ tác vụ hạ nguồn nào, ví dụ: phân loại nút{\tt/}đồ thị hoặc dự đoán cạnh.

Graph nodes in an MPNN update their representation aggregation information from their immediate neighbors. As such, stacking $n$ MPNN layers means that 1 node will be able to communicate with nodes that are at most $n$ ``hops'' away. In principle, to ensure that every node receives information from every other node, one would need to stack a number of MPNN layers equal to the graph diameter. However, stacking many MPNN layers may cause issues e.g. oversmoothing \& oversquashing. Oversmoothing refers to issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures e.g. skip connections (as in residual neural networks), gated updated rules \& jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer, i.e., by considering the graph as a complete graph, can mitigate oversquashing in problems where long-range dependencies are required.

-- Các nút đồ thị trong MPNN cập nhật thông tin tổng hợp biểu diễn của chúng từ các nút lân cận trực tiếp. Do đó, việc xếp chồng $n$ lớp MPNN có nghĩa là 1 nút sẽ có thể giao tiếp với các nút cách xa tối đa $n$ ``bước nhảy''. Về nguyên tắc, để đảm bảo rằng mọi nút đều nhận được thông tin từ mọi nút khác, người ta cần xếp chồng một số lớp MPNN bằng với đường kính đồ thị. Tuy nhiên, việc xếp chồng nhiều lớp MPNN có thể gây ra các vấn đề, ví dụ như làm mịn quá mức \& đè bẹp quá mức. Làm mịn quá mức đề cập đến vấn đề biểu diễn nút trở nên không thể phân biệt được. Đè bẹp quá mức đề cập đến nút thắt cổ chai được tạo ra bằng cách nén các phụ thuộc tầm xa vào các biểu diễn có kích thước cố định. Các biện pháp đối phó, ví dụ như kết nối bỏ qua (như trong mạng nơ-ron dư), các quy tắc cập nhật có cổng \& kiến thức nhảy có thể giảm thiểu việc làm mịn quá mức. Việc sửa đổi lớp cuối cùng thành một lớp hoàn toàn liền kề, tức là bằng cách coi đồ thị là một đồ thị hoàn chỉnh, có thể giảm thiểu việc đè bẹp quá mức trong các vấn đề yêu cầu các phụ thuộc tầm xa.

Other ``flavors'' of MPNN have been developed in the literature, e.g. graph convolutional networks \& graph attention networks, whose definitions can be expressed in terms of the MPNN formalism.

-- Các ``hương vị'' khác của MPNN đã được phát triển trong tài liệu, ví dụ như mạng tích chập đồ thị \& mạng chú ý đồ thị, định nghĩa của chúng có thể được thể hiện theo hình thức MPNN.

%------------------------------------------------------------------------------%

\subsection{Graph convolutional network (GCN) -- Mạng lưới tích chập đồ thị}
The graph convolutional network (GCN) was 1st introduced by {\sc Thomas Kipf \& Max Welling} in 2017. A GCN layer defines a 1st-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data.

-- Mạng tích chập đồ thị (GCN) được giới thiệu lần đầu tiên bởi {\sc Thomas Kipf \& Max Welling} vào năm 2017. Một lớp GCN định nghĩa phép xấp xỉ bậc 1 của bộ lọc phổ cục bộ trên đồ thị. GCN có thể được hiểu là sự tổng quát hóa của mạng nơ-ron tích chập lên dữ liệu có cấu trúc đồ thị.

The formal expression of a GCN layer reads as follows:
\begin{equation*}
    {\bf H} = \sigma\left(\widetilde{\bf D}^{-\frac{1}{2}}\widetilde{\bf A}\widetilde{\bf D}^{-\frac{1}{2}}{\bf X}\Theta\right),
\end{equation*}
where ${\bf H}$ is the matrix of node representations ${\bf h}_u$, ${\bf X}$ is the matrix of node features ${\bf x}_u$, $\sigma(\cdot)$ is an activation function (e.g., ReLU), $\widetilde{\bf A}$ is the graph adjacency matrix with the addition of self-loops, $\widetilde{\bf D}$ is the graph degree matrix with the addition of self-loops, \& $\Theta$ is a matrix of trainable parameters.

-- Biểu thức chính thức của một lớp GCN được đọc như sau:
\begin{equation*}
    {\bf H} = \sigma\left(\widetilde{\bf D}^{-\frac{1}{2}}\widetilde{\bf A}\widetilde{\bf D}^{-\frac{1}{2}}{\bf X}\Theta\right),
\end{equation*}
trong đó ${\bf H}$ là ma trận biểu diễn nút ${\bf h}_u$, ${\bf X}$ là ma trận đặc trưng nút ${\bf x}_u$, $\sigma(\cdot)$ là một hàm kích hoạt (ví dụ: ReLU), $\widetilde{\bf A}$ là ma trận kề đồ thị với việc bổ sung các vòng lặp tự thân, $\widetilde{\bf D}$ là ma trận bậc đồ thị với việc bổ sung các vòng lặp tự thân, \& $\Theta$ là ma trận các tham số có thể đào tạo được.

In particular, let ${\bf A}$ be the graph adjacency matrix: then, one can define $\widetilde{\bf A}\coloneqq{\bf A} + {\bf I}$ \& $\widetilde{\bf D}_{ii} = \sum_{j\in V} \widetilde{A}_{ij}$, where $I$ denotes identity matrix. This normalization ensures: eigenvalues of $\widetilde{\bf D}^{-\frac{1}{2}}\widetilde{\bf A}\widetilde{\bf D}^{-\frac{1}{2}}$ are bounded in range $[0,1]$, avoiding numerical instabilities \& exploding{\tt/}vanishing gradients.

-- Cụ thể, giả sử ${\bf A}$ là ma trận kề đồ thị: khi đó, ta có thể định nghĩa $\widetilde{\bf A}\coloneqq{\bf A} + {\bf I}$ \& $\widetilde{\bf D}_{ii} = \sum_{j\in V} \widetilde{A}_{ij}$, trong đó $I$ biểu thị ma trận đơn vị. Chuẩn hóa này đảm bảo: các trị riêng của $\widetilde{\bf D}^{-\frac{1}{2}}\widetilde{\bf A}\widetilde{\bf D}^{-\frac{1}{2}}$ bị chặn trong khoảng $[0,1]$, tránh được hiện tượng bất ổn định số \& bùng nổ{\tt/}gradient biến mất.

A limitation of GCNs: they do not allow multidimensional edge features ${\bf e}_{uv}$. It is however possible to associate scalar weight $w_{uv}$ to each edge by imposing $A_{uv} = w_{uv}$, i.e., by setting each nonzero entry in adjacency matrix equal to weight of corresponding edge.

-- 1 hạn chế của GCN: chúng không cho phép các đặc trưng cạnh đa chiều ${\bf e}_{uv}$. Tuy nhiên, có thể liên kết trọng số vô hướng $w_{uv}$ với mỗi cạnh bằng cách áp đặt $A_{uv} = w_{uv}$, tức là bằng cách đặt mỗi phần tử khác không trong ma trận kề bằng với trọng số của cạnh tương ứng.

%------------------------------------------------------------------------------%

\subsection{Graph attention network -- Mạng lưới chú ý đồ thị}
Graph attention network (GAT) was introduced by {\sc Petar Veličković} et al. in 2018. Graph attention network is a combination of a GNN \& an attention layer. Implementation of attention layer in graphical neural networks helps provide attention or focus to important information from data instead of focusing on whole data.

-- Mạng lưới chú ý đồ thị (GAT) được giới thiệu bởi {\sc Petar Veličković} \& cộng sự vào năm 2018. Mạng lưới chú ý đồ thị là sự kết hợp giữa mạng nơ-ron nhân tạo (GNN) và một lớp chú ý. Việc triển khai lớp chú ý trong mạng nơ-ron đồ họa giúp tập trung sự chú ý hoặc tập trung vào thông tin quan trọng từ dữ liệu thay vì tập trung vào toàn bộ dữ liệu.

A multi-head GAT layer can be expressed as follows:
\begin{equation*}
    {\bf h}_u = {\overset{K}{\underset{k=1}{\Big\Vert }}} \sigma\left(\sum_{v\in N_u} \alpha_{uv}{\bf W}^k{\bf x}_v\right),
\end{equation*}
where $K$ is the number of attention heads, $\big\Vert$ denotes vector concatenation, $\sigma(\cdot)$ is an activation function (e.g., ReLU), $\alpha_{ij}$ are attention coefficients, \& $W^k$ is a matrix of trainable parameters for the $k$th attention head.

-- 1 lớp GAT nhiều đầu có thể được biểu diễn như sau:
\begin{equation*}
    {\bf h}_u = {\overset{K}{\underset{k=1}{\Big\Vert }}} \sigma\left(\sum_{v\in N_u} \alpha_{uv}{\bf W}^k{\bf x}_v\right),
\end{equation*}
trong đó $K$ là số đầu chú ý, $\big\Vert$ biểu thị phép nối vectơ, $\sigma(\cdot)$ là hàm kích hoạt (ví dụ: ReLU), $\alpha_{ij}$ là các hệ số chú ý, \& $W^k$ là ma trận các tham số có thể huấn luyện được cho đầu chú ý thứ $k$.

+++

%------------------------------------------------------------------------------%

\subsection{Gated graph sequence neural network -- Mạng nơ-ron chuỗi đồ thị có cổng}


%------------------------------------------------------------------------------%

\section{Local Pooling Layers -- Lớp Gộp Cục Bộ}

%------------------------------------------------------------------------------%

\section{Heterophilic Graph Learning -- Học Đồ Thị Dị Hướng}

%------------------------------------------------------------------------------%

\section{Applications of GNNs -- Ứng Dụng của GNNs}

%------------------------------------------------------------------------------%

\chapter{Miscellaneous}
\minitoc

%------------------------------------------------------------------------------%

\section{Contributors}

\begin{enumerate}
    \item {\sc Võ Ngọc Trâm Anh [VNTA].}
    \item {\sc Đặng Phúc An Khang [DPAK]}
    \item {\sc Nguyễn Ngọc Thạch [NNT].}
    \item {\sc Phan Vĩnh Tiến [PVT].}
\end{enumerate}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]

\end{document}