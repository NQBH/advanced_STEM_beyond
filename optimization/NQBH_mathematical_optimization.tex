\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Mathematical Optimization -- Toán Tối Ưu}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Mathematical Optimization -- Toán Tối Ưu}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/optimization/NQBH_mathematical_optimization.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/optimization/NQBH_mathematical_optimization.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basic}

%------------------------------------------------------------------------------%

\section{Linear Programming -- Quy Hoạch Tuyến Tính}
\begin{definition}[Linear programming]
	``\emph{Linear programming (LP)}, also called \emph{linear optimization}, is a method to achieve the best outcome, e.g., maximum profit or lower cost, in a \href{https://en.wikipedia.org/wiki/Mathematical_model}{mathematical model} whose requirements \& objective are represented by \href{https://en.wikipedia.org/wiki/Linear_function#As_a_polynomial_function}{linear relationships}. Linear programming is a special case of mathematical programming $\equiv$ \href{https://en.wikipedia.org/wiki/Mathematical_optimization}{mathematical optimization}.'' -- \href{https://en.wikipedia.org/wiki/Linear_programming}{Wikipedia{\tt/}linear programming}
\end{definition}
More formally, linear programming is a technique for the \href{https://en.wikipedia.org/wiki/Mathematical_optimization}{optimization} of a linear \href{https://en.wikipedia.org/wiki/Objective_function}{linear objective function}, subject to \href{https://en.wikipedia.org/wiki/Linear_equality}{linear equality} \& \href{https://en.wikipedia.org/wiki/Linear_inequality}{linear inequality} \href{https://en.wikipedia.org/wiki/Constraint_(mathematics)}{constraints}. Its \href{https://en.wikipedia.org/wiki/Feasible_region}{feasible region} is a \href{https://en.wikipedia.org/wiki/Convex_polytope}{convex polytope}, which is a set defined as the \href{https://en.wikipedia.org/wiki/Intersection_(mathematics)}{intersection} of finitely many \href{https://en.wikipedia.org/wiki/Half-space_(geometry)}{half spaces}, each of which is defined by a linear inequality. Its objective function is a real-valued \href{https://en.wikipedia.org/wiki/Affine_function}{affine (linear) function} defined on this polytope. A linear programming \href{https://en.wikipedia.org/wiki/Algorithm}{algorithm} finds a point in the \href{https://en.wikipedia.org/wiki/Polytope}{polytope} where this function has the largest (or smallest) value if such a point exists.

Linear programs are problems that can be expressed in \href{https://en.wikipedia.org/wiki/Canonical_form}{standard form} as
\begin{equation}
	\label{linear program}
	\tag{lp}
	\mbox{Find a vector }{\bf x}\mbox{ that maximizes{\tt/}minimizes }{\bf c}^\top{\bf x}\mbox{ subject to } A{\bf x}\le{\bf b}\mbox{ \& }{\bf x}\ge{\bf 0}.
\end{equation}
Here the components of ${\bf x}$ are the variables to be determined, ${\bf b},{\bf c}$ are given vectors, \& $A$ is a given matrix. The function whose value is to be maximized (${\bf x}\mapsto{\bf c}^\top{\bf x}$ in this case) is called the \href{https://en.wikipedia.org/wiki/Objective_function}{objective function}. The constraint $A{\bf x}\le{\bf x}$ \& ${\bf x}\ge{\bf 0}$ specify a \href{https://en.wikipedia.org/wiki/Convex_polytope}{convex polytope} over which the objective function is to be optimized.

Linear programming can be applied to various fields of study, which is widely used in mathematics \&, to a lesser extent, in business, economics, \& to some engineering problems. There is a close connection between linear programs, eigenequations, \href{https://en.wikipedia.org/wiki/John_von_Neumann}{John von Neumann}'s general equilibrium model, \& structural equilibrium models (see \href{https://en.wikipedia.org/wiki/Dual_linear_program}{dual linear program}). Industries using linear programming models include transportation, energy, telecommunications, \& manufacturing. It has proven useful in modeling diverse types of problems in \href{https://en.wikipedia.org/wiki/Automated_planning_and_scheduling}{planning}, \href{https://en.wikipedia.org/wiki/Routing}{routing}, \href{https://en.wikipedia.org/wiki/Scheduling_(production_processes)}{scheduling}, \href{https://en.wikipedia.org/wiki/Assignment_problem}{assignment}, \& design.

\begin{dinhnghia}[Quy hoạch tuyến tính]
	Bài toán \emph{quy hoạch tuyến tính} là bài toán tìm {\rm GTLN{\tt/}GTNN} của \emph{hàm mục tiêu} trong điều kiện hàm mục tiêu là hàm bậc nhất đối với các biến \& mỗi 1 điều kiện ràng buộc là bất phương trình bậc nhất đối với các biến (không kể điều kiện ràng buộc biến thuộc tập số nào, e.g., $\mathbb{N},\mathbb{Q},\mathbb{R},\mathbb{C}$.
\end{dinhnghia}
Ta có thể viết bài toán quy hoạch tuyến tính 2 biến $x,y$ về dạng sau:
\begin{align}
	\max T&\coloneqq\alpha x + \beta y\mbox{ s.t } a_ix + b_iy\le c_i,\ \forall i = 1,2,\ldots,n,\label{linear programming 2 vars max}\tag{lp2max}\\
	\min T&\coloneqq\alpha x + \beta y\mbox{ s.t } a_ix + b_iy\le c_i,\ \forall i = 1,2,\ldots,n,\label{linear programming 2 vars min}\tag{lp2min}
\end{align}
trong đó các điều kiện ràng buộc đều là các bất phương trình bậc nhất đối với $x,y$. See also:
\begin{itemize}
	\item {\it Problem: Inequation \& Linear System of Inequations -- Bài Tập: Bất Phương Trình \& Hệ Bất Phương Trình}.
	
	Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 10{\tt/}linear system inequations{\tt/}problem}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.tex}.}].
	\begin{itemize}
		\item {\it Problem \& Solution: Inequation \& Linear System of Inequations -- Bài Tập \& Lời Giải: Bất Phương Trình \& Hệ Bất Phương Trình}.
		
		Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 10{\tt/}linear system inequations{\tt/}solution}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.tex}.}].
	\end{itemize}
	\item {\it Problem: Mathematical Optimization -- Bài Tập: Ứng Dụng Toán Học Để Giải Quyết 1 Số Bài Toán Tối Ưu}.
	
	Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 12{\tt/}optimization{\tt/}problem}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.tex}.}].
	\begin{itemize}
		\item {\it Problem \& Solution: Mathematical Optimization -- Bài Tập \& Lời Giải: Ứng Dụng Toán Học Để Giải Quyết 1 Số Bài Toán Tối Ưu}.
		
		Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 12{\tt/}optimization{\tt/}solution}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.tex}.}].
	\end{itemize}
\end{itemize}

\subsection{How to solve some linear programmings -- Cách giải 1 số bài toán quy hoạch tuyến tính}
Có thể giải 1 số bài toán quy hoạch tuyến tính dạng \eqref{linear programming 2 vars max} hay \eqref{linear programming 2 vars min} theo 2 bước:
\begin{enumerate}
	\item Xác định miền nghiệm $S\subset\mathbb{R}^2$ của hệ bất phương trình $a_ix + b_iy\le c_i$, $\forall i = 1,\ldots,n$.
	\item Tìm điểm $(x,y)\in S$ sao cho biểu thức $T = T(x,y) = \alpha x + \beta y$ có {\rm GTLN} ở bài toán \eqref{linear programming 2 vars max} hoặc có {\rm GTNN} ở bài toán \eqref{linear programming 2 vars min}.
	
	Khi miền nghiệm $S$ là đa giác (polygon), biểu thức $T(x,y) = \alpha x + \beta y$ đạt GTLN{\tt/}GTNN (gộp chung gọi là {\it cực trị}) tại $(x,y)\in\mathbb{R}^2$ là tọa độ 1 trong các đỉnh của đa giác đó. Khi đó, bước 2 có thể được thực hiện như sau:
	\begin{enumerate}
		\item Xác định tọa độ các đỉnh của đa giác đó.
		\item Tính giá trị của biểu thức $T(x,y) = \alpha x + \beta y$ tại các đỉnh của đa giác đó.
		\item So sánh các giá trị \& kết luận.
	\end{enumerate}
\end{enumerate}
\cite[Chuyên đề II, \S1, LT1--3, 1., 2., 3., 4., 5., pp. 20--25]{CDHT_Toan_12_Canh_Dieu}.

%------------------------------------------------------------------------------%

\section{Optimal Control -- Điều Khiển Tối Ưu}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Lions1969}. {\sc Jacques-Louis Lions}. {\it Quelques m\'{e}thodes de r\'{e}solution des probl\`emes aux limites nonlin\'{e}aires} -- Some Methods for Solving Nonlinear Boundary Problems.
	\item \cite{Lions1971}. {\sc Jacques-Louis Lions}. {\it Optimal control of systems governed by PDEs}.
	\item \cite{Troltzsch2010}. {\sc Fredi Tr\"{o}ltzsch}. {\it Optimal Control of PDEs}.
\end{enumerate}

\begin{definition}[Optimal control theory]
	``\emph{Optimal control theory} is a branch of \href{https://en.wikipedia.org/wiki/Control_theory}{control theory} that deals with finding a \href{https://en.wikipedia.org/wiki/Control_(optimal_control_theory)}{control} for a \href{https://en.wikipedia.org/wiki/Dynamical_system}{dynamical system} over a period of time s.t. an \href{https://en.wikipedia.org/wiki/Objective_function}{objective function} is optimized. It has numerous applications in science, engineering, \& operational research. E.g., the dynamical system might be a \href{https://en.wikipedia.org/wiki/Spacecraft}{spacecraft} with controls corresponding to rocket thrusters, \& the objective might be to reach the Moon with minimum fuel expenditure. Or the dynamical system could be a nation's \href{https://en.wikipedia.org/wiki/Economy}{economy}, with the objective to minimize \href{https://en.wikipedia.org/wiki/Unemployment}{unemployment}; the controls in this case could be \href{https://en.wikipedia.org/wiki/Fiscal_policy}{fiscal} \& \href{https://en.wikipedia.org/wiki/Monetary_policy}{monetary policy}. A dynamical system may also be introduced to embed \href{https://en.wikipedia.org/wiki/Operations_research}{operations research problems} within the framework of optimal control theory.'' -- \href{https://en.wikipedia.org/wiki/Optimal_control}{Wikipedia{\tt/}optimal control}
\end{definition}
``Optimal control is an extension of the \href{https://en.wikipedia.org/wiki/Calculus_of_variations}{calculus of variations}, \& is a mathematical optimization method for deriving \href{https://en.wikipedia.org/wiki/Control_theory}{control policies}. The method is largely due to the work of \href{https://en.wikipedia.org/wiki/Lev_Pontryagin}{Lev Pontryagin} \& \href{https://en.wikipedia.org/wiki/Richard_Bellman}{Richard Bellman} in the 1950s, after contributions to calculus of variations by \href{https://en.wikipedia.org/wiki/Edward_J._McShane}{Edward J. McShane}. Optimal control can be seen as a \href{https://en.wikipedia.org/wiki/Control_strategy}{control strategy} in \href{https://en.wikipedia.org/wiki/Control_theory}{control theory}.'' -- \href{https://en.wikipedia.org/wiki/Optimal_control}{Wikipedia{\tt/}optimal control}

\paragraph{General method.} Optimal control deals with the problem of finding a control law for a given system s.t. a certain \href{https://en.wikipedia.org/wiki/Optimality_criterion}{optimality criterion} is achieved. A control problem includes a \href{https://en.wikipedia.org/wiki/Cost_functional}{cost functional} that is a function of state \& control variables. An {\it optimal control} is a set of \href{https://en.wikipedia.org/wiki/Differential_equation}{differential equations} describing the paths of the control variables that minimize the cost function. The optimal control can be derived using \href{https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle}{Pontryagin's maximum principle} (a \href{https://en.wikipedia.org/wiki/Necessary_condition}{necessary condition} also known as Pontryagin's minimum principle or simply Pontryagin's principle), or by solving the \href{https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation}{Hamilton--Jacobi--Bellman equation0} (a \href{https://en.wikipedia.org/wiki/Sufficient_condition}{sufficient condition}).

\begin{example}
	Consider a car traveling in a straight line on a hilly road. Question: How should the driver press the accelerator pedal in order to minimize the total traveling time? The term \emph{control law} refers specifically to the way in which the driver presses the accelerator \& shifts the gears. The \emph{system} consists of both the car \& the road, \& the \emph{optimality criterion} is the minimization of the total traveling time. Control problems usually include ancillary \href{https://en.wikipedia.org/wiki/Constraint_(mathematics)}{constraints}. E.g., the amount of available fuel might be limited, the accelerator pedal cannot be pushed through the floor of the car, speed limits, etc.
\end{example}
A proper cost function will be a mathematical expression giving the traveling time as a function of the speed, geometrical considerations, \& \href{https://en.wikipedia.org/wiki/Initial_condition}{initial conditions} of the system. \href{https://en.wikipedia.org/wiki/Constraint_(mathematics)}{Constraints} are often interchangeable with the cost function.

Another related optimal control problem may be to find the way to drive the car so as to minimize its fuel consumption, given that it must complete a given course in a time not exceeding some amount. Yet another related control problem may be to minimize the total monetary cost of completing the trip, given assumed monetary prices for time \& fuel.

\paragraph{An abstract framework.} Minimize the continuous-time cost functional
\begin{equation}
	J(t_0,t_{\rm f},{\bf x}(\cdot),{\bf u}(\cdot))\coloneqq E({\bf x}(t_0),t_0,{\bf x}(t_{\rm f}),t_{\rm f}) + \int_{t_0}^{t_{\rm f}} F(t,{\bf x}(t),{\bf u}(t))\,{\rm d}t,
\end{equation}
subject to the 1st-order dynamic constraints (the {\it state equation})
\begin{equation}
	\dot{\bf x}(t) = {\bf f}(t,{\bf x}(t),{\bf u}(t)),
\end{equation}
the algebraic {\it path constraints}
\begin{equation}
	{\bf h}(t,{\bf x}(t),{\bf u}(t))\le{\bf 0},
\end{equation}
\& the \href{https://en.wikipedia.org/wiki/Boundary_condition}{endpoint conditions}
\begin{equation}
	{\bf e}(t_0,{\bf x}(t_0),t_{\rm f},{\bf x}(t_{\rm f})) = {\bf 0},
\end{equation}
where ${\bf x}(t)$: the {\it state}, ${\bf u}(t)$ is the {\it control}, $t$: the independent variable (generally speaking, time), $t_0$: the initial time, \& $t_{\rm f}$: the terminal time. The terms $E,F$ are called the {\it endpoint cost} \& {\it running cost}, respectively. In the calculus of variations, $E,F$ are referred to as the Mayer term \& the \href{https://en.wikipedia.org/wiki/Lagrange_multiplier}{Lagrangian}, respectively. Furthermore, it is noted that the path constraints are in general {\it inequality} constraints \& thus may not be active (i.e., $= 0$) at the optimal solution. It is also noted that the optimal control problem as stated above may have multiple solutions (i.e., the solution may not be unique). Thus, it is most often the case that any solution $(t_0^\star,t_{\rm f}^\star,{\bf x}^\star(t),{\bf u}^\star(t))$ to the optimal control problem is {\it locally minimizing{\tt/}minimizer}.

\subsection{Linear Quadratic Control}
A special case of the general nonlinear optimal control problem is the \href{https://en.wikipedia.org/wiki/Linear-quadratic_regulator}{linear quadratic (LQ) optimal control problem}. The LQ problem is stated as follows. Minimize the {\it quadratic} continuous-time cost functional
\begin{equation}
	J = \frac{1}{2}{\bf x}^\top(t_{\rm f})S_{\rm f}{\bf x}(t_{\rm f}) + \frac{1}{2}\int_{t_0}^{t_{\rm f}} {\bf x}^\top(t)Q(t){\bf x}(t) + {\bf u}^\top(t)R(t){\bf u}(t)\,{\rm d}t,
\end{equation}
subject to the linear 1st-order dynamic constraints
\begin{equation}
	\left\{\begin{split}
		\dot{\bf x}(t) &= A(t){\bf x}(t) + B(t){\bf u}(t),\\
		{\bf x}(t_0) &= {\bf x}_0.
	\end{split}\right.	
\end{equation}
A particular form of the LQ problem arising in many control system problems is that of the {\it linear quadratic regulator} (LQR) where all of the matrices (i.e., $A,B,Q,R$) are constant, the initial time is arbitrarily set to 0, \& the terminal time is taken in the limit $t_{\rm f}\to0$ (this last assumption is what is known as {\it infinite horizon}). The LQR problem is stated as follows. Minimize the infinite horizon quadratic continuous-time cost functional
\begin{equation}
	J = \frac{1}{2}\int_0^\infty {\bf x}^\top(t)Q{\bf x}(t) + {\bf u}^\top(t)R{\bf u}(t)\,{\rm d}t,
\end{equation}
subject to the {\it linear time-invariant} 1st-order dynamic constraints
\begin{equation}
	\left\{\begin{split}
		\dot{\bf x}(t) &= A{\bf x}(t) + B{\bf u}(t),\\
		{\bf x}(t_0) &= {\bf x}_0.
	\end{split}\right.
\end{equation}
In the finite-horizon case the matrices are restricted in that $Q,R$ are positive semi-definite \& positive definite, respectively. In the infinite-horizon case, however, the matrices $Q,R$ are not only positive-semidefinite \& positive-definite, respectively, but are also constant. These additional restrictions on $Q,R$ in the infinite-horizon case are enforced to ensure that the cost functional remains positive. Furthermore, in order to ensure that the cost function is {\it bounded}, the additional restriction is imposed that the pair $(A,B)$ is \href{https://en.wikipedia.org/wiki/Controllability}{\it controllable}. Note that the LQ or LQR cost functional can be thought of physically as attempting to minimize the {\it control energy} (measured as a quadratic form).

The infinite horizon problem, i.e., LQR, may seem overly restrictive \& essentially useless because it assumes that the operator is driving the system to 0-state \& hence driving the output of the system to 0. This is indeed correct. However the problem of driving the output to a desired nonzero level can be solved {\it after} the zero output one is. In fact, can prove that this secondary LQR problem can be solved in a very straightforward manner. It has been shown in classical optimal control theory that the LQ (or LQR) optimal control has the feedback form
\begin{equation}
	{\bf u}(t) = -K(t){\bf x}(t),
\end{equation}
where $K(t)$ is a properly dimensioned matrix, given as
\begin{equation}
	K(t) = R^{-1}B^\top S(t),
\end{equation}
\& $S(t)$ is the solution of the differential \href{https://en.wikipedia.org/wiki/Riccati_equation}{Riccati equation}. The differential Riccati equation is given as
\begin{equation}
	\dot S(t) = -S(t)A - A^\top S(t) + S(t)BR^{-1}B^\top S(t) - Q.
\end{equation}
For the finite horizon LQ problem, the Riccati equation is integrated backward in time using the terminal boundary condition
\begin{equation}
	S(t_{\rm f}) = S_{\rm f}.
\end{equation}
For the infinite horizon LQR problem, the differential Riccati equation is replaced with the {\it algebraic} Riccati equation (ARE) given as
\begin{equation}
	-SA - A^\top S + SBR^{-1}B^\top S - Q = {\bf 0}.
\end{equation}
Understanding that the ARE arises from infinite horizon problem, the matrices $A,B,Q,R$ are all constant. It is noted that there are in general multiple solutions to the algebraic Riccati equation \& the {\it positive definite} (or positive semi-definite) solution is the one that is used to compute the feedback again. The LQ (LQR) problem was elegantly solved by \href{https://en.wikipedia.org/wiki/Rudolf_E._K%C3%A1lm%C3%A1n}{Rudolf E. K\'alm\'an}.

\subsection{Numerical Methods for Optimal Control}
``Optimal control problems are generally nonlinear \& therefore, generally do not have analytic solutions, e.g., like the linear-quadratic optimal control problem. As a result, it is necessary to employ numerical methods to solve optimal control problems. In the early years of optimal control (c. 1950s--1980s) the favored approach for solving optimal control problems was that of {\it indirect methods}. In an indirect method, the calculus of variations is employed to obtain the 1st-order optimality conditions. These conditions result in a 2-point (or, in the case of a complex problem, a multi-point) \href{https://en.wikipedia.org/wiki/Boundary-value_problem}{BVP}. This BVP actually has a special structure because it arises from taking the derivative of a \href{https://en.wikipedia.org/wiki/Hamiltonian_(control_theory)}{Hamiltonian}. Thus, the resulting \href{https://en.wikipedia.org/wiki/Dynamical_system}{dynamical system} is a \href{https://en.wikipedia.org/wiki/Hamiltonian_system}{Hamiltonian system} of the form
\begin{equation}
	\left\{\begin{split}
		\dot{\bf x} &= \partial_{\boldsymbol{\lambda}}H,\\
		\dot{\boldsymbol{\lambda}} &= -\partial_{\bf x}H,
	\end{split}\right.
\end{equation}
where $H = F + \boldsymbol{\lambda}^\top{\bf f} - \boldsymbol{\mu}^\top{\bf h}$ is the {\it augmented Hamiltonian} \& in an indirect method, the BVP is solved (using the appropriate boundary or {\it transversality} conditions). The beauty of using an indirect method is that the state \& adjoint, i.e., $\boldsymbol{\lambda}$, are solved for \& the resulting solution is readily verified to be an extremal trajectory. The disadvantage of indirect methods is that the BVP is often extremely difficult to solve (particularly for problems that span large time intervals or problems with interior point constraints). A well-known software program that implements direct methods is BNDSCO.

The approach that has risen to prominence in numerical optimal control since the 1980s is that of so-called {\it direct methods}. In a direct method, the state or the control, or both, are approximated using an appropriate function approximation (e.g., polynomial approximation or piecewise constant parameterization). Simultaneously, the cost functional is approximated as a {\it cost function}. Then, the coefficients of the function approximations are treated as optimization variables \& the problem is ``transcribed'' to a nonlinear optimization problem of the form
\begin{equation}
	\min F({\bf z})
\end{equation}
subject to the algebraic constraints
\begin{equation}
	{\bf g}({\bf z}) = {\bf 0},\ {\bf h}({\bf z})\le{\bf 0}.
\end{equation}
Depending upon the type of direct method employed, the size of the nonlinear optimization problem can be quite small (e.g., as in a direct shooting or \href{https://en.wikipedia.org/wiki/Quasilinearization}{quasilinearization} method), moderate (e.g., \href{https://en.wikipedia.org/wiki/Pseudospectral_optimal_control}{pseudospectral optimal control}) or may be quite large (e.g., a direct \href{https://en.wikipedia.org/wiki/Collocation_method}{collocation method}). In the latter case (i.e., a collocation method), the nonlinear optimization problem may be literally thousands $a\cdot10^3$ to tens of thousands $a\cdot10^4$ of variables \& constraints. Given the size of many NLPs arising from a direct method, it may appear somewhat counter-intuitive that solving the nonlinear optimization problem is easier than solving the BVP. It is, however, the fact that the NLP is easier to solve than the BVP. The reason for the relative ease of computation, particularly of a direct collocation method, is that the NLP is {\it sparse} \& many well-known software programs exist (e.g., \href{https://en.wikipedia.org/wiki/SNOPT}{SNOPT}) to solve large sparse NLPs. As a result, the range of problems that can be solved via direct methods (particularly direct {\it collocation methods} which are very popular these days) is significantly larger than the range of problems that can be solved via indirect methods. In fact, direct methods have become so popular these days that many people have written elaborate software programs employing these methods. In particular, many such programs include DIRCOL, SOCS, OTIS, GESOP{\tt/}\href{https://en.wikipedia.org/wiki/ASTOS}{ASTOS}, DITAN., \& PyGMO{\tt/}PyKEP. In recent years, due to the advent of the \href{https://en.wikipedia.org/wiki/MATLAB}{MATLAB} programming language, optimal control software in MATLAB has become more common. Examples of academically developed MATLAB software tools implementing direct methods include RIOTs, \href{https://en.wikipedia.org/wiki/DIDO_(optimal_control)}{DIDO}, DIRECT, FALCON.m, \& GPOPs, while an example of an industry developed MATLAB tool is \href{https://en.wikipedia.org/wiki/PROPT}{PROPT}. These software tools have increased significantly the opportunity for people to explore complex optimal control problems both for academic research \& industrial problems. Finally, it is noted that general-purpose MATLAB optimization environments such as \href{https://en.wikipedia.org/wiki/TOMLAB}{TOMLAB} have made coding complex optimal control problems significantly easier than was previously possible in languages such as C \& \href{https://en.wikipedia.org/wiki/FORTRAN}{FORTRAN}.

\subsection{Discrete-Time Optimal Control}
``The examples thus far have shown \href{https://en.wikipedia.org/wiki/Continuous_time}{continuous time} systems \& control solutions. In fact, as optimal control solutions are now often implemented \href{https://en.wikipedia.org/wiki/Digital_data}{digitally}, contemporary control theory is now primarily concerned with \href{https://en.wikipedia.org/wiki/Discrete_time}{discrete time} systems \& solutions. The theory of Consistent Approximations provides conditions under which solutions to a series of increasingly accurate discretized optimal control problem converge to the solution of the original, continuous-time problem. Not all discretization methods have this property, even seemingly obvious ones. E.g., using a variable step-size routine to integrate the problem's dynamic equations may generate a gradient which does not converge to 0 (or point in the right direction) as the solution is approached. The direct method \href{http://www.schwartz-home.com/RIOTS}{RIOTS} is based on the Theory of Consistent Approximation.

A common solution strategy in many optimal control problems is to solve for the costate (sometimes called the \href{https://en.wikipedia.org/wiki/Shadow_price}{shadow price}) $\lambda(t)$. The costate summaries in 1 number the marginal value of expanding or contracting the state variable next turn. The marginal value is not only the gains accruing to it next turn but associated with the duration of the program. It is nice when $\lambda(t)$ can be solved analytically, but usually, the most one can do is describe it sufficiently well that the intuition can grasp the character of the solution \& an equation solver can solve numerically for the values.

Having obtained $\lambda(t)$, the turn-$t$ optimal value for the control can usually be solved as a differential equation conditional on knowledge of $\lambda(t)$. Again it is infrequent, especially in continuous-time problems, that one obtains the value of the control or the state explicitly. Usually, the strategy is to solve for thresholds \& regions that characterize the optimal control \& use a numerical solver to isolate the actual choice values in time.

\begin{example}
	``Consider the problem of a mine owner who must decide at what rate to extract ore from their mine. They own rights to the ore from date $0$ to date $T$. At date $0$ there is $x_0$ ore in the ground, \& the time-dependent amount of ore $x(t)$ left in the ground declines at the rate of $u(t)$ that the mine owner extracts it. The mine owner extracts ore at cost $\frac{u(t)^2}{x(t)}$ (the cost of extraction increasing with the square of the extraction speed \& the inverse of the amount of ore left) \& sells ore at a constant price $p$. Any ore left in the ground at time $T$ cannot be sold \& has no value (there is no ``scrap value''). The owner chooses the rate of extraction varying with time $u(t)$ to maximize profits over the period of ownership with no time discounting. See discrete-time version vs. Continuous-time version.'' -- \href{https://en.wikipedia.org/wiki/Optimal_control#Finite_time}{Wikipedia{\tt/}optimal control{\tt/}finite time}
\end{example}

\subsection{Optimal Control for PDEs -- Điều Khiển Tối Ưu Cho Phương Trình Vi Phân Đạo Hàm Riêng}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Lions1971}. {\sc Jacques-Louis Lions}. {\it Optimal control of systems governed by PDEs}.
	\item \cite{Troltzsch2010}. {\sc Fredi Tr\"{o}ltzsch}. {\it Optimal Control of PDEs}.
\end{enumerate}

\subsubsection{Optimal Control for Navier--Stokes Equations -- Điều Khiển Tối Ưu Cho Phương Trình Navier--Stokes}
	
%------------------------------------------------------------------------------%

\section{Shape Optimization -- Tối Ưu Hình Dạng}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Allaire_Henrot2001}. {\sc Gr\'egoire Allaire, Antoine Henrot}. {\it On some recent advances in shape optimization}.
	\item \cite{Azegami2020}. {\sc Hideyuki Azegami}. {\it Shape Optimization Problems}.
	\item \cite{Bandle_Wagner2023}. {\sc Catherine Bandle, Alfred Wagner}. {\it Shape Optimization: Variations of Domains \& Applications}.
	\item \cite{Delfour_Zolesio2001,Delfour_Zolesio2011}. {\sc Michael C. Delfour, Jean-Paul Zol\'{e}sio}. {\it Shapes \& Geometries}.
	\item \cite{Haslinger_Makinen2003}. {\sc J. Haslinger, R. A. E. M\"{a}kinen}. {\it Introduction to Shape Optimization}.
	\item \cite{Mohammadi_Pironneau2010}. {\sc Bijan Mohammadi, Olivier Pironneau}. {\it Applied Shape Optimization for Fluids}.
	\item \cite{Moubachir_Zolesio2006}. {\sc Marwan Moubachir, Jean-Paul Zol\'{e}sio}. {\it Moving Shape Analysis \& Control}.
	\item {\sc Stephan Schmidt}. Master course: {\it Shape \& Geometry}. Humboldt University of Berlin. [written in German, taught in English \& German].
	\item \cite{Sokolowski_Zolesio1992}. {\sc Jan Soko\l owski, Jean-Paul Zol\'{e}sio}. {\it Introduction to Shape Optimization}.
	\item \cite{Walker2015}. {\sc Shawn W. Walker}. {\it The Shapes of Things}.
	
	{\bf Differential equations on surfaces.} Differential geometry is useful for understanding mathematical models containing geometric PDEs, e.g., surface{\tt/}manifold version of the standard Laplace equation, which requires the development of the surface gradient \& surface Laplacian operators -- the usual gradient $\nabla$ \& Laplacian $\Delta = \nabla\cdot\nabla$ operators defined on a surface (manifold) instead of standard Euclidean space $\mathbb{R}^n$. {\it Advantage}: provide alternative formulas for geometric quantities, e.g., the summed (mean) curvature, that are much clearer than the usual presentation of texts on differential geometry.
	
	{\bf Differentiating w.r.t. Shape.} The approach to differential geometry is advantageous for developing the framework of {\it shape differential calculus} -- the study of how quantities change w.r.t. changes of independent ``shape variable''.
	
	``The framework of shape differential calculus provides the tools for developing the equations of mean curvature flow \& Willmore flow, which are geometric flows occurring in many applications such as fluid dynamics \& biology.'' -- \cite[p. 2]{Walker2015}
	
	The shape perturbation $\delta J(\Omega;V)$ is similar to the gradient operator, which is a directional derivative, analogous to $V\cdot\nabla f$ where $V$ is a given direction, providing information about the local slope, or the sensitivity of a quantity w.r.t. some parameters.
	
	It takes only 2 or 3 numbers to specify a point $(x,y)$ in 2D \& a point $(x,y,z)$ in 3D, whereas an ``infinite'' number of coordinate pairs is needed to specify a domain $\Omega$. $V$ is a 2D{\tt/}3D vector in the scalar function setting; for a shape functional, $V$ is a full-blown function requiring definition at every point in $\Omega$. This ``infinite dimensionality'' is the reason for using the notation $\delta J(\Omega;V)$ to denote a shape perturbation. $\delta J(\Omega;V)$ indicates how we should change $\Omega$ to decrease $J$, similarly to how $\nabla f(x,y)$ indicates how the coordinate pair $(x,y)$ should change to decrease $f$, which opens up the world of shape optimization.
	
	{\bf3 schools of shape optimization.} Cf. engineering shape optimization vs. applied shape optimization \cite{Mohammadi_Pironneau2010} vs. theoretical shape optimization \cite{Sokolowski_Zolesio1992,Delfour_Zolesio2011}.
	
	Shape perturbations allow us to ``climb down the hill'' in the infinite dimensional setting of shape, which is a powerful tool for producing sophisticated engineering designs in an automatic way.
	
	{\bf Extrinsic vs. intrinsic point of views.} To make the discussion as clear as possible, we adopt the {\it extrinsic} point of view: curves \& surfaces are assumed to lie in a Euclidean space of higher dimension. The ambient space is 3D Euclidean space. Alternatively, there is the {\it intrinsic} point of view, i.e., the surface is not assumed to lie in an ambient space, i.e., one is not allowed to reference anything ``outside'' of the surface when defining it. Moreover, no mathematical structures ``outside'' of the surface can be utilized. Walker \cite{Walker2015} did not adopt the intrinsic view or consider higher dimensional manifolds, general embedding dimensions, etc. for the reasons: \cite{Walker2015} is meant as a {\it practical guide} to differential geometry \& shape differentiation that can be used by researchers in other fields.
	\begin{itemize}
		\item \cite{Walker2015} is meant to be used as background information for deriving physical models where geometry plays a critical role. Because most physical problems of interest take place in 3D Euclidean space, the extrinsic viewpoint is sufficient.
		\item Many of the proofs \& derivations of differential geometry relations simplify dramatically for 2D surfaces in 3D \& require only basic multivariable calculus \& linear algebra.
		\item The concepts of {\it normal vectors} \& {\it curvature} are harder to motivate with the intrinsic viewpoint. {\it What does it meant for a surface to ``curve through space'' if you cannot talk about the ambient space?}
		\item Walker wants to keep in mind applications of this machinery to geometric PDEs, fluid dynamics, numerical analysis, optimization, etc. An interesting application of this methodology is for the development of numerical methods for mean curvature flow \& surface tension driven fluid flow. Ergo ($=$ therefore), the extrinsic viewpoint is often more convenient for computational purposes.
		\item Walker wants his framework to be useful for analyzing \& solving {\it shape optimization} problems, i.e., optimization problems where geometry{\tt/}shape is the control variable.
	\end{itemize}
	{\bf Prerequisites.} ``When reading any mathematical text, the reader must have a certain level of mathematical ``maturity'' in order to efficiently learn what is in the text.
\end{enumerate}

\begin{example}[\cite{Walker2015}, Sect. 1.2.1, pp. 1--2]
	Let $f = f(r,\theta)$ be a smooth function defined on the disk $B_{R,2}(0,0)$ of radius $R$ in terms of polar coordinates. The integral of $f$ over $B_{2,R}(0,0)$ $J\coloneqq\int_{B_{2,R}(0,0)} f\,{\rm d}{\bf x} = \int_0^{2\pi}\int_0^R f(r,\theta)\,{\rm d}r\,{\rm d}\theta$ depends on $R$. Assume $f$ also depends on $R$, i.e., $f = f(r,\theta,R)$ with a physical example: $J$ is the \emph{net flow rate} of liquid through a pipe with cross-section $\Omega$, then $f$ is the flow rate per unit area \& could be the solution of a PDE defined on $\Omega$, e.g., a Navier--Stokes fluid flowing in a circular pipe. Advantageous to know the \emph{sensitivity} of $J$ w.r.t. $R$, e.g., for optimization purposes. Differentiate $J$ w.r.t. $R$:
	\begin{equation*}
		\frac{d}{dR}J = \int_0^{2\pi}\left(\frac{d}{dR}\int_0^R f(r,\theta;R)r\,{\rm d}r\right){\rm d}\theta = \int_0^{2\pi}\int_0^R f'(r,\theta;R)r\,{\rm d}r\,{\rm d}\theta + \int_0^{2\pi} f(R,\theta;R)\,{\rm d}\theta.
	\end{equation*}
	The dependence of $f$ on $R$ can more generally be viewed as dependence on $B_{R,2}(0,0)$, i.e., $f(\cdot;R)\equiv f(\cdot;B_{R,2}(0,0))$. Rewriting $d/dR J$ using Cartesian coordinates ${\bf x}$:
	\begin{equation}
		\frac{d}{dR}J = \int_{B_{R,2}(0,0)} f'({\bf x};\Omega)\,{\rm d}{\bf x} + \int_{S_{R,2}(0,0)} f({\bf x};\Omega)\,{\rm d}S({\bf x}),
	\end{equation}
	where ${\rm d}{\bf x}$ is the volume measure, ${\rm d}S({\bf x})$ is the surface area measure.
\end{example}

\begin{example}[Surface height function of a hill]
	Let $f = f(x,y)$ be a function describing the surface height of the hill, where $(x,y)$ are the coordinates of our position. Then, by using basic multivariate calculus, finding a direction that will move us downhill is equivalent to computing the gradient (vector) of $f$ \& moving in the opposite direction to the gradient. In this sense, we do not need to ``see'' the whole function. We just need to \emph{locally} compute the gradient $\nabla f$, analogous to feeling the ground beneath.
\end{example}

\begin{example}[Engineering shape optimization: minimizing drag with Navier--Stokes flow of fluid past a rigid body, \cite{Walker2015}, pp. 3--5]
	A shape functional representing the drag:
	\begin{equation}
		J_{\rm d}(\Omega)\coloneqq-{\bf u}_{\rm out}\cdot\int_{\Gamma_0} \boldsymbol{\sigma}({\bf u},p){\bf n}\,{\rm d}\Gamma = \frac{2}{{\rm Re}}\int_\Omega |\boldsymbol{\varepsilon}({\bf u})|^2\,{\rm d}{\bf x}\ge0,
	\end{equation}
	which physically represents the net force that must be applied to $\Omega_{\rm B}$ to keep it stationary while being acted upon by the imposed flow field \& represents the total amount of viscous dissipation of energy (per unit of time) in the fluid domain $\Omega$. Using the machinery of shape perturbations, $\delta J_{\rm d}(\Omega;V)$ indicates how $J_{\rm d}$ changes when we perturb $\Omega$ in the direction $V$. Hence, we can use this information to change $\Omega$ in small steps so as to slowly deform $\Omega$ into a shape that has better (lower) drag characteristics. A numerical computation: 2 large vortices appear behind the body, which indicate a large amount of viscous dissipation, i.e., large drag. The optimization process then computes $\delta J_{\rm d}(\Omega^0;V)$ for many different choices of $V$ \& chooses the one that drives down $J_{\rm d}$ the most. This choice of $V$ is used to deform $\Gamma_{\rm B}^0$ into a new shape $\Gamma_{\rm B}^1$ at iteration 1, with only a small difference between $\Gamma_{\rm B}^0$ \& $\Gamma_{\rm B}^1$. This process is repeated many times. Note how the vortices are eliminated by the more slender shape.
\end{example}

\begin{itemize}
	\item Condition (V)***
	\item Family of transformations $\{T_s:0\le s\le\tau\}$.
	\item Perturbed domain $\Omega_s = \Omega_s(V) = T_s(V)(\Omega)$.
	\item Assume that the velocity field $V$ satisfies $V_D$ and, in addition, $V\in C^0([0,\tau];C_{\rm loc}^1(\mathbb{R}^d,\mathbb{R}^d))$ and $\tau > 0$ is small enough such that the Jacobian $J_s$ is strictly positive: $J_s(X)\coloneqq\det DT_s(X) > 0$, $\forall s\in[0,\tau]$, where $DT_s(X)$ is the \textit{Jacobian matrix} of the transformation $T_s = T_s(V)$ associated with the velocity vector field $V$.
\end{itemize}

\subsection{Domain Integrals}

\begin{itemize}
	\item Given $\varphi\in W_{\rm loc}^{1,1}(\mathbb{R}^d)$, consider for $s\in[0,\tau]$ the volume integral
	\begin{equation}
		J(\Omega_s(V))\coloneqq\int_{\Omega_s(V)} \varphi\,{\rm d}{\bf x} = \int_\Omega \varphi\circ T_sJ_s\,{\rm d}{\bf x}.
	\end{equation}
	\begin{equation}
		dJ(\Omega;V) = \frac{d}{ds}J(\Omega_s(V))|_{s = 0} = \int_\Omega \nabla\varphi\cdot V(0) + \varphi\operatorname{div}V(0)\,{\rm d}{\bf x} = \int_\Omega \operatorname{div}(\varphi V(0))\,{\rm d}{\bf x}.
	\end{equation}
	If $\Omega$ has a Lipschitzian boundary, by Stokes's theorem:
	\begin{equation}
		dJ(\Omega;V) = \int_\Gamma \varphi V(0)\cdot{\bf n}\,{\rm d}\Gamma.
	\end{equation}
	
	\begin{theorem}[\cite{Delfour_Zolesio2011}, Thm. 4.1, pp. 482--483]
		Let $\varphi\in W_{\rm loc}^{1,1}(\mathbb{R}^d)$. Assume that the vector field $V = \{V(s):0\le s\le\tau\}$ satisfies condition (V).
		\item[(i)] For each $s\in[0,\tau]$ the map $\varphi\mapsto\varphi\circ T_s:W_{\rm loc}^{1,1}(\mathbb{R}^d)\to W_{\rm loc}^{1,1}(\mathbb{R}^d)$ and its inverse are both locally Lipschitzian and $\nabla(\varphi\circ T_s) = {}^*DT_s\nabla\varphi\circ T_s$.
		\item[(ii)] If $V\in C^0([0,\tau];C_{\rm loc}^1(\mathbb{R}^d,\mathbb{R}^d))$, then the map $s\mapsto J_s:[0,\tau]\to C_{\rm loc}^0(\mathbb{R}^d)$ is differentiable and
		\begin{equation}
			\frac{dJ_s}{ds} = [\nabla\cdot V(s)]\circ T_sJ_s\in C_{\rm loc}^0(\mathbb{R}^d).
		\end{equation}
		Hence the map $s\mapsto J_s$ belongs to $C^1([0,\tau];C_{\rm loc}^0(\mathbb{R}^d))$.
	\end{theorem}
	
	\begin{align}
		\frac{d}{ds}DT_s(X) &= DV(s,T_s(X))DT_s(X),\ DT_0(X) = I,\\
		\frac{d}{ds}\det DT_s(X) &= \operatorname{tr}DV(s,T_s(X))\det DT_s(X) = \nabla\cdot V(s,T_s(X))\det DT_s(X),\ \det DT_0(X) = 1.
	\end{align}
	
	\begin{theorem}[\cite{Delfour_Zolesio2011}, Thm. 4.2, pp. 483--484]
		Assume that there exists $\tau > 0$ such that the velocity field $V(t)$ satisfies conditions (V) and $V\in C^0([0,\tau];C_{\rm loc}^1(\mathbb{R}^d,\mathbb{R}^d))$. Given a function $\varphi\in C(0,\tau;W_{\rm loc}^{1,1}(\mathbb{R}^d))\cap C^1(0,\tau;L_{\rm loc}^1(\mathbb{R}^d))$ and a bounded measurable domain $\Omega$ with boundary $\Gamma$, the semiderivative of the function $J_V(s)\coloneqq\int_{\Omega_s(V)} \varphi(s)\,{\rm d}{\bf x}$ at $s = 0$ is given by
		\begin{equation}
			dJ_V(0) = \int_\Omega \varphi'(0) + \operatorname{div}(\varphi(0)V(0))\,{\rm d}{\bf x},
		\end{equation}
		where $\varphi(0)({\bf x})\coloneqq\varphi(0,{\bf x})$ and $\varphi'(0)({\bf x})\coloneqq\partial_t\varphi(0,{\bf x})$. In addition, $\Omega$ is an open domain with a Lipschitzian boundary $\Gamma$, then
		\begin{equation}
			dJ_V(0) = \int_\Omega \varphi'(0)\,{\rm d}{\bf x} + \int_\Gamma \varphi(0)V(0)\cdot{\bf n}\,{\rm d}{\bf x}.
		\end{equation}
	\end{theorem}
	
\end{itemize}

\subsection{Boundary Integrals}

\subsection{Material derivatives}
Let $\Omega\subset D$ be a bounded domain, 

%------------------------------------------------------------------------------%

\section{Topology Optimization -- Tối Ưu Tôpô}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}