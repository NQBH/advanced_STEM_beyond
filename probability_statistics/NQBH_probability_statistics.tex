\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Probability {\it\&} Statistics -- Xác Suất {\it\&} Thống Kê}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Probability \& Statistics -- Xác Suất \& Thống Kê}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/probability_statistics/NQBH_probability_statistics.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/probability_statistics/NQBH_probability_statistics.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basic}
{\bf Relationship among AL, ML, \& DL.} $\rm DL\subset ML\subset AI$.

%------------------------------------------------------------------------------%

\section{Probability -- Xác Suất}
\textbf{\textsf{Community -- Cộng đồng.}} {\sc Andrey Nikolaevich Kolmogorov}.

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item {\sc Simon J. D. Prince}. {\it Computer Vision: Models, Learning, \& Inference}.
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Statistics -- Thống Kê}
\textbf{\textsf{Community -- Cộng đồng.}} 

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item 
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Stochastic -- Ngẫu Nhiên}
\textbf{\textsf{Community -- Cộng đồng.}} {\sc Caroline Geiersbach}, {\sc Michael Hinterm\"uller}.

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item 
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Data Science (DS)}
\textbf{\textsf{Community -- Cộng đồng.}} 

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item 
\end{enumerate}

\subsection{Wikipedia{\tt/}Data Science}
``{\it Data science} is an \href{https://en.wikipedia.org/wiki/Interdisciplinary}{interdisciplinary} academic field that uses \href{https://en.wikipedia.org/wiki/Statistics}{statistics}, \href{https://en.wikipedia.org/wiki/Scientific_computing}{scientific computing}, \href{https://en.wikipedia.org/wiki/Scientific_method}{scientific methods}, processing, \href{https://en.wikipedia.org/wiki/Scientific_visualization}{scientific visualization}, \href{https://en.wikipedia.org/wiki/Algorithm}{algorithms} \& systems to extract or extrapolate \href{https://en.wikipedia.org/wiki/Knowledge}{knowledge} \& insights from potentially noisy, structured, or \href{https://en.wikipedia.org/wiki/Unstructured_data}{unstructured data}.

Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, \& medicine). Data science is multifaceted \& can be described as a science, a research paradigm, a research method, a discipline, a workflow, \& a profession.

Data science is ``a concept to unify statistics, \href{https://en.wikipedia.org/wiki/Data_analysis}{data analysis}, \href{https://en.wikipedia.org/wiki/Informatics}{informatics}, \& their related \href{https://en.wikipedia.org/wiki/Scientific_method}{methods}'' to ``understand \& analyze actual \href{https://en.wikipedia.org/wiki/Phenomena}{phenomena}'' with \href{https://en.wikipedia.org/wiki/Data}{data}. It uses techniques \& theories drawn from many fields within the context of mathematics, statistics, \href{https://en.wikipedia.org/wiki/Computer_science}{computer science}, \href{https://en.wikipedia.org/wiki/Information_science}{information science}, \& \href{https://en.wikipedia.org/wiki/Domain_knowledge}{domain knowledge}. However, data science is different from \href{https://en.wikipedia.org/wiki/Computer_science}{computer science} \& \href{https://en.wikipedia.org/wiki/Information_science}{information science}. \href{https://en.wikipedia.org/wiki/Turing_Award}{Turing Award} winner \href{https://en.wikipedia.org/wiki/Jim_Gray_(computer_scientist)}{Jim Gray} imagined data science as a ``4th paradigm'' of science (\href{https://en.wikipedia.org/wiki/Empirical_research}{empirical}, \href{https://en.wikipedia.org/wiki/Basic_research}{theoretical}, \href{https://en.wikipedia.org/wiki/Computational_science}{computational}, \& now data-driven) \& asserted that ``everything about science is changing because of the impact of \href{https://en.wikipedia.org/wiki/Information_technology}{information technology}'' \& the \href{https://en.wikipedia.org/wiki/Information_explosion}{data deluge}.

A {\it data scientist} is a professional who creates programming code \& combines it with statistical knowledge to create insights from data.

\subsubsection{Foundations}
Data science is an \href{https://en.wikipedia.org/wiki/Interdisciplinarity}{interdisciplinary} \href{https://en.wikipedia.org/wiki/Academic_discipline}{field} focused on \href{https://en.wikipedia.org/wiki/Knowledge_extraction}{extracting knowledge} from typically \href{https://en.wikipedia.org/wiki/Big_data}{large} \href{https://en.wikipedia.org/wiki/Data_set}{data sets} \& applying the knowledge \& insights from that data to \href{https://en.wikipedia.org/wiki/Problem-solving}{solve problems} in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, \href{https://en.wikipedia.org/wiki/Analysis}{analyzing} data, developing data-driven solutions, \& presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, \href{https://en.wikipedia.org/wiki/Data_visualization}{data visualization}, \href{https://en.wikipedia.org/wiki/Information_visualization}{information visualization}, \href{https://en.wikipedia.org/wiki/Data_sonification}{data sonification}, data \href{https://en.wikipedia.org/wiki/Data_integration}{integration}, \href{https://en.wikipedia.org/wiki/Graphic_design}{graphic design}, \href{https://en.wikipedia.org/wiki/Complex_systems}{complex systems}, \href{https://en.wikipedia.org/wiki/Communication}{communication} \& \href{https://en.wikipedia.org/wiki/Business}{business}. Statistician \href{https://en.wikipedia.org/wiki/Nathan_Yau}{Nathan Yau}, drawing on \href{https://en.wikipedia.org/wiki/Ben_Fry}{Ben Fry}, also links data science to \href{https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction}{human-computer interaction}: users should be able to intuitively control \& \href{https://en.wikipedia.org/wiki/Exploration}{explore} data. In 2015, the \href{https://en.wikipedia.org/wiki/American_Statistical_Association}{American Statistical Association} identified \href{https://en.wikipedia.org/wiki/Database}{database} management, statistics \& \href{https://en.wikipedia.org/wiki/Machine_learning}{machine learning}, \& \href{https://en.wikipedia.org/wiki/Distributed_computing}{distributed \& parallel systems} as the 3 emerging foundational professional communities.

\paragraph{Relationship to statistics.} Many statisticians, including \href{https://en.wikipedia.org/wiki/Nate_Silver}{Nate Silver}, have argued that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems \& techniques unique to digital data. \href{https://en.wikipedia.org/wiki/Vasant_Dhar}{Vasant Dhar} writes that statistics emphasizes quantitative data \& description. In contrast, data science deals with quantitative \& qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) \& emphasizes prediction \& action. \href{https://en.wikipedia.org/wiki/Andrew_Gelman}{Andrew Gelman} of Columbia University has described statistics as a non-essential part of data science.

Stanford professor \href{https://en.wikipedia.org/wiki/David_Donoho}{David Donoho} writes that data science is not distinguished from statistics by the size of datasets or use of computing \& that many graduate programs misleadingly advertise their analytics \& statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.

\subsubsection{Etymology}

\subsubsection{Data science \& data analysis}
Data science \& data analysis are both important disciplines in the field of \href{https://en.wikipedia.org/wiki/Data_management}{data management} \& analysis, but they differ in several key ways. While both fields involve working with data, data science is more of an \href{https://en.wikipedia.org/wiki/Interdisciplinary_field}{interdisciplinary field} that involves the application of statistical, computational, \& \href{https://en.wikipedia.org/wiki/Machine_learning}{machine learning} methods to extract insights from data \& make predictions, while data analysis is more focused on the examination \& interpretation of data to identify patterns \& trends.

Data analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. This can involve tasks such as \href{https://en.wikipedia.org/wiki/Data_cleaning}{data cleaning}, \href{https://en.wikipedia.org/wiki/Data_visualization}{data visualization}, \& exploratory data analysis to gain insights into the data \& develop hypotheses about relationships between \href{https://en.wikipedia.org/wiki/Variable_(research)}{variables}. Data analysts typically use statistical methods to test these hypotheses \& draw conclusions from the data. E.g., a \href{https://en.wikipedia.org/wiki/Data_analyst}{data analyst} might analyze sales data to identify trends in customer behavior \& make recommendations for marketing strategies.

Data science, on the other hand, is a more complex \& \href{https://en.wikipedia.org/wiki/Iterative}{iterative} process that involves working with larger, more complex datasets that often require advanced computational \& statistical methods to analyze. Data scientists often work with \href{https://en.wikipedia.org/wiki/Unstructured_data}{unstructured data} such as text or images \& use machine learning algorithms to build predictive models \& make data-driven decisions. In addition to \href{https://en.wikipedia.org/wiki/Statistical_analysis}{statistical analysis}, data science often involves tasks such as \href{https://en.wikipedia.org/wiki/Data_preprocessing}{data preprocessing}, \href{https://en.wikipedia.org/wiki/Feature_engineering}{feature engineering}, \& model selection. E.g., a data scientist might develop a recommendation system for an e-commerce platform by analyzing user behavior patterns \& using \href{https://en.wikipedia.org/wiki/Machine_learning_algorithms}{machine learning algorithms} to predict user preferences.

While data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development \& implementation of predictive models to make informed decisions. Data scientists are often responsible for collecting \& cleaning data, selecting appropriate analytical techniques, \& deploying models in real-world scenarios. They work at the intersection fo mathematics, computer science, \& \href{https://en.wikipedia.org/wiki/Domain_expertise}{domain expertise} to solve complex problems \& uncover hidden patterns in large datasets.

Despite these differences, data science \& data analysis are closely related fields \& often require similar skills sets. Both fields require a solid foundation in statistics, \href{https://en.wikipedia.org/wiki/Computer_programming}{programming}, \& \href{https://en.wikipedia.org/wiki/Data_visualization}{data visualization}, as well as the ability to communicate findings effectively to both technical \& non-technical audiences. Both fields benefit from \href{https://en.wikipedia.org/wiki/Critical_thinking}{critical thinking} \& \href{https://en.wikipedia.org/wiki/Domain_knowledge}{domain knowledge}, as understanding the context \& nuances of the data is essential for accurate analysis \& modeling.

In summary, data analysis \& data science are distinct yet interconnected disciplines within the broader field of \href{https://en.wikipedia.org/wiki/Data_management}{data management} \& analysis. Data analysis focuses on extracting insights \& drawing conclusions from \href{https://en.wikipedia.org/wiki/Structured_data}{structured data}, while data science involves a more comprehensive approach that combines \href{https://en.wikipedia.org/wiki/Statistical_analysis}{statistical analysis}, computational methods, \& machine learning to extract insights, build predictive models, \& drive data-driven \href{https://en.wikipedia.org/wiki/Decision-making}{decision-making}. Both fields use data to understand patterns, make informed decisions, \& solve complex problems across various domains.

\subsubsection{Cloud computing for data science}
\href{https://en.wikipedia.org/wiki/Cloud_computing}{Cloud computing} can offer access to large amounts of computational power \& \href{https://en.wikipedia.org/wiki/Data_storage}{storage}. In \href{https://en.wikipedia.org/wiki/Big_data}{big data}, where volumes of information are continually generated \& processed, these platforms can be used to handle complex \& resource-intensive analytical tasks.

Some distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process \& analyze large datasets in parallel, which can reducing processing times.

\subsubsection{Ethical consideration in data science}
Data science involve collecting, processing, \& analyzing data which often including personal \& sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, \& negative societal impacts.

Machine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.'' -- \href{https://en.wikipedia.org/wiki/Data_science}{Wikipedia{\tt/}data science}

%------------------------------------------------------------------------------%

\section{Deep Learning (DL)}
\textbf{\textsf{Community -- Cộng đồng.}} 

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{LeCun_Bengio_Hinton2015}. {\sc Yann LeCun, Yoshua Bengio, Geoffrey Hinton}. {\it Deep Learning}.
\end{enumerate}
Những năm gần đây, sự phát triển của các hệ thống tính toán cùng lượng dữ liệu khổng lồ được thu thập bởi các hãng công nghệ lớn đã giúp machine learning tiến thêm 1 bước dài. 1 lĩnh vực mới được ra đời được gọi là {\it học sâu} (deep learning, DL). Deep learning đã giúp máy tính thực thi những việc vào 10 năm trước tưởng chừng là không thể: phân loại cả ngàn vật thể khác nhau trong các bức ảnh, tự tạo chú thích cho ảnh, bắt chước giọng nói \& chữ viết, giao tiếp với con người, chuyển đổi ngôn ngữ, hay thậm chí cả sáng tác văn thơ \& âm nhạc.'' -- \cite[p. 15]{Tiep_ML_co_ban}

{\bf$\Phi$: Start with simple things -- Luôn bắt đầu từ những điều đơn giản.} Khi bắt tay vào giải quyết 1 bài toán ML hay bất cứ bài toán nào, nên bắt đầu từ các thuật toán đơn giản. Không phải chỉ có các thuật toán phức tạp mới có thể giải quyết được vấn đề. Các thuật toán phức tạp thường có yêu cầu cao về khả năng tính toán \& đôi khi nhạy cảm với cách chọn tham số. Ngược lại, các thuật toán đơn giản giúp ta nhanh chóng có 1 bộ khung cho mỗi bài toán. Kết quả của các thuật toán đơn giản cũng mang lại cái nhìn sơ bộ về sự phức tạp của mỗi bài toán. Việc cải thiện kết quả sẽ được thực hiện dần ở các bước sau. '' -- \cite[p. 17]{Tiep_ML_co_ban}

{\bf Approach.} Để giải quyết mỗi bài toán ML, cần chọn 1 mô hình phù hợp. Mô hình này được mô tả bởi bộ các tham số ta cần đi tìm. Thông thường, lượng tham số có thể lên tới hàng triệu \& được tìm bằng cách giải 1 bài toán tối ưu. Khi viết về các thuật toán ML, VKTiệp sẽ bắt đầu từ các ý tưởng trực quan. Các ý tưởng này được mô hình hóa dưới dạng 1 bài toán tối ưu. Các suy luận toán học \& ví dụ mẫu trên Python sẽ giúp hiểu rõ hơn về nguồn gốc, ý nghĩa, \& cách sử dụng mỗi thuật toán. Xen kẽ giữa các thuật toán ML, trình bày các kỹ thuật tối ưu cơ bản, với hy vọng giúp hiểu rõ hơn bản chất của vấn đề.

{\bf Audiences.} Cuốn sách được thực hiện hướng tới nhiều nhóm độc giả khác nhau. Nếu không thực sự muốn đi sâu vào phần toán, vẫn có thể tham khảo mã nguồn \& cách sử dụng các thư viện. Nhưng để sử dụng các thư viện 1 cách hiệu quả, cũng cần hiểu nguồn gốc của mô hình \& ý nghĩa của các tham số. Còn nếu thực sự muốn tìm hiểu nguồn gốc, ý nghĩa của các thuật toán, có thể học được nhiều điều từ cách xây dựng \& tối ưu các mô hình.

{\bf Python.} Python là 1 ngôn ngữ lập trình miễn phí, có thể được cài đặt dễ dàng trên các nền tảng hệ điều hành khác nhau. Có rất nhiều thư viện hỗ trợ ML cũng như DL trên Python. Có 2 thư viện Python chính thường được sử dụng là {\tt numpy, scikit-learn}.
\begin{itemize}
	\item {\tt numpy} \url{www.numpy.org} là 1 thư viện phổ biến giúp xử lý các phép toán liên quan đến các mảng nhiều chiều, hỗ trợ các hàm gần gũi với đại số tuyến tính. Cách xử lý các mảng nhiều chiều.
	\item {\tt scikit-learn/sklearn} \url{scikit-learn.org}: 1 thư viện chứa đầy đủ các thuật toán ML cơ bản \& rất dễ sử dụng. Tài liệu của scikit-learn cũng là 1 nguồn tham khảo chất lượng cho MLer. Scikit-learn được dùng để kiểm chứng các suy luận toán học \& các mô hình được xây dựng thông qua {\tt numpy}.
\end{itemize}
{\bf Inevitability of mathematics in ML.} Có rất nhiều thư viện giúp tạo ra các sản phẩm ML{\tt/}DL mà không yêu cầu nhiều kiến thức toán. Hướng tới việc giúp hiểu bản chất toán học đằng sau mỗi mô hình trước khi áp dụng các thư viện sẵn có. Việc sử dụng thư viện $+$ yêu cầu kiến thức nhất định về việc lựa chọn mô hình \& điều chỉnh các tham số.

%------------------------------------------------------------------------------%

\section{Machine Learning (ML)}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item {\sc Andrew Ng}. {\it Machine Learning Course} on Coursera.
	\item Machine Learning Mastery: Making Developers Awesome at Machine Learning: \url{https://machinelearningmastery.com}.
	\begin{itemize}
		\item \href{https://machinelearningmastery.com/inspirational-applications-deep-learning/}{Machine Learning Mastery{\tt/}8 Inspirational Applications of Deep Learning}.
	\end{itemize}
	\item Machine Learning cơ bản:
	 \url{https://machinelearningcoban.com/}.
	\item \cite{Tiep_ML_co_ban}. {\sc Vũ Hữu Tiệp}. {\it Machine Learning Cơ Bản}.
	
	Mã nguồn cuốn ebook ``Machine Learning Cơ Bản'': \url{https://github.com/tiepvupsu/ebookMLCB}.
\end{enumerate}

\begin{definition}
	``\emph{Machine learning (ML)} is a field of study in \href{https://en.wikipedia.org/wiki/Artificial_intelligence}{AI} concerned with the development \& study of \href{https://en.wikipedia.org/wiki/Computational_statistics}{statistical algorithms} that can learn from \href{https://en.wikipedia.org/wiki/Data}{data} \& generalize to unseen data, \& thus perform \href{https://en.wikipedia.org/wiki/Task_(computing)}{tasks} without explicit \href{https://en.wikipedia.org/wiki/Machine_code}{instructions}. Quick progress in the fields of \href{https://en.wikipedia.org/wiki/Deep_learning}{deep learning}, beginning in 2010s, allowed neural networks to surpass many previous approaches in performance.'' -- \href{https://en.wikipedia.org/wiki/Machine_learning}{Wikipedia{\tt/}machine learning}
\end{definition}

\begin{dinhnghia}
	``\emph{Học máy} (machine learning, ML) là 1 tập con của trí tuệ nhân tạo. Machine learning là 1 lĩnh vực nhỏ trong Khoa học Máy tính, có khả năng tự học hỏi dựa trên dữ liệu được đưa vào mà không cần phải được lập trình cụ thể: ``Machine Learning is the subfield of computer science, that ``gives computers the ability to learn without being explicitly programmed'' -- Wikipedia.'' -- \cite[p. 15]{Tiep_ML_co_ban}
\end{dinhnghia}
``ML finds application in many fields, including \href{https://en.wikipedia.org/wiki/Natural_language_processing}{natural language processing}, \href{https://en.wikipedia.org/wiki/Computer_vision}{computer vision}, \href{https://en.wikipedia.org/wiki/Speech_recognition}{speech recognition}, \href{https://en.wikipedia.org/wiki/Email_filtering}{email filtering}, \href{https://en.wikipedia.org/wiki/Agriculture}{agriculture}, \& \href{https://en.wikipedia.org/wiki/Medicine}{medicine}. The application of ML to business problems is known as \href{https://en.wikipedia.org/wiki/Predictive_analytics}{predictive analysis}.

Statistics \& mathematical optimization{\tt/}mathematical programming methods comprise the foundations of machine learning. \href{https://en.wikipedia.org/wiki/Data_mining}{Data mining} is related field of study, focusing on \href{https://en.wikipedia.org/wiki/Exploratory_data_analysis}{exploratory data analysis} (EDA) via \href{https://en.wikipedia.org/wiki/Unsupervised_learning}{unsupervised learning}.

From a theoretical viewpoint, \href{https://en.wikipedia.org/wiki/Probably_approximately_correct_learning}{probably approximately correct (PAC) learning} provides a framework for describing machine learning.'' -- \href{https://en.wikipedia.org/wiki/Machine_learning}{Wikipedia{\tt/}machine learning}

{\bf Relationships of ML to AI.} As a scientific endeavor, machine learning grew out of the quest for AI. In the early days of AI as an \href{https://en.wikipedia.org/wiki/Discipline_(academia)}{academic discipline}, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ``\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{neural networks}''; these were mostly \href{https://en.wikipedia.org/wiki/Perceptron}{perceptrons} \& other models e.g. \href{https://en.wikipedia.org/wiki/ADALINE}{ADALINE} that were later found to be reinventions of the \href{https://en.wikipedia.org/wiki/Generalized_linear_model}{generalized linear models} of statistics. \href{https://en.wikipedia.org/wiki/Probabilistic_reasoning}{Probabilistic reasoning} was also employed, especially in \href{https://en.wikipedia.org/wiki/Automated_medical_diagnosis}{automated medical diagnosis}. However, an increasing emphasis on the \href{https://en.wikipedia.org/wiki/Symbolic_AI}{logical, knowledge-based approach} caused a rift between AI \& machine learning. Probabilistic systems were plagued by theoretical \& practical problems of data acquisition \& representation.

\subsection{Wikipedia{\tt/}Machine Learning}
``{\it Machine learning} (ML) is a \href{https://en.wikipedia.org/wiki/Field_of_study}{field of study} in AI concerned with the development \& study of \href{https://en.wikipedia.org/wiki/Computational_statistics}{statistical algorithms} that can learn from \href{https://en.wikipedia.org/wiki/Data}{data} \& \href{https://en.wikipedia.org/wiki/Generalize}{generalize} to unseen data, \& thus perform \href{https://en.wikipedia.org/wiki/Task_(computing)}{tasks} without explicit \href{https://en.wikipedia.org/wiki/Machine_code}{instructions}. Quick progress in the field of \href{https://en.wikipedia.org/wiki/Deep_learning}{deep learning}, beginning in 2010s, allowed neutral networks to surpass many previous approaches in performance.

ML finds application in many fields, including \href{https://en.wikipedia.org/wiki/Natural_language_processing}{natural language processing}, \href{https://en.wikipedia.org/wiki/Computer_vision}{computer vision}, \href{https://en.wikipedia.org/wiki/Speech_recognition}{speed recognition}, \href{https://en.wikipedia.org/wiki/Email_filtering}{email filtering}, \href{https://en.wikipedia.org/wiki/Agriculture}{agriculture}, \& \href{https://en.wikipedia.org/wiki/Medicine}{medicine}. The application of ML to business problems is known as \href{https://en.wikipedia.org/wiki/Predictive_analytics}{predictive analytics}.

Statistics \& \href{https://en.wikipedia.org/wiki/Mathematical_optimization}{mathematical optimization{\tt/}programming} methods comprise the foundations of machine learning. \href{https://en.wikipedia.org/wiki/Data_mining}{Data mining} is a related field of study, focusing on \href{https://en.wikipedia.org/wiki/Exploratory_data_analysis}{exploratory data analysis} (EDA) via \href{https://en.wikipedia.org/wiki/Unsupervised_learning}{unsupervised learning}.

From a theoretical viewpoint, \href{https://en.wikipedia.org/wiki/Probably_approximately_correct_learning}{probably approximately correct (PAC) learning} provides a framework for describing machine learning.

\subsubsection{History}

\subsubsection{Relationships to other fields}

\paragraph{AI.} As a scientific endeavor, machine learning grew out of the quest for AI. In the early days of AI as an \href{https://en.wikipedia.org/wiki/Discipline_(academia)}{academic discipline}, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ``\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{neural networks}''; these were mostly \href{https://en.wikipedia.org/wiki/Perceptron}{perceptrons} \& \href{https://en.wikipedia.org/wiki/ADALINE}{other models ADALINE} that were later found to be reinventions of the \href{https://en.wikipedia.org/wiki/Generalized_linear_model}{generalized linear models} of statistics. \href{https://en.wikipedia.org/wiki/Probabilistic_reasoning}{Probablistic reasoning} was also employed, especially in \href{https://en.wikipedia.org/wiki/Automated_medical_diagnosis}{automated medical diagnosis}.

However, an increasing emphasis on the \href{https://en.wikipedia.org/wiki/Symbolic_AI}{logical, knowledge-based approach} caused a rift between AI \& machine learning. Probabilistic systems were plagued by theoretical \& practical problems of data acquisition \& representation. By 1980, \href{https://en.wikipedia.org/wiki/Expert_system}{expert systems} had come to dominate AI, \& statistics was out of favor. Work on symbolic{\tt/}knowledge-based learning did continue within AI, leading to \href{https://en.wikipedia.org/wiki/Inductive_logic_programming}{inductive logic programming} (ILP), but the more statistical line of research was now outside the field of AI proper, in \href{https://en.wikipedia.org/wiki/Pattern_recognition}{pattern recognition} \& \href{https://en.wikipedia.org/wiki/Information_retrieval}{information retrieval}. Neural networks research had been abandoned by AI \& computer science around the same time. This line, too, was continued outside the AI{\tt/}CS field, as ``\href{https://en.wikipedia.org/wiki/Connectionism}{connectionism}'', by researchers from other disciplines including \href{https://en.wikipedia.org/wiki/John_Hopfield}{John Hopfield}, \href{https://en.wikipedia.org/wiki/David_Rumelhart}{David Rumelhart}, \& \href{https://en.wikipedia.org/wiki/Geoffrey_Hinton}{Geoffrey Hinton}. Their main success came in the mid-1980s with the reinvention of \href{https://en.wikipedia.org/wiki/Backpropagation}{backpropagation}.

Machine learning (ML), reorganized \& recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving AI to tackling solvable problems of a practical nature. It shifted focus away from the \href{https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence}{symbolic approaches} it had inherited from AI, \& toward methods \& models borrowed from statistics, \href{https://en.wikipedia.org/wiki/Fuzzy_logic}{fuzzy logic}, \& \href{https://en.wikipedia.org/wiki/Probability_theory}{probability theory}.

\paragraph{Data compression.} Main: \href{https://en.wikipedia.org/wiki/Data_compression#Machine_learning}{Wikipedia{\tt/}data compression{\tt/}ML}. There is a close connection between ML \& compression. A system that predicts the \href{https://en.wikipedia.org/wiki/Posterior_probabilities}{posterior probabilities} of a sequence given its entire history can be used for optimal data compression (by using \href{https://en.wikipedia.org/wiki/Arithmetic_coding}{arithmetic coding} on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for ``general intelligence''.

An alternative view can show compression algorithms implicitly map strings into implicit \href{https://en.wikipedia.org/wiki/Feature_space_vector}{feature space vectors}, \& compression-based similarity measures compute similarity within these feature spaces. For each compressor $C(\cdot)$ we define an associated vector space $\mathfrak{N}$ s.t. $C(\cdot)$ maps an input string $x$, corresponding to the vector norm $\|\widetilde{x}\|$. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine 3 representative lossless compression methods, LZW, LZ77, \& PPM.

According to \href{https://en.wikipedia.org/wiki/AIXI}{AIXI} theory, a connection more directly explained in \href{https://en.wikipedia.org/wiki/Hutter_Prize}{Hutter Prize}, the best possible compression of $x$ is the smallest possible software that generates $x$. E.g., in that model, a zip file's compressed size includes both the zip file \& the unzipping software, since you cannot unzip it without both, but there may be an even smaller combined form.

Examples of AI-powered audio{\tt/}video compression software include \href{https://en.wikipedia.org/wiki/NVIDIA_Maxine}{NVIDIA Maxine}, AIVC. Examples of software that can perform AI-powered image compression include \href{https://en.wikipedia.org/wiki/OpenCV}{OpenCV}, \href{https://en.wikipedia.org/wiki/TensorFlow}{TensorFlow}, \href{https://en.wikipedia.org/wiki/MATLAB}{MATLAB}'s Imaging Processing Toolbox (IPT) \& High-Fidelity Generative Image Compression.

In \href{https://en.wikipedia.org/wiki/Unsupervised_machine_learning}{unsupervised machine learning}, \href{https://en.wikipedia.org/wiki/K-means_clustering}{$k$-mean clustering} can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels \& finds widespread use in fields such as \href{https://en.wikipedia.org/wiki/Image_compression}{image compression}.

Data compression aims to reduce the size of data files, enhancing storage efficiency \& speeding up data transmission. $k$-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, $k$, each represented by the \href{https://en.wikipedia.org/wiki/Centroid}{centroid} of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in \href{https://en.wikipedia.org/wiki/Image_processing}{image} \& \href{https://en.wikipedia.org/wiki/Signal_processing}{signal processing}, $k$-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.

\href{https://en.wikipedia.org/wiki/Large_language_model}{Large language models} (LLMs) are also capable of lossless data compression, as demonstrated by \href{https://en.wikipedia.org/wiki/DeepMind}{DeepMind}'s research with the Chinchilla 70B model. Developed by DeepMind, Chinchilla 70B effectively compressed data, outperforming conventional methods such as \href{https://en.wikipedia.org/wiki/Portable_Network_Graphics}{Portable Network Graphics} (PNG) for images \& \href{https://en.wikipedia.org/wiki/Free_Lossless_Audio_Codec}{Free Lossless Audio Codec} (FLAC) for audio. It achieved compression of image \& audio data to 43.4\% \& 16.4\% of their original sizes, respectively.

\paragraph{Data mining.} ML \& \href{https://en.wikipedia.org/wiki/Data_mining}{data mining} often employ the same methods \& overlap significantly, but while ML focuses on prediction, based on {\it known} properties learned from the training data, data mining focuses on the \href{https://en.wikipedia.org/wiki/Discovery_(observation)}{discovery} of (previously) {\it unknown} properties in the data (this is the analysis step of \href{https://en.wikipedia.org/wiki/Knowledge_discovery}{knowledge discovery} in databases). Data mining uses many ML methods, but with different goals; on the other hand, ML also employs data mining methods as ``\href{https://en.wikipedia.org/wiki/Unsupervised_learning}{unsupervised learning}'' or as a preprocessing step to improve learner accuracy. Much of the confusion between these 2 research communities (which do often have separate conferences \& separate journals, \href{https://en.wikipedia.org/wiki/ECML_PKDD}{ECML PKDD} being a major exception) comes from the basic assumptions they work with: in ML, performance is usually evaluated w.r.t. the ability to {\it reproduce known} knowledge, while in knowledge discovery \& data mining (KDD) the key task is the discovery of previously {\it unknown} knowledge. Evaluated w.r.t. known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.

ML also has intimate ties to optimization: Many learning problems are formulated as minimization of some \href{https://en.wikipedia.org/wiki/Loss_function}{loss function} on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained \& the actual problem instances (e.g., in classification, one wants to assign a \href{https://en.wikipedia.org/wiki/Labeled_data}{label} to instances, \& models are trained to correctly predict the preassigned labels of a set of examples).

\paragraph{Generalization.} Characterizing the generalization of various learning algorithms is an active topic of current research, especially for \href{https://en.wikipedia.org/wiki/Deep_learning}{deep learning} algorithms.

\paragraph{Statistics.} ML \& statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population \href{https://en.wikipedia.org/wiki/Statistical_inference}{inferences} from a \href{https://en.wikipedia.org/wiki/Sample_(statistics)}{sample}, while ML finds generalizable predictive patterns. According to \href{https://en.wikipedia.org/wiki/Michael_I._Jordan}{Michael I. Jordan}, the ideas of ML, from methodological principles to theoretical tools, have had a long pre-history in statistics. he also suggested the term data science as a placeholder to call the overall field.

Conventional statistical analyzes require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, ML is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.

\href{https://en.wikipedia.org/wiki/Leo_Breiman}{Leo Breiman} distinguished 2 statistical modeling paradigms: data model \& algorithmic model, wherein ``algorithm model'' means more or less the ML algorithms like \href{https://en.wikipedia.org/wiki/Random_forest}{Random Forest}.

Some statisticians have adopted methods from ML, leading to a combined field that they call {\it statistical learning}.

\paragraph{Statistical physics.} Analytical \& computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including ML, e.g., to analyze the weight space of \href{https://en.wikipedia.org/wiki/Deep_neural_network}{deep neural networks}. Statistical physics is thus finding applications in the area of \href{https://en.wikipedia.org/wiki/Medical_diagnostics}{medical diagnostics}.

\subsubsection{Theory}
Main: \href{https://en.wikipedia.org/wiki/Computational_learning_theory}{Wikipedia{\tt/}computational learning theory} \& \href{https://en.wikipedia.org/wiki/Statistical_learning_theory}{Wikipedia{\tt/}statistical learning theory}. A core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples{\tt/}tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) \& the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.

The computational analysis of ML algorithms \& their performance is a branch of \href{https://en.wikipedia.org/wiki/Theoretical_computer_science}{theoretical computer science} known as \href{https://en.wikipedia.org/wiki/Computational_learning_theory}{computational learning theory} via the \href{https://en.wikipedia.org/wiki/Probably_approximately_correct_learning}{Probably Approximately Correct Learning} (PAC) model. Because training sets are finite \& the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The \href{https://en.wikipedia.org/wiki/Bias%E2%80%93variance_decomposition}{bias-variance decompositon} is 1 way to quantify generalization \href{https://en.wikipedia.org/wiki/Errors_and_residuals}{error}.

For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of he function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to \href{https://en.wikipedia.org/wiki/Overfitting}{overfitting} \& generalization will be poorer.

In addition to performance bounds, learning theorists study the time complexity \& feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in \href{https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time}{polynomial time}. There are 2 kinds of \href{https://en.wikipedia.org/wiki/Time_complexity}{time complexity} results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time. 

\subsubsection{Approaches}
ML approaches are traditionally divided into 3 broad categories, which corresponding to learning paradigms, depending on the nature of the ``signal'' or ``feedback'' available to the learning system:
\begin{itemize}
	\item \href{https://en.wikipedia.org/wiki/Supervised_learning}{Supervised learning}: The computer is presented with example inputs \& their desired outputs, given by a ``teacher'', \& the goal to learn a general rule that maps inputs to outputs.
	\item \href{https://en.wikipedia.org/wiki/Unsupervised_learning}{Unsupervised learning}: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means toward an end (\href{https://en.wikipedia.org/wiki/Feature_learning}{feature learning}).
	\item \href{https://en.wikipedia.org/wiki/Reinforcement_learning}{Reinforcement learning}: A computer program interacts with a dynamic environment in which it must perform a certain goal (e.g. \href{https://en.wikipedia.org/wiki/Autonomous_car}{driving a vehicle} or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize.
\end{itemize}
Although each algorithm has advantages \& limitations, no single algorithm works for all problems.

\paragraph{Supervised learning.}

\paragraph{Semi-supervised learning.}

\paragraph{Reinforcement learning.}

\paragraph{Dimensionality reduction.}

\paragraph{Other types.}

'' -- \href{https://en.wikipedia.org/wiki/Machine_learning}{Wikipedia{\tt/}machine learning}

%------------------------------------------------------------------------------%

\section{Artificial Intelligence (AI)}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Bac_Viet_AI}. {\sc Lê Hoài Bắc, Tô Hoài Việt}. {\it Cơ Sở Trí Tuệ Nhân Tạo}.
	\item \cite{Aoun_robot-proof}. {\sc Joseph E. Aoun}. {\it Robot-Proof: Higher Education in the Age of Artificial Intelligence}.
	\item \cite{Aoun_robot-proof_VN}. {\sc Joseph E. Aoun}. {\it Robot-Proof: Higher Education in the Age of Artificial Intelligence -- Chạy Đua Với Robot: Học Tập Thời Trí Tuệ Nhân Tạo}.
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}