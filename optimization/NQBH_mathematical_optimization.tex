\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{goal}{Goal}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Mathematical Optimization -- Toán Tối Ưu}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:

	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.

	Latest version:
	\begin{itemize}
		\item {\it Mathematical Optimization -- Toán Tối Ưu}.

		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/optimization/NQBH_mathematical_optimization.pdf}.

		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/optimization/NQBH_mathematical_optimization.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Adjoint}
2 main types: discrete adjoint, continuous adjoint.

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item {\sc Tan Bui-Thanh}. {\sc Adjoint \& Its roles in Sciences, Engineering, \& Mathematics: A Tutorial}.

	{\sf Abstract.} Synergize roles of adjoint in various disciplines of mathematics, sciences, \& engineering. Though materials developed \& presented here are not new -- as each or some could be found in (or inferred from) publications in different fields -- believe: 1st effort to systematically unify these materials on the same mathematical footing starting from the basic defs. Aim: provide a unified perspective \& understanding of adjoint applications. As a result, this work could give broader views \& better insights into the application of adjoint beyond a single community. By rigorously establishing general results \& then developing materials specific to each application, bring forth details on how abstract concepts{\tt/}defs can be translated into particular applications \& connections among them. This paper is written as an interdisciplinary tutorial on adjoint with discussions \& with many examples from different fields including linear algebra (e.g., eigendecomposition \& SVD), ODEs (asymptotic stability of an epidemic model), PDEs (well-posedness of elliptic, hyperbolic, \& Friedrichs' types), neural networks (backpropagation of feed-forward deep neural networks), least squares \& inverse problems (with Tikhonov regularization), \& PDE-constrained optimization. Exposition covers results \& applications in both finite-dimensional \& infinite-dimensional Hilbert spaces.

	{\sf Keywords.}Adjoint, optimization, backpropagation, eigenvalue problem, singular value decomposition, asymptotic stability, wellposedness, least squares, PDE-constrained optimization, reproduction number.
	\begin{itemize}
		\item {\sf1. Introduction.} Adjoint is ubiquitous in mathematics. History of adjoint can be traced back to as far as {\sc Lagrange} in 1766, in a memoir extending the letter that he wrote to {\sc D'Alembert} in Jun 1765 discussing his new method of solving $n$th-order differential equations. Term ``adjoint equation'', 1st used to call corresponding equation developed from Lagrange memoir, is due to {\sc Fuchs}. Adjoint operator was introduced by {\sc Riesz} in his seminal paper ({\sc Riesz} also established {\it functional analysis} as a new mathematical discipline) to study inverse of linear operators.

		Since then adjoint has been pervasive in vast literature across mathematics, engineering, \& sciences disciplines. This is not surprising as adjoint has many appealing features including:
		\begin{enumerate}
			\item adjoint operator typically possesses nicer properties than the original operator (e.g. adjoint of a densely defined linear operator is a closed operator though the original operator may not)
			\item adjoint equation is always linear even when the original equation is not.
		\end{enumerate}
		Though a comprehensive survey on adjoint accounting for its application in many disciplines{\tt/}fields (\& their sub-disciplines) could be desirable to appreciate crucial role that adjoint plays, it is perhaps an impossible task. Or more precisely, it is rather the task for a book than for a paper.
		\begin{goal}
			Provide a window into the adjoint \& its crucial role in certain subsets of computational science, engineering, \& mathematics.
		\end{goal}
		Exposition is necessarily personal \& biased based on topics familiar with. Materials developed \& presented are not new, as each or some could be found in (or inferred from) publications in different fields.
		\begin{goal}
			Systematically unify these materials on the same mathematical footing starting from basic defs.
		\end{goal}
		This expectantly provides a more unified perspective on usefulness of adjoint in variety of applications. As a result, this work could give broader views \& better insights into applications of adjoint beyond 1 field. By establishing general results \& then developing materials specific to each application, bring forth details on how abstract concepts{\tt/}defs can be translated into particular applications \& connections among them. This paper is written as a tutorial on adjoint with many examples presented with discussions. Though trike for a self-contained expositor, necessary to state a few results without proof to keep length of paper manageable \& to focus on adjoint \& its roles.

		{\sf Structure.}
		\begin{itemize}
			\item Sect. 2: Introduce various notations, defs, \& some examples.
			\item Sect. 3: Part I in finite dimensions. Start with celebrated Riesz representation theorem that is then used to prove existence of adjoints of continuous linear operators. Followed by closed range theorem that will be useful in many places later. Built upon these basic materials, shall develop several applications of adjoint.
			\begin{itemize}
				\item Subsect. 3.1: highlight role of adjoint in assessing solvability of linear operator equations before solving them.
				\item Subsect. 3.2: 1 of most important applications of adjoint is in study of eigenvalue problems. Main result: spectral decomposition of self-adjoiont operators in finite dimensions. Tight relationship between orthogonality of a projection \& its self-adjointness, immediately leading to a generalized Pythagorean theorem.
				\item Subsect. 3.3: start with classical projection theorem \& then deploy it together with close range theorem to find necessary \& sufficient condition for optimality of an abstract linear least squares problem.
				\item Subsect. 3.4: Next important application: SVD, in which employ spectral decomposition to establish an SVD decomposition for general linear operator in finite dimensions. This SVD decomposition is then deployed to provide trivial proofs for closed range theorem, rank-nullity theorem, \& fundamental theorem of linear algebra for abstract linear operators. Discuss equivalence of SVD of an abstract linear operator \& SVD of its matrix representation.
				\item Subsect. 3.5: optimization with equality constraints. Expose at length role of adjoint in optimization theory valid for both finite \& infinite dimensions. Accomplished by working with Fr\'echet derivative \& its Riesz representation counterpart as the gradient. Though can be further developed (e.g. to 2nd-order optimality conditions) focus on 1st-order optimality condition. Recall an implicit function theorem \& use it to prove an abstract inverse function theorem, then deployed to derive 1st-order optimality condition for abstract optimization problems with equality constraints. Important role of adjoint comes into picture when prove an abstract Lagrangian multiplier theorem using closed range theorem. Importance of adjoint is further amplified when study constrained optimization problems with separable structure. Here, adjoint facilitates an efficient gradient-based optimization algorithm in unconstrained reduced subspace while ensuring feasibility of constraints at all time.
				\item Subsect. 3.6: show that when applying this reduced space approach for optimization problem arisen from training deep neural networks (DNNs), recover backpropagation from reduced space approach provides further insights into algorithm.
				\item Subsect. 3.7: stability of autonomous ODEs. Main goal: exhibit vital role of adjoint in establishing necessary \& sufficient conditions for asymptotic stability (in sense of Lyapunov) of ODEs' equilibria.
			\end{itemize}
			\item Sect. 4: Part II in infinite dimensions. Start with a more general adjoint definition for densely defined linear operators. Useful for all subsects. except Subsect. 4.1.
			\begin{itemize}
				\item Subsect. 4.1: illposedness (in Hadamard's sense) nature of inverting compact operators. Extend spectral theorem \& SVD decomposition theorem to Hilbert-Schmidt theorem \& a general SVD theorem for compact (linear) operators in infinite dimensions. Main result: a Picard theorem stating necessary \& sufficient conditions under which task of inverting a compact operator is solvable. Often employed to show that inverting a compact operator violates at least stability condition of well-posedness. Deploy Riesz-Fredholm theory to show how Tikhonov regularization can restore well-posedness at expense of getting a nearby solution.
				\item Subsect. 4.2: Discuss role of adjoint in establishing well-posedness of abstract linear operator equations with application to PDEs. 2 main results developed: Banach-Nečas-Babuška theorem, Lax-Milgram lemma.
				\item Subsect. 4.3: study Sturm-Liouville eigenvalue problem \& generalized Fourier series in $L^2$. For closed linear operators. Using Hilbert-Schmidt theorem \& Lax-Milgram lemma, establish a spectral decomposition theorems for quite general linear operators, \& then apply them to Sturm-Liouville eigenvalue problems to derive Fourier series \& its generalizations.
				\item Subsect. 4.4: PDE-constrained optimization. Show how to rigorously translate abstract Lagrangian multiplier theorem to derive adjoint equation \& reduced gradient for prototype elliptic \& hyperbolic PDEs. Show: differential operators of adjoint equations are indeed adjoint operators derived at beginning of Sect. 4.
			\end{itemize}
		\end{itemize}
		To keep exposition succinct, defs \& results valid for both cases are presented{\tt/}proved once \& when that happens we will explicitly state so. Most of our developments start from abstract operator settings \& then reduce to standard finite-dimensional settings in $\mathbb{R}^d$ as a special case. In some cases, e.g. optimization, order is reversed as we believe it is more natural that way. Each sect. of paper is equipped with examples on which we show how to apply preceding abstract theoretical results. Make an effort to include practical examples from different fields including linear algebra (e.g., eigendecomposition \& SVD), ODEs (an epidemic model), PDEs (of elliptic, hyperbolic, \& Friedrichs' types), neural networks (feed-forward deep neural networks), least squares \& inverse problems (with Tikhonov regularization), PDE-constrained optimization, etc. Due to interdisciplinary nature of paper, do not attempt to provide an exhaustive list of refs but a few for each sect. to keep references at a manageable length.

		\item {\sf2. Notations.} boldface lower case letters, e.g., ${\bf u}$: vector-valued functions in $\mathbb{R}^d$. Calligraphic uppercase letters, e.g., $\mathcal{A}$: matrices, script uppercase letters: operators. Bold blackboard upper cases: spaces \& sets. Lowercase letters: scalar-valued functions, also for results valid for both finite \& infinite dimensional settings. Bold uppercase letters: bases of vector spaces. Frequently identify dual of any Hilbert space with itself. $\theta$ denotes either ``zero'' function or ``zero'' vector in appropriate space.

		Defs: Linear transformation{\tt/}map{\tt/}operator \& its domain, range, kernel{\tt/}null space. Any matrix is a linear operator. Integrals are operators: $\mathcal{A}f = \int_0^1 \omega(t)f(t)\,{\rm d}t$. Differentiation is a linear operator: $\mathcal{A}u = \frac{d^2}{dt^2}u(t)$.

		\item {\sf3. Part I: Adjoint operators in finite dimensional Hilbert spaces.} Assume $X,Y$ are finite dimensional vector spaces, $\dim X = n < \infty,\dim Y = m < \infty$. If $A\in L(X,Y)$ \& $\dim X < \infty$, then $A\in\mathcal{B}(X,Y)$. Let $E = \{e_i\}_{i=1}^n,G = \{g_i\}_{i=1}^m$ be orthonormal bases for $X,Y$, resp. $\forall u\in X$, denote by $u^E$ the unique vector of coordinates of $u$ in $E$, then $(u^E,v^E)_{\mathbb{F}^d} = (u,v)_X$. Matrix representation of $A$ w.r.t. bases $E,G$ is denoted as $A^{EG}$. When there is no ambiguity on bases referred to, simply ignore superscripts for both coordinate vector \& matrix representation. Denote $i$th element of a vector ${\bf u}$ as ${\bf u}(i)$ \& element at $i$th row \& $j$th column of a matrix $A$ as $A(i,j)$. Also use ${\bf u}_i\coloneqq{\bf u}(i)$. Use square brackets to express matrices \& vectors with a finite number of components. Unless otherwise stated, vectors with finite number of components are column vectors. Organize: celebrated Riesz representation theorem \& closed range theorem, upon which develop several applications of adjoint.

		\begin{theorem}[Riesz representation]
			Let $L$ be a bounded linear functional on a Hilbert space $X$. There exists a unique $u\in X$ s.t. $L(v) = (u,v)_X$, $\forall v\in X$. Furthermore, operator norm of $L$ is given as $\|L\|\coloneqq\sup_{v\in X} \frac{|L(v)|}{\|v\|_X} = \|u\|_X$.
		\end{theorem}

		\begin{definition}[Adjoint operator]
			Let $A\in B(X,Y)$. Say that $A^\star:Y\to X$ is the adjoint of $A$ iff $(Au,v)_Y = (u,A^\star v)_X$, $\forall u\in X,v\in Y$.
		\end{definition}

		\begin{proposition}
			Let $A\in B(X,Y)$. Then $A^\star$ exists \& is unique. Furthermore, it is linear with $\|A^\star\| = \|A\|$, where operator norm is defined as $\|A\|\coloneqq\sup_{u\in X} \frac{\|Au\|_Y}{\|u\|_X} = \sup_{\|u\|_X = 1} \|Au\|_Y$.
		\end{proposition}
		An $M$-weighted inner product $(\cdot,\cdot)_{\mathbb{R}^d,M}$ where $({\bf u},{\bf v})_{\mathbb{R}^d,M}\coloneqq\sum_{i,j} {\bf u}(i)M(i,j){\bf v}(i)\coloneqq{\bf u}^TM{\bf v}$, $\forall{\bf u},{\bf v}\in V$, $M$: a symmetric \& positive definite matrix. Adjoint operator $A^\star = A^TM$ of a matrix $A$. Set $\mathbb{P}^n[0,1]$ of complex-valued polynomial of order $\le n$ on $[0,1]$.

		\begin{proposition}
			Let $E,G$ be orthonormal bases of $X,Y$, resp., \& $\dim X = n,\dim Y = m$. Let $A,B$ be the matrix representations of $A,A^\star$ w.r.t. bases $E,G$. Then $B = A^\star$ where $A^\star$ be the conjugate transpose of $A$.
		\end{proposition}
		Orthogonal complement $S^\bot$ of $S\subset X$ is a closed subspace of $X$ \& $S\cap S^\bot =\{0\}$. Close $\overline{S}$ of $S$ is the smallest closed set containing $S$. $(S^\bot)^\bot = \overline{S}$.

		\begin{theorem}[Closed range]
			Let $A:X\to Y$. $[{\rm R}(A)]^\bot = {\rm N}{A^\star},\overline{{\rm R}(A)} = [N(A^\star)]^\bot,[{\rm R}(A^\star)]^\bot = {\rm N}(A),\overline{{\rm R}(A^\star)} = [{\rm N}(A)]^\bot$. Consequently, $X = {\rm N}(A)\oplus\overline{{\rm R}(A^\star)}$, $Y = {\rm N}(A^\star)\oplus\overline{{\rm R}(A)}$.
		\end{theorem}
		Since consider only Hilbert spaces, which are reflexive, $\overline{{\rm R}(A^\star)} = [{\rm N}(A)]^\bot$ holds. In general $\overline{{\rm R}(A^\star)}\subset[{\rm N}(A)]^\bot$. For finite dimensional vector spaces $X,Y$, ${\rm R}|(A)$, \& hence ${\rm R}(A^\star)$, is obviously closed. Proof of closed range theorem for finite dimensional cases is trivial using SVD decomposition.

		\begin{itemize}
			\item {\sf Subsect. 3.1: Application of adjoint to solvability of linear operator equations.} Solvability of linear operator equations before solving them. Existence of a solution of linear operator equation $Au = f$ where $A:X\to Y$.

			\begin{lemma}
				\item(i) Existence: The linear equation $Au = f$ has a solution iff $y\in{\rm N}(A^\star)^\bot$.
				\item(ii) Uniqueness: The solution of the linear equation $Au = f$ is unique iff ${\rm N}(A) = \{0\}$.
				\item(iii) If $\dim X = \dim Y$, uniqueness $\Leftrightarrow$ existence.
			\end{lemma}
			Existence condition can be simply $y\in{\rm R}(A)$. However, easier to work with ${\rm N}(A^\star)^\bot$ as it gives us equations to determine{\tt/}characterize ${\rm N}(A^\star)^\bot$.

			An operator setting for problem of fitting a quadratic polynomial $u(x)$ with 2 pieces of information about $u(x)$.

			\item {\sf Subsect. 3.2: Application of adjoint to eigenvalue problems.} Role of adjoint in study of eigenvalue problems.

			\begin{definition}[Eigenvalue problem]
				Let $A:X\to X$ be a linear operator. $Au = \lambda u$ is called an \emph{eigenvalue problem} if there exists a nontrivial pair $(\lambda,x)$ ($x$ is not a zero vector{\tt/}function but $\lambda$ could be zero). In particular: $\lambda$ is called an \emph{eigenvalue}. $x$ is called an \emph{eigenfunction}, associated with eigenvalue $\lambda$, of $A$. If $X$ is a finite-dimensional space, i.e., $X = \mathbb{R}^d$, $x$ is typically called \emph{eigenvector}.
			\end{definition}

			\begin{definition}[Self-adjoint operator]
				If $A^\star = A$, then $A$ is called \emph{self-adjoint}.
			\end{definition}

			\begin{lemma}
				Let $A:X\to X$ be a linear operator \& $A$ is self-adjoint. Then:
				\item(i) Eigenvalues of $A$ are real.
				\item(ii) Eigenfunctions corresponding to distinct eigenvalues are orthogonal to each other. I.e., if $(\lambda,u)$ \& $(\alpha,v)$ are 2 eigen-pairs \& $\lambda\ne\alpha$ then $(u,v)_X = 0$.
			\end{lemma}

			\begin{proposition}
				Let $A:X\to X$ be a linear operator. Then $A$ has at least 1 eigenvalue.
			\end{proposition}

			\begin{theorem}
				Let $A:X\to X$ be a self-adjoint linear operator. Then, an orthonormal basis of $X$ can be constructed from eigenfunctions of $A$.
			\end{theorem}

			\begin{theorem}[Spectral decomposition of self-adjoint operators in finite dimensions]
				Let $\dim X = n$ \& $A:X\to X$ be a linear \& self-adjoint operator. There exists $n$ real values $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_n$ \& orthonormal vectors $u_1,u_2,\ldots,u_n$ s.t. $Au_i = \lambda_iu_i$. $\forall x\in X$, $Ax = \sum_{i=1} \lambda_i(x,u_i)_Xu_i\Rightarrow A = \sum_{i=1}^n \lambda_i(\cdot,u_i)_Xu_i$, i.e., $A$ is completely determined by its eigenpairs.
			\end{theorem}

			\item {\sf Subsect. 3.3.} Employ classical projection theorem $+$ closed range theorem to find necessary \& sufficient condition for optimality of an abstract linear least squares problem.

			\item {\sf Subsect. 3.4.} Singular value decomposition (SVD). Deploy SVD decomposition to provide trivial proofs for closed range theorem, rank-nullity theorem, \& fundamental theorem of linear algebra for abstract linear operators.

			\item {\sf Subsect. 3.5.} Optimization with equality constraints, expose at length role of adjoint in optimization theory valid for both finite \& infinite dimensions.

			\item {\sf Subsect. 3.6: Application of adjoint to backpropagation in deep learning.} A reduced spaced approach using adjoint reduces to backpropagation of deep neural networks. Consider standard fully connected deep neural network \& use adjoint method to derive backpropagation method for computing gradient of loss function w.r.t. weights \& biases of a general fully-connected deep neural work (DNN). Review papers on deep learning \cite{LeCun_Bengio_Hinton2015}, [69], history of backpropagation: [54,55]. Gradient is needed for gradient-based methods e.g. stochastic gradient descent. Extension of adjoint method for other type of neural networks e.g. ResNet \& CNN are straightforward. Backpropagation is nothing more than a reduced space approach to compute gradient using gradient method.

			\begin{definition}[$L$-layer Neural network]
				Given $n_l,s_0,s_1,\ldots,s_{n_l}\in\mathbb{N}$, an $n_l$-layer neural network is defined as the following series of composition: Input layer: ${\bf a}^0 - {\bf x} = {\bf0}$, The $i$th layer: ${\bf a}^i - \sigma(\mathcal{W}^i{\bf a}^{i-1} + {\bf b}^i) = {\bf0}$, $i = 1,\ldots,n_l$, where ${\bf x}\in\mathbb{R}^{s_0},\mathcal{W}^i\in\mathbb{R}^{s_i}\times\mathbb{R}^{s_i - 1}$, ${\bf b}^i\in\mathbb{R}^{s_i}$, $i = 1,\ldots,n_l$, are weight matrix \& bias vector of the $i$th layer; ${\bf a}^i\in\mathbb{R}^{s_i}$ is the output of the $i$th layer; \& the activation function $\sigma$ acts componentwise when its argument is a vector.
			\end{definition}
			Define ${\bf u}\coloneqq[{\bf a}^0,\ldots,{\bf a}^{n_l}]^T,{\bf z}\coloneqq[\mathcal{W}^1,{\bf b}^1,\ldots,W^{n_l},{\bf b}^{n_l}]^T,{\bf c}({\bf u},{\bf z}) = {\bf0}$ as concatenation of all subequation. For concreteness, consider loss (objective) function $f({\bf u},{\bf z})\coloneqq\frac{1}{2}\|{\bf a}^{\rm obs} - {\bf a}^{n_l}\|^2$ where ${\bf a}^{\rm obs}$ is a given data (label). Neural network training problem is constrained optimization problem in Ex 3.59.

			Using adjoint equations, backpropagate to compute adjoint solution from output layer to $i$th layer, \& then compute gradients. From backpropagation point of view, ${\bf y}^i,i = 1,\ldots,n_l$ are simply temporary variables to help compute{\tt/}write chain rule in a succinct manner. The adjoint approach, however, reveals their precise role as adjoint solutions -- also known as Lagrangian multiliers -- of adjoint equations stemming from 1st-order optimality condition using reduced space approach. DNN training problem, from adjoint point of view, is a constrained optimization problem with forward pass as forward equations. The backpropagation is thus nothing more than a reduced space approach to compute gradient using adjoint method.

			\item {\sf Subsect. 3.7.} role of adjoint in establishing stability of autonomous ODEs. A brief view of role of adjoint in study of stability of equilibria of ODEs. Limit to autonomous systems of form $\dot{\bf x}\coloneqq\frac{d{\bf x}}{dt} = {\bf f}({\bf x})$ where ${\bf x}\in\mathbb{R}^d,{\bf f}:G\subset\mathbb{R}^d\to\mathbb{R}^d$ is assumed to be continuous \& locally Lipschitz.

			\begin{definition}[Lyapunov stability]
				The equilibrium point ${\bf0}$ is stable in the sense of Lyapunov if for any $\varepsilon > 0,\exists\delta > 0$ s.t. for every (maximal) solution ${\bf x}:I\to G$ s.t. ${\bf x}(0)\le\delta$, have ${\bf x}(t)\le\varepsilon$, $\forall t\in I\cap(0,\infty)$.
			\end{definition}

			\begin{theorem}[Lyapunov direct method]
				If there exists an open neighborhood $U$ of ${\bf0}$ \& a continuous differentiable function $V$ s.t.
				\item(i) $V({\bf0}) = 0$, $V({\bf z}) > 0$, $\forall{\bf z}\in U\backslash\{{\bf 0}\}$, \&
				\item(ii) $V_{\bf f}({\bf z})\coloneqq(\nabla V({\bf z}),{\bf f}({\bf z}))\coloneqq\sum_{i=1}^n \partial_{{\bf z}_i}V {\bf f}_i({\bf z})\le0$, $\forall{\bf z}\in U$.

				Then $\bf0$ is a stable equilibrium point of $\dot{\bf x}\coloneqq\frac{d{\bf x}}{dt} = {\bf f}({\bf x})$.
			\end{theorem}

			\begin{definition}[Asymptotic stability]
				The equilibrium $\bf0$ is \emph{attractive} if there exists $\delta > 0$ s.t. $\forall{\bf x}_0\in G$ s.t. $\|{\bf x}_0\|\le\delta$, then the solution ${\bf x}(t)\to{\bf0}$ as $t\to\infty$. Say $\bf0$ \emph{asymptotically stable} in the sense of Lyapunov if it is both stable \& attractive.
			\end{definition}

			\begin{theorem}[A sufficient condition for asymptotic stability]
				Assume that there exists a neighborhood $U$ of ${\bf x}_0$ \& a continuously differentiable function $V$ s.t.
				\item(i) $V({\bf0}) = 0$, $V({\bf z}) > 0$, $\forall{\bf z}\in U\backslash\{{\bf 0}\}$, \& $V_{\bf f}\le0$, $\forall{\bf z}\in U$, \&
				\item(ii) $\bf0$ is the inverse image of $V_{\bf f}({\bf z}) = 0$, i.e., $V_{\bf f}^{-1}(0) = {\bf0}$.

				Then $\bf0$ is asymptotically stable.
			\end{theorem}
			Study stability of systems of linear ODEs, \& this is where the adjoint comes into picture. Infer stability of nonlinear systems using stability of their linearizations.

		\end{itemize}
		\item {\sf4. Part II: Adjoint operators in Infinite dimensional Hilbert spaces.}
		\item {\sf5. Conclusions.}
	\end{itemize}
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Algorithms for Optimization -- Thuật Toán Tối Ưu}

\subsection{\cite{Kochenderfer_Wheeler2019}. {\sc Mykel J. Kochenderfer, Tim A. Wheeler}. Algorithms for Optimization. 2019}
{\sf[118 Amazon ratings]}

{\sf Amazon review.} A comprehensive introduction to optimization with a focus on practical algorithms for design of engineering systems.

This book offers a comprehensive introduction to optimization with a focus on practical algorithms. Book approaches optimization from an engineering perspective, where objective: design a system that optimizes a set of metrics subject to constraints. Readers will learn about computational approaches for a range of challenges, including searching high-dimensional spaces, handling problems where there are multiple competing objectives, \& accommodating uncertainty in metrics. Figures, examples, \& exercises convey intuition behind mathematical approaches. Text provides concrete implementation in Julia programming language.

Topics covered include derivatives \& their generalization to multiple dimensions; local descent \& 1st- \& 2nd-order methods that inform local descent; stochastic methods, which introduce randomness into optimization process; linear constrained optimization, when both objective function \& constraints are linear; surrogate models, probabilistic surrogate models, \& using probabilistic surrogate models to guide optimization; expression optimization, \& multidisciplinary design optimization. Appendixes offer an introduction to Julia language, test functions for evaluating algorithm performance, \& mathematical concepts used in derivation \& analysis of optimization methods discussed in text. Book can be used by advanced undergraduates \& graduates students in mathematics, statistics, CS, any engineering field, (including electrical engineering \& aerospace engineering), \& operations research, \& as a reference for professionals.
\begin{itemize}
	\item {\sf Preface.} This book provides a broad introduction to optimization with a focus on practical algorithms for design of engineering systems. Cover a wide variety of optimization topics, introducing underlying mathematical problem formulations \& algorithms for solving them. Figures, examples, \& exercises are provided to convey intuition behind various approaches.

	This text is intended for advanced undergraduates \& graduate students as well as professionals. Book requires some mathematical maturity \& assumes prior exposure to multivariable calculus, linear algebra, \& probability concepts. Some review material is provided in appendix. Disciplines where book would be especially useful include mathematics, statistics, CS, aerospace, electrical engineering, \& operations research.

	Fundamental to this textbook are algorithms, which are all implemented in Julia programming language. Have found language to be ideal for specifying algorithms in human readable form. Permission is granted, free of charge, to use code snippets associated with this book, subject to condition: source of code is acknowledged. Anticipate (Dự đoán): others may want to contribute translations of these algorithms to other programming languages. As translations become available, link to them from book's webpage.
	\item {\sf1. Introduction.}
	\item {\sf2. Derivatives \& Gradients.}
	\item {\sf3. Bracketing.}
	\item {\sf4. Local Descent.}
	\item {\sf5. 1st-Order Methods.}
	\item {\sf6. 2nd-Order Methods.}
	\item {\sf7. Direct Methods.}
	\item {\sf8. Stochastic Methods.}
	\item {\sf9. Population Methods.}
	\item {\sf10. Constraints.}
	\item {\sf11. Linear Constrained Optimization.}
	\item {\sf12. Multiobjective Optimization.}
	\item {\sf13. Sampling Plans.}
	\item {\sf14. Surrogate Models.}
	\item {\sf15. Probabilistic Surrogate Models.}
	\item {\sf16. Surrogate Optimization.}
	\item {\sf17. Optimization under Uncertainty.}
	\item {\sf18. Uncertainty Propagation.}
	\item {\sf19. Discrete Optimization.}
	\item {\sf20. Expression Optimization.}
	\item {\sf21. Multidisciplinary Optimization.}
	\item {\sf A. Julia.}
	\item {\sf B. Test Functions.}
	\item {\sf C. Mathematical Concepts.}
\end{itemize}

%------------------------------------------------------------------------------%

\section{Mathematical Optimization}
\textbf{\textsf{Community -- Cộng đồng.}} {\sc Marc Quincampoix, C\'{e}dric Villani}.

%------------------------------------------------------------------------------%

\subsection{\cite{Fischetti2019}. {\sc Matteo Fischetti}. Introduction to Mathematical Optimization}

\begin{itemize}
    \item {\sf Preface.} This book is intended to be a teaching aid for students of courses in Operations Research \& Mathematical Optimization for scientific faculties. Some of basic topics of Operations Research \& Optimization will be considered: Linear Programming, nteger Linear Programming, Computational Complexity, \& Graph Theory. Particular emphasis is given to Integer Linear Programming, with an exposition of the most recent resolution techniques, \& in particular of {\it branch-\&-cut} method. The work is accompanied by numerous examples \& exercises.
    \item {\sf1. Mathematical Programming.}
    \item {\sf2. Linear (Integer) Programming Models.}
    \item {\sf3. Advanced Models.}
    \item {\sf4. Simplex Algorithm.}
    \item {\sf5. Duality in Linear Programming.}
    \item {\sf6. Integer Linear Programming.}
    \item {\sf7. Graph Theory.}
    \item {\sf8. Some NP-hard Problems.}
    \item {\sf9. Exercises.}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{\cite{Gueler2010}. {\sc Osman G\"uler}. Foundations of Optimization}

\begin{itemize}
    \item {\sf Preface.} Optimization is everywhere. It is human nature to seek best option among all available. Nature, too, seems to be guided by optimization -- many laws of nature have a variational character. Among geometric figures in plane with a fixed perimeter, circle has greatest area. Such isoperimetric problems involving geometric figures date back to ancient Greece. Fermat's principle, discovered in 1629, stating: tangent line is horizontal at a minimum point, seems to have influenced development of calculus. Proofs of Rolle's theorem \& mean value theorem in calculus use Weierstrass theorem on existence of maximizers \& minimizers. Introduction of brachistochrone problem in 1696 by {\sc Johann Bernoulli} had a tremendous impact on development of calculus of variations \& influenced development of functional analysis. Variational character of laws of mechanics \& optics were discovered in 17th \& 18th centuries. Euler \& Lagrange forged foundations of calculus of variations in 18th century. In 19th century, {\sc Riemann} used Dirichlet's principle, which has a variational character, in his investigations in complex analysis. Simplex method for linear programming was discovered shortly after advent of computers in 1940s, \& influenced subsequent development of mathematical programming. Emergence of theory of optimal control in 1950s was in response to need for controlling space vehicles \& various industrial processes. Today, optimization is a vast subject with many subfields, \& is growing at a rapid pace. Research is proceeding various directions -- advancement of theory, development of new applications \& computer codes, \& establishment or renewal of ties with many fields in science \& industry.

    -- Tối ưu hóa ở khắp mọi nơi. Bản chất con người là tìm kiếm phương án tốt nhất trong số tất cả các phương án có sẵn. Bản chất tự nhiên cũng có vẻ được hướng dẫn bởi sự tối ưu hóa -- nhiều định luật tự nhiên có tính chất biến phân. Trong số các hình học trên mặt phẳng có chu vi cố định, hình tròn có diện tích lớn nhất. Các bài toán đẳng chu vi liên quan đến các hình học có nguồn gốc từ Hy Lạp cổ đại. Nguyên lý Fermat, được phát hiện vào năm 1629, phát biểu rằng: đường tiếp tuyến nằm ngang tại điểm nhỏ nhất, dường như đã ảnh hưởng đến sự phát triển của phép tính. Các bằng chứng về định lý Rolle \& định lý giá trị trung bình trong phép tính sử dụng định lý Weierstrass về sự tồn tại của các hàm cực đại \& cực tiểu. Sự ra đời của bài toán brachistochrone vào năm 1696 của {\sc Johann Bernoulli} đã có tác động to lớn đến sự phát triển của phép tính biến phân \& ảnh hưởng đến sự phát triển của giải tích hàm. Tính chất biến phân của các định luật cơ học \& quang học đã được phát hiện vào thế kỷ 17 \& 18. Euler \& Lagrange đã xây dựng nền tảng cho phép tính biến phân vào thế kỷ 18. Vào thế kỷ 19, {\sc Riemann} đã sử dụng nguyên lý Dirichlet, có tính chất biến phân, trong các nghiên cứu của ông về giải tích phức tạp. Phương pháp simplex cho lập trình tuyến tính được phát hiện ngay sau khi máy tính ra đời vào những năm 1940, \& ảnh hưởng đến sự phát triển sau này của lập trình toán học. Sự xuất hiện của lý thuyết điều khiển tối ưu vào những năm 1950 là để đáp ứng nhu cầu điều khiển các phương tiện vũ trụ \& nhiều quy trình công nghiệp khác nhau. Ngày nay, tối ưu hóa là một chủ đề rộng lớn với nhiều lĩnh vực phụ, \& đang phát triển với tốc độ nhanh chóng. Nghiên cứu đang tiến triển theo nhiều hướng khác nhau -- sự tiến bộ của lý thuyết, phát triển các ứng dụng mới \& mã máy tính, \& thiết lập hoặc đổi mới mối quan hệ với nhiều lĩnh vực trong khoa học \& công nghiệp.

    Main focus of this book is optimization in finite-dimensional spaces. In broad terms, this is problem of optimizing a function $f$ in $n$ variables over a subset of $\mathbb{R}^n$. Thus, decision variable ${\bf x} = (x_1,\ldots,x_n)$ is finite-dimensional. A typical problem in this area is a mathematical program $(P)$, which concerns minimization (or maximization) of a function $f(x)$ subject to finitely many functional constraints of form $g_i({\bf x})\le0,h_j({\bf x}) = 0$ \& a set constraint of form ${\bf x}\in C$, where $f,g_i,h_j$ are real-valued functions defined on some subsets of $\mathbb{R}^n$ \& $C\subset\mathbb{R}^n$. Any finite-dimensional vector spae may be substituted for $\mathbb{R}^n$ w.l.o.g. If domain of $f$ is an open set \& there are no constraints (or more generally if domains of $g_i,h_j,C$ are open sets), then we have an unconstrained optimization problem. If all functions are affine \& $C$ is defined by linear equations \& inequalities, then $(P)$ is called a {\it linear program}. If $f,g_i$ are convex functions, $h_j$ is an affine function, \& $C$ is a convex set, then $(P)$ is a convex program. If number of functional constraints is infinite, then $(P)$ is called a {\it semi-infinite program}. Mathematical programs have many real-life applications. In particular, linear programming, \& more recently semidefinite programming, are enormously popular \& have many industrial \& scientific applications. Latter problem optimizes a linear function subject to linear equality constraints over cone of symmetric positive semidefinite matrices.

    -- Trọng tâm chính của cuốn sách này là tối ưu hóa trong không gian hữu hạn chiều. Theo nghĩa rộng, đây là vấn đề tối ưu hóa hàm $f$ trong $n$ biến trên một tập con của $\mathbb{R}^n$. Do đó, biến quyết định ${\bf x} = (x_1,\ldots,x_n)$ là hữu hạn chiều. Một vấn đề điển hình trong lĩnh vực này là một chương trình toán học $(P)$, liên quan đến việc giảm thiểu (hoặc tối đa hóa) hàm $f(x)$ theo hữu hạn nhiều ràng buộc hàm dạng $g_i({\bf x})\le0,h_j({\bf x}) = 0$ \& ràng buộc tập hợp dạng ${\bf x}\in C$, trong đó $f,g_i,h_j$ là các hàm giá trị thực được xác định trên một số tập con của $\mathbb{R}^n$ \& $C\subset\mathbb{R}^n$. Bất kỳ vectơ spae hữu hạn chiều nào cũng có thể được thay thế cho $\mathbb{R}^n$ w.l.o.g. Nếu miền của $f$ là một tập mở \& không có ràng buộc nào (hoặc tổng quát hơn nếu miền của $g_i,h_j,C$ là các tập mở), thì chúng ta có một bài toán tối ưu hóa không ràng buộc. Nếu tất cả các hàm là afin \& $C$ được định nghĩa bởi các phương trình tuyến tính \& bất đẳng thức, thì $(P)$ được gọi là {\it chương trình tuyến tính}. Nếu $f,g_i$ là các hàm lồi, $h_j$ là một hàm afin, \& $C$ là một tập lồi, thì $(P)$ là một chương trình lồi. Nếu số ràng buộc hàm là vô hạn, thì $(P)$ được gọi là {\it chương trình bán vô hạn}. Các chương trình toán học có nhiều ứng dụng trong đời sống thực. Đặc biệt, lập trình tuyến tính, \& gần đây hơn là lập trình bán xác định, cực kỳ phổ biến \& có nhiều ứng dụng trong công nghiệp \& khoa học. Bài toán sau tối ưu hóa một hàm tuyến tính tuân theo các ràng buộc đẳng thức tuyến tính trên hình nón của các ma trận bán xác định dương đối xứng.

    Main goal of theory of mathematical programming: obtain optimality conditions (necessary \& sufficient) for a local or global minimizer of $(P)$. This is an impossible task unless some kind of regularity is assumed about data of $(P)$ -- functions $f,g_i,h_j$, \& set $C$. This can be differentiability (in some form) of functions, or convexity of functions as well as of set $C$. In this book, assume: functions $f,g_i,h_j$ are differentiable as many times as needed (except in cases where there is no advantage to do so), \& do not develop nonsmooth analysis in any systematic way. Optimization from viewpoint of nonsmooth analysis is competently covered in several recent books; see e.g. {\it Variational Analysis} by {\sc Rockafellar \& Wets}, {\it Variational Analysis \& Generalized Differentiation} by {\sc Mordukhovich}. Another goal of theory, important especially in convex programming, is {\it duality theory}, whereby a 2nd convex program $(D)$ is associated with $(P)$ s.t. the pair $(P)$-$(D)$ have remarkable properties which can be exploited in several useful ways. If problem $(P)$ has a lot of structure, it may be possible to use optimality conditions to solve analytically for solutions to $(P)$. This desirable situation is very valuable when it is successful, but it is rare, so it becomes necessary to devise numerical optimization techniques or algorithms to search for optimal solutions. Process of designing efficient algorithms requires a great deal of ingenuity, \& optimality conditions contribute to process in several ways, e.g. by suggesting algorithm itself, or by verifying correctness of numerical solutions returned by algorithm. Role of duality theory in designing algorithms is similar, \& often is more decisive.

    -- Mục tiêu chính của lý thuyết lập trình toán học: thu được các điều kiện tối ưu (cần \& đủ) cho một bộ tối thiểu cục bộ hoặc toàn cục của $(P)$. Đây là một nhiệm vụ bất khả thi trừ khi một số loại quy luật được giả định về dữ liệu của $(P)$ -- các hàm $f,g_i,h_j$, \& tập $C$. Điều này có thể là tính khả vi (dưới một số dạng) của các hàm, hoặc tính lồi của các hàm cũng như của tập $C$. Trong cuốn sách này, giả sử: các hàm $f,g_i,h_j$ có thể khả vi nhiều lần tùy theo nhu cầu (trừ những trường hợp không có lợi thế nào khi làm như vậy), \& không phát triển phân tích không trơn tru theo bất kỳ cách có hệ thống nào. Tối ưu hóa theo quan điểm phân tích không trơn tru được trình bày một cách có thẩm quyền trong một số cuốn sách gần đây; ví dụ, hãy xem {\it Variational Analysis} của {\sc Rockafellar \& Wets}, {\it Variational Analysis \& Generalized Differentiation} của {\sc Mordukhovich}. Một mục tiêu khác của lý thuyết, đặc biệt quan trọng trong lập trình lồi, là {\it lý thuyết đối ngẫu}, theo đó một chương trình lồi thứ 2 $(D)$ được liên kết với $(P)$ sao cho cặp $(P)$-$(D)$ có các tính chất đáng chú ý có thể được khai thác theo một số cách hữu ích. Nếu bài toán $(P)$ có nhiều cấu trúc, có thể sử dụng các điều kiện tối ưu để giải tích tìm nghiệm cho $(P)$. Tình huống mong muốn này rất có giá trị khi thành công, nhưng lại hiếm, do đó cần phải đưa ra các kỹ thuật hoặc thuật toán tối ưu hóa số để tìm kiếm các nghiệm tối ưu. Quá trình thiết kế các thuật toán hiệu quả đòi hỏi rất nhiều sự khéo léo, \& các điều kiện tối ưu góp phần vào quá trình theo một số cách, ví dụ như bằng cách đề xuất chính thuật toán hoặc bằng cách xác minh tính đúng đắn của các nghiệm số do thuật toán trả về. Vai trò của lý thuyết đối ngẫu trong việc thiết kế thuật toán cũng tương tự, \& thường mang tính quyết định hơn.

    All chaps except Chap. 14 are concerned with theory of optimization. Have tried to present all major results in theory of finite-dimensional optimization, \& strived to provide best available proofs whenever possible. Moreover, include several independent proofs of some of most important results in order to give reader \& instructor of a course using this book flexibility in learning or teaching key subjects. On several occasions give proofs that may be new. Not all chaps deal exclusively with finite-dimensional spaces, however. Chaps. 3, 5, 6, 14, \& Appendices A, C contain, in part or fully, important results in nonlinear analysis \& in theory of convexity in infinite-dimensional settings.

    -- Tất cả các chương ngoại trừ Chương 14 đều liên quan đến lý thuyết tối ưu hóa. Đã cố gắng trình bày tất cả các kết quả chính trong lý thuyết tối ưu hóa hữu hạn chiều, \& cố gắng cung cấp các bằng chứng tốt nhất có thể bất cứ khi nào có thể. Hơn nữa, bao gồm một số bằng chứng độc lập của một số kết quả quan trọng nhất để cung cấp cho người đọc \& giảng viên của một khóa học sử dụng cuốn sách này sự linh hoạt trong việc học hoặc giảng dạy các môn học chính. Trong một số trường hợp đưa ra các bằng chứng có thể là mới. Tuy nhiên, không phải tất cả các chương đều chỉ đề cập đến không gian hữu hạn chiều. Các Chương 3, 5, 6, 14, \& Phụ lục A, C chứa, một phần hoặc toàn bộ, các kết quả quan trọng trong phân tích phi tuyến tính \& trong lý thuyết lồi trong các thiết lập vô hạn chiều.

    Chap. 14 may be viewed as a short course on 3 basic optimization algorithms: steepest descent method, Newton's method, \& conjugate-gradient method. In particular, conjugate-gradient method is presented in great detail. 3 algorithms are chosen to be included because many computational schemes in mathematical programming have their origins in these algorithms.

    -- Chương 14 có thể được xem như một khóa học ngắn về 3 thuật toán tối ưu hóa cơ bản: phương pháp dốc nhất, phương pháp Newton, phương pháp \& gradien liên hợp. Đặc biệt, phương pháp gradien liên hợp được trình bày rất chi tiết. 3 thuật toán được chọn để đưa vào vì nhiều lược đồ tính toán trong lập trình toán học có nguồn gốc từ các thuật toán này.
    \item {\sf Audience \& background.} Book is suitable as a textbook for a 1st course in theory of optimization in finite-dimensional spaces at graduate level. Book is also suitable for self-study or as a reference book for more advanced readers. It evolved out of my experience in teaching a graduate-level course 12 times since 1993, 11 times at University of Maryland, Baltimore County (UMBC), \& once in 2001 at Bilkent University, Ankara, Turkey. An important feature of book is inclusion of $> 200$ carefully selected exercises $+$ a fair number of completely solved examples within text.

    Prerequisites for course are analysis \& linear algebra. Reader is assumed to be familiar with basic concepts \& results of analysis in finite-dimensional vector spaces -- limits, continuity, completeness, compactness, connectedness, \& so on. In some of more advanced chaps \& sects, necessary to be familiar with same concepts in metric \& Banach spaces. Reader is also assumed to be familiar with fundamental concepts \& results of linear algebra -- vector space, matrix, linear combination, span, linear independence, linear map (transformation), \& so on.

    -- Điều kiện tiên quyết cho khóa học là phân tích \& đại số tuyến tính. Người đọc được coi là quen thuộc với các khái niệm cơ bản \& kết quả phân tích trong không gian vectơ hữu hạn chiều -- giới hạn, tính liên tục, tính đầy đủ, tính chặt chẽ, tính liên thông, \& v.v. Trong một số chương nâng cao hơn \& giáo phái, cần phải quen thuộc với các khái niệm tương tự trong không gian metric \& Banach. Người đọc cũng được coi là quen thuộc với các khái niệm cơ bản \& kết quả của đại số tuyến tính -- không gian vectơ, ma trận, tổ hợp tuyến tính, khoảng, độc lập tuyến tính, ánh xạ tuyến tính (biến đổi), \& v.v.
    \item {\sf Suggestions for using this book in a course.} Ideally, a 1st course in finite-dimensional optimization should cover 1st-order \& 2nd-order optimality conditions in unconstrained optimization, fundamental concepts of convexity, separation theorems involving convex sets (at least in finite-dimensional spaces), theory of linear inequalities \& convex polyhedra, optimality conditions in nonlinear programming, \& duality theory of convex programming. These are treated in Chaps. 2,4,6,7,9, 11, resp., \& can be covered in a 1-semester course. Chap. 1 on differential calculus can be covered in such a course, or referred to as needed, depending on background of students. In any case, important to be familiar with multivariate Taylor formulas, because they are used in deriving optimality conditions \& in differentiating functions.

    -- Lý tưởng nhất là khóa học đầu tiên về tối ưu hóa hữu hạn chiều nên bao gồm các điều kiện tối ưu bậc 1 \& bậc 2 trong tối ưu hóa không bị ràng buộc, các khái niệm cơ bản về tính lồi, các định lý phân tách liên quan đến các tập lồi (ít nhất là trong không gian hữu hạn chiều), lý thuyết bất đẳng thức tuyến tính \& đa diện lồi, các điều kiện tối ưu trong lập trình phi tuyến tính, \& lý thuyết đối ngẫu của lập trình lồi. Những điều này được trình bày trong các Chương 2, 4, 6, 7, 9, 11, tương ứng, \& có thể được trình bày trong một khóa học kéo dài 1 học kỳ. Chương 1 về phép tính vi phân có thể được trình bày trong một khóa học như vậy hoặc được gọi khi cần, tùy thuộc vào nền tảng của sinh viên. Trong mọi trường hợp, điều quan trọng là phải quen thuộc với các công thức Taylor đa biến, vì chúng được sử dụng để suy ra các điều kiện tối ưu \& trong việc phân biệt các hàm.

    This [course] emphasizes use of separation theorems for convex sets for deriving optimality conditions for nonlinear programming. This approach is both natural \& widely applicable -- possible to use same idea to derive optimality conditions for many types of problems, from nonlinear programming to optimal control problems, as was shown by {\sc Dubovitskii \& Milyutin}.

    Chap. 3: cover theory of linear inequalities, basic theorems of nonlinear analysis, Chap. 14 on algorithms, Chap. 8 on linear programming, Chap. 10 on nonlinear programming, Chap. 12 on semi-infinite programming. Chaps. 5--6 contain very detailed, advanced results on convexity. Chaps. 4--8, 11, 13 can be used for a stand-alone 1-semester course on theory of convexity. If desired, one may supplement course by presenting theory of Fenchel duality using, e.g., Chaps. 1--3 \& 6 of book {\it Convex Analysis \& Variational Problems} by {\sc Ekeland \& Temam}. Theory of convexity has an important place in optimization. Already mentioned role of separation theorems for convex sets in deriving optimality conditions in mathematical programming. Theory of duality is a powerful tool with many uses, both in theory of optimization \& in design of numerical optimization algorithms. Role of convexity in complexity theory of optimization in even more central; since work of {\sc Nemirovskii \& Yudin} in 1970s on ellipsoid method, know: convex programming (\& some close relatives) is only known class of problems that are computationally tractable, i.e., for which polynomial-time methods can be developed.
    \item {\sf Comments on contents of individual chaps.} Chap. 1 includes background material on differential calculus. 2 novel features of chap are converse of Taylor's formula \& Danskin's theorem. 1st result validates role of Taylor's formula for computing derivatives, \& Danskin's formula is a useful tool in optimization.

    Chap. 2 develops 1st-order \& 2nd-order optimality conditions in unconstrained optimization. Sect. 2.4 deals with quadratic forms \& symmetric matrices. Recall spectral decomposition of a symmetric matrix, give eigenvalue characterizations of definite \& semidefinite matrices, state Descartes's exact rule of sign (whose proof is given in Appendix B), \& use it as tool for recognizing definite \& semidefinite matrices. Also include a proof of Sylvester's theorem on positive definiteness of a symmetric matrix. (An elegant optimization-based proof is given in an exercise at end of chap.) In Sect. 2.5, give proofs of inverse \& implicit function theorems \& Lyusternik's theorem using an optimization-based approach going back at least to Carathéodory. A proof of Morse's lemma is given in Sect. 2.6 because of light it throws on 2nd-order optimality conditions.

    Chap. 3 is devoted to Ekeland's $\epsilon$-variational principle (\& its relatives) \& its applications. Use it to prove central result on linear inequalities (Motzkin's transposition theorem), \& basic theorems of nonlinear analysis in a general setting. Variational principles are fascinating, \& their importance in optimization is likely to grow even more in future.

    Next 3 chaps are devoted to convexity. Chap. 4 treats fundamentals of convex analysis. Include Sect. 4.1 on affine geometry because of its intrinsic importance, \& because it helps make certain results in convexity more transparent.

    Chap. 5 delves into structure of convex sets. A proper understanding of concepts e.g. relative interior, closure, \& faces of convex sets is essential for proving separation theorems involving convex sets \& much else. Concept of relative interior is developed in both algebraic \& topological settings.

    Chap. 6 is devoted to separation of convex sets, essential source of duality, at least in convex programming. Chap is divided into 2 parts. Sects. 6.1--6.5 deal with separation theorems in finite dimensions \& do not depend heavily on Chap. 5. They are sufficient for somebody who is interested in only finite-dimensional situation. Sect. 6.5 is devoted to finite-dimensional version of Dubovitskii--Milyutin theorem, a convenient separation theorem, applicable to separation of several convex sets. Sects. 6.6--6.98 treat separation theorems involving 2 or several convex sets in a very general setting. Chap. 5 is a prerequisite for these sects, which are intended for more advanced readers.

    Chaps. 7--8 treat theories of convex polyhedra \& linear programming, resp. 2 secs, Sect. 7.5 on Tucker's complementarity theorem \& Sect. 8.5 on existence of strictly complementary solutions in linear programming, are important in interior-point methods.

    Chaps. 9--10 treat nonlinear programming. Standard, basic theory consisting of 1st-order (Fritz John \& KKT) \& 2nd-order conditions for optimality is given in Chap. 9. A novel feature of chap is inclusion of a 1st-order sufficient optimality condition that goes back to {\sc Fritz John}, \& several completely solved examples of nonlinear programs. Chap. 19 gives complete solutions for 7 structured optimization problems. These problems are chosen for their intrinsic importance \& to demonstrate: optimization techniques can resolve important problems.

    Chap. 11 deals with duality theory. Have chosen to treat duality using Lagrangian function. This approach is completely general for convex programming, because it is equivalent to approach by Fenchel duality in that context, \& more general because it is sometimes applicable beyond convex programming. Establish general correspondence between saddle point \& duality in Sect. 11.2 \& apply it to nonlinear programming in Sect. 11.3. Most important result of chap is strong duality theorem for convex programming given in Sect. 11.4, under very weak conditions. Necessary to use sophisticated separation theorems to achieve this result. After treating several examples of duality in Sect. 11.5, turn to duality theory of conic programming in Sect. 11.6. As a novel application, give a proof of Hoffman's lemma using duality in Sect. 11.8.

    Chap. 12: semi-infinite programming. This subject is not commonly included in most optimization textbooks, but many important problems in finite dimensions require it, e.g. problem of finding extremal-volume ellipsoids associated with a convex body in $\mathbb{R}^n$. Derive Fritz John optimality conditions for these problems using Danskin's theorem when set indexing constraints is compact. In rest of chap solve several specific, important semi-infinite programming problem rather than giving a systematic theory. Another method to treat convex semi-infinite programs, using Helly's theorem, is given in Sect. 13.2.

    Chap. 13 is devoted to several special topics in convexity that we deem interesting: combinatorial theory of convex sets, homogeneous convex functions, decomposition of convex cones, \& norms of polynomials. Last topic finds an interesting application to self-concordant functions in interior-point methods.

    Focus of Chap. 14 is on algorithms. Development of numerical algorithms for optimization problems is a highly intricate art \& science, \& anything close to a proper treatment would require several volumes. This chap is included in our book out of conviction that there should be a place in a book on theory for a chap e.g. this, which treats in some depth a few select algorithms. This should help reader put theory in perspective, \& accomplish at least 3 goals: reader should see how theory \& algorithms fit together, how they are different, \& whether there are differences in thought processes that go into developing each part. It should also give reader additional incentive to learn more about algorithms.

    -- Trọng tâm của Chương 14 là về thuật toán. Phát triển thuật toán số cho các bài toán tối ưu hóa là một nghệ thuật \& khoa học vô cùng phức tạp, \& bất kỳ điều gì gần với cách xử lý phù hợp đều cần đến nhiều tập. Chương này được đưa vào sách của chúng tôi với niềm tin rằng nên có một vị trí trong một cuốn sách về lý thuyết cho một chương như chương này, chương này sẽ đề cập sâu hơn đến một số thuật toán được chọn. Điều này sẽ giúp người đọc đưa lý thuyết vào đúng bối cảnh, \& đạt được ít nhất 3 mục tiêu: người đọc sẽ thấy lý thuyết \& thuật toán phù hợp với nhau như thế nào, chúng khác nhau như thế nào, \& liệu có sự khác biệt nào trong các quá trình suy nghĩ khi phát triển từng phần hay không. Nó cũng sẽ cung cấp cho người đọc thêm động lực để tìm hiểu thêm về thuật toán.

    Choose to treat 3 fundamental optimization algorithms: steepest-descent (\& gradient projection), Newton's, \& conjugate-gradient methods. Develop each in some depth \& provide convergence rate estimates where possible. E.g., provide convergence rate for steepest-descent method for minimization of a convex quadratic function, \& for minimization of a convex function with Lipschitz gradient. Convergence theory of Newton's method is treated, including convergence theory due to {\sc Kantorovich}. Finally, give a very extensive treatment of conjugate-gradient method. Prove its remarkable convergence properties \& show its connection with orthogonal polynomials.

    In Appendix A, give theory for consistency of a system of finitely many linear (both strict \& weak) inequalities in arbitrary vector spaces. Algebraic proof has considerable merits: it is very general, does not need any prerequisites, \& doe not use completeness of field over which vector space is defined. Consequently, applicable to linear inequalities with rational coefficients.

    In Appendix B, give a short proof of Descartes's exact rule of sign, \& in Appendix C, classical proofs of open mapping theorem \& Graves's theorem.
    \item {\sf1. Differential Calculus.} This chap is devoted to differential calculus. Tools from differential calculus are widely used in many branches of analysis, including in optimization. In optimization, they are used, among other things, to derive optimality conditions in extremal problems which are described by differentiable functions.

    \item {\sf2. Unconstrained Optimization.}
    \item {\sf3. Variational Principles.}
    \item {\sf4. Convex Analysis.}
    \item {\sf5. Structure of Convex Sets \& Functions.}
    \item {\sf6. Separation of Convex Sets.}
    \item {\sf7. Convex Polyhedra.}
    \item {\sf8. Linear Programming.}
    \item {\sf9. Nonlinear Programming.}
    \item {\sf10. Structured Optimization Problems.}
    \item {\sf11. Duality Theory \& Convex Programming.}
    \item {\sf12. Semi-infinite Programming.}
    \item {\sf13. Topics in Convexity.}
    \item {\sf14. 3 Basic Optimization Algorithms.}
    \item {\sf A. Finite Systems of Linear Inequalities in Vector Spaces.}
    \item {\sf B. Descartes's Rule of Sign.}
    \item {\sf C. Classical Proofs of Open Mapping \& Graves's Theorems.}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{\cite{Villani2003}. {\sc C\'{e}dric Villani}. Topics in Optimal Transportation}

\begin{enumerate}
	\item \cite{Villani2003}. {\sc C\'{e}dric Villani}. {\it Topics in Optimal Transportation}.

	\begin{itemize}
		\item {\sf Preface.} Optimal mass transportation was born in France in 1781, with a very famous paper by {\sc Gaspard Monge}, {\it M\'emoire sur la th\'eorie des d\'eblais et des remblais}. Since, become a classical subject in probability theory, economics, \& optimization. Gained extreme popularity, since many researches in different areas of mathematics understood that optimal mass transportation was strongly linked to their subject. A precise birthdate for this revival: 1987 note by {\sc Yann Brenier}, {\it D\'ecomposition polaire et r\'earrangement des champs de vecteurs} paved the way towards a beautiful interplay between PDEs, fluid mechanics, geometry, probability theory, \& function analysis, developed over last 10 years, through contributions of a number of authors, with optimal transportation problems as a common denominator. 2 volumes of {\it Mass transportation problems} by {\sc Rachev \& R\"uschendorf} depicting many applications of Monge-Kantorovich distance to various problems, together with classical theory of optimal transportation problem in a very abstract setting; survey by {\sc Evans}, which can also be considered as an introduction to subject, describes several applications of $L^1$ theory (i.e., when cost function is a distance), extremely clear lecture notes by {\sc Ambrosio}, centered on $L^1$ theory from point of view of calculus of variations, lecture notes by {\sc Urbas} -- a marvelous reference for regularity theory of Monge-Amp\`ere equation arising in mass transportation.

		\item {\sf Introduction.} {\it Formulation of optimal transportation problem.} Assume given a pile of sand, \& a hole having to be completely filled up with the sand. Obviously pile \& hole must have same volume. Normalize mass of pile $= 1$. Will model both pile \& hole by probability measures $\mu,\nu$, defined respectively on some measure spaces $X,Y$. Whenever $A,B$: measurable subsets of $X,Y$, resp., $\mu[A]$ gives a measure of how much sand is located inside $A$, $\nu[B]$ of how much sand can be piled in $B$. Moving sand around needs some effort, modeled by a measurable {\it cost function} defined on $X\times Y$. Informally, $c(x,y)$ tells how much it costs to transport 1 unit of mass from location $x$ to location $y$. Natural to assume at least: $c$ is measurable \& nonnegative. Should not a priori exclude the possibility that $c$ takes infinite values, \& so $c$ should be a measurable map from $X\times Y$ to $\mathbb{R}\cup\{+\infty\}$.

		\begin{problem}[Central question]
			How to realize transportation at minimal cost?
		\end{problem}
		Before studying this question, have to make clear what a way of transportation, or a {\it transference\footnote{the process of moving something from one place, person or use to another.} plan}, is. Will model transference plans by probability measures $\pi$ on the product space $X\times Y$. Informally, $d\pi(x,y)$ measures amount of mass transferred from location $x$ to location $y$. Do not a priori exclude the possibility that some mass located at point $x$ may be split into several parts (several possible destination $y$'s). For a transference plan $\pi\in P(X\times Y)$ to be admissible, of course necessary that all the mass taken from point $x$ coincide with $d\mu(x)$, \& all the mass transferred to $y$ coincide with $d\nu(y)$, i.e., $\int_Y d\pi(x,y) = d\mu(x)$, $\int_X d\pi(x,y) = d\nu(y)$. More rigorously, require $\pi[A\times Y] = \mu[A]$, $\pi[X\times B] = \nu[B]$ for all measurable subsets $A\subset X$ \& $B\subset Y$. Equivalent to stating: for all functions $\phi,\psi$ in a suitable class of test functions,
		\begin{equation}
			\int_{X\times Y} \phi(x) + \psi(y)\,{\rm d}\pi(x,y) = \int_X \phi(x)\,{\rm d}\mu(x) + \int_Y \psi(y)\,{\rm d}\nu(y).
		\end{equation}
		In general, the natural set of admissible test functions for $(\phi,\psi)$ is $L^1(d\mu)\times L^1(d\nu)$, or equivalently, $L^\infty(d\mu)\times L^\infty(d\nu)$. In most situations of interest, this class can be narrowed to just $C_b(X)\times C_b(Y)$, or $C_0(X)\times C_0(Y)$. Those probability measures $\pi$ satisfying $\pi[A\times Y] = \mu[A]$, $\pi[X\times B] = \nu[B]$ are said to have {\it marginals} $\mu,\nu$, \& will be the admissible transference plans. Denote the set of all such probability measures by $\Pi(\mu,\nu)\coloneqq\{\pi\in P(X\times Y);\pi[A\times Y] = \mu[A]$, $\pi[X\times B] = \nu[B]\mbox{ holds for all measurable } A,B\}\ne\emptyset$ since tensor product $\mu\otimes\nu\in\Pi(\mu,\nu)$, corresponding to the most stupid transportation plan that one may imagine: any piece of sand, regardless of its location, is distributed over the entire hole, proportionally to the depth. A clear mathematical def of basic problem:

		\begin{problem}[Kantorovich's optimal transportation problem]
			Minimize $I[\pi] = \int_{X\times Y} c(x,y)\,{\rm d}\pi(x,y)$ for $\pi\in\Pi(\mu,\nu)$.
		\end{problem}
		This minimization problem was studied in 40s by {\sc Kantorovich} awarded a Nobel prize for related work in economics. The optimal transference problem is related to basic questions in economics becomes clear if one thinks of $\mu$ as a density of production units, \& of $\nu$ as aa density of consumers. For a given transference plan $\pi$, the nonnegative (possibly infinite) quantity $I[\pi]$ is called the {\it total transportation cost} associated to $\pi$. The {\it optimal transportation cost} between $\mu,\nu$ is the value $\mathcal{T}_{\rm c}(\mu,\nu)\coloneqq\inf_{\pi\in\Pi(\mu,\nu)} I[\pi]$. The optimal $\pi$'s, i.e., those s.t. $I[\pi] = \mathcal{T}_{\rm c}(\mu,\nu)$, if they exist, will be called {\it optimal transference plans}.

		Translate Kantorovich problem into its probabilistic equivalent:

		\begin{problem}[Probabilistic interpretation]
			Given 2 probability measures $\mu,\nu$, minimize expectation $I(U,V) = \mathbb{E}[c(U,V)]$ over all pairs $(U,V)$ of random variables $U$ in $X$, $V$ in $Y$, s.t. ${\rm law}(U) = \mu$, ${\rm law}(V) = \nu$.
		\end{problem}
		{\sf Basic probabilistic theory.} a random variable $U$ in $X$ is a measurable map with values in $X$, defined on a probability space $\Omega$ equipped with a probability measure $\mathbb{P}$, that the {\rm law} of $U$ is the probability measure $\mu$ on $X$ defined by $\mu[A]\coloneqq\mathbb{P}[U^{-1}(A)]$, \& the expectation stands for the integral w.r.t. $\mathbb{P}$. Transference plans $\pi\in\Pi(\mu,\nu)$ are all possible laws of the couple $(U,V)$. Such a $\pi$ is often said to be the {\it joint law} of random variables $U,V$; also says that it constitutes a {\it coupling} of $U,V$.

		Kantorovich's problem is a relaxed version of the original mass transportation problem considered by {\sc Monge}. {\sc Monge}'s problem is just the same as {\sc Kantorovich}'s, except for 1 thing: additionally required: {\it no mass be split}. I.e., to each location $x$ is associated a unique destination $y$. In terms of random variables, this requirement means that we ask for $V$ to be a function of $U$ in $I(U,V) = \mathbb{E}[c(U,V)]$. In terms of transference plans, it means that we ask for $\pi$ in $I[\pi]$ to have special form $d\pi(x,y) = d\pi_T(x,y)\equiv d\mu(x)\delta[y = T(x)]$, where $T$ is a measurable map $X\to Y$.

		\item {\sf Chap. 1: Kantorovich Duality.}
		\item {\sf Chap. 2: Geometry of Optimal Transportation.}
		\item {\sf Chap. 3: Brenier's Polar Factorization Theorem.}
		\item {\sf Chap. 4: Monge-Amp\`ere Equation.}
		\item {\sf Chap. 5: Displacement Interpolation \& Displacement Convexity.}
		\item {\sf Chap. 6: Geometric \& Gaussian Inequalities.}
		\item {\sf Chap. 7: Metric Side of Optimal Transportation.}
		\item {\sf Chap. 8: A Differential Point of View on Optimal Transportation.}
		\item {\sf Chap. 9: Entropy Production \& Transportation Inequalities.}
		\item {\sf Chap. 10: Problems.}
	\end{itemize}
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Dynamic Programming -- Quy Hoạch Động}



%------------------------------------------------------------------------------%

\section{Linear Programming -- Quy Hoạch Tuyến Tính}
\begin{definition}[Linear programming]
	``\emph{Linear programming (LP)}, also called \emph{linear optimization}, is a method to achieve the best outcome, e.g., maximum profit or lower cost, in a \href{https://en.wikipedia.org/wiki/Mathematical_model}{mathematical model} whose requirements \& objective are represented by \href{https://en.wikipedia.org/wiki/Linear_function#As_a_polynomial_function}{linear relationships}. Linear programming is a special case of mathematical programming $\equiv$ \href{https://en.wikipedia.org/wiki/Mathematical_optimization}{mathematical optimization}.'' -- \href{https://en.wikipedia.org/wiki/Linear_programming}{Wikipedia{\tt/}linear programming}
\end{definition}
More formally, linear programming is a technique for the \href{https://en.wikipedia.org/wiki/Mathematical_optimization}{optimization} of a linear \href{https://en.wikipedia.org/wiki/Objective_function}{linear objective function}, subject to \href{https://en.wikipedia.org/wiki/Linear_equality}{linear equality} \& \href{https://en.wikipedia.org/wiki/Linear_inequality}{linear inequality} \href{https://en.wikipedia.org/wiki/Constraint_(mathematics)}{constraints}. Its \href{https://en.wikipedia.org/wiki/Feasible_region}{feasible region} is a \href{https://en.wikipedia.org/wiki/Convex_polytope}{convex polytope}, which is a set defined as the \href{https://en.wikipedia.org/wiki/Intersection_(mathematics)}{intersection} of finitely many \href{https://en.wikipedia.org/wiki/Half-space_(geometry)}{half spaces}, each of which is defined by a linear inequality. Its objective function is a real-valued \href{https://en.wikipedia.org/wiki/Affine_function}{affine (linear) function} defined on this polytope. A linear programming \href{https://en.wikipedia.org/wiki/Algorithm}{algorithm} finds a point in the \href{https://en.wikipedia.org/wiki/Polytope}{polytope} where this function has the largest (or smallest) value if such a point exists.

Linear programs are problems that can be expressed in \href{https://en.wikipedia.org/wiki/Canonical_form}{standard form} as
\begin{equation}
	\label{linear program}
	\tag{lp}
	\mbox{Find a vector }{\bf x}\mbox{ that maximizes{\tt/}minimizes }{\bf c}^\top{\bf x}\mbox{ subject to } A{\bf x}\le{\bf b}\mbox{ \& }{\bf x}\ge{\bf 0}.
\end{equation}
Here the components of ${\bf x}$ are the variables to be determined, ${\bf b},{\bf c}$ are given vectors, \& $A$ is a given matrix. The function whose value is to be maximized (${\bf x}\mapsto{\bf c}^\top{\bf x}$ in this case) is called the \href{https://en.wikipedia.org/wiki/Objective_function}{objective function}. The constraint $A{\bf x}\le{\bf x}$ \& ${\bf x}\ge{\bf 0}$ specify a \href{https://en.wikipedia.org/wiki/Convex_polytope}{convex polytope} over which the objective function is to be optimized.

Linear programming can be applied to various fields of study, which is widely used in mathematics \&, to a lesser extent, in business, economics, \& to some engineering problems. There is a close connection between linear programs, eigenequations, \href{https://en.wikipedia.org/wiki/John_von_Neumann}{John von Neumann}'s general equilibrium model, \& structural equilibrium models (see \href{https://en.wikipedia.org/wiki/Dual_linear_program}{dual linear program}). Industries using linear programming models include transportation, energy, telecommunications, \& manufacturing. It has proven useful in modeling diverse types of problems in \href{https://en.wikipedia.org/wiki/Automated_planning_and_scheduling}{planning}, \href{https://en.wikipedia.org/wiki/Routing}{routing}, \href{https://en.wikipedia.org/wiki/Scheduling_(production_processes)}{scheduling}, \href{https://en.wikipedia.org/wiki/Assignment_problem}{assignment}, \& design.

\begin{dinhnghia}[Quy hoạch tuyến tính]
	Bài toán \emph{quy hoạch tuyến tính} là bài toán tìm {\rm GTLN{\tt/}GTNN} của \emph{hàm mục tiêu} trong điều kiện hàm mục tiêu là hàm bậc nhất đối với các biến \& mỗi 1 điều kiện ràng buộc là bất phương trình bậc nhất đối với các biến (không kể điều kiện ràng buộc biến thuộc tập số nào, e.g., $\mathbb{N},\mathbb{Q},\mathbb{R},\mathbb{C}$.
\end{dinhnghia}
Ta có thể viết bài toán quy hoạch tuyến tính 2 biến $x,y$ về dạng sau:
\begin{align}
	\max T&\coloneqq\alpha x + \beta y\mbox{ s.t } a_ix + b_iy\le c_i,\ \forall i = 1,2,\ldots,n,\label{linear programming 2 vars max}\tag{lp2max}\\
	\min T&\coloneqq\alpha x + \beta y\mbox{ s.t } a_ix + b_iy\le c_i,\ \forall i = 1,2,\ldots,n,\label{linear programming 2 vars min}\tag{lp2min}
\end{align}
trong đó các điều kiện ràng buộc đều là các bất phương trình bậc nhất đối với $x,y$. See also:
\begin{itemize}
	\item {\it Problem: Inequation \& Linear System of Inequations -- Bài Tập: Bất Phương Trình \& Hệ Bất Phương Trình}.

	Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 10{\tt/}linear system inequations{\tt/}problem}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.tex}.}].
	\begin{itemize}
		\item {\it Problem \& Solution: Inequation \& Linear System of Inequations -- Bài Tập \& Lời Giải: Bất Phương Trình \& Hệ Bất Phương Trình}.

		Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 10{\tt/}linear system inequations{\tt/}solution}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.tex}.}].
	\end{itemize}
	\item {\it Problem: Mathematical Optimization -- Bài Tập: Ứng Dụng Toán Học Để Giải Quyết 1 Số Bài Toán Tối Ưu}.

	Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 12{\tt/}optimization{\tt/}problem}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.tex}.}].
	\begin{itemize}
		\item {\it Problem \& Solution: Mathematical Optimization -- Bài Tập \& Lời Giải: Ứng Dụng Toán Học Để Giải Quyết 1 Số Bài Toán Tối Ưu}.

		Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 12{\tt/}optimization{\tt/}solution}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.tex}.}].
	\end{itemize}
\end{itemize}

\subsection{How to solve some linear programmings -- Cách giải 1 số bài toán quy hoạch tuyến tính}
Có thể giải 1 số bài toán quy hoạch tuyến tính dạng \eqref{linear programming 2 vars max} hay \eqref{linear programming 2 vars min} theo 2 bước:
\begin{enumerate}
	\item Xác định miền nghiệm $S\subset\mathbb{R}^2$ của hệ bất phương trình $a_ix + b_iy\le c_i$, $\forall i = 1,\ldots,n$.
	\item Tìm điểm $(x,y)\in S$ sao cho biểu thức $T = T(x,y) = \alpha x + \beta y$ có {\rm GTLN} ở bài toán \eqref{linear programming 2 vars max} hoặc có {\rm GTNN} ở bài toán \eqref{linear programming 2 vars min}.

	Khi miền nghiệm $S$ là đa giác (polygon), biểu thức $T(x,y) = \alpha x + \beta y$ đạt GTLN{\tt/}GTNN (gộp chung gọi là {\it cực trị}) tại $(x,y)\in\mathbb{R}^2$ là tọa độ 1 trong các đỉnh của đa giác đó. Khi đó, bước 2 có thể được thực hiện như sau:
	\begin{enumerate}
		\item Xác định tọa độ các đỉnh của đa giác đó.
		\item Tính giá trị của biểu thức $T(x,y) = \alpha x + \beta y$ tại các đỉnh của đa giác đó.
		\item So sánh các giá trị \& kết luận.
	\end{enumerate}
\end{enumerate}
\cite[Chuyên đề II, \S1, LT1--3, 1., 2., 3., 4., 5., pp. 20--25]{CDHT_Toan_12_Canh_Dieu}.

%------------------------------------------------------------------------------%

\section{Control Theory -- Lý Thuyết Điều Khiển}
``{\it Control theory} is a field of \href{https://en.wikipedia.org/wiki/Control_engineering}{control engineering} \& \href{https://en.wikipedia.org/wiki/Applied_mathematics}{applied mathematics} that deals with the \href{https://en.wikipedia.org/wiki/Control_system}{control} of \href{https://en.wikipedia.org/wiki/Dynamical_system}{dynamical systems} in engineered processes \& machines. The objective is to develop a model or algorithm governing the application of system inputs to drive the system to a desired state, while minimizing any {\it delay, overshoot}, or {\it steady-state error} \& ensuring a level of control \href{https://en.wikipedia.org/wiki/Stability_theory}{stability}; often with the aim to achieve a degree of \href{https://en.wikipedia.org/wiki/Optimal_control}{optimality}.

To do this, a {\it controller} with the requisite corrective behavior is required. This controller monitors the controlled \href{https://en.wikipedia.org/wiki/Process_variable}{process variable} (PV), \& compares it with the reference or \href{https://en.wikipedia.org/wiki/Setpoint_(control_system)}{set point} (SP). The difference between actual \& desired value of the process variable, called the {\it error} signal, or SP-PV error, is applied as feedback to generate a control action to bring the controlled process variable to the same value as the set point. Other aspects which are also studied are \href{https://en.wikipedia.org/wiki/Controllability}{controllability} \& \href{https://en.wikipedia.org/wiki/Observability}{observability}. Control theory is used in \href{https://en.wikipedia.org/wiki/Control_system_engineering}{control system engineering} to design automation that have revolutionized manufacturing, aircraft, communications, \& other industries, \& created new fields such as \href{https://en.wikipedia.org/wiki/Robotics}{robotics}.

Extensive use is usually made of a diagrammatic style known as the \href{https://en.wikipedia.org/wiki/Block_diagram}{block diagram}. In it the \href{https://en.wikipedia.org/wiki/Transfer_function}{transfer function}, also known as the system function or network function, is a mathematical model of the relation between the input \& output based on the \href{https://en.wikipedia.org/wiki/Differential_equation}{differential equations} describing the system.

Control theory dates from the 19th century, when the theoretical basis for the operation of governors was 1st described by \href{https://en.wikipedia.org/wiki/James_Clerk_Maxwell}{James Clerk Maxwell}. Control theory was further advanced by \href{https://en.wikipedia.org/wiki/Edward_Routh}{Edward Routh} in 1874, \href{https://en.wikipedia.org/wiki/Jacques_Charles_Fran%C3%A7ois_Sturm}{Charles Sturm} \& in 1895, \href{https://en.wikipedia.org/wiki/Adolf_Hurwitz}{Adolf Hurwitz}, who all contributed to the establishment of control stability criteria; \& from 1922 onwards, the development of \href{https://en.wikipedia.org/wiki/PID_control}{PID control} theory by Nicolas Minorsky. Although a major application of mathematical control theory is in \href{https://en.wikipedia.org/wiki/Control_Systems_Engineering}{control systems engineering}, which deals with the design of \href{https://en.wikipedia.org/wiki/Process_control}{process control} systems for industry, other applications range far beyond this. As the general theory of feedback systems, control theory is useful wherever feedback occurs -- thus control theory also has applications in life sciences, computer engineering, sociology, \& \href{https://en.wikipedia.org/wiki/Operations_research}{operations research}.'' -- \href{https://en.wikipedia.org/wiki/Control_theory}{Wikipedia{\tt/}control theory}

\paragraph{Open-loop \& closed-loop (feedback) control.} ``Fundamentally, there are 2 types of control loop: \href{https://en.wikipedia.org/wiki/Open-loop_control}{open-loop control} (feedforward), \& \href{https://en.wikipedia.org/wiki/Closed-loop_control}{closed-loop control} (feedback).

In open-loop control, the control action from the controller is independent of the ``process output'' (or ``controlled process variable''). A good example of this is a central heating boiler controlled only by a timer, so that heat is applied for a constant time, regardless of the temperature of the building. The control action is the switching on{\tt/}off of the boiler, but the controlled variable should be the building temperature, but is not because this is open-loop control of the boiler, which does not give closed-loop control of the temperature.

In closed loop control, the control action from the controller is dependent on the process output. In the case of the boiler analogy this would include a thermostat to monitor the building temperature, \& thereby feed back a signal to ensure the controller maintains the building at the temperature set on the thermostat. A closed loop controller therefore has a feedback loop which ensures the controller exerts a control action to give a process output the same as the ``reference input'' or ``set point''. For this reason, closed loop controllers are also called {\it feedback controllers}.

The definition of a closed loop control system according to the \href{https://en.wikipedia.org/wiki/British_Standards_Institution}{British Standards Institutin} is ``a control system possessing monitoring feedback, the deviation signal formed as a result of this feedback being used to control the action of a final control element in such a way as to tend to reduce the deviation to 0.''

Likewise; ``A {\it Feedback Control System} is a system which tends to maintain a prescribed relationship of 1 system variable to another by comparing functions of these variables \& using the difference as a means of control.''

\paragraph{Classical control theory.}

\subsection{Application of $C_0$-Semigroup in Control Theory -- Ứng Dụng của Nửa Nhóm $C_0$ Trong Lý Thuyết Điều Khiển}

%------------------------------------------------------------------------------%

\section{$C_0$ Semigroup -- Nửa Nhóm $C_0$}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Anh_Ke_semigroup}. {\sc Cung Thế Anh, Trần Đình Kế}. {\it Nửa Nhóm Các Toán Tử Tuyến Tính \& Ứng Dụng}.
\end{enumerate}
``Ứng dụng của lý thuyết nửa nhóm các toán tử tuyến tính trong lý thuyết điều khiển toán học, bao gồm bài toán điều khiển được, bài toán quan sát, bài toán điều khiển tối ưu, \& bài toán ổn định hóa. Nhấn mạnh đến các hệ điều khiển vô hạn chiều, i.e., các hệ điều khiển sinh bởi các PDEs.'' -- \cite[Chap. IV: {\it Ứng Dụng Trong Lý Thuyết Điều Khiển}]{Anh_Ke_semigroup}

%------------------------------------------------------------------------------%

\section{Feedback Control -- Kiểm Soát Phản Hồi{\tt/}Điều Khiển Hồi Tiếp}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Raymond2007}. {\sc J.-P. Raymond}. {\it Feedback Boundary Stabilization of 3D Incompressible NSEs}.

	Study local stabilization of 3D NSEs around an unstable stationary solution ${\bf w}$, by means of a feedback boundary control. 1st determine a feedback law for the linearized system around ${\bf w}$. Show: this feedback provides a local stabilization of NSEs. To deal with the nonlinear term, the solutions to the closed loop system must be in $H^{\frac{3}{2} + \varepsilon,\frac{3}{4} + \frac{\varepsilon}{2}}(Q)$ with $\varepsilon > 0$. Such a regularity is achieved with a feedback obtained by minimizing a functional involving a norm of the state variable strong enough. The feedback controller cannot be determined by a well posed Riccati equation. Here choose a controller at $t = 0$, is achieved by choosing a time varying control operator in a neighborhood of $t = 0$.

	{\it Keywords}: Dirichlet control; Navier–Stokes equations; Feedback control; Stabilization; Riccati equation.

	An important issue in control theory is the controllability of systems. The local stability of 3D NSEs in a neighborhood of an unstable stationary solution may be deduced from this controllability result. Another important issue is the characterization of stabilizing feedback control laws, pointwise in time.
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Optimal Control -- Điều Khiển Tối Ưu}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Lasiecka_Triggiani2000}. {\sc Irena Lasiecka, Roberto Triggiani}. {\it Control Theory for PDEs: Continuous \& Approximation Theories II: Abstract Hyperbolic-Like Systems Over a Finite Time Horizon}.
	\item \cite{Lions1969}. {\sc Jacques-Louis Lions}. {\it Quelques m\'{e}thodes de r\'{e}solution des probl\`emes aux limites nonlin\'{e}aires -- Some Methods for Solving Nonlinear Boundary Problems}.
	\item \cite{Lions1971}. {\sc Jacques-Louis Lions}. {\it Optimal control of systems governed by PDEs}.
	\item \cite{Troltzsch2010}. {\sc Fredi Tr\"{o}ltzsch}. {\it Optimal Control of PDEs}.
\end{enumerate}

\begin{definition}[Optimal control theory]
	``\emph{Optimal control theory} is a branch of \href{https://en.wikipedia.org/wiki/Control_theory}{control theory} that deals with finding a \href{https://en.wikipedia.org/wiki/Control_(optimal_control_theory)}{control} for a \href{https://en.wikipedia.org/wiki/Dynamical_system}{dynamical system} over a period of time s.t. an \href{https://en.wikipedia.org/wiki/Objective_function}{objective function} is optimized. It has numerous applications in science, engineering, \& operational research. E.g., the dynamical system might be a \href{https://en.wikipedia.org/wiki/Spacecraft}{spacecraft} with controls corresponding to rocket thrusters, \& the objective might be to reach the Moon with minimum fuel expenditure. Or the dynamical system could be a nation's \href{https://en.wikipedia.org/wiki/Economy}{economy}, with the objective to minimize \href{https://en.wikipedia.org/wiki/Unemployment}{unemployment}; the controls in this case could be \href{https://en.wikipedia.org/wiki/Fiscal_policy}{fiscal} \& \href{https://en.wikipedia.org/wiki/Monetary_policy}{monetary policy}. A dynamical system may also be introduced to embed \href{https://en.wikipedia.org/wiki/Operations_research}{operations research problems} within the framework of optimal control theory.'' -- \href{https://en.wikipedia.org/wiki/Optimal_control}{Wikipedia{\tt/}optimal control}
\end{definition}
``Optimal control is an extension of the \href{https://en.wikipedia.org/wiki/Calculus_of_variations}{calculus of variations}, \& is a mathematical optimization method for deriving \href{https://en.wikipedia.org/wiki/Control_theory}{control policies}. The method is largely due to the work of \href{https://en.wikipedia.org/wiki/Lev_Pontryagin}{Lev Pontryagin} \& \href{https://en.wikipedia.org/wiki/Richard_Bellman}{Richard Bellman} in the 1950s, after contributions to calculus of variations by \href{https://en.wikipedia.org/wiki/Edward_J._McShane}{Edward J. McShane}. Optimal control can be seen as a \href{https://en.wikipedia.org/wiki/Control_strategy}{control strategy} in \href{https://en.wikipedia.org/wiki/Control_theory}{control theory}.'' -- \href{https://en.wikipedia.org/wiki/Optimal_control}{Wikipedia{\tt/}optimal control}

\paragraph{General method.} Optimal control deals with the problem of finding a control law for a given system s.t. a certain \href{https://en.wikipedia.org/wiki/Optimality_criterion}{optimality criterion} is achieved. A control problem includes a \href{https://en.wikipedia.org/wiki/Cost_functional}{cost functional} that is a function of state \& control variables. An {\it optimal control} is a set of \href{https://en.wikipedia.org/wiki/Differential_equation}{differential equations} describing the paths of the control variables that minimize the cost function. The optimal control can be derived using \href{https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle}{Pontryagin's maximum principle} (a \href{https://en.wikipedia.org/wiki/Necessary_condition}{necessary condition} also known as Pontryagin's minimum principle or simply Pontryagin's principle), or by solving the \href{https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation}{Hamilton--Jacobi--Bellman equation0} (a \href{https://en.wikipedia.org/wiki/Sufficient_condition}{sufficient condition}).

\begin{example}
	Consider a car traveling in a straight line on a hilly road. Question: How should the driver press the accelerator pedal in order to minimize the total traveling time? The term \emph{control law} refers specifically to the way in which the driver presses the accelerator \& shifts the gears. The \emph{system} consists of both the car \& the road, \& the \emph{optimality criterion} is the minimization of the total traveling time. Control problems usually include ancillary \href{https://en.wikipedia.org/wiki/Constraint_(mathematics)}{constraints}. E.g., the amount of available fuel might be limited, the accelerator pedal cannot be pushed through the floor of the car, speed limits, etc.
\end{example}
A proper cost function will be a mathematical expression giving the traveling time as a function of the speed, geometrical considerations, \& \href{https://en.wikipedia.org/wiki/Initial_condition}{initial conditions} of the system. \href{https://en.wikipedia.org/wiki/Constraint_(mathematics)}{Constraints} are often interchangeable with the cost function.

Another related optimal control problem may be to find the way to drive the car so as to minimize its fuel consumption, given that it must complete a given course in a time not exceeding some amount. Yet another related control problem may be to minimize the total monetary cost of completing the trip, given assumed monetary prices for time \& fuel.

\paragraph{An abstract framework.} Minimize the continuous-time cost functional
\begin{equation}
	J(t_0,t_{\rm f},{\bf x}(\cdot),{\bf u}(\cdot))\coloneqq E({\bf x}(t_0),t_0,{\bf x}(t_{\rm f}),t_{\rm f}) + \int_{t_0}^{t_{\rm f}} F(t,{\bf x}(t),{\bf u}(t))\,{\rm d}t,
\end{equation}
subject to the 1st-order dynamic constraints (the {\it state equation})
\begin{equation}
	\dot{\bf x}(t) = {\bf f}(t,{\bf x}(t),{\bf u}(t)),
\end{equation}
the algebraic {\it path constraints}
\begin{equation}
	{\bf h}(t,{\bf x}(t),{\bf u}(t))\le{\bf 0},
\end{equation}
\& the \href{https://en.wikipedia.org/wiki/Boundary_condition}{endpoint conditions}
\begin{equation}
	{\bf e}(t_0,{\bf x}(t_0),t_{\rm f},{\bf x}(t_{\rm f})) = {\bf 0},
\end{equation}
where ${\bf x}(t)$: the {\it state}, ${\bf u}(t)$ is the {\it control}, $t$: the independent variable (generally speaking, time), $t_0$: the initial time, \& $t_{\rm f}$: the terminal time. The terms $E,F$ are called the {\it endpoint cost} \& {\it running cost}, respectively. In the calculus of variations, $E,F$ are referred to as the Mayer term \& the \href{https://en.wikipedia.org/wiki/Lagrange_multiplier}{Lagrangian}, respectively. Furthermore, it is noted that the path constraints are in general {\it inequality} constraints \& thus may not be active (i.e., $= 0$) at the optimal solution. It is also noted that the optimal control problem as stated above may have multiple solutions (i.e., the solution may not be unique). Thus, it is most often the case that any solution $(t_0^\star,t_{\rm f}^\star,{\bf x}^\star(t),{\bf u}^\star(t))$ to the optimal control problem is {\it locally minimizing{\tt/}minimizer}.

\subsection{Linear Quadratic Control}
A special case of the general nonlinear optimal control problem is the \href{https://en.wikipedia.org/wiki/Linear-quadratic_regulator}{linear quadratic (LQ) optimal control problem}. The LQ problem is stated as follows. Minimize the {\it quadratic} continuous-time cost functional
\begin{equation}
	J = \frac{1}{2}{\bf x}^\top(t_{\rm f})S_{\rm f}{\bf x}(t_{\rm f}) + \frac{1}{2}\int_{t_0}^{t_{\rm f}} {\bf x}^\top(t)Q(t){\bf x}(t) + {\bf u}^\top(t)R(t){\bf u}(t)\,{\rm d}t,
\end{equation}
subject to the linear 1st-order dynamic constraints
\begin{equation}
	\left\{\begin{split}
		\dot{\bf x}(t) &= A(t){\bf x}(t) + B(t){\bf u}(t),\\
		{\bf x}(t_0) &= {\bf x}_0.
	\end{split}\right.
\end{equation}
A particular form of the LQ problem arising in many control system problems is that of the {\it linear quadratic regulator} (LQR) where all of the matrices (i.e., $A,B,Q,R$) are constant, the initial time is arbitrarily set to 0, \& the terminal time is taken in the limit $t_{\rm f}\to0$ (this last assumption is what is known as {\it infinite horizon}). The LQR problem is stated as follows. Minimize the infinite horizon quadratic continuous-time cost functional
\begin{equation}
	J = \frac{1}{2}\int_0^\infty {\bf x}^\top(t)Q{\bf x}(t) + {\bf u}^\top(t)R{\bf u}(t)\,{\rm d}t,
\end{equation}
subject to the {\it linear time-invariant} 1st-order dynamic constraints
\begin{equation}
	\left\{\begin{split}
		\dot{\bf x}(t) &= A{\bf x}(t) + B{\bf u}(t),\\
		{\bf x}(t_0) &= {\bf x}_0.
	\end{split}\right.
\end{equation}
In the finite-horizon case the matrices are restricted in that $Q,R$ are positive semi-definite \& positive definite, respectively. In the infinite-horizon case, however, the matrices $Q,R$ are not only positive-semidefinite \& positive-definite, respectively, but are also constant. These additional restrictions on $Q,R$ in the infinite-horizon case are enforced to ensure that the cost functional remains positive. Furthermore, in order to ensure that the cost function is {\it bounded}, the additional restriction is imposed that the pair $(A,B)$ is \href{https://en.wikipedia.org/wiki/Controllability}{\it controllable}. Note that the LQ or LQR cost functional can be thought of physically as attempting to minimize the {\it control energy} (measured as a quadratic form).

The infinite horizon problem, i.e., LQR, may seem overly restrictive \& essentially useless because it assumes that the operator is driving the system to 0-state \& hence driving the output of the system to 0. This is indeed correct. However the problem of driving the output to a desired nonzero level can be solved {\it after} the zero output one is. In fact, can prove that this secondary LQR problem can be solved in a very straightforward manner. It has been shown in classical optimal control theory that the LQ (or LQR) optimal control has the feedback form
\begin{equation}
	{\bf u}(t) = -K(t){\bf x}(t),
\end{equation}
where $K(t)$ is a properly dimensioned matrix, given as
\begin{equation}
	K(t) = R^{-1}B^\top S(t),
\end{equation}
\& $S(t)$ is the solution of the differential \href{https://en.wikipedia.org/wiki/Riccati_equation}{Riccati equation}. The differential Riccati equation is given as
\begin{equation}
	\dot S(t) = -S(t)A - A^\top S(t) + S(t)BR^{-1}B^\top S(t) - Q.
\end{equation}
For the finite horizon LQ problem, the Riccati equation is integrated backward in time using the terminal boundary condition
\begin{equation}
	S(t_{\rm f}) = S_{\rm f}.
\end{equation}
For the infinite horizon LQR problem, the differential Riccati equation is replaced with the {\it algebraic} Riccati equation (ARE) given as
\begin{equation}
	-SA - A^\top S + SBR^{-1}B^\top S - Q = {\bf 0}.
\end{equation}
Understanding that the ARE arises from infinite horizon problem, the matrices $A,B,Q,R$ are all constant. It is noted that there are in general multiple solutions to the algebraic Riccati equation \& the {\it positive definite} (or positive semi-definite) solution is the one that is used to compute the feedback again. The LQ (LQR) problem was elegantly solved by \href{https://en.wikipedia.org/wiki/Rudolf_E._K%C3%A1lm%C3%A1n}{Rudolf E. K\'alm\'an}.

\subsection{Numerical Methods for Optimal Control}
``Optimal control problems are generally nonlinear \& therefore, generally do not have analytic solutions, e.g., like the linear-quadratic optimal control problem. As a result, it is necessary to employ numerical methods to solve optimal control problems. In the early years of optimal control (c. 1950s--1980s) the favored approach for solving optimal control problems was that of {\it indirect methods}. In an indirect method, the calculus of variations is employed to obtain the 1st-order optimality conditions. These conditions result in a 2-point (or, in the case of a complex problem, a multi-point) \href{https://en.wikipedia.org/wiki/Boundary-value_problem}{BVP}. This BVP actually has a special structure because it arises from taking the derivative of a \href{https://en.wikipedia.org/wiki/Hamiltonian_(control_theory)}{Hamiltonian}. Thus, the resulting \href{https://en.wikipedia.org/wiki/Dynamical_system}{dynamical system} is a \href{https://en.wikipedia.org/wiki/Hamiltonian_system}{Hamiltonian system} of the form
\begin{equation}
	\left\{\begin{split}
		\dot{\bf x} &= \partial_{\boldsymbol{\lambda}}H,\\
		\dot{\boldsymbol{\lambda}} &= -\partial_{\bf x}H,
	\end{split}\right.
\end{equation}
where $H = F + \boldsymbol{\lambda}^\top{\bf f} - \boldsymbol{\mu}^\top{\bf h}$ is the {\it augmented Hamiltonian} \& in an indirect method, the BVP is solved (using the appropriate boundary or {\it transversality} conditions). The beauty of using an indirect method is that the state \& adjoint, i.e., $\boldsymbol{\lambda}$, are solved for \& the resulting solution is readily verified to be an extremal trajectory. The disadvantage of indirect methods is that the BVP is often extremely difficult to solve (particularly for problems that span large time intervals or problems with interior point constraints). A well-known software program that implements direct methods is BNDSCO.

The approach that has risen to prominence in numerical optimal control since the 1980s is that of so-called {\it direct methods}. In a direct method, the state or the control, or both, are approximated using an appropriate function approximation (e.g., polynomial approximation or piecewise constant parameterization). Simultaneously, the cost functional is approximated as a {\it cost function}. Then, the coefficients of the function approximations are treated as optimization variables \& the problem is ``transcribed'' to a nonlinear optimization problem of the form
\begin{equation}
	\min F({\bf z})
\end{equation}
subject to the algebraic constraints
\begin{equation}
	{\bf g}({\bf z}) = {\bf 0},\ {\bf h}({\bf z})\le{\bf 0}.
\end{equation}
Depending upon the type of direct method employed, the size of the nonlinear optimization problem can be quite small (e.g., as in a direct shooting or \href{https://en.wikipedia.org/wiki/Quasilinearization}{quasilinearization} method), moderate (e.g., \href{https://en.wikipedia.org/wiki/Pseudospectral_optimal_control}{pseudospectral optimal control}) or may be quite large (e.g., a direct \href{https://en.wikipedia.org/wiki/Collocation_method}{collocation method}). In the latter case (i.e., a collocation method), the nonlinear optimization problem may be literally thousands $a\cdot10^3$ to tens of thousands $a\cdot10^4$ of variables \& constraints. Given the size of many NLPs arising from a direct method, it may appear somewhat counter-intuitive that solving the nonlinear optimization problem is easier than solving the BVP. It is, however, the fact that the NLP is easier to solve than the BVP. The reason for the relative ease of computation, particularly of a direct collocation method, is that the NLP is {\it sparse} \& many well-known software programs exist (e.g., \href{https://en.wikipedia.org/wiki/SNOPT}{SNOPT}) to solve large sparse NLPs. As a result, the range of problems that can be solved via direct methods (particularly direct {\it collocation methods} which are very popular these days) is significantly larger than the range of problems that can be solved via indirect methods. In fact, direct methods have become so popular these days that many people have written elaborate software programs employing these methods. In particular, many such programs include DIRCOL, SOCS, OTIS, GESOP{\tt/}\href{https://en.wikipedia.org/wiki/ASTOS}{ASTOS}, DITAN., \& PyGMO{\tt/}PyKEP. In recent years, due to the advent of the \href{https://en.wikipedia.org/wiki/MATLAB}{MATLAB} programming language, optimal control software in MATLAB has become more common. Examples of academically developed MATLAB software tools implementing direct methods include RIOTs, \href{https://en.wikipedia.org/wiki/DIDO_(optimal_control)}{DIDO}, DIRECT, FALCON.m, \& GPOPs, while an example of an industry developed MATLAB tool is \href{https://en.wikipedia.org/wiki/PROPT}{PROPT}. These software tools have increased significantly the opportunity for people to explore complex optimal control problems both for academic research \& industrial problems. Finally, it is noted that general-purpose MATLAB optimization environments such as \href{https://en.wikipedia.org/wiki/TOMLAB}{TOMLAB} have made coding complex optimal control problems significantly easier than was previously possible in languages such as C \& \href{https://en.wikipedia.org/wiki/FORTRAN}{FORTRAN}.

\subsection{Discrete-Time Optimal Control}
``The examples thus far have shown \href{https://en.wikipedia.org/wiki/Continuous_time}{continuous time} systems \& control solutions. In fact, as optimal control solutions are now often implemented \href{https://en.wikipedia.org/wiki/Digital_data}{digitally}, contemporary control theory is now primarily concerned with \href{https://en.wikipedia.org/wiki/Discrete_time}{discrete time} systems \& solutions. The theory of Consistent Approximations provides conditions under which solutions to a series of increasingly accurate discretized optimal control problem converge to the solution of the original, continuous-time problem. Not all discretization methods have this property, even seemingly obvious ones. E.g., using a variable step-size routine to integrate the problem's dynamic equations may generate a gradient which does not converge to 0 (or point in the right direction) as the solution is approached. The direct method \href{http://www.schwartz-home.com/RIOTS}{RIOTS} is based on the Theory of Consistent Approximation.

A common solution strategy in many optimal control problems is to solve for the costate (sometimes called the \href{https://en.wikipedia.org/wiki/Shadow_price}{shadow price}) $\lambda(t)$. The costate summaries in 1 number the marginal value of expanding or contracting the state variable next turn. The marginal value is not only the gains accruing to it next turn but associated with the duration of the program. It is nice when $\lambda(t)$ can be solved analytically, but usually, the most one can do is describe it sufficiently well that the intuition can grasp the character of the solution \& an equation solver can solve numerically for the values.

Having obtained $\lambda(t)$, the turn-$t$ optimal value for the control can usually be solved as a differential equation conditional on knowledge of $\lambda(t)$. Again it is infrequent, especially in continuous-time problems, that one obtains the value of the control or the state explicitly. Usually, the strategy is to solve for thresholds \& regions that characterize the optimal control \& use a numerical solver to isolate the actual choice values in time.

\begin{example}
	``Consider the problem of a mine owner who must decide at what rate to extract ore from their mine. They own rights to the ore from date $0$ to date $T$. At date $0$ there is $x_0$ ore in the ground, \& the time-dependent amount of ore $x(t)$ left in the ground declines at the rate of $u(t)$ that the mine owner extracts it. The mine owner extracts ore at cost $\frac{u(t)^2}{x(t)}$ (the cost of extraction increasing with the square of the extraction speed \& the inverse of the amount of ore left) \& sells ore at a constant price $p$. Any ore left in the ground at time $T$ cannot be sold \& has no value (there is no ``scrap value''). The owner chooses the rate of extraction varying with time $u(t)$ to maximize profits over the period of ownership with no time discounting. See discrete-time version vs. Continuous-time version.'' -- \href{https://en.wikipedia.org/wiki/Optimal_control#Finite_time}{Wikipedia{\tt/}optimal control{\tt/}finite time}
\end{example}

\subsection{Optimal Control for PDEs -- Điều Khiển Tối Ưu Cho Phương Trình Vi Phân Đạo Hàm Riêng}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Lions1971}. {\sc Jacques-Louis Lions}. {\it Optimal control of systems governed by PDEs}.
	\item \cite{Troltzsch2010}. {\sc Fredi Tr\"{o}ltzsch}. {\it Optimal Control of PDEs}.
\end{enumerate}

\subsubsection{Optimal Control for Navier--Stokes Equations -- Điều Khiển Tối Ưu Cho Phương Trình Navier--Stokes}

%------------------------------------------------------------------------------%

\section{Stochastic Control}
\textbf{\textsf{Community -- Cộng đồng.}} {\sc Michael Hinterm\"uller, Caroline Geiersbach}.

{\it Stochastic control} or {\it stochastic \href{https://en.wikipedia.org/wiki/Optimal_control}{optimal control}} is a subfield of \href{https://en.wikipedia.org/wiki/Control_theory}{control theory} that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system. The system designer assumes, in a \href{https://en.wikipedia.org/wiki/Bayesian_probability}{Bayesian probability}-driven fashion, that random noise with known \href{https://en.wikipedia.org/wiki/Probability_distribution}{probability distribution} affects the evolution \& observation of the state variables. Stochastic control aims to design the time path of the controlled variables that performs the desired control task with minimum cost, somehow defined, despite the presence of this noise. The context may be either \href{https://en.wikipedia.org/wiki/Discrete_time}{discrete time} or \href{https://en.wikipedia.org/wiki/Continuous_time}{continuous time}.

\paragraph{Certainty equivalence.} ``An extremely well-studied formulation in stochastic control is that of \href{https://en.wikipedia.org/wiki/Linear_quadratic_Gaussian_control}{linear quadratic Gaussian control}. Here the model is linear, the objective function is the expected value of a quadratic form, \& the disturbances are purely additive. A basic result for discrete-time centralized systems with only additive uncertainty is the {\it certainty equivalence property}: that the optimal control solution in this case is the same as would be obtained in the absence of the additive disturbances. This property is applicable to all centralized systems with linear equations of evolution, quadratic cost function, \& noise entering the model only additively; the quadratic assumption allows for the optimal control laws, which follow the certainty-equivalence property to be linear functions of the observations of the controllers.

Any deviation from the above assumptions -- a nonlinear stat equation, a non-quadratic objective function, \href{https://en.wikipedia.org/wiki/Multiplier_uncertainty}{noise in the multiplicative parameters} of the model, or decentralization of control -- causes the certainty equivalence property not to hold. E.g., its failure to hold for decentralized control was demonstrated in \href{https://en.wikipedia.org/wiki/Witsenhausen%27s_counterexample}{Witsenhausen's counterexample}.'' -- \href{https://en.wikipedia.org/wiki/Stochastic_control#Certainty_equivalence}{Wikipedia{\tt/}stochastic control{\tt}certainty equivalence}.

\paragraph{Discrete time.} ``In a discrete-time context, the decision-maker observes the state variable, possibly with observational noise, in each time period. The objective may be to optimize the sum of expected values of a nonlinear (possibly quadratic) objective function over all the time periods from the present to the final period of concern, or to optimize the value of the objective function as of the final period only. At each time period new observations are made, \& the control variables are to be adjusted optimally. Finding the optimal solution for the present time may involve iterating a \href{https://en.wikipedia.org/wiki/Linear-quadratic-Gaussian_control#Discrete_time}{matrix Riccati equation} backwards in time from the last period to the present period.

In the discrete-time case with uncertainty about the parameter values in the transition matrix (giving the effect of current values of the state variables on their own evolution) \&{\tt/}or the control response matrix of the state equation, but still with a linear state equation \& quadratic objective function, a Riccati equation can still be obtained for iterating backward to each period's solution even though certainty equivalence does not apply. The discrete-time case of a non-quadratic loss function but only additive disturbances can also be handled, albeit with more complications.

\begin{example}
	A typical specification of the discrete-time stochastic linear quadratic control problem is to minimize
	\begin{equation}
		{\rm E}_1\sum_{t=1}^S y_t^\top Qy_t + u_t^\top Ru_t,
	\end{equation}
	where ${\rm E}_1$ is the \href{https://en.wikipedia.org/wiki/Expected_value}{expected value} operator conditional on $y_0$, $S$: the time horizon, subject to the state equation $y_t = A_ty_{t-1} + B_tu_t$.
\end{example}

\paragraph{Continuous time.} ``If the model is in continuous time, the controller knows the state of the system at each instant of time. The objective is to maximize either an integral of, e.g., a concave function of a state variable over a horizon from time 0 (the present) to a terminal time $T$, or a concave function of a state variable at some future date $T$. As time evolves, new observations are continuously made \& the control variables are continuously adjusted in optimal fashion.''

\paragraph{Stochastic model predictive control.} ``In the literature, there are 2 types of MPCs for stochastic systems; Robust model predictive control \& Stochastic Model Predictive Control (SMPC). Robust model predictive control is a more conservative method which considers the worst scenario in the optimization procedure. However, this method, similar to other robust controls, deteriorates the overall controller's performance \& also is applicable only for systems with bounded uncertainties. The alternative method, SMPC, considers soft constraints which limit the risk of violation by a probabilistic inequality.

\begin{example}[Stochastic model predictive control in finance]
	``In a continuous time approach in a \href{https://en.wikipedia.org/wiki/Finance}{finance} context, the state variable in the stochastic differential equation is usually wealth or net worth, \& the controls are the shares placed at each time in the various assets. Given the \href{https://en.wikipedia.org/wiki/Asset_allocation}{asset allocation} chosen at any time, the determinants of the change in wealth are usually the stochastic returns to assets \& the interest rate on the risk-free asset. The field of stochastic control has developed greatly since the 1970s, particularly in its applications to finance. Robert Merton used stochastic control to study \href{https://en.wikipedia.org/wiki/Optimal_portfolio}{optimal portfolios} of safe \& risky assets. \href{https://en.wikipedia.org/wiki/Merton%27s_portfolio_problem}{Merton's portfolio problem} \& that of \href{https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model}{Black Scholes} changed the nature of the finance literature. Influential mathematical textbook treatments were by Fleming \& Rishel, \& by Fleming \& Soner. These techniques were applied by Stein to the \href{https://en.wikipedia.org/wiki/Financial_crisis_of_2007%E2%80%9308}{financial crisis of 2007--08}.

	The maximization, say of the expected logarithm of net work at a terminal date $T$, is subject to stochastic processes on the components of wealth. In this case, in continuous time \href{https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma}{It\^o's equation} is the main tool of analysis. In the case where the maximization is an integral of a concave function of utility over an horizon $(0,T)$, dynamic programming is used. There is no certainty equivalence as in the older literature, because the coefficients of the control variables -- i.e., the returns received by the chosen shares of assets -- are stochastic.
\end{example}

%------------------------------------------------------------------------------%

\section{Shape Optimization -- Tối Ưu Hình Dạng}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Allaire_Henrot2001}. {\sc Gr\'egoire Allaire, Antoine Henrot}. {\it On some recent advances in shape optimization}.
	\item \cite{Azegami2020}. {\sc Hideyuki Azegami}. {\it Shape Optimization Problems}.
	\item \cite{Bandle_Wagner2023}. {\sc Catherine Bandle, Alfred Wagner}. {\it Shape Optimization: Variations of Domains \& Applications}.
	\item \cite{Delfour_Zolesio2001,Delfour_Zolesio2011}. {\sc Michael C. Delfour, Jean-Paul Zol\'{e}sio}. {\it Shapes \& Geometries}.
	\item \cite{Haslinger_Makinen2003}. {\sc J. Haslinger, R. A. E. M\"{a}kinen}. {\it Introduction to Shape Optimization}.
	\item \cite{Mohammadi_Pironneau2010}. {\sc Bijan Mohammadi, Olivier Pironneau}. {\it Applied Shape Optimization for Fluids}.
	\item \cite{Moubachir_Zolesio2006}. {\sc Marwan Moubachir, Jean-Paul Zol\'{e}sio}. {\it Moving Shape Analysis \& Control}.
	\item {\sc Stephan Schmidt}. Master course: {\it Shape \& Geometry}. Humboldt University of Berlin. [written in German, taught in English \& German].
	\item \cite{Sokolowski_Zolesio1992}. {\sc Jan Soko\l owski, Jean-Paul Zol\'{e}sio}. {\it Introduction to Shape Optimization}.
	\item \cite{Walker2015}. {\sc Shawn W. Walker}. {\it The Shapes of Things}.

	{\bf Differential equations on surfaces.} Differential geometry is useful for understanding mathematical models containing geometric PDEs, e.g., surface{\tt/}manifold version of the standard Laplace equation, which requires the development of the surface gradient \& surface Laplacian operators -- the usual gradient $\nabla$ \& Laplacian $\Delta = \nabla\cdot\nabla$ operators defined on a surface (manifold) instead of standard Euclidean space $\mathbb{R}^n$. {\it Advantage}: provide alternative formulas for geometric quantities, e.g., the summed (mean) curvature, that are much clearer than the usual presentation of texts on differential geometry.

	{\bf Differentiating w.r.t. Shape.} The approach to differential geometry is advantageous for developing the framework of {\it shape differential calculus} -- the study of how quantities change w.r.t. changes of independent ``shape variable''.

	``The framework of shape differential calculus provides the tools for developing the equations of mean curvature flow \& Willmore flow, which are geometric flows occurring in many applications such as fluid dynamics \& biology.'' -- \cite[p. 2]{Walker2015}

	The shape perturbation $\delta J(\Omega;V)$ is similar to the gradient operator, which is a directional derivative, analogous to $V\cdot\nabla f$ where $V$ is a given direction, providing information about the local slope, or the sensitivity of a quantity w.r.t. some parameters.

	It takes only 2 or 3 numbers to specify a point $(x,y)$ in 2D \& a point $(x,y,z)$ in 3D, whereas an ``infinite'' number of coordinate pairs is needed to specify a domain $\Omega$. $V$ is a 2D{\tt/}3D vector in the scalar function setting; for a shape functional, $V$ is a full-blown function requiring definition at every point in $\Omega$. This ``infinite dimensionality'' is the reason for using the notation $\delta J(\Omega;V)$ to denote a shape perturbation. $\delta J(\Omega;V)$ indicates how we should change $\Omega$ to decrease $J$, similarly to how $\nabla f(x,y)$ indicates how the coordinate pair $(x,y)$ should change to decrease $f$, which opens up the world of shape optimization.

	{\bf3 schools of shape optimization.} Cf. engineering shape optimization vs. applied shape optimization \cite{Mohammadi_Pironneau2010} vs. theoretical shape optimization \cite{Sokolowski_Zolesio1992,Delfour_Zolesio2011}.

	Shape perturbations allow us to ``climb down the hill'' in the infinite dimensional setting of shape, which is a powerful tool for producing sophisticated engineering designs in an automatic way.

	{\bf Extrinsic vs. intrinsic point of views.} To make the discussion as clear as possible, we adopt the {\it extrinsic} point of view: curves \& surfaces are assumed to lie in a Euclidean space of higher dimension. The ambient space is 3D Euclidean space. Alternatively, there is the {\it intrinsic} point of view, i.e., the surface is not assumed to lie in an ambient space, i.e., one is not allowed to reference anything ``outside'' of the surface when defining it. Moreover, no mathematical structures ``outside'' of the surface can be utilized. Walker \cite{Walker2015} did not adopt the intrinsic view or consider higher dimensional manifolds, general embedding dimensions, etc. for the reasons: \cite{Walker2015} is meant as a {\it practical guide} to differential geometry \& shape differentiation that can be used by researchers in other fields.
	\begin{itemize}
		\item \cite{Walker2015} is meant to be used as background information for deriving physical models where geometry plays a critical role. Because most physical problems of interest take place in 3D Euclidean space, the extrinsic viewpoint is sufficient.
		\item Many of the proofs \& derivations of differential geometry relations simplify dramatically for 2D surfaces in 3D \& require only basic multivariable calculus \& linear algebra.
		\item The concepts of {\it normal vectors} \& {\it curvature} are harder to motivate with the intrinsic viewpoint. {\it What does it meant for a surface to ``curve through space'' if you cannot talk about the ambient space?}
		\item Walker wants to keep in mind applications of this machinery to geometric PDEs, fluid dynamics, numerical analysis, optimization, etc. An interesting application of this methodology is for the development of numerical methods for mean curvature flow \& surface tension driven fluid flow. Ergo ($=$ therefore), the extrinsic viewpoint is often more convenient for computational purposes.
		\item Walker wants his framework to be useful for analyzing \& solving {\it shape optimization} problems, i.e., optimization problems where geometry{\tt/}shape is the control variable.
	\end{itemize}
	{\bf Prerequisites.} ``When reading any mathematical text, the reader must have a certain level of mathematical ``maturity'' in order to efficiently learn what is in the text.
\end{enumerate}

\begin{example}[\cite{Walker2015}, Sect. 1.2.1, pp. 1--2]
	Let $f = f(r,\theta)$ be a smooth function defined on the disk $B_{R,2}(0,0)$ of radius $R$ in terms of polar coordinates. The integral of $f$ over $B_{2,R}(0,0)$ $J\coloneqq\int_{B_{2,R}(0,0)} f\,{\rm d}{\bf x} = \int_0^{2\pi}\int_0^R f(r,\theta)\,{\rm d}r\,{\rm d}\theta$ depends on $R$. Assume $f$ also depends on $R$, i.e., $f = f(r,\theta,R)$ with a physical example: $J$ is the \emph{net flow rate} of liquid through a pipe with cross-section $\Omega$, then $f$ is the flow rate per unit area \& could be the solution of a PDE defined on $\Omega$, e.g., a Navier--Stokes fluid flowing in a circular pipe. Advantageous to know the \emph{sensitivity} of $J$ w.r.t. $R$, e.g., for optimization purposes. Differentiate $J$ w.r.t. $R$:
	\begin{equation*}
		\frac{d}{dR}J = \int_0^{2\pi}\left(\frac{d}{dR}\int_0^R f(r,\theta;R)r\,{\rm d}r\right){\rm d}\theta = \int_0^{2\pi}\int_0^R f'(r,\theta;R)r\,{\rm d}r\,{\rm d}\theta + \int_0^{2\pi} f(R,\theta;R)\,{\rm d}\theta.
	\end{equation*}
	The dependence of $f$ on $R$ can more generally be viewed as dependence on $B_{R,2}(0,0)$, i.e., $f(\cdot;R)\equiv f(\cdot;B_{R,2}(0,0))$. Rewriting $d/dR J$ using Cartesian coordinates ${\bf x}$:
	\begin{equation}
		\frac{d}{dR}J = \int_{B_{R,2}(0,0)} f'({\bf x};\Omega)\,{\rm d}{\bf x} + \int_{S_{R,2}(0,0)} f({\bf x};\Omega)\,{\rm d}S({\bf x}),
	\end{equation}
	where ${\rm d}{\bf x}$ is the volume measure, ${\rm d}S({\bf x})$ is the surface area measure.
\end{example}

\begin{example}[Surface height function of a hill]
	Let $f = f(x,y)$ be a function describing the surface height of the hill, where $(x,y)$ are the coordinates of our position. Then, by using basic multivariate calculus, finding a direction that will move us downhill is equivalent to computing the gradient (vector) of $f$ \& moving in the opposite direction to the gradient. In this sense, we do not need to ``see'' the whole function. We just need to \emph{locally} compute the gradient $\nabla f$, analogous to feeling the ground beneath.
\end{example}

\begin{example}[Engineering shape optimization: minimizing drag with Navier--Stokes flow of fluid past a rigid body, \cite{Walker2015}, pp. 3--5]
	A shape functional representing the drag:
	\begin{equation}
		J_{\rm d}(\Omega)\coloneqq-{\bf u}_{\rm out}\cdot\int_{\Gamma_0} \boldsymbol{\sigma}({\bf u},p){\bf n}\,{\rm d}\Gamma = \frac{2}{{\rm Re}}\int_\Omega |\boldsymbol{\varepsilon}({\bf u})|^2\,{\rm d}{\bf x}\ge0,
	\end{equation}
	which physically represents the net force that must be applied to $\Omega_{\rm B}$ to keep it stationary while being acted upon by the imposed flow field \& represents the total amount of viscous dissipation of energy (per unit of time) in the fluid domain $\Omega$. Using the machinery of shape perturbations, $\delta J_{\rm d}(\Omega;V)$ indicates how $J_{\rm d}$ changes when we perturb $\Omega$ in the direction $V$. Hence, we can use this information to change $\Omega$ in small steps so as to slowly deform $\Omega$ into a shape that has better (lower) drag characteristics. A numerical computation: 2 large vortices appear behind the body, which indicate a large amount of viscous dissipation, i.e., large drag. The optimization process then computes $\delta J_{\rm d}(\Omega^0;V)$ for many different choices of $V$ \& chooses the one that drives down $J_{\rm d}$ the most. This choice of $V$ is used to deform $\Gamma_{\rm B}^0$ into a new shape $\Gamma_{\rm B}^1$ at iteration 1, with only a small difference between $\Gamma_{\rm B}^0$ \& $\Gamma_{\rm B}^1$. This process is repeated many times. Note how the vortices are eliminated by the more slender shape.
\end{example}

\begin{itemize}
	\item Condition (V)***
	\item Family of transformations $\{T_s:0\le s\le\tau\}$.
	\item Perturbed domain $\Omega_s = \Omega_s(V) = T_s(V)(\Omega)$.
	\item Assume that the velocity field $V$ satisfies $V_D$ and, in addition, $V\in C^0([0,\tau];C_{\rm loc}^1(\mathbb{R}^d,\mathbb{R}^d))$ \& $\tau > 0$ is small enough such that the Jacobian $J_s$ is strictly positive: $J_s(X)\coloneqq\det DT_s(X) > 0$, $\forall s\in[0,\tau]$, where $DT_s(X)$ is the \textit{Jacobian matrix} of the transformation $T_s = T_s(V)$ associated with the velocity vector field $V$.
\end{itemize}

\subsection{Domain Integrals}

\begin{itemize}
	\item Given $\varphi\in W_{\rm loc}^{1,1}(\mathbb{R}^d)$, consider for $s\in[0,\tau]$ the volume integral
	\begin{equation}
		J(\Omega_s(V))\coloneqq\int_{\Omega_s(V)} \varphi\,{\rm d}{\bf x} = \int_\Omega \varphi\circ T_sJ_s\,{\rm d}{\bf x}.
	\end{equation}
	\begin{equation}
		dJ(\Omega;V) = \frac{d}{ds}J(\Omega_s(V))|_{s = 0} = \int_\Omega \nabla\varphi\cdot V(0) + \varphi\operatorname{div}V(0)\,{\rm d}{\bf x} = \int_\Omega \operatorname{div}(\varphi V(0))\,{\rm d}{\bf x}.
	\end{equation}
	If $\Omega$ has a Lipschitzian boundary, by Stokes's theorem:
	\begin{equation}
		dJ(\Omega;V) = \int_\Gamma \varphi V(0)\cdot{\bf n}\,{\rm d}\Gamma.
	\end{equation}

	\begin{theorem}[\cite{Delfour_Zolesio2011}, Thm. 4.1, pp. 482--483]
		Let $\varphi\in W_{\rm loc}^{1,1}(\mathbb{R}^d)$. Assume that the vector field $V = \{V(s):0\le s\le\tau\}$ satisfies condition (V).
		\item[(i)] For each $s\in[0,\tau]$ the map $\varphi\mapsto\varphi\circ T_s:W_{\rm loc}^{1,1}(\mathbb{R}^d)\to W_{\rm loc}^{1,1}(\mathbb{R}^d)$ \& its inverse are both locally Lipschitzian \& $\nabla(\varphi\circ T_s) = {}^*DT_s\nabla\varphi\circ T_s$.
		\item[(ii)] If $V\in C^0([0,\tau];C_{\rm loc}^1(\mathbb{R}^d,\mathbb{R}^d))$, then the map $s\mapsto J_s:[0,\tau]\to C_{\rm loc}^0(\mathbb{R}^d)$ is differentiable and
		\begin{equation}
			\frac{dJ_s}{ds} = [\nabla\cdot V(s)]\circ T_sJ_s\in C_{\rm loc}^0(\mathbb{R}^d).
		\end{equation}
		Hence the map $s\mapsto J_s$ belongs to $C^1([0,\tau];C_{\rm loc}^0(\mathbb{R}^d))$.
	\end{theorem}

	\begin{align}
		\frac{d}{ds}DT_s(X) &= DV(s,T_s(X))DT_s(X),\ DT_0(X) = I,\\
		\frac{d}{ds}\det DT_s(X) &= \operatorname{tr}DV(s,T_s(X))\det DT_s(X) = \nabla\cdot V(s,T_s(X))\det DT_s(X),\ \det DT_0(X) = 1.
	\end{align}

	\begin{theorem}[\cite{Delfour_Zolesio2011}, Thm. 4.2, pp. 483--484]
		Assume that there exists $\tau > 0$ such that the velocity field $V(t)$ satisfies conditions (V) \& $V\in C^0([0,\tau];C_{\rm loc}^1(\mathbb{R}^d,\mathbb{R}^d))$. Given a function $\varphi\in C(0,\tau;W_{\rm loc}^{1,1}(\mathbb{R}^d))\cap C^1(0,\tau;L_{\rm loc}^1(\mathbb{R}^d))$ \& a bounded measurable domain $\Omega$ with boundary $\Gamma$, the semiderivative of the function $J_V(s)\coloneqq\int_{\Omega_s(V)} \varphi(s)\,{\rm d}{\bf x}$ at $s = 0$ is given by
		\begin{equation}
			dJ_V(0) = \int_\Omega \varphi'(0) + \operatorname{div}(\varphi(0)V(0))\,{\rm d}{\bf x},
		\end{equation}
		where $\varphi(0)({\bf x})\coloneqq\varphi(0,{\bf x})$ \& $\varphi'(0)({\bf x})\coloneqq\partial_t\varphi(0,{\bf x})$. In addition, $\Omega$ is an open domain with a Lipschitzian boundary $\Gamma$, then
		\begin{equation}
			dJ_V(0) = \int_\Omega \varphi'(0)\,{\rm d}{\bf x} + \int_\Gamma \varphi(0)V(0)\cdot{\bf n}\,{\rm d}{\bf x}.
		\end{equation}
	\end{theorem}

\end{itemize}

\subsection{Boundary Integrals}

\subsection{Material derivatives}
Let $\Omega\subset D$ be a bounded domain,

%------------------------------------------------------------------------------%

\section{Topology Optimization -- Tối Ưu Tôpô}

%------------------------------------------------------------------------------%

\section{Google OR-Tools}

%------------------------------------------------------------------------------%

\subsection{Get Started}

%------------------------------------------------------------------------------%

\subsubsection{About OR-Tools}
OR-Tools is open source software for {\it combinatorial optimization}, which seeks to find best solution to a problem out of a very large set of possible solutions. Some examples of problems that OR-Tools solves:
\begin{enumerate}
    \item Vehicle routing: Find optimal routes for vehicle fleets that pick up \& deliver packages given constraints, e.g., ``this truck can't hold $> 20000$ pounds'' or ``all deliveries must be made within a 2-hour window''.
    \item Scheduling: Find optimal schedule for a complex set of tasks, some of which need to be performed before others, on a fixed set of machines, or other resources.
    \item Bin packing: Pack as many objects of various sizes as possible into a fixed number of bins with maximum capacities.
\end{enumerate}
In most cases, problems like these have a vast number of possible solutions -- too many for a computer to search them all. To overcome this, OR-Tools uses state-of-art algorithms to narrow down search set, in order to find an optimal (or close to optimal) solution.

-- OR-Tools là phần mềm mã nguồn mở dùng cho {\it tối ưu hóa tổ hợp}, tìm kiếm giải pháp tốt nhất cho một vấn đề từ một tập hợp rất lớn các giải pháp khả thi. Một số ví dụ về các vấn đề mà OR-Tools giải quyết:
\begin{enumerate}
    \item Định tuyến xe: Tìm tuyến đường tối ưu cho đội xe nhận \& giao hàng với các ràng buộc, ví dụ: ``xe tải này không thể chở > 20000 pound'' hoặc ``tất cả các lần giao hàng phải được thực hiện trong vòng 2 giờ''.
    \item Lập lịch: Tìm lịch trình tối ưu cho một tập hợp các tác vụ phức tạp, một số tác vụ cần được thực hiện trước các tác vụ khác, trên một tập hợp máy móc cố định hoặc các nguồn lực khác.
    \item Đóng gói thùng: Đóng gói càng nhiều vật thể có kích thước khác nhau càng tốt vào một số lượng thùng cố định với sức chứa tối đa.
\end{enumerate}
Trong hầu hết các trường hợp, những vấn đề như thế này có rất nhiều giải pháp khả thi -- quá nhiều đến mức máy tính không thể tìm kiếm hết được. Để khắc phục điều này, OR-Tools sử dụng các thuật toán tiên tiến để thu hẹp phạm vi tìm kiếm, nhằm tìm ra giải pháp tối ưu (hoặc gần tối ưu).

OR-Tools includes solvers for:
\begin{enumerate}
    \item {\bf Constraint Programming.} A set of techniques for finding feasible solutions to a problem expressed as {\it constraints} (e.g., a room can't be used for 2 events simultaneously, or distance to crops must be less than length of hose, or no more than 5 TV shows can be recorded at once).
    \item {\bf Linear \& Mixed-Integer Programming.} Glop linear optimizer finds optimal value of a linear objective function, given a set of linear inequalities as constraints (e.g., assigning people to jobs, or finding best allocation of a set of resources while minimizing cost). Glop \& mixed-integer programming software SCIP \url{http://scip.zib.de/} are also available via Google Apps Script \href{https://developers.google.com/apps-script/reference/optimization}{Optimization Service}.
    \item {\bf Vehicle Routing.} A specialized library for identifying best vehicle routes given constraints.
    \item {\bf Graph Algorithms.} Code for finding shortest paths in graphs, min-cost flows, max flows, \& linear sum assignments.
\end{enumerate}
-- OR-Tools bao gồm các trình giải cho:
\begin{enumerate}
    \item {\bf Lập trình Ràng buộc.} Một tập hợp các kỹ thuật để tìm ra các giải pháp khả thi cho một vấn đề được biểu thị dưới dạng {\it ràng buộc} (ví dụ: một phòng không thể được sử dụng cho 2 sự kiện cùng lúc, hoặc khoảng cách đến cây trồng phải nhỏ hơn chiều dài của vòi, hoặc không thể ghi hình quá 5 chương trình truyền hình cùng một lúc).
    \item {\bf Lập trình Tuyến tính \& Số nguyên Hỗn hợp.} Trình tối ưu hóa tuyến tính Glop tìm giá trị tối ưu của hàm mục tiêu tuyến tính, cho trước một tập hợp các bất đẳng thức tuyến tính làm ràng buộc (ví dụ: phân công nhân viên cho các công việc, hoặc tìm cách phân bổ tốt nhất một tập hợp các nguồn lực trong khi giảm thiểu chi phí). Phần mềm lập trình Glop \& số nguyên hỗn hợp SCIP \url{http://scip.zib.de/} cũng có sẵn thông qua Google Apps Script \href{https://developers.google.com/apps-script/reference/optimization}{Optimization Service}.
    \item {\bf Định tuyến xe.} Một thư viện chuyên biệt để xác định các tuyến đường xe tốt nhất với các ràng buộc nhất định.
    \item {\bf Thuật toán đồ thị.} Mã để tìm đường đi ngắn nhất trong đồ thị, luồng chi phí tối thiểu, luồng tối đa, \& phép gán tổng tuyến tính.
\end{enumerate}

%------------------------------------------------------------------------------%

\subsubsection{Get Started Guides}
OR-Tools is written in C++, but can also use it with Python, Java, or .Net. Following sects will quickly get you started creating \& running OR-Tools programs in each of supported languages.

\paragraph{Get started with OR-Tools for C++.}

{\bf What is an optimization problem?} Goal of optimization: find best solution to a problem out of a large set of possible solutions. (Sometimes you will be satisfied with finding any feasible solution; OR-Tools can do that as well.)

-- {\bf Bài toán tối ưu hóa là gì?} Mục tiêu của tối ưu hóa: tìm ra giải pháp tốt nhất cho một vấn đề từ một tập hợp lớn các giải pháp khả thi. (Đôi khi bạn sẽ hài lòng với việc tìm ra bất kỳ giải pháp khả thi nào; OR-Tools cũng có thể làm được điều đó.)

Here is a typical optimization problem. Suppose a shipping company delivers packages to its customers using a fleet of trucks. Every day, company must assign packages to trucks, \& then choose a route for each truck to deliver its packages. Each possible assignment of packages \& routes has a cost, based on total travel distance for trucks, \& possibly other factors as well. Problem: choose assignments of packages \& routes that has least costs.

-- Đây là một bài toán tối ưu hóa điển hình. Giả sử một công ty vận chuyển giao hàng cho khách hàng bằng một đội xe tải. Mỗi ngày, công ty phải phân công hàng hóa cho các xe tải, \& sau đó chọn tuyến đường cho mỗi xe tải để giao hàng. Mỗi cách phân công hàng hóa \& tuyến đường có thể có đều có chi phí, dựa trên tổng quãng đường di chuyển của xe tải, \& có thể có các yếu tố khác. Bài toán: chọn cách phân công hàng hóa \& tuyến đường có chi phí thấp nhất.

Like all optimization problems, this problem has following elements:
\begin{itemize}
    \item {\bf Objective}: quantity you want to optimize. In example above, objective: minimize cost. To set up an optimization problem, you need to define a function that calculates value of objective for any possible solution. This is called {\it objective function}. In preceding example, objective function would calculate total cost of any assignment of packages \& routes. An {\it optimal} solution is one for which value of objective function is best. (``Best'' can be either a maximum or a minimum.)
    \item {\bf Constraints}: restrictions on set of possible solutions, based on specific requirements of problem. E.g., if shipping company can't assign packages above a given weight to trucks, this would impose a constraint on solutions. A {\it feasible} solution is one that satisfies all given constraints for problem, without necessarily being optimal.
\end{itemize}
1st step in solving an optimization problem is identifying objective \& constraints.

-- Giống như tất cả các bài toán tối ưu hóa, bài toán này có các yếu tố sau:
\begin{itemize}
    \item {\bf Mục tiêu}: số lượng bạn muốn tối ưu hóa. Trong ví dụ trên, mục tiêu: giảm thiểu chi phí. Để thiết lập một bài toán tối ưu hóa, bạn cần định nghĩa một hàm tính toán giá trị của mục tiêu cho bất kỳ giải pháp khả thi nào. Hàm này được gọi là {\it hàm mục tiêu}. Trong ví dụ trước, hàm mục tiêu sẽ tính toán tổng chi phí của bất kỳ phép gán nào cho các gói hàng \& tuyến đường. Một giải pháp {\it tối ưu} là giải pháp mà giá trị của hàm mục tiêu là tốt nhất. (``Tốt nhất'' có thể là giá trị cực đại hoặc cực tiểu.)
    \item {\bf Ràng buộc}: các ràng buộc về tập hợp các giải pháp khả thi, dựa trên các yêu cầu cụ thể của bài toán. Ví dụ: nếu công ty vận chuyển không thể gán các gói hàng có trọng lượng vượt quá giới hạn cho xe tải, điều này sẽ áp đặt một ràng buộc lên các giải pháp. Một giải pháp {\it khả thi} là giải pháp thỏa mãn tất cả các ràng buộc cho trước của bài toán, mà không nhất thiết phải là giải pháp tối ưu.
\end{itemize}
Bước đầu tiên trong việc giải quyết một bài toán tối ưu hóa là xác định các ràng buộc \& mục tiêu.

{\bf Solving an optimization problem in C++.} Next, give an example of an optimization problem, \& show how to set up \& solve it in C++.
\begin{itemize}
    \item {\bf A linear optimization example.} 1 of oldest \& most widely-used areas of optimization is linear optimization or linear programming, in which objective function \& constraints can be written as linear expressions. Here is a simple example of this type of problem. Maximize $3x + y$ subjects to constraints $x\in[0,1],y\in[0,2],x + y\le2$. Objective function in this example is $3x + y$. Both objective function \& constraints are given by linear expressions, which makes this a linear problem.
    \item {\bf Main steps in solving problem.} For each language, basic steps for setting up \& solving a problem are same:
    \begin{enumerate}
        \item Import required libraries
        \item Declare solver
        \item Create variables
        \item Define constraints
        \item Define objective function
        \item Invoke solver
        \item Display results.
    \end{enumerate}
    \item {\bf C++ program.} This sect walks through a C++ program that sets up \& solves problem. Here are the steps:
    \begin{enumerate}
        \item Import required libraries.
        \item Declare solver. {\tt MPSolver} is a wrapper for solving any linear programming or mixed integer programming problems.
        \item Create variables.
        \item Define constraints. 1st 2 constraints $x\in[0,1],y\in[0,2]$, are already set by defs of variables. Following code defines constraint $x + y\le2$. Method {\tt SetCoefficient} sets coefficients of $x,y$ in expression for constraint.
        \item Define objective function. Method {\tt SetMaximization} declares this to be a maximization problem.
        \item Invoke solver \& display results.
    \end{enumerate}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{Scheduling}

%------------------------------------------------------------------------------%

\subsubsection{Scheduling Overview}
Companies that manage extensive operations, which require assigning people \& resources to tasks at specific times, need to solve difficult scheduling problems on a regular basis. Here are a couple of examples of such problems:
\begin{enumerate}
    \item Schedule employees in multiple shifts, subject to a complex set of constraints \& staffing requirements.
    \item Schedule a manufacturing process that involves performing many tasks on a limited set of machines, each of which can do only 1 task at a time.
\end{enumerate}
OR-Tools provides powerful techniques for solving problems like these. Following sects illustrate some scheduling problems \& their solutions.

%------------------------------------------------------------------------------%

\subsubsection{Employee Scheduling}

%------------------------------------------------------------------------------%

\subsubsection{Job Shop Problem}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
    \item \url{https://developers.google.com/optimization/scheduling/job_shop}.
\end{enumerate}
1 common scheduling problem is {\it job shop}, in which multiple jobs are processed on several machines. Each job consists of a sequence of tasks, which must be performed in a given order, \& each task must be processed on a specific machine. E.g., job could be manufacture of a single consumer item, e.g. an automobile. Problem: schedule tasks on machines so as to minimize {\tt length} of schedule -- time it takes for all jobs to be completed.

-- Một vấn đề lập lịch phổ biến là {\tt job shop}, trong đó nhiều công việc được xử lý trên nhiều máy. Mỗi công việc bao gồm một chuỗi các tác vụ, phải được thực hiện theo một thứ tự nhất định, \& mỗi tác vụ phải được xử lý trên một máy cụ thể. Ví dụ: công việc có thể là sản xuất một mặt hàng tiêu dùng duy nhất, ví dụ như ô tô. Vấn đề: lập lịch các tác vụ trên máy sao cho tối thiểu hóa {\tt length} của lịch trình -- thời gian cần thiết để hoàn thành tất cả các công việc.

There are several constraints for the job shop problem:
\begin{enumerate}
    \item No task for a job can be started until previous task for that job is completed.
    \item A machine can only work on 1 task at a time.
    \item A task, once started, must run to completion.
\end{enumerate}
-- Có một số ràng buộc đối với bài toán job shop:
\begin{enumerate}
    \item Không thể bắt đầu bất kỳ tác vụ nào cho một tác vụ cho đến khi tác vụ trước đó của tác vụ đó hoàn thành.
    \item Một máy chỉ có thể xử lý 1 tác vụ tại một thời điểm.
    \item Một tác vụ, sau khi đã bắt đầu, phải chạy cho đến khi hoàn thành.
\end{enumerate}

\paragraph{Example Problem.} Below is a simple example of a job shop problem, in which each task is labeled by a pair of numbers $(m,p)$ where $m$ is number of machine the task must be processed on \& $p$ is {\it processing time} of task -- amount of time it requires. (Numbering of jobs \& machines starts at 0.)
\begin{enumerate}
    \item job 0 = [(0, 3), (1, 2), (2, 2)]
    \item job 1 = [(0, 2), (2, 1), (1, 4)]
    \item job 2 = [(1, 4), (2, 3)]
\end{enumerate}
In the example, job 0 has three tasks. The 1st, (0, 3), must be processed on machine 0 in 3 units of time. The 2nd, (1, 2), must be processed on machine 1 in 2 units of time, \& so on. Altogether, there are 8 tasks.

{\bf A solution for problem.} A solution to job shop problem is an assignment of a start time for each task, which meets constraints given above. Diagram below shows 1 possible solution for problem. Can check: tasks for each job are scheduled at non-overlapping time intervals, in order given by problem. Length of this solution is 12, which is 1st time when all 3 obs are complete. However, this is not optimal solution to problem.

-- {\bf Một giải pháp cho bài toán này.} Một giải pháp cho bài toán Job Shop là gán thời gian bắt đầu cho mỗi tác vụ, đáp ứng các ràng buộc nêu trên. Sơ đồ bên dưới cho thấy 1 giải pháp khả thi cho bài toán. Có thể kiểm tra: các tác vụ cho mỗi tác vụ được lên lịch theo các khoảng thời gian không chồng chéo, theo thứ tự được cho bởi bài toán. Độ dài của giải pháp này là 12, tức là lần đầu tiên cả 3 tác vụ đều hoàn thành. Tuy nhiên, đây không phải là giải pháp tối ưu cho bài toán.

{\bf Variables \& constraints for problem.} This sect describes how to set up variables \& constraints for problem. 1st, let {\tt task(i, j)} denote $j$th task in sequence for job $i$. E.g., {\tt task(0, 2)} denotes 2nd task for job 0, which corresponds to pair $(1,2)$ in problem description.

-- {\bf Biến \& ràng buộc cho bài toán.} Phần này mô tả cách thiết lập biến \& ràng buộc cho bài toán. 1. Đầu tiên, hãy để {\tt task(i, j)} biểu thị nhiệm vụ thứ $j$ trong chuỗi cho công việc $i$. Ví dụ: {\tt task(0, 2)} biểu thị nhiệm vụ thứ 2 cho công việc 0, tương ứng với cặp $(1,2)$ trong mô tả bài toán.

Next, define $t_{ij}$ to be start time for {\tt task(i, j)}. $t_{ij}$ are variables in JSSP. Finding a solution involves determining values for these variables that meet requirement of problem.

-- Tiếp theo, hãy định nghĩa $t_{ij}$ là thời gian bắt đầu cho {\tt task(i, j)}. $t_{ij}$ là các biến trong JSSP. Việc tìm kiếm giải pháp liên quan đến việc xác định giá trị của các biến này sao cho đáp ứng được yêu cầu của bài toán.

There are 2 types of constraints for JSSP:
\begin{enumerate}
    \item {\bf Precedence constraints.} These arise from condition that for any 2 consecutive tasks in same job, the 1st must be completed before the 2nd can be started. E.g., {\tt task(0,2), task(0,3)} are consecutive tasks for job 0. Since processing time for {\tt task(0, 2)} is 2, start time for {\tt task(0, 3)} must be $\ge2$ units of time after start time for task 2. (Perhaps task 2 is painting a door, \& it takes 2 hours for paint to dry.) As a result, get following constraint $t_{0,2} + 2\le t_{0,3}$.
    \item {\bf No overlap constraints.} These arise from restriction that a machine cannot work on 2 tasks at same time. E.g., {\tt task(0,2), task(2,1)} are both processed on machine 1. Since their processing times are 2 \& 4, resp., 1 of following constraints must hold:
    \begin{enumerate}
        \item $t_{0,2} + 2\le t_{2,1}$ (if {\tt task(0,2)} is scheduled before {\tt task(2,1)}) or
        \item $t_{2,1} + 4\le t_{0,2}$ (if {\tt task(2,1)} is scheduled before {\tt task(0,2)}).
    \end{enumerate}
\end{enumerate}
-- Có 2 loại ràng buộc cho JSSP:
\begin{enumerate}
    \item {\bf Ràng buộc thứ tự ưu tiên.} Những ràng buộc này phát sinh từ điều kiện rằng đối với bất kỳ 2 tác vụ liên tiếp nào trong cùng một công việc, tác vụ thứ nhất phải được hoàn thành trước khi có thể bắt đầu tác vụ thứ hai. Ví dụ: {\tt task(0,2), task(0,3)} là các tác vụ liên tiếp cho công việc 0. Vì thời gian xử lý cho {\tt task(0, 2)} là 2, nên thời gian bắt đầu cho {\tt task(0, 3)} phải cách thời gian bắt đầu của tác vụ 2 $\ge2$ đơn vị. (Ví dụ, tác vụ 2 là sơn cửa, \& mất 2 giờ để sơn khô.) Kết quả là, ta có ràng buộc sau $t_{0,2} + 2\le t_{0,3}$.
    \item {\bf Ràng buộc không chồng chéo.} Những ràng buộc này phát sinh từ hạn chế rằng một máy không thể làm việc trên 2 tác vụ cùng một lúc. Ví dụ: {\tt task(0,2), task(2,1)} đều được xử lý trên máy 1. Vì thời gian xử lý của chúng là 2 \& 4, tương ứng, 1 trong các ràng buộc sau phải được thỏa mãn:
    \begin{enumerate}
        \item $t_{0,2} + 2\le t_{2,1}$ (nếu {\tt task(0,2)} được lên lịch trước {\tt task(2,1)}) hoặc
        \item $t_{2,1} + 4\le t_{0,2}$ (nếu {\tt task(2,1)} được lên lịch trước {\tt task(0,2)}).
    \end{enumerate}
\end{enumerate}
{\bf Objective for problem.} Objective of job shop problem: minimize makespan: length of time from earliest start time of jobs to latest end time.

-- {\bf Mục tiêu của bài toán.} Mục tiêu của bài toán xưởng gia công: giảm thiểu makespan: khoảng thời gian từ thời điểm bắt đầu sớm nhất của công việc đến thời điểm kết thúc muộn nhất.

\paragraph{A Program Solution.} The following sections describe main elements of a program that solves job shop problem.

-- Các phần sau đây mô tả các thành phần chính của một chương trình giải quyết vấn đề xưởng gia công.

{\bf Import libraries.} The following code imports the required library.
\begin{itemize}
    \item {\sf Python.}
    \begin{verbatim}
import collections
from ortools.sat.python import cp_model
    \end{verbatim}
    \item {\sf C++.}
    \begin{verbatim}
#include <stdlib.h>

#include <algorithm>
#include <cstdint>
#include <map>
#include <numeric>
#include <string>
#include <tuple>
#include <vector>

#include "absl/base/log_severity.h"
#include "absl/log/globals.h"
#include "absl/strings/str_format.h"
#include "ortools/base/init_google.h"
#include "ortools/base/logging.h"
#include "ortools/sat/cp_model.h"
#include "ortools/sat/cp_model.pb.h"
#include "ortools/sat/cp_model_solver.h"
    \end{verbatim}
    \item {\sf Java.}
    \begin{verbatim}
import static java.lang.Math.max;

import com.google.ortools.Loader;
import com.google.ortools.sat.CpModel;
import com.google.ortools.sat.CpSolver;
import com.google.ortools.sat.CpSolverStatus;
import com.google.ortools.sat.IntVar;
import com.google.ortools.sat.IntervalVar;
import com.google.ortools.sat.LinearExpr;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.IntStream;
    \end{verbatim}
    \item {\sf C\#.}
    \begin{verbatim}
using System;
using System.Collections;
using System.Collections.Generic;
using System.Linq;
using Google.OrTools.Sat;
    \end{verbatim}
\end{itemize}
{\bf Define data.} Next, program defines the data for the problem.

{\bf Declare model.} Following code declares the model for the problem.

{\bf Define variables.} Following code defines the variables in problem. For each job \& task, program uses model's \verb|NewIntVar/new_int_var/newIntVar| method to create variables:
\begin{enumerate}
    \item \verb|start_var|: start time of task.
    \item \verb|end_var|: end time of task.
\end{enumerate}
Upper bound for \verb|start_var, end_var| is {\tt horizon}, sum of processing times for all tasks in all jobs. {\tt horizon} is sufficiently large to complete all tasks for following reason: if schedule tasks in non-overlapping time intervals (a non-optimal solution), total length of schedule is exactly {\tt horizon}. So duration of optimal solution cannot be any greater than {\tt horizon}.

-- {\bf Định nghĩa biến.} Đoạn mã sau định nghĩa các biến trong bài toán. Đối với mỗi công việc \& task, chương trình sử dụng phương thức \verb|NewIntVar/new_int_var/newIntVar| của mô hình để tạo biến:
\begin{enumerate}
    \item \verb|start_var|: thời gian bắt đầu của tác vụ.
    \item \verb|end_var|: thời gian kết thúc của tác vụ.
\end{enumerate}
Giới hạn trên của \verb|start_var, end_var| là {\tt horizon}, tổng thời gian xử lý cho tất cả các tác vụ trong tất cả các job. {\tt horizon} đủ lớn để hoàn thành tất cả các tác vụ vì lý do sau: nếu các tác vụ được lên lịch theo các khoảng thời gian không chồng lấn (một giải pháp không tối ưu), tổng độ dài của lịch trình chính xác là {\tt horizon}. Vì vậy, thời gian của giải pháp tối ưu không thể lớn hơn {\tt horizon}.

Next, program uses \verb|NewIntervalVar/new_interval_var/newIntervalVar| method to create an {\it interval variable} -- whose value is a variable time interval -- for task. Inputs to this method are:
\begin{itemize}
    \item Start time of task.
    \item Length of time interval for task.
    \item End time of task.
    \item Name for interval variable.
\end{itemize}
In any solution, \verb|end_var| minus \verb|start_var| must equal {\tt duration}.

-- Tiếp theo, chương trình sử dụng phương thức \verb|NewIntervalVar/new_interval_var/newIntervalVar| để tạo một {\tt biến khoảng thời gian} -- có giá trị là một khoảng thời gian biến đổi -- cho tác vụ. Đầu vào của phương thức này là:
\begin{itemize}
    \item Thời gian bắt đầu của tác vụ.
    \item Độ dài khoảng thời gian của tác vụ.
    \item Thời gian kết thúc của tác vụ.
    \item Tên của biến khoảng thời gian.
\end{itemize}
Trong bất kỳ lời giải nào, \verb|end_var| trừ \verb|start_var| phải bằng {\tt duration}.

{\bf Define constraints.} Program uses model's \verb|AddNoOverlap/add_no_overlap/addNoOverlap| method to create no overlap constraints, which prevent tasks for same machine from overlapping in time.

-- {\bf Định nghĩa ràng buộc.} Chương trình sử dụng phương thức \verb|AddNoOverlap/add_no_overlap/addNoOverlap| của mô hình để tạo ra các ràng buộc không chồng chéo, ngăn các tác vụ cho cùng một máy chồng chéo về mặt thời gian.

Next, program adds precedence constraints, which prevent consecutive tasks for same job from overlapping in time. For each job \& each task in job, a linear constraint is added to specify: end time of a task to occur before start time of next task in job.

-- Tiếp theo, chương trình thêm các ràng buộc ưu tiên, giúp ngăn các tác vụ liên tiếp của cùng một công việc chồng chéo về thời gian. Đối với mỗi tác vụ \& mỗi tác vụ trong công việc, một ràng buộc tuyến tính được thêm vào để chỉ định: thời gian kết thúc của một tác vụ phải xảy ra trước thời gian bắt đầu của tác vụ tiếp theo trong công việc.

{\bf Define objective.} This code creates an objective variable \& constraints it to be max of end of all jobs.

{\bf Invoke solver.} The following code calls the solver.

{\bf Display results.} Following code displays results, including optimal schedule \& task intervals. Optimal schedule is shown below:
\begin{verbatim}
 Optimal Schedule Length: 11
Machine 0: job_0_0   job_1_0
           [0,3]     [3,5]
Machine 1: job_2_0   job_0_1   job_1_2
           [0,4]     [4,6]     [7,11]
Machine 2: job_1_1   job_0_2   job_2_1
           [5,6]     [6,8]     [8,11]
\end{verbatim}
Eagle-eyed readers examining machine 1 might wonder why \verb|job_1_2| was scheduled at time 7 instead of time 6. Both are valid solutions, but remember: objective is to minimize makespan. Moving \verb|job_1_2| earlier wouldn't reduce makespan, so 2 solutions are equal from solver's perspective.

-- {\bf Hiển thị kết quả.} Đoạn mã sau hiển thị kết quả, bao gồm lịch trình tối ưu \& khoảng thời gian tác vụ. Lịch trình tối ưu được hiển thị bên dưới:
\begin{verbatim}
    Độ dài Lịch trình Tối ưu: 11
    Máy 0: job_0_0 job_1_0
    [0,3] [3,5]
    Máy 1: job_2_0 job_0_1 job_1_2
    [0,4] [4,6] [7,11]
    Máy 2: job_1_1 job_0_2 job_2_1
    [5,6] [6,8] [8,11]
\end{verbatim}
Những người đọc tinh mắt khi xem xét máy 1 có thể tự hỏi tại sao \verb|job_1_2| lại được lên lịch vào thời điểm 7 thay vì thời điểm 6. Cả hai đều là giải pháp hợp lệ, nhưng hãy nhớ: mục tiêu là giảm thiểu thời gian chờ. Di chuyển \verb|job_1_2| trước đó sẽ không làm giảm makespan, do đó 2 giải pháp là bằng nhau theo quan điểm của người giải.

%------------------------------------------------------------------------------%

\section{Wikipedia}

\subsection{Wikipedia{\tt/}adjoint}
``In mathematics, the term {\it adjoint} applies in several situations. Several of these share a similar formalism: if $A$ is adjoint to $B$, then there is typically some formula of the type $(Ax,y) = (x,By)$. Specifically, {\it adjoint} or {\it adjunction} may mean:
\begin{enumerate}
	\item \href{https://en.wikipedia.org/wiki/Transpose_of_a_linear_map}{Adjoint of a linear map}, also called its transpose in case of matrices
	\item \href{https://en.wikipedia.org/wiki/Hermitian_adjoint}{Hermitian adjoint} (adjoint of a linear operator) in functional analysis
	\item \href{https://en.wikipedia.org/wiki/Adjoint_endomorphism}{Adjoint endomorphism} of a Lie algebra
	\item \href{https://en.wikipedia.org/wiki/Adjoint_endomorphism}{Adjoint endomorphism} of a Lie algebra
	\item \href{https://en.wikipedia.org/wiki/Adjoint_representation_of_a_Lie_group}{Adjoint functors} in category theory
	\item \href{https://en.wikipedia.org/wiki/Adjunction_(field_theory)}{Adjunction (field theory)}
	\item \href{https://en.wikipedia.org/wiki/Adjunction_formula_(algebraic_geometry)}{Adjunction formula (algebraic geometry)}
	\item \href{https://en.wikipedia.org/wiki/Adjunction_space}{Adjunction space} in topology
	\item \href{https://en.wikipedia.org/wiki/Conjugate_transpose}{Conjuate transpose} of a matrix in linear algebra
	\item \href{https://en.wikipedia.org/wiki/Adjugate_matrix}{Adjugate matrix}, related to its inverse
	\item \href{https://en.wikipedia.org/wiki/Adjoint_equation}{Adjoint equation}
	\item The upper \& lower adjoints of a \href{https://en.wikipedia.org/wiki/Galois_connection}{Galois connection} in order theory
	\item The adjoint of a \href{https://en.wikipedia.org/wiki/Differential_operator}{differential operator} with general polynomial coefficients
	\item \href{https://en.wikipedia.org/wiki/Kleisli_adjunction}{Kleisli adjunction}
	\item \href{https://en.wikipedia.org/wiki/Monoidal_adjunction}{Monoidal adjunction}
	\item \href{https://en.wikipedia.org/wiki/Quillen_adjunction}{Qhillen adjunction}
	\item \href{https://en.wikipedia.org/wiki/Axiom_of_adjunction}{Axiom of adjunction} in set theory
	\item \href{https://en.wikipedia.org/wiki/Adjunction_(rule_of_inference)}{Adjunction (rule of inference)}'' -- \href{https://en.wikipedia.org/wiki/Adjoint}{Wikipedia{\tt/}adjoint}
\end{enumerate}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}dynamic programming}
``{\sf Finding shortest path in a graph using optimal substructure; a straight line indicates a single edge; a wavy line indicates a shortest path between 2 vertices it connects (among other paths, not shown, sharing same 2 vertices); bold line is overall shortest path from start to goal.} {\it Dynamic programming} is both a mathematical optimization method \& an \href{https://en.wikipedia.org/wiki/Algorithmic_paradigm}{algorithmic paradigm}. Method was developed by \href{https://en.wikipedia.org/wiki/Richard_Bellman}{\sc Richard Bellman} in 1950s \& has found applications in numerous fields, from aerospace engineering to economics.

In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a \href{https://en.wikipedia.org/wiki/Recursion}{recursive} manner. While some decision problems cannot be taken apart this way, decisions that span several points in  time do often break apart recursively. Likewise, in CS, if a problem, if a problem can be solved optimally by breaking it into sub-problems \& then recursively finding optimal solutions to sub-problems, then it is said to have \href{https://en.wikipedia.org/wiki/Optimal_substructure}{optimal substructure}.

If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between value of larger problem \& values of sub-problems. In optimization literature this relationship is called \href{https://en.wikipedia.org/wiki/Bellman_equation}{Bellmann equation}.

\subsubsection{Overview}

\begin{enumerate}
	\item {\bf Mathematical optimization.} In terms of mathematical optimization, dynamic programming usually refers to simplifying a decision by breaking it down into a sequence of decision steps over time.

	This is done by defining a sequence of {\it value functions} $V_1,\ldots,V_n$ taking $y$ as an argument representing \href{https://en.wikipedia.org/wiki/State_variable}{state} of system at times $i$ from 1 to $n$.

	Def of $V_n(y)$ is value obtained in state $y$ at last time $n$.

	Values $V_i$ at earlier times $i = n - 1,n - 2,\ldots,2,1$ can be found by working backwards, using a \href{https://en.wikipedia.org/wiki/Recursion}{recursive} relationship called \href{https://en.wikipedia.org/wiki/Bellman_equation}{Bellmann equation}.

	For $i = 2,\ldots,n$, $V_{i-1}$ at any state $y$ is calculated from $V_i$ by maximizing a simple function (usually sum) of gain from a decision at time $i - 1$ \& function $V_i$ at new state of system if this decision is made.

	Since $V_i$ has already been calculated for needed states, above operation yields $V_{i-1}$ for those states.

	Finally, $V_1$ at initial state of system is value of optimal solution. Optimal values of decision variables can be recovered, 1 by 1, by tracking back calculations already performed.
	\item {\bf Control theory.} In \href{https://en.wikipedia.org/wiki/Control_theory}{control theory}, a typical problem: find an admissible control ${\bf u}^*$ which causes system $\dot{\bf x}(t) = {\bf g}({\bf x}(t),{\bf u}(t),t)$ to follow an admissible trajectory ${\bf x}^*$ on a continuous time interval $t_0\le t\le t_1$ that minimizes a \href{https://en.wikipedia.org/wiki/Loss_function}{cost function}
	\begin{equation*}
		J = b({\bf x}(t_1),t_1) + \int_{t_0}^{t_1} f({\bf x}(t),{\bf u}(t),t)\,{\rm d}t.
	\end{equation*}
	Solution to this problem is an optimal control law or policy ${\bf u}^* = h({\bf x}(t),t)$, which produces an optimal trajectory ${\bf x}^*$ \& a \href{https://en.wikipedia.org/wiki/Cost-to-go_function}{cost-to-go function} $J^*$. Latter obeys fundamental equation of dynamic programming:
	\begin{equation*}
		-J_t^* = \min_{\bf u} \{f({\bf x}(t),{\bf u}(t),t) + {J_x^^{*\top}{\bf g}({\bf x}(t),{\bf u}(t),t)\}
	\end{equation*}
	a PDE known as \href{https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation}{Hamilton--Jacobi--Bellman equation}, in which
	\begin{equation*}
		J_x^* = \frac{\partial J^*}{\partial{\bf x}} = [\partial_{x_1}J^*,\ldots,\partial_{x_n}J^*]^\top,\ J_t^* = \partial_tJ^*.
	\end{equation*}
	One finds: minimizing ${\bf u}$ in terms of $t,{\bf x}$, \& unknown function $J_x^*$ \& then substitutes result into Hamilton--Jacobi--Bellman equation to get PDE to be solved with boundary condition $J(t_1) = b({\bf x}(t_1),t_1)$. In practice, this generally requires \href{https://en.wikipedia.org/wiki/Numerical_partial_differential_equations}{numerical techniques} for some discrete approximation to exact optimization relationship.

	Alternatively, continuous process can be approximated by a discrete system, which leads to a following recurrence relation analog to Hamilton--Jacobi--Bellman equation:
	\begin{equation*}
		J_k^*({\bf x}_{n-k}) = \min_{{\bf u}_{n-k}} \left\{\hat{f}({\bf x}_{n-k},{\bf u}_{n-k}) + J_{k-1}^*(\hat{\bf g}({\bf x}_{n-k},{\bf u}_{n-k}))\right\}
	\end{equation*}
	at $k$th state of $n$ equally spaced discrete time intervals, \& where $\hat{f},\hat{\bf g}$ denote discrete approximations to $f,{\bf g}$. This functional equation is known as Bellman equation, which can be solved for an exact solution of discrete approximation of optimization equation.
	\begin{itemize}
		\item {\bf Example from economics: Ramsey's problem of optimal saving.} See also: \href{https://en.wikipedia.org/wiki/Ramsey%E2%80%93Cass%E2%80%93Koopmans_model}{Ramsey--Cass--Koopmans model}. In economics, objective is generally to maximize (rather than minimize) some dynamic \href{https://en.wikipedia.org/wiki/Social_welfare_function}{social welfare function}. In Ramsey's problem, this function relates amounts of consumption to levels of \href{https://en.wikipedia.org/wiki/Utility}{utility}. Loosely speaking, planner faces trade-off between contemporaneous consumption \& future consumption (via investment in \href{https://en.wikipedia.org/wiki/Physical_capital}{capital stock} used in production), known as \href{https://en.wikipedia.org/wiki/Intertemporal_choice}{intertemporal choice}. Future consumption is discounted at a constant rate $\beta\in(0,1)$. A discrete approximation to transition equation of capital is given by
		\begin{equation*}
			k_{t+1} = \hat{g}(k_t,c_t) = f(k_t) - c_t,
		\end{equation*}
		where $c$ is consumption, $k$ is capital, $f$: a \href{https://en.wikipedia.org/wiki/Production_function}{production function} satisfying \href{https://en.wikipedia.org/wiki/Inada_conditions}{Inada conditions}. An initial capital stock $k_0 > 0$ is assumed.

		Let $c_t$ be consumption in period $t$, \& assume consumption yields utility $u(c_t) = \ln c_t$ as long as consumer lives. Assume consumer is impatient, so that he \href{https://en.wikipedia.org/wiki/Discounting}{discounts} future utility by a factor $b$ each period, where $0 < b < 1$. Let $k_t$ be \href{https://en.wikipedia.org/wiki/Capital_(economics)}{capital} in period $t$. Assume initial capital is a given amount $k_0 > 0$, \& suppose: this period's capital \& consumption determine next period's capital as $k_{t+1} = Ak_t^a - c_t$, where $A$: a positive constant \& $0 < a < 1$. Assume capital cannot be negative. Then consumer's decision problem can be written as follows:
		\begin{equation*}
			\max\sum_{t=0}^T b^t\ln c_t\mbox{ subject to } k_{t+1} = Ak_t^a - c_t\ge0,\ \forall t = 0,1,\ldots,T.
		\end{equation*}
		Written this way, problem looks complicated, because it involves solving for all choice variables $c_0,c_1,\ldots,c_T$. (Capital $k_0$ is not a choice variable -- consumer's initial capital is taken as given.)

		DP approach to solve this problem involves breaking it apart into a sequence of smaller decisions. To do so, define a sequence of {\it value functions} $V_t(k)$, for $t = 0,1,\ldots,T,T+1$ which represent value of having any amount of capital $k$ at each time $t$. There is (by assumption) no utility from having capital after death, $V_{T+1}(k) = 0$.

		Value of any quantity of capital at any previous time can be calculated by \href{https://en.wikipedia.org/wiki/Backward_induction}{backward induction} using Bellman equation. In this problem, for each $t = 0,1,\ldots,T$, Bellman equation is
		\begin{equation*}
			V_t(k_t) = \max(\ln c_t + bV_{t+1}(k_{t+1}))\mbox{ subject to } k_{t+1} = Ak_t^a - c_t\ge0.
		\end{equation*}
		This problem is much simpler than the once wrote down before, because it involves only 2 decision variables, $c_t$ \& $k_{t+1}$. Intuitively, instead of choosing his whole lifetime plan at birth, consumer can take things 1 step at a time. At time $t$, his current capital $k_t$ is given, \& he only needs to choose current consumption $c_t$ \& saving $k_{t+1}$.

		To actually solve this problem, work backwards. For simplicity, current level of capital is denoted as $k$. $V_{T+1}(k)$ is already known, so using Bellman equation once can calculate $V_T(k)$, \& so on until get to $V_0(k)$, which is {\it value} of initial decision problem for whole lifetime. I.e., once know $V_{T-j+1}(k)$, can calculate $V_{T_j}(k)$, which is maximum of $\ln c_{T-j} + bV_{T-j+1}(Ak^a - c_{T-j})$, where $c_{T-j}$ is choice variable \& $Ak^a - c_{T-j}\ge0$.

		Working backwards, it can be shown: value function at time $t = T - j$ is
		\begin{equation*}
			V_{T-j}(k) = a\sum_{i=0}^j a^ib^i\ln k + v_{T-j},
		\end{equation*}
		where each $v_{T-j}$ is a constant, \& optimal amount to consume at time $t = T - j$ is
		\begin{equation*}
			c_{T-j}(k) = \frac{1}{\sum_{i=0}^j a^ib^i}Ak^a.
		\end{equation*}
		See: optimal to consume a larger fraction of current wealth as one gets older, finally consuming all remaining wealth in period $T$, the last period of life.
	\end{itemize}
	\item {\bf CS.} There are 2 key attributes that a problem must have in order for DP to be applicable: optimal substructure \& \href{https://en.wikipedia.org/wiki/Overlapping_subproblem}{overlapping sub-problems}. If a problem can be solved by combining optimal solutions to {\it non-overlapping} sub-problems, strategy is called ``\href{https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm}{divide \& conquer}'' instead. This is why \href{https://en.wikipedia.org/wiki/Mergesort}{merge sort} \& \href{https://en.wikipedia.org/wiki/Quicksort}{quick sort} are not classified as DP problems.

	{\it Optimal substructure} means: solution to a given optimization problem can be obtained by combination of optimal solutions to its sub-problems. Such optimal substructures are usually described by means of \href{https://en.wikipedia.org/wiki/Recursion}{recursion}. E.g., given a graph $G = (V,E)$, shortest path $p$ from a vertex $u$ to a vertex $v$ exhibits optimal substructure: take any intermediate vertex $w$ on this shortest path $p$. If $p$ is truly shortest path, then it can be split into sub-paths $p_1$ from $u$ to $w$ \& $p_2$ from $w$ to $v$ s.t. these, in turn, are indeed shortest paths between corresponding vertices (by simple cut-\&-paste argument described in \href{https://en.wikipedia.org/wiki/Introduction_to_Algorithms}{\it Introduction to Algorithms}). Hence, one can easily formulate solution for finding shortest paths in a recursive manner, which is what \href{https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm}{Bellman--Ford algorithm} or \href{https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm}{Floyd--Warshall algorithm} does.

	{\it Overlapping} sub-problems means: space of sub-problems must be small, i.e., any recursive algorithm solving problem should solve same sub-problems over \& over, rather than generating new sub-problems. E.g., consider recursive formulation for generating Fibonacci sequence $F_i = F_{i-1} + F_{i-2}$, with base case $F_1 = F_2 = 1$. Then $F_{43} = F_{42} + F_{41}$, \& $F_{42} = F_{41} + F_{40}$. Now $F_{41}$ is being solved in recursive sub-trees of both $F_{43}$ as well as $F_{42}$. Even though total number of sub-problems is actually small (only 43 of them), end up solving same problems over \& over if adopt a naive recursive solution e.g. this. DP takes account of this fact \& solves each sub-problem only once.

	{\sf Fig. Subproblem graph for Fibonacci sequence. Fact: it is not a \href{https://en.wikipedia.org/wiki/Tree_structure}{tree} indicates overlapping subproblems.} This can be achieved in either of 2 ways:
	\begin{itemize}
		\item \href{https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design}{\it Top-down approach}: This is direct fall-out of recursive formulation of any problem. if solution to any problem can be formulated recursively using solution to its sub-problems, \& if its sub-problems are overlapping, then one can easily \href{https://en.wikipedia.org/wiki/Memoize}{memoize} or store solutions to sub-problems in a table (often an \href{https://en.wikipedia.org/wiki/Array_(data_structure)}{array} or \href{https://en.wikipedia.org/wiki/Hash_table}{hastable} in practice). Whenever attempt to solve a new sub-problem, 1st check table to see if already solved. If a solution has been recorded, can use it directly, otherwise solve sub-problem \& add its solution to table.
		\item \href{https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design}{\it Bottom-up approach}: Once formulate solution to a problem recursively as in terms of its sub-problems, can try reformulating problem in a bottom-up fashion: try solving sub-problems 1st \& use their solutions to build-on \& arrive at solutions to bigger sub-problems. This is also usually done in a tabular form by iteratively generating solutions to bigger \& bigger sub-problems by using solutions to small sub-problems. E.g., if already know values of $F_{41}$ \& $F_{40}$, can directly calculate value of $F_{42}$.
	\end{itemize}
	Some \href{https://en.wikipedia.org/wiki/Programming_language}{programming languagues} can automatically \href{https://en.wikipedia.org/wiki/Memoization}{memoize} result of a function call with a particular set of arguments, in order to speed up \href{https://en.wikipedia.org/wiki/Call-by-name}{call-by-name} evaluation (this mechanism is referred to as \href{https://en.wikipedia.org/wiki/Call-by-need}{\it call-by-need}). Some languages make it possible portably (e.g., \href{https://en.wikipedia.org/wiki/Scheme_(programming_language)}{Scheme}, \href{https://en.wikipedia.org/wiki/Common_Lisp}{Common Lisp}, \href{https://en.wikipedia.org/wiki/Perl}{Perl}, \& \href{https://en.wikipedia.org/wiki/D_(programming_language)}{D}). Some languages have automatic \href{https://en.wikipedia.org/wiki/Memoization}{memoization} built in, e.g. tabled \href{https://en.wikipedia.org/wiki/Prolog}{Prolog} \& \href{https://en.wikipedia.org/wiki/J_(programming_language)}{J}, which supports memoization with {\it M.} adverb. In any case, this is only possible for a \href{https://en.wikipedia.org/wiki/Referentially_transparent}{referentially transparent} function. Memoization is also encountered as an easily accessible design pattern within term-rewrite based languages e.g. \href{https://en.wikipedia.org/wiki/Wolfram_Language}{Wolfram Language}.
	\item {\bf Bioinformatics.} DP is widely used in bioinformatics for tasks e.g. \href{https://en.wikipedia.org/wiki/Sequence_alignment}{sequence alignment}, \href{https://en.wikipedia.org/wiki/Protein_folding}{protein folding}, RNA structure prediction \& protein-DNA binding. 1st DP algorithms for protein-DNA binding were developed in 1970s independently by \href{https://en.wikipedia.org/wiki/Charles_DeLisi}{\sc Charles DeLisi} in US \& by {\sc Georgii Gurskii \& Alexander Zasedatelev} in Soviet Union. Recently these algorithms have become very popular in bioinformatics \& \href{https://en.wikipedia.org/wiki/Computational_biology}{computational biology}, particularly in studies of \href{https://en.wikipedia.org/wiki/Nucleosome}{nucleosome} positioning \& \href{https://en.wikipedia.org/wiki/Transcription_factor}{transcription factor} binding.
\end{enumerate}

\subsubsection{Examples: computer algorithms}

\begin{enumerate}
	\item {\bf Dijkstra's algorithm for shortest path problem.} From a DP point of view, \href{https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm}{Dijkstra's algorithm} for \href{https://en.wikipedia.org/wiki/Shortest_path_problem}{shortest path problem} is a successive approximation scheme that solves DP functional equation for shortest path problem by {\it Reaching} method.

	In fact, Dijkstra's explanation of logic behind algorithm, namely
	\begin{problem}
		Find path of minimum total length between 2 given nodes $P,Q$.
	\end{problem}
	Use fact: if $R$ is a node on minimal path from $P$ to $Q$, knowledge of latter implies knowledge of minimal path from $P$ to $R$.

	is a paraphrasing of \href{https://en.wikipedia.org/wiki/Richard_Bellman}{\sc Bellman}'s famous \href{https://en.wikipedia.org/wiki/Principle_of_Optimality}{Principle of Optimality} in context of \href{https://en.wikipedia.org/wiki/Shortest_path_problem}{shortest path problem}.
	\item {\bf Fibonacci sequence.} Using DP in calculation of $n$th member of \href{https://en.wikipedia.org/wiki/Fibonacci_sequence}{Fibonacci sequence} improves its performance greatly. Here is a naive implementation, based directly on mathematical def:
	\begin{verbatim}
		function fib(n)
		    if n <= 1 return n
		    return fib(n - 1) + fib(n - 2)
	\end{verbatim}
	Notice if call, say, {\tt fib(5)}, produce a call tree that calls function on same value many different times [$\ldots$]. In particular, {\tt fib(2)} was calculated 3 times from scratch. In larger examples, many more values of {\tt fib}, or {\it subproblems}, are recalculated, leading to an exponential time algorithm.

	Now, suppose have a simple \href{https://en.wikipedia.org/wiki/Associative_array}{map} object $m$ which maps each value of {\tt fib} that has already been calculated to its result, \& modify our function to use it \& update it. Resulting function requires only $O(n)$ time instead of exponential time (but requires $O(n)$ space):
	\begin{verbatim}
		var m := map(0 -> 0, 1 -> 1)
		function fib(n)
		    if key n is not in map m
		        m[n] := fib(n - 1) + fib(n - 2)
		    return m[n]
	\end{verbatim}
	This technique of saving values that have already been calculated is called \href{https://en.wikipedia.org/wiki/Memoization}{memoization}; this is top-down approach, since 1st break problem into subproblems \& then calculate \& store values.

	In {\it bottom-up} approach, calculate smaller values of {\tt fib} 1st, then build larger values from them. This method also uses $O(n)$ time since it contains a loop that repeats $n - 1$ times, but it only takes constant $O(1)$ space, in contrast to top-down approach which requires $O(n)$ space to store map.
	\begin{verbatim}
		function fib(n)
		    if n = 0
		        return 0
		    else
		        var previousFib := 0, currentFib := 1
		        repeat n - 1 times // loop is skipped if n = 1
		            var newFib := previousFib + currentFib
		            previousFib := currentFib
		            currentFib := newFib
		        return currentFib
	\end{verbatim}
	In both examples, only calculate {\tt fib(2)} 1 time, \& then use it to calculate both {\tt fib(4)} \& {\tt fib(3)}, instead of computing it every time either of them is evaluated.
	\item {\bf A type of balanced 0--1 matrix.} Consider problem of assigning values, either 0 or 1, to positions of an $n\times n$ matrix, with $n$ even, so that each row \& each column contains exactly $\frac{n}{2}$ 0s \& $\frac{n}{2}$ 1s. Ask how many different assignments there are for a given $n$. E.g., when $n = 4$, 5 possible solutions are [$\ldots$] There are at least 3 possible approaches: \href{https://en.wikipedia.org/wiki/Brute-force_search}{brute force}, \href{https://en.wikipedia.org/wiki/Backtracking}{backtracking}, \& DP.

	Brute force consists of checking all assignments of 0s \& 1s \& counting those that have balanced rows \& columns ($\frac{n}{2}$ 0s \& $\frac{n}{2}$ 1s). As there are $2^{n^2}$ possible assignments \& $\binom{n}{\frac{n}{2}}^2$ sensible assignments, this strategy is not practical except maybe up to $n = 6$.

	Backtracking for this problem consists of choosing some order of matrix elements \& recursively placing 1s or 0s, while checking: in every row \& column number of elements that have not been assigned plus number of 1s or 0s are both at least $\frac{n}{2}$. While more sophisticated than brute force, this approach will visit every solution once, making it impractical for $n > 6$, since number of solutions is already 116,963,796,250 for $n = 8$.

	DP makes it possible to count number of solutions without visiting them all. Imagine backtracking values for 1st row -- what information would we require about remaining rows, in order to be able to accurately count solutions obtained for each 1st row value? Consider $k\times n$ boards, where $1\le k\le n$, whose $k$ rows contain $\frac{n}{2}$ 0s \& $\frac{n}{2}$ 1s. Function $f$ to which memoization is applied maps vectors of $n$ pairs of integers to number of admissible boards (solutions). There is 1 pair for each column, \& its 2 components indicate respectively number of 0s \& 1s that have yet to be placed in that column. Seek value of $f\left(\left(\frac{n}{2},\frac{n}{2}\right),\left(\frac{n}{2},\frac{n}{2}\right),\ldots,\left(\frac{n}{2},\frac{n}{2}\right)\right)$ ($n$ arguments or 1 vector of $n$ elements). Process of subproblem creation involves iterating over every 1 of $\binom{n}{\frac{n}{2}}$ possible assignments for top row of board, \& going through every column, subtracting 1 from appropriate element of pair for that column, depending on whether assignment for top row contained a 0 or a 1 at that position. If any 1 of results is negative, then assignment is invalid \& does not contribute to set of solutions (recursion stops). Otherwise, have an assignment for top row of $k\times n$ board \& recursively compute number of solutions to remaining $(k - 1)\times n$ board, adding numbers of solutions for every admissible assignment of top row \& returning sum, which is being memoized. Base case is trivial subproblem, which occurs for a $1\times n$ board. Number of solutions for this board is either 0 or 1, depending on whether vector is a permutation of $\frac{n}{2}$ $(0,1)$ \& $\frac{n}{2}$ $(1,0)$ pairs or not.

	E.g., in 1st 2 boards shown above sequences of vectors would be [$\ldots$] The number of solutions (sequence) A058527 in \href{https://en.wikipedia.org/wiki/On-Line_Encyclopedia_of_Integer_Sequences}{OEIS}) is $1,2,90,297200,116963796250,6736218287430460752,\ldots$ Links to MAPLE implementation of DP approach may be found among external links.
	\item {\bf Checkerboard.} Consider a \href{https://en.wikipedia.org/wiki/Checkerboard}{checkerboard} with $n\times n$ squares \& a cost function {\tt c(i, j)} which returns a cost associated with square {\tt(i,j)} $i$: row, $j$: column. E.g., on a [$5\times5$ checkerboard]. Say there was a checker that could start at any square on 1st rank (i.e., row) \& wanted to know shortest path (sum of minimum costs at each visited rank) to get to last rank; assuming checker could move only diagonally left forward, diagonally right forward, or straight forward. I.e., a checker on $(1,3)$ can move to $(2,2),(2,3)$, or $(2,4)$. This problem exhibits {\it optimal substructure}. I.e., solution to entire problem relies on solutions to subproblems. Define a function $q(i,j)\coloneqq$ minimum cost to reach square $(i,j)$. Starting at rank $n$ \& descending to rank 1, compute value of this function for all squares at each successive rank. Picking square that holds minimum value at each rank gives shortest path between rank $n$ \& rank 1.

	Function $q(i,j) =$ minimum cost to get to any of 3 squares below it (since those are the only squares that can reach it) plus $c(i,j)$. E.g.: $q(A) = \min(q(B),q(C),q(D)) + c(A)$. Define $q(i,j)$ in somewhat more general terms:
	\begin{equation*}
		q(i,j) = \left\{\begin{split}
			&\infty&&\mbox{if } j < 1\mbox{ or } j > n,\\
			&c(i,j)&&\mbox{if } i = 1,\\
			&\min(q(i - 1,j - 1),q(i - 1,j),q(i - 1,j + 1)) + c(i,j)&&\mbox{otherwise}.
		\end{split}\right.
	\end{equation*}
	1st line of this equation deals with a board modeled as squares indexed on 1 at lowest bound \& $n$ at highest bound. 2nd line specifies what happens at 1st rank; providing a base case. 3rd line, recursion, is important part. It represents $A,B,C,D$ terms in example. From this def can derive straightforward recursive code for $q(i,j)$. In following pseudo code, $n$: size of board, $c(i,j)$: cost function, $\min()$ returns minimum of a number of values.
	\begin{verbatim}
		function minCost(i, j)
		     if j < 1 or j > n
		          return infinity
		     else if i = 1
		          return c(i, j)
		     else
		          return min(minCost(i - 1, j - 1), minCost(i - 1, j), minCost(i - 1, j + 1)) + c(i, j)
	\end{verbatim}
	This function only computes path cost, not actual path. Discuss actual path. This, like Fibonacci-numbers example, is horribly slow because it too exhibits {\it overlapping subproblems} attribute. I.e., it recomputes same path costs over \& over. However, can compute it much faster in a bottom-up fashion if store path costs in a 2D array {\tt q[i, j]} rather than using a function. This avoids recomputation; all values needed for array {\tt q[i, j]} are computed ahead of time only once. Precomputed values for $(i,j)$ are simply looked up whenever needed.

	Also need to know what actual shortest path is. To do this, use another array {\tt p[i, j]}; a {\it predecessor array}. This array records path to any square $s$. Predecessor of $s$ is modeled as an offset relative to index (in {\tt q[i, j]}) of precomputed path cost of $s$. To reconstruct complete path, lookup predecessor of $s$, then predecessor of that square, then predecessor of that square, \& so on recursively, until reach starting square. Consider following pseudocode:
	\begin{verbatim}
		function computeShortestPathArrays()
		    for x from 1 to n
		        q[1, x] := c(1, x)
		    for y from 1 to n
		        q[y, 0] := infinity
		        q[y, n + 1] := infinity
		    for y from 2 to n
		        for x from 1 to n
		            m := min(q[y-1, x-1], q[y-1, x], q[y-1, x+1])
		            q[y, x] := m + c(y, x)
		            if m = q[y-1, x-1]
		                p[y, x] := -1
		            else if m = q[y-1, x]
		                p[y, x] :=  0
		            else
		                p[y, x] :=  1
	\end{verbatim}
	Now the rest is a simple matter of finding minimum \& printing it.
	\begin{verbatim}
		function computeShortestPath()
		    computeShortestPathArrays()
		    minIndex := 1
		    min := q[n, 1]
		    for i from 2 to n
		        if q[n, i] < min
		            minIndex := i
		            min := q[n, i]
		    printPath(n, minIndex)

		function printPath(y, x)
		    print(x)
		    print("<-")
		    if y = 2
		        print(x + p[y, x])
		    else
		        printPath(y-1, x + p[y, x])
	\end{verbatim}
	\item {\bf Sequence alignment.} In \href{https://en.wikipedia.org/wiki/Genetics}{genetics}, \href{https://en.wikipedia.org/wiki/Sequence_alignment}{sequence alignment} is an important application where DP is essential. Typically, problem consists of transforming 1 sequence into another using edit operations that replace, insert, or remove an element. Each operation has an associated cost, \& goal: find \href{https://en.wikipedia.org/wiki/Edit_distance}{sequence of edits with lowest total cost}.

	Problem can be stated naturally as a recursion, a sequence $A$ is optimally edited into a sequence $B$ by either:
	\begin{enumerate}
		\item inserting 1st character of $B$, \& performing an optimal alignment of $A$ \& tail of $B$
		\item deleting 1st character of $A$, \& performing optimal alignment of tail of $A$ \& $B$
		\item replacing 1st character of $A$ with 1st character of $B$, \& performing optimal alignments of tails of $A$ \& $B$.
	\end{enumerate}
	Partial alignments can be tabulated in a matrix, where cell(i, j) contains cost of optimal alignment of A[1..i] to B[1..j]. Cost in cell(i, j) can be calculated by adding cost of relevant operations to cost of its neighboring cells, \& selecting optimum.

	Different variants exist, see \href{https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm}{Smith--Waterman algorithm} \& \href{https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm}{Needleman--Wunsch algorithm}.
	\item {\bf Tower of Hanoi puzzle.} \href{https://en.wikipedia.org/wiki/Tower_of_Hanoi}{Tower of Hanoi} or Towers of Hanoi is a \href{https://en.wikipedia.org/wiki/Mathematical_game}{mathematical game} or \href{https://en.wikipedia.org/wiki/Puzzle}{puzzle}. It consists of 3 rods, \& a number of disks of different sizes which can slide onto any rod. Puzzle starts with disks in a neat stack in ascending order of size on 1 rod, smallest at top, thus making a conical shape.

	Objective of puzzle: move entire stack to another rod, obey rules:
	\begin{enumerate}
		\item Only 1 disk may be moved at a time.
		\item Each move consists of taking upper disk from 1 of rods \& sliding it onto another rod, on top of other disks that may already be present on that rod.
		\item No disk may be placed on top of a smaller disk.
	\end{enumerate}
	DP solution consists of solving \href{https://en.wikipedia.org/wiki/Bellman_equation}{functional equation}
	\begin{equation*}
		S(n,h,t) = S(n - 1,h,not(h,t));S(1,h,t);S(n - 1,not(h,t),t)
	\end{equation*}
	where $n$ denotes number of disks to be moved, $h$ denotes home rod, $t$ denotes target rod, $not(h,t)$ denotes 3rd rod (neither $h$ nor $t$), ``;'' denotes concatenation, \& $S(n,h,t)\coloneqq$ solution to a problem consisting of $n$ disks that are to be moved from rod $h$ to rod $t$. For $n = 1$ problem is trivial, namely $S(1,h,t) =$ ``move a disk from rod $h$ to rod $t$'' (there is only 1 disk left).

	Number of moves required by this solution is $2^n - 1$. If objective is to {\it maximize} number of moves (without cycling) then DP \href{https://en.wikipedia.org/wiki/Bellman_equation}{functional equation} is slightly more complicated \& $3^n - 1$ moves are required.
	\item {\bf Egg dropping puzzle.} A description of instance of this famous puzzle involving $N = 2$ eggs \& a building with $H = 36$ floors:
	\begin{problem}
		Suppose wish to know which stories in a 36-story building are safe to drop eggs from, \& which will cause eggs to break on landing (using U.S. English terminology, in which 1st floor is at ground level). Make a few assumptions:
		\begin{itemize}
			\item An egg that survives a fall can be used again.
			\item A broken egg must be discarded.
			\item Effect of a fall is same for all eggs.
			\item If an egg breaks when dropped, then it would break if dropped from a higher window.
			\item If an egg survives a fall, then it would survive a shorter fall.
			\item It is not ruled out: 1st-floor windows break eggs, nor is it ruled out that eggs can survive 36th-floor windows.
		\end{itemize}
		If only 1 egg is available \& wish to be sure of obtaining right result, experiment can be carried out in only 1 way. Drop egg from 1st-floor window; if it survives, drop it from 2nd-floor window. Continue upward until it breaks. In worst case, this method may require 36 droppings. Suppose 2 eggs are available. What is lowest number of egg-droppings that is guaranteed to work in all cases?
	\end{problem}
	To derive a DP functional equation for this puzzle, let {\it state} of DP model be a pair $s = (n,k)$, where $n =$ number of test eggs available, $n = 0,1,,\ldots,N - 1$, $k =$ number of (consecutive) floors yet to be tested, $k = 0,1,\ldots,H - 1$. E.g., $s = (2,6)$ indicates: 2 test eggs are available \& 6 (consecutive) floors ar yet to be tested. Initial state of process is $s = (N,H)$ where $N$ denotes number of test eggs available at commencement of experiment. Process terminates either when there are no more test eggs $n = 0$ or when $k = 0$, whichever occurs 1st. If termination occurs at state $s = (0,k)$ \& $k > 0$, then test failed. Now let $W(n,k) =$  minimum number of trials required to identify value of critical floor under worst-case scenario given that process is in state $s = (n,k)$. Then it can be shown: $W(n,k) = 1 + \min\{\max(W(n - 1,x - 1), W(n,k - x)):x = 1,\ldots,k\}$ with $W(n,0) = 0$, $\forall n\in\mathbb{N}^\star$ \& $W(1,k) = k$ for all $k$. Easy to solve this equation iteratively by systematically increasing values of $n,k$.
	\begin{itemize}
		\item {\bf Faster DP solution using a different parametrization.} Notice above solution takes $O(nk^2)$ time with a DP solution. This can be improved to $O(nk\log k)$ time by binary searching on optiaml $x$ in above recurrence, since $W(n - 1,x - 1)$ is increasing in $x$ while $W(n,k - x)$ is decreasing in $x$, thus a local minimum of $\max(W(n - 1,x - 1),W(n,k - x))$ is a global minimum. Also, by storing optimal $x$ for each cell in DP table \& referring to its value for prev cell, optimal $x$ for each cell can be found in constant time, improving it to $O(nk)$ time. However, there is an even faster solution that involves a different parametrization of problem: Let $k$: total number of floors s.t. eggs break when dropped from $k$th floor (example above is equivalent to taking $k = 37$). Let $m$: minimum floor from which egg must be dropped to be broken. Let $f(t,n)$: maximum number of values of $m$ that are distinguishable using $t$ tries \& $n$ eggs. Then $f(t,0) = f(0,n) = 1$ $\forall t,n\ge0$. Let $a$: floor from which 1st egg is dropped in optimal strategy. If 1st egg broke, $m$ is from 1 to $a$ \& distinguishable using at most $t - 1$ tries \& $n - 1$ eggs. If 1st eggs did not break, $m$ is from $a + 1$ to $k$ \& distinguishable using $t - 1$ tries \& $n$ eggs. Therefore, $f(t,n) = f(t - 1,n - 1) + f(t - 1,n)$. Then problem is equivalent to finding minimum $x$ s.t. $f(x,n)\ge k$. To do so, could compute $\{f(t,i):0\le i\le n\}$ in order of increasing $t$, which would take $O(nx)$ time. Thus, if separately handle case of $n = 1$, algorithm would take $O(n\sqrt{k})$ time. But recurrence relation can in fact be solved, giving $f(t,n) = \sum_{i=0}^n \binom{t}{i}$, which can be computed in $O(n)$ time using identity $\binom{t}{i + 1} = \binom{t}{i}\dfrac{t - i}{i + 1}$, $\forall i\ge0$. Since $f(t,n)\le f(t + 1,n)$, $\forall t\ge0$, can binary search on $t$ to find $x$, giving an $O(n\log k)$ algorithm.
	\end{itemize}
	\item {\bf Matrix chain multiplication.} \href{https://en.wikipedia.org/wiki/Matrix_chain_multiplication}{Wikipedia{\tt/}matrix chain multiplication}. Matrix chain multiplication is a well-known example that demonstrates utility of DP. E.g., engineering applications often have to multiply a chain of matrices. It is not surprising to find matrices of large dimensions, e.g. $100\times100$. Therefore, our task: multiply matrices $A_1,\ldots,A_n$. Matrix multiplication is not commutative, but is associative; \& can multiply only 2 matrices at a time. So, can multiply this chain of matrices in many different ways, e.g.: [$\ldots$] There are numerous ways to multiply this chain of matrices. They will all produce same final result, however they will take more or less time to compute, based on which particular matrices are multiplied. If matrix $A$ has dimensions $m\times n$ \& matrix $B$ has dimension $n\times q$, then matrix $A = AB$ will have dimensions $m\times Q$, \& will require $mnq$ scalar multiplications (using a simplistic \href{https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm}{matrix multiplication algorithm} for purposes of illustration).

	E.g., multiply matrices $A,B,C$. Assume: their dimensions are $m\times n,n\times p,p\times s$, resp. Matrix $ABC$ will be of size $m\times s$ \& can be calculated in 2 ways shown:
	\begin{enumerate}
		\item $A(BC)$: this order of matrix multiplication will require $nps + mns$ scalar multiplications.
		\item $(AB)C$: this order of matrix multiplication will require $mnp + mps$ scalar calculations.
	\end{enumerate}
	Assume $m = 10,n = 100,p = 10,s = 1000$. So, 1st way to multiply chain will require $10^6 + 10^6$ calculations. 2nd way will require only $10000 + 100000$ calculations. Obviously, 2nd way is faster, \& should multiply matrices using that arrangement of parenthesis.

	Therefore, our conclusion: order of parenthesis matters, \& our task: find optimal order of parenthesis.

	At this point, have several choices, 1 of which: design a DP algorithm that will split problem into overlapping problems \& calculate optimal arrangement of parenthesis. DP solution is presented below.

	Call $m[i,j]$ minimum number of scalar multiplications needed to multiply a chain of matrices from matrix $i$ to matrix $j$, i.e., $A_i\cdots A_j$, i.e., $i\le j$. Split chain at some matrix $k$, s.t. $i\le k\le j$, \& try to find out which combination produces minimum $m[i,j]$. Formula is:
	\begin{verbatim}
		if i = j, m[i,j]= 0
		if i < j, m[i,j]= min over all possible values of k (m[i,k]+m[k+1,j] + p[i-1]*p[k]*p[j]
	\end{verbatim}
	where $k$ ranges from $i$ to $j - 1$, $p_{i - 1}$: row dimension of matrix $i$, $p_k$: column dimension of matrix $k$, $p_j$: column dimension of matrix $j$. This formula can be coded as shown below, where input parameter ``chain'' is chain of matrices, i.e., $A_1,\ldots,A_n$:
	\begin{verbatim}
		function OptimalMatrixChainParenthesis(chain)
		    n = length(chain)
		    for i = 1, n
		        m[i,i] = 0    // Since it takes no calculations to multiply one matrix
		    for len = 2, n
		        for i = 1, n - len + 1
		            j = i + len - 1
		            m[i,j] = infinity      // So that the first calculation updates
		            for k = i, j - 1
		                q = m[i, k] + m[k+1, j] + p[i-1]*p[k]*p[j]
		                if q < m[i, j]     // The new order of parentheses is better than what we had
		                    m[i, j] = q    // Update
		                    s[i, j] = k    // Record which k to split on, i.e. where to place the parenthesis
	\end{verbatim}
	So far, have calculated values for all possible $m[i,j]$, minimum number of calculations to multiply a chain from matrix $i$ to matrix $j$, \& have recorded corresponding ``split point'' $s[i,j]$. E.g., if multiplying chain $A_1A_2A_3A_4$, \& it turns out $m[1,3] = 100$ \& $s[1,3] = 2$, i.e. optimal placement of parenthesis for matrices 1 to 3 is $(A_1A_2)A_3$ \& to multiply those matrices will require 100 scalar calculations.

	This algorithm will produce ``tables'' $m[\cdot,\cdot],s[\cdot,\cdot]$ that will have entries for all possible values of $i,j$. Final solution for entire chain is $m[1,n]$, with corresponding split at $s[1,n]$. Unraveling solution will be recursive, starting from top \& continuing until reach base case, i.e., multiplication of single matrices.

	Therefore, next step: actually split chain, i.e., place parenthesis where they (optimally) belong. For this purpose could use algorithm:
	\begin{verbatim}
		function PrintOptimalParenthesis(s, i, j)
		    if i = j
		        print "A"i
		    else
		        print "("
		        PrintOptimalParenthesis(s, i, s[i, j])
		        PrintOptimalParenthesis(s, s[i, j] + 1, j)
		        print ")"
	\end{verbatim}
	Of course, this algorithm is not useful for actual multiplication. This algorithm is just a user-friendly way to see what result looks like. To actually multiply matrices using proper splits, need algorithm:
	\begin{verbatim}
		function MatrixChainMultiply(chain from 1 to n)       // returns the final matrix, i.e. A1xA2x... xAn
		    OptimalMatrixChainParenthesis(chain from 1 to n)   // this will produce s[ . ] \& m[ . ] "tables"
		    OptimalMatrixMultiplication(s, chain from 1 to n)  // actually multiply
		function OptimalMatrixMultiplication(s, i, j)   // returns the result of multiplying a chain of matrices from Ai to Aj in optimal way
		    if i < j
		        // keep on splitting the chain \& multiplying the matrices in left \& right sides
		        LeftSide = OptimalMatrixMultiplication(s, i, s[i, j])
		        RightSide = OptimalMatrixMultiplication(s, s[i, j] + 1, j)
		        return MatrixMultiply(LeftSide, RightSide)
		    else if i = j
		        return Ai   // matrix at position i
		    else
		        print "error, i <= j must hold"
	    function MatrixMultiply(A, B)    // function that multiplies two matrices
	        if columns(A) = rows(B)
	            for i = 1, rows(A)
	                for j = 1, columns(B)
	                    C[i, j] = 0
	                    for k = 1, columns(A)
	                        C[i, j] = C[i, j] + A[i, k]*B[k, j]
	                    return C
	        else
	            print "error, incompatible dimensions."
	\end{verbatim}
\end{enumerate}

\subsubsection{History of name}
Term {\it dynamic programming} was originally used in 1940s by {\sc Richard Bellman} to describe process of solving problems where one needs to find best decisions one after another. By 1953, he refined this to modern meaning, referring specifically to nesting smaller decision problems inside larger decisions, \& field was therefore recognized by IEEE as a \href{https://en.wikipedia.org/wiki/Systems_analysis}{system analysis} \& engineering topic. {\sc Bellman}'s contribution is remembered in name of Bellman equation, a central result of DP which restates an optimization problem in recursive form.

{\sc Bellman} explains reasoning behind term DP in his autobiography, {\it Eye of the Hurricane: An Autobiography}:
\begin{quote}
	``I spent Fall quarter (of 1950) at \href{https://en.wikipedia.org/wiki/RAND_Corporation}{RAND}. My 1st task was to find a name for multistage decision processes. An interesting question is, ``Where did name, DP, come from?'' 1950s were not good years for mathematical research. We had a very interesting gentleman in Washing named \href{https://en.wikipedia.org/wiki/Charles_Erwin_Wilson}{\sc Wilson}. He was Secretary of Defense, \& actually had a pathological fear \& hatred of word ``research''. I'm not using term lightly; I'm using it precisely. His face would suffuse, he would turn red, \& he would get violent if people used term research in his presence. Can imagine how he felt, then, about term mathematical. RAND Corporation was employed by Air Force, \& Air Force had {\sc Wilsn} as its boss, essentially. Hence, I felt I had to do sth to shield {\sc Wilson} \& Air Force from fact I was really doing mathematics inside RAND Corporation. What title, what name, could I choose? In 1st place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use word ``programming''. I wanted to get across idea that this was dynamic, this was multistage, this was time-varying. I thought, let's kill 2 birds with 1 stone. Let's take a word that has an absolutely precise meaning, namely dynamic, in classical physical sense. It also has a very interesting property as an adjective, \& i.e., impossible to use word dynamic in a pejorative sense (ý nghĩa miệt thị). Try thinking of some combination that will possibly give it a pejorative meaning. Impossible. Thus, I thought dynamic programming was a good name. It was sth not even a Congressman could object to. So I used it as an umbrella for my activities.'' -- {\sc Richard Bellman}, {\it Eye of the Hurricane: An Autobiography} (1984, p. 159)
\end{quote}
Word {\it dynamic} was chosen by {\sc Bellman} to capture time-varying aspect of problems, \& because it sounded impressive. Word {\it programming} referred to use of method to find an optimal {\it program}, in sense of a military schedule for training or logistics. This usage is same as that in phrases \href{https://en.wikipedia.org/wiki/Linear_programming}{linear programming} \& mathematical programming, a synonym for mathematical optimization.

Above explanation of origin of term may be inaccurate: According to {\sc Russell \& Norvig}, above story ``cannot be strictly true, because his 1st paper using the term (Bellman1952) appeared before {\sc Wilson} became Secretary of Defense in 1953.'' Also, \href{https://en.wikipedia.org/wiki/Harold_J._Kushner}{\sc Harold J. Kushner} stated in a speech: ``On other hand, when I asked {\sc Bellman} same question, he replied: he was trying to upstage \href{https://en.wikipedia.org/wiki/George_Dantzig}{\sc Dantzig}'s linear programming by adding dynamic. Perhaps both motivations were true.'''' -- \href{https://en.wikipedia.org/wiki/Dynamic_programming}{Wikipedia{\tt/}dynamic programming}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}optimal substructure}
``In CS, a problem is said to have {\it optimal substructure} if an optimal solution can be constructed from optimal solutions of its subproblems. This property is used to determine usefulness of greedy algorithms for a problem.

Typically, a \href{https://en.wikipedia.org/wiki/Greedy_algorithm}{greedy algorithm} is used to solve a problem with optimal substructure if it can be proven by induction that this is optimal at each step. Otherwise, provided problem exhibits \href{https://en.wikipedia.org/wiki/Overlapping_subproblems}{overlapping subproblems} as well, \href{https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm}{divide-\&-conquer} methods or DP may be used. If there are no appropriate greedy algorithms \& problem fails to exhibit overlapping subproblems, often a lengthy but straightforward search of solution space is best alternative.

In application of DP to mathematical optimization, \href{https://en.wikipedia.org/wiki/Richard_Bellman}{\sc Richard Bellman}'s \href{https://en.wikipedia.org/wiki/Bellman_equation#Bellman's_principle_of_optimality}{Principle of Optimality} is based on idea that in order to solve a dynamic optimization problem from some starting period $t$ to some ending period $T$, one implicitly has to solve subproblems starting from later dates $s$, where $t < s < T$. This is an example of optimal substructure. Principle of Optimality is used to derive \href{https://en.wikipedia.org/wiki/Bellman_equation}{Bellman equation}, which shows how value of problem starting from $t$ is related to value of problem starting from $s$.

\subsubsection{Example}
Consider finding a \href{https://en.wikipedia.org/wiki/Shortest_path_problem}{shortest path} for traveling between 2 cities by car, as illustrated in Fig. 1. Such an example is likely to exhibit optimal substructure. I.e., if shortest route from Seattle to Los Angeles passes through Portland \& then Sacramento, then shortest route from Portland to Los Angeles must pass through Sacramento too. I.e., problem of how to get from Portland to Los Angeles is nested inside problem of how to get from Seattle to Los Angeles. (Wavy lines in graph represent solutions to subproblems.)

As an example of a problem that is unlikely to exhibit optimal substructure, consider problem of finding cheapest airline ticket from Buenos Aires to Moscow. Even if that ticket involves stops in Miami \& then London, can't conclude: cheapest ticket from Miami to Moscow stops in London, because price at which an airline sells a multi-flight trip is usually not sum of prices at which it would sell individual flights in trip.

\subsubsection{Def}
A slightly more formal def of optimal substructure can be given. Let a ``problem'' be a collection of ``alternatives'', \& let each alternative have an associated cost $c(a)$. Task: find a set of alternatives that minimizes $c(a)$. Suppose: alternatives can be \href{https://en.wikipedia.org/wiki/Partition_of_a_set}{partitioned} into subsets, i.e., each alternative belongs to only 1 subset. Suppose each subset has its own cost function. Minima of each of these cost functions can be found, as can minima of global cost function, {\it restricted to same subsets}. If these minima match for each subsets, then it's almost obvious that a global minimum can be picked  not out of full set of alternatives, but out of only set that consists of minima of smaller, local cost functions we have defined. If minimizing local functions is a problem of ``lower order'', \& (specifically) if, after a finite number of these reductions, problem becomes trivial, then problem has an optimal substructure.

\subsubsection{Problems with optimal substructure}

\begin{itemize}
	\item \href{https://en.wikipedia.org/wiki/Longest_common_subsequence_problem}{Longest common subsequence problem}
	\item \href{https://en.wikipedia.org/wiki/Longest_increasing_subsequence}{Longest increasing subsequence}
	\item \href{https://en.wikipedia.org/wiki/Longest_palindromic_substring}{Longest palindromic substring}
	\item \href{https://en.wikipedia.org/wiki/Shortest_path_problem#All-pairs_shortest_paths}{All-Pairs Shortest Path}
	\item Any problem that can be solved by DP.
\end{itemize}

\subsubsection{Problems without optimal substructure}

\begin{itemize}
	\item \href{https://en.wikipedia.org/wiki/Longest_path_problem}{Longest path problem}
	\item \href{https://en.wikipedia.org/wiki/Addition-chain_exponentiation}{Addition-chain exponentiation}
	\item {\it Least-cost airline fare.} Using online flight search, will frequently find: cheapest flight from airport A to airport B involves a single connection through airport C, but cheapest flight from airport A to airport C involves a connection through some other airport D. However, if problem takes maximum number of layovers as a parameter, then problem has optimal substructure. Cheapest flight from A to B that involves at most $k$ layovers is either direct flight; or cheapest flight from A to some airport C that involves at most $t$ layovers for some integer $t$ with $0\le t < k$, plus cheapest flight from C to B that involves at most $k - 1 - t$ layovers.'' -- \href{https://en.wikipedia.org/wiki/Optimal_substructure}{Wikipedia{\tt/}optimal substructure}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}overlapping subproblems}
``In CS, a \href{https://en.wikipedia.org/wiki/Computational_problem}{computational problem} is said to have {\it overlapping subproblems} if problem can be broken down into subproblems which are reused several times or a recursive algorithm for problem solves same subproblem over \& over rather than always generating new subproblems.

-- Trong CS, 1 vấn đề tính toán được gọi là có {\it các vấn đề con chồng lấn} nếu vấn đề có thể được chia nhỏ thành các vấn đề con được sử dụng lại nhiều lần hoặc một thuật toán đệ quy để giải quyết cùng 1 vấn đề con nhiều lần thay vì luôn tạo ra các vấn đề con mới.

E.g., problem of computing \href{https://en.wikipedia.org/wiki/Fibonacci_sequence}{Fibonacci sequence} exhibits overlapping subproblems. Problem of computing $n$th \href{https://en.wikipedia.org/wiki/Fibonacci_number}{Fibonacci number} $F(n)$, can be broken down into subproblems of computing $F(n - 1),F(n - 2)$, \& then adding the 2. Subproblem of computing $F(n - 1)$ can itself be broken down into a subproblem that involves computing $F(n - 2)$. Therefore, computation of $F(n - 2)$ is reused, \& Fibonacci sequence thus exhibits overlapping subproblems.

A naive recursive approach to such a problem generally fails due to an \href{https://en.wikipedia.org/wiki/Exponential_time}{exponential complexity}. If problem also shares an \href{https://en.wikipedia.org/wiki/Optimal_substructure}{optimal substructure} property, DP is a good way to work it out.

\subsubsection{Fibonacci sequence example}
In following 2 implementations for calculating \href{https://en.wikipedia.org/wiki/Fibonacci_sequence}{Fibonacci sequence} {\tt fibonacci} uses regular recursion \& \verb|fibonacci_mem| uses \href{https://en.wikipedia.org/wiki/Memoization}{memoization}. \verb|fibonacci_mem| is much more efficient as value for any particular $n$ is computed only once.
\begin{verbatim}
	def fibonacci(n):
	    if n <= 1:
	        return n
	    return fibonacci(n - 1) + fibonacci(n - 2)
	def fibonacci_mem(n, cache):
	    if n <= 1:
	        return n
	    if n in cache:
	        return cache[n]
	    cache[n] = fibonacci_mem(n - 1, cache) + fibonacci_mem(n - 2, cache)
	    return cache[n]
	print(fibonacci_mem(5, {}))  # 5
	print(fibonacci(5))  # 5
\end{verbatim}
When executed, {\tt fibonacci} function computes value of some of numbers in sequence many times over, whereas \verb|fibonacci_mem| reuses value of $n$ which was computed previously [Figs.] Difference in performance may appear minimal with an $n$ value of 5; however, as $n$ increases, \href{https://en.wikipedia.org/wiki/Computational_complexity}{computational complexity} of original {\tt fibonacci} function grows exponentially. In contrast, \verb|fibonacci_mem| version exhibits a more linear increase in complexity.'' -- \href{https://en.wikipedia.org/wiki/Overlapping_subproblems}{Wikipedia{\tt/}overlapping subproblems}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}value function}
``{\it Value function} of an \href{https://en.wikipedia.org/wiki/Optimization_problem}{optimization problem} gives value attained by \href{https://en.wikipedia.org/wiki/Objective_function}{objective function} at a solution, while only depending on \href{https://en.wikipedia.org/wiki/Parameter}{parameters} of problem. In a \href{https://en.wikipedia.org/wiki/Control_theory}{controlled} \href{https://en.wikipedia.org/wiki/Dynamical_system}{dynamical system}, value function represents optimal payoff of system over interval $[t,t_1]$ when started at time-$t$ \href{https://en.wikipedia.org/wiki/State_variable}{state variable} $x(t) = x$. If objective function represents some cost that is to be minimized, value function can be interpreted as cost to finish optimal program, \& is thus referred to as ``cost-to-go function''. In an economic context, where objective function usually represents \href{https://en.wikipedia.org/wiki/Utility}{utility}, value function is conceptually equivalent to \href{https://en.wikipedia.org/wiki/Indirect_utility_function}{indirect utility function}.

In a problem of \href{https://en.wikipedia.org/wiki/Optimal_control}{optimal control}, value function is defined as supremum of objective function taken over set of admissible controls. Given $(t_0,x_0)\in[0,t_1]\times\mathbb{R}^d$, a typical optimal control problem is to
\begin{equation*}
	\mbox{maximize } J(t_0,x_0;u) = \int_{t_0}^{t_1} I(t,x(t),u(t))\,{\rm d}t + \phi(x(t_1))\mbox{ subject to }\frac{dx(t)}{dt} = f(t,x(t),u(t))
\end{equation*}
with initial state variable $x(t_0) = x_0$. Objective function $J(t_0,x_0;u)$ is to be maximized over all admissible controls $u\in U[t_0,t_1]$, where $u$ is a \href{https://en.wikipedia.org/wiki/Measurable_function}{Lebesgue measurable function} from $[t_0,t_1]$ to some prescribed arbitrary set in $\mathbb{R}^m$. Value function is then defined as
\begin{equation*}
	V(t,x(t)) = \max_{u\in U} \int_t^{t_1} I(\tau,x(\tau),u(\tau))\,{\rm d}\tau + \phi(x(t_1)),
\end{equation*}
with $V(t_1,x(t_1)) = \phi(x(t_1))$, where $\phi(x(t_1))$ is ``scrap value''. If optimal pair of control \& state trajectories is $(x^*,u^*)$, then $V(t_0,x_0) = J(t_0,x_0;u^*)$. Function $h$ that gives a optimal control $u^*$ based on current state $x$ is called a {\it feedback control policy}, or simply a {\it policy function}.

{\sc Bellman}'s principle of optimality roughly states: any optimal policy at time $t$, $t_0\le t\le t_1$ taking current state $x(t)$ as ``new'' initial condition must be optimal for remaining problem. If value function happens to be continuous differentiable, this gives rise to an important PDE known as \href{https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation}{Hamilton--Jacobi--Bellman equation}
\begin{equation*}
	-\frac{\partial V(t,x)}{\partial t} = \max_u \left\{I(t,x,u) + \frac{\partial V(t,x)}{\partial x}f(t,xu)\right\},
\end{equation*}
where \href{https://en.wiktionary.org/wiki/maximand}{maximand} on RHS can also be rewritten as \href{https://en.wikipedia.org/wiki/Hamiltonian_(control_theory)}{Hamiltonian} $H(t,x,u,\lambda) = I(t,x,u) + \lambda(t)f(t,x,u)$, as
\begin{equation*}
	-\frac{\partial V(t,x)}{\partial t} = \max_u H(t,x,u,\lambda)
\end{equation*}
with $\partial_xV(t,x) = \lambda(t)$ playing role of \href{https://en.wikipedia.org/wiki/Costate_variable}{costate variables}. Given this def, further have $\frac{d\lambda(t)}{dt} = \frac{\partial^2V(t,x)}{\partial x\partial t} + \frac{\partial^2V(t,x)}{\partial x^2}\cdot f(x)$, \& after differentiating both sides of HJB equation w.r.t. $x$
\begin{equation*}
	-\frac{\partial^2V(t,x)}{\partial t\partial x} = \frac{\partial I}{\partial x} + \frac{\partial^2V(t,x)}{\partial x^2}f(x) + \frac{\partial V(t,x)}{\partial x}\frac{\partial f(x)}{\partial x},
\end{equation*}
which after replacing appropriate terms recovers \href{https://en.wikipedia.org/wiki/Costate_equation}{costate equation}
\begin{equation*}
	-\dot{\lambda}(t) = \partial_xH = \partial_xI + \lambda(t)\partial_xf(x),
\end{equation*}
where $\dot{\lambda}(t)$ is \href{https://en.wikipedia.org/wiki/Newton_notation}{Newton notation} for derivative w.r.t. time.

Value function is unique \href{https://en.wikipedia.org/wiki/Viscosity_solution}{viscosity solution} to Hamilton--Jacobi--Bellman equation. In an \href{https://en.wikipedia.org/wiki/Online_algorithm}{online} closed-loop appropriate optimal control, value function is also a \href{https://en.wikipedia.org/wiki/Lyapunov_function}{Lyapunov function} that establishes global asymptotic stability of closed-loop system.'' -- \href{https://en.wikipedia.org/wiki/Value_function}{Wikipedia{\tt/}value function}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]

\end{document}