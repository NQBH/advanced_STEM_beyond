\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{goal}{Goal}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Mathematical Optimization -- Toán Tối Ưu}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Mathematical Optimization -- Toán Tối Ưu}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/optimization/NQBH_mathematical_optimization.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/optimization/NQBH_mathematical_optimization.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Adjoint}
2 main types: discrete adjoint, continuous adjoint.

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item {\sc Tan Bui-Thanh}. {\sc Adjoint \& Its roles in Sciences, Engineering, and Mathematics: A Tutorial}.
	
	{\sf Abstract.} Synergize roles of adjoint in various disciplines of mathematics, sciences, \& engineering. Though materials developed \& presented here are not new -- as each or some could be found in (or inferred from) publications in different fields -- believe: 1st effort to systematically unify these materials on the same mathematical footing starting from the basic defs. Aim: provide a unified perspective \& understanding of adjoint applications. As a result, this work could give broader views \& better insights into the application of adjoint beyond a single community. By rigorously establishing general results \& then developing materials specific to each application, bring forth details on how abstract concepts{\tt/}defs can be translated into particular applications \& connections among them. This paper is written as an interdisciplinary tutorial on adjoint with discussions \& with many examples from different fields including linear algebra (e.g., eigendecomposition \& SVD), ODEs (asymptotic stability of an epidemic model), PDEs (well-posedness of elliptic, hyperbolic, \& Friedrichs' types), neural networks (backpropagation of feed-forward deep neural networks), least squares \& inverse problems (with Tikhonov regularization), \& PDE-constrained optimization. Exposition covers results \& applications in both finite-dimensional \& infinite-dimensional Hilbert spaces.
	
	{\sf Keywords.}Adjoint, optimization, backpropagation, eigenvalue problem, singular value decomposition, asymptotic stability, wellposedness, least squares, PDE-constrained optimization, reproduction number.
	\begin{itemize}
		\item {\sf1. Introduction.} Adjoint is ubiquitous in mathematics. History of adjoint can be traced back to as far as {\sc Lagrange} in 1766, in a memoir extending the letter that he wrote to {\sc D'Alembert} in Jun 1765 discussing his new method of solving $n$th-order differential equations. Term ``adjoint equation'', 1st used to call corresponding equation developed from Lagrange memoir, is due to {\sc Fuchs}. Adjoint operator was introduced by {\sc Riesz} in his seminal paper ({\sc Riesz} also established {\it functional analysis} as a new mathematical discipline) to study inverse of linear operators.
		
		Since then adjoint has been pervasive in vast literature across mathematics, engineering, \& sciences disciplines. This is not surprising as adjoint has many appealing features including:
		\begin{enumerate}
			\item adjoint operator typically possesses nicer properties than the original operator (e.g. adjoint of a densely defined linear operator is a closed operator though the original operator may not)
			\item adjoint equation is always linear even when the original equation is not.
		\end{enumerate}
		Though a comprehensive survey on adjoint accounting for its application in many disciplines{\tt/}fields (\& their sub-disciplines) could be desirable to appreciate crucial role that adjoint plays, it is perhaps an impossible task. Or more precisely, it is rather the task for a book than for a paper.
		\begin{goal}
			Provide a window into the adjoint \& its crucial role in certain subsets of computational science, engineering, \& mathematics.
		\end{goal}
		Exposition is necessarily personal \& biased based on topics familiar with. Materials developed \& presented are not new, as each or some could be found in (or inferred from) publications in different fields.
		\begin{goal}
			Systematically unify these materials on the same mathematical footing starting from basic defs.
		\end{goal}
		This expectantly provides a more unified perspective on usefulness of adjoint in variety of applications. As a result, this work could give broader views \& better insights into applications of adjoint beyond 1 field. By establishing general results \& then developing materials specific to each application, bring forth details on how abstract concepts{\tt/}defs can be translated into particular applications \& connections among them. This paper is written as a tutorial on adjoint with many examples presented with discussions. Though trike for a self-contained expositor, necessary to state a few results without proof to keep length of paper manageable \& to focus on adjoint \& its roles.
		
		{\sf Structure.}
		\begin{itemize}
			\item Sect. 2: Introduce various notations, defs, \& some examples.
			\item Sect. 3: Part I in finite dimensions. Start with celebrated Riesz representation theorem that is then used to prove existence of adjoints of continuous linear operators. Followed by closed range theorem that will be useful in many places later. Built upon these basic materials, shall develop several applications of adjoint.
			\begin{itemize}
				\item Subsect. 3.1: highlight role of adjoint in assessing solvability of linear operator equations before solving them.
				\item Subsect. 3.2: 1 of most important applications of adjoint is in study of eigenvalue problems. Main result: spectral decomposition of self-adjoiont operators in finite dimensions. Tight relationship between orthogonality of a projection \& its self-adjointness, immediately leading to a generalized Pythagorean theorem.
				\item Subsect. 3.3: start with classical projection theorem \& then deploy it together with close range theorem to find necessary \& sufficient condition for optimality of an abstract linear least squares problem.
				\item Subsect. 3.4: Next important application: SVD, in which employ spectral decomposition to establish an SVD decomposition for general linear operator in finite dimensions. This SVD decomposition is then deployed to provide trivial proofs for closed range theorem, rank-nullity theorem, \& fundamental theorem of linear algebra for abstract linear operators. Discuss equivalence of SVD of an abstract linear operator \& SVD of its matrix representation.
				\item Subsect. 3.5: optimization with equality constraints. Expose at length role of adjoint in optimization theory valid for both finite \& infinite dimensions. Accomplished by working with Fr\'echet derivative \& its Riesz representation counterpart as the gradient. Though can be further developed (e.g. to 2nd-order optimality conditions) focus on 1st-order optimality condition. Recall an implicit function theorem \& use it to prove an abstract inverse function theorem, then deployed to derive 1st-order optimality condition for abstract optimization problems with equality constraints. Important role of adjoint comes into picture when prove an abstract Lagrangian multiplier theorem using closed range theorem. Importance of adjoint is further amplified when study constrained optimization problems with separable structure. Here, adjoint facilitates an efficient gradient-based optimization algorithm in unconstrained reduced subspace while ensuring feasibility of constraints at all time. 
				\item Subsect. 3.6: show that when applying this reduced space approach for optimization problem arisen from training deep neural networks (DNNs), recover backpropagation from reduced space approach provides further insights into algorithm.
				\item Subsect. 3.7: stability of autonomous ODEs. Main goal: exhibit vital role of adjoint in establishing necessary \& sufficient conditions for asymptotic stability (in sense of Lyapunov) of ODEs' equilibria.
			\end{itemize}
			\item Sect. 4: Part II in infinite dimensions. Start with a more general adjoint definition for densely defined linear operators. Useful for all subsects. except Subsect. 4.1.
			\begin{itemize}
				\item Subsect. 4.1: illposedness (in Hadamard's sense) nature of inverting compact operators. Extend spectral theorem \& SVD decomposition theorem to Hilbert-Schmidt theorem \& a general SVD theorem for compact (linear) operators in infinite dimensions. Main result: a Picard theorem stating necessary \& sufficient conditions under which task of inverting a compact operator is solvable. Often employed to show that inverting a compact operator violates at least stability condition of well-posedness. Deploy Riesz-Fredholm theory to show how Tikhonov regularization can restore well-posedness at expense of getting a nearby solution.
				\item Subsect. 4.2: Discuss role of adjoint in establishing well-posedness of abstract linear operator equations with application to PDEs. 2 main results developed: Banach-Nečas-Babuška theorem, Lax-Milgram lemma.
				\item Subsect. 4.3: study Sturm-Liouville eigenvalue problem \& generalized Fourier series in $L^2$. For closed linear operators. Using Hilbert-Schmidt theorem \& Lax-Milgram lemma, establish a spectral decomposition theorems for quite general linear operators, \& then apply them to Sturm-Liouville eigenvalue problems to derive Fourier series \& its generalizations.
				\item Subsect. 4.4: PDE-constrained optimization. Show how to rigorously translate abstract Lagrangian multiplier theorem to derive adjoint equation \& reduced gradient for prototype elliptic \& hyperbolic PDEs. Show: differential operators of adjoint equations are indeed adjoint operators derived at beginning of Sect. 4.
			\end{itemize}
		\end{itemize}
		To keep exposition succinct, defs \& results valid for both cases are presented{\tt/}proved once \& when that happens we will explicitly state so. Most of our developments start from abstract operator settings \& then reduce to standard finite-dimensional settings in $\mathbb{R}^d$ as a special case. In some cases, e.g. optimization, order is reversed as we believe it is more natural that way. Each sect. of paper is equipped with examples on which we show how to apply preceding abstract theoretical results. Make an effort to include practical examples from different fields including linear algebra (e.g., eigendecomposition \& SVD), ODEs (an epidemic model), PDEs (of elliptic, hyperbolic, \& Friedrichs' types), neural networks (feed-forward deep neural networks), least squares \& inverse problems (with Tikhonov regularization), PDE-constrained optimization, etc. Due to interdisciplinary nature of paper, do not attempt to provide an exhaustive list of refs but a few for each sect. to keep references at a manageable length.
				
		\item {\sf2. Notations.} boldface lower case letters, e.g., ${\bf u}$: vector-valued functions in $\mathbb{R}^d$. Calligraphic uppercase letters, e.g., $\mathcal{A}$: matrices, script uppercase letters: operators. Bold blackboard upper cases: spaces \& sets. Lowercase letters: scalar-valued functions, also for results valid for both finite \& infinite dimensional settings. Bold uppercase letters: bases of vector spaces. Frequently identify dual of any Hilbert space with itself. $\theta$ denotes either ``zero'' function or ``zero'' vector in appropriate space.
		
		Defs: Linear transformation{\tt/}map{\tt/}operator \& its domain, range, kernel{\tt/}null space. Any matrix is a linear operator. Integrals are operators: $\mathcal{A}f = \int_0^1 \omega(t)f(t)\,{\rm d}t$. Differentiation is a linear operator: $\mathcal{A}u = \frac{d^2}{dt^2}u(t)$.
		
		\item {\sf3. Part I: Adjoint operators in finite dimensional Hilbert spaces.} Assume $X,Y$ are finite dimensional vector spaces, $\dim X = n < \infty,\dim Y = m < \infty$. If $A\in L(X,Y)$ \& $\dim X < \infty$, then $A\in\mathcal{B}(X,Y)$. Let $E = \{e_i\}_{i=1}^n,G = \{g_i\}_{i=1}^m$ be orthonormal bases for $X,Y$, resp. $\forall u\in X$, denote by $u^E$ the unique vector of coordinates of $u$ in $E$, then $(u^E,v^E)_{\mathbb{F}^d} = (u,v)_X$. Matrix representation of $A$ w.r.t. bases $E,G$ is denoted as $A^{EG}$. When there is no ambiguity on bases referred to, simply ignore superscripts for both coordinate vector \& matrix representation. Denote $i$th element of a vector ${\bf u}$ as ${\bf u}(i)$ \& element at $i$th row \& $j$th column of a matrix $A$ as $A(i,j)$. Also use ${\bf u}_i\coloneqq{\bf u}(i)$. Use square brackets to express matrices \& vectors with a finite number of components. Unless otherwise stated, vectors with finite number of components are column vectors. Organize: celebrated Riesz representation theorem \& closed range theorem, upon which develop several applications of adjoint.
		
		\begin{theorem}[Riesz representation]
			Let $L$ be a bounded linear functional on a Hilbert space $X$. There exists a unique $u\in X$ s.t. $L(v) = (u,v)_X$, $\forall v\in X$. Furthermore, operator norm of $L$ is given as $\|L\|\coloneqq\sup_{v\in X} \frac{|L(v)|}{\|v\|_X} = \|u\|_X$.
		\end{theorem}
		
		\begin{definition}[Adjoint operator]
			Let $A\in B(X,Y)$. Say that $A^\star:Y\to X$ is the adjoint of $A$ iff $(Au,v)_Y = (u,A^\star v)_X$, $\forall u\in X,v\in Y$.
		\end{definition}
		
		\begin{proposition}
			Let $A\in B(X,Y)$. Then $A^\star$ exists \& is unique. Furthermore, it is linear with $\|A^\star\| = \|A\|$, where operator norm is defined as $\|A\|\coloneqq\sup_{u\in X} \frac{\|Au\|_Y}{\|u\|_X} = \sup_{\|u\|_X = 1} \|Au\|_Y$.
		\end{proposition}
		An $M$-weighted inner product $(\cdot,\cdot)_{\mathbb{R}^d,M}$ where $({\bf u},{\bf v})_{\mathbb{R}^d,M}\coloneqq\sum_{i,j} {\bf u}(i)M(i,j){\bf v}(i)\coloneqq{\bf u}^TM{\bf v}$, $\forall{\bf u},{\bf v}\in V$, $M$: a symmetric \& positive definite matrix. Adjoint operator $A^\star = A^TM$ of a matrix $A$. Set $\mathbb{P}^n[0,1]$ of complex-valued polynomial of order $\le n$ on $[0,1]$.
		
		\begin{proposition}
			Let $E,G$ be orthonormal bases of $X,Y$, resp., \& $\dim X = n,\dim Y = m$. Let $A,B$ be the matrix representations of $A,A^\star$ w.r.t. bases $E,G$. Then $B = A^\star$ where $A^\star$ be the conjugate transpose of $A$.
		\end{proposition}
		Orthogonal complement $S^\bot$ of $S\subset X$ is a closed subspace of $X$ \& $S\cap S^\bot =\{0\}$. Close $\overline{S}$ of $S$ is the smallest closed set containing $S$. $(S^\bot)^\bot = \overline{S}$.
		
		\begin{theorem}[Closed range]
			Let $A:X\to Y$. $[{\rm R}(A)]^\bot = {\rm N}{A^\star},\overline{{\rm R}(A)} = [N(A^\star)]^\bot,[{\rm R}(A^\star)]^\bot = {\rm N}(A),\overline{{\rm R}(A^\star)} = [{\rm N}(A)]^\bot$. Consequently, $X = {\rm N}(A)\oplus\overline{{\rm R}(A^\star)}$, $Y = {\rm N}(A^\star)\oplus\overline{{\rm R}(A)}$.
		\end{theorem}
		Since consider only Hilbert spaces, which are reflexive, $\overline{{\rm R}(A^\star)} = [{\rm N}(A)]^\bot$ holds. In general $\overline{{\rm R}(A^\star)}\subset[{\rm N}(A)]^\bot$. For finite dimensional vector spaces $X,Y$, ${\rm R}|(A)$, \& hence ${\rm R}(A^\star)$, is obviously closed. Proof of closed range theorem for finite dimensional cases is trivial using SVD decomposition.
		
		\begin{itemize}
			\item {\sf Subsect. 3.1: Application of adjoint to solvability of linear operator equations.} Solvability of linear operator equations before solving them. Existence of a solution of linear operator equation $Au = f$ where $A:X\to Y$.
			
			\begin{lemma}
				\item(i) Existence: The linear equation $Au = f$ has a solution iff $y\in{\rm N}(A^\star)^\bot$.
				\item(ii) Uniqueness: The solution of the linear equation $Au = f$ is unique iff ${\rm N}(A) = \{0\}$.
				\item(iii) If $\dim X = \dim Y$, uniqueness $\Leftrightarrow$ existence.
			\end{lemma}
			Existence condition can be simply $y\in{\rm R}(A)$. However, easier to work with ${\rm N}(A^\star)^\bot$ as it gives us equations to determine{\tt/}characterize ${\rm N}(A^\star)^\bot$.
			
			An operator setting for problem of fitting a quadratic polynomial $u(x)$ with 2 pieces of information about $u(x)$.
			
			\item {\sf Subsect. 3.2: Application of adjoint to eigenvalue problems.} Role of adjoint in study of eigenvalue problems.
			
			\begin{definition}[Eigenvalue problem]
				Let $A:X\to X$ be a linear operator. $Au = \lambda u$ is called an \emph{eigenvalue problem} if there exists a nontrivial pair $(\lambda,x)$ ($x$ is not a zero vector{\tt/}function but $\lambda$ could be zero). In particular: $\lambda$ is called an \emph{eigenvalue}. $x$ is called an \emph{eigenfunction}, associated with eigenvalue $\lambda$, of $A$. If $X$ is a finite-dimensional space, i.e., $X = \mathbb{R}^d$, $x$ is typically called \emph{eigenvector}.
			\end{definition}
			
			\begin{definition}[Self-adjoint operator]
				If $A^\star = A$, then $A$ is called \emph{self-adjoint}.
			\end{definition}
			
			\begin{lemma}
				Let $A:X\to X$ be a linear operator \& $A$ is self-adjoint. Then:
				\item(i) Eigenvalues of $A$ are real.
				\item(ii) Eigenfunctions corresponding to distinct eigenvalues are orthogonal to each other. I.e., if $(\lambda,u)$ \& $(\alpha,v)$ are 2 eigen-pairs \& $\lambda\ne\alpha$ then $(u,v)_X = 0$.
			\end{lemma}
			
			\begin{proposition}
				Let $A:X\to X$ be a linear operator. Then $A$ has at least 1 eigenvalue.
			\end{proposition}
			
			\begin{theorem}
				Let $A:X\to X$ be a self-adjoint linear operator. Then, an orthonormal basis of $X$ can be constructed from eigenfunctions of $A$.
			\end{theorem}
			
			\begin{theorem}[Spectral decomposition of self-adjoint operators in finite dimensions]
				Let $\dim X = n$ \& $A:X\to X$ be a linear \& self-adjoint operator. There exists $n$ real values $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_n$ \& orthonormal vectors $u_1,u_2,\ldots,u_n$ s.t. $Au_i = \lambda_iu_i$. $\forall x\in X$, $Ax = \sum_{i=1} \lambda_i(x,u_i)_Xu_i\Rightarrow A = \sum_{i=1}^n \lambda_i(\cdot,u_i)_Xu_i$, i.e., $A$ is completely determined by its eigenpairs.
			\end{theorem}
			
			\item {\sf Subsect. 3.3.} Employ classical projection theorem $+$ closed range theorem to find necessary \& sufficient condition for optimality of an abstract linear least squares problem.
			
			\item {\sf Subsect. 3.4.} Singular value decomposition (SVD). Deploy SVD decomposition to provide trivial proofs for closed range theorem, rank-nullity theorem, \& fundamental theorem of linear algebra for abstract linear operators.
			
			\item {\sf Subsect. 3.5.} Optimization with equality constraints, expose at length role of adjoint in optimization theory valid for both finite \& infinite dimensions.
			
			\item {\sf Subsect. 3.6: Application of adjoint to backpropagation in deep learning.} A reduced spaced approach using adjoint reduces to backpropagation of deep neural networks. Consider standard fully connected deep neural network \& use adjoint method to derive backpropagation method for computing gradient of loss function w.r.t. weights \& biases of a general fully-connected deep neural work (DNN). Review papers on deep learning \cite{LeCun_Bengio_Hinton2015}, [69], history of backpropagation: [54,55]. Gradient is needed for gradient-based methods e.g. stochastic gradient descent. Extension of adjoint method for other type of neural networks e.g. ResNet \& CNN are straightforward. Backpropagation is nothing more than a reduced space approach to compute gradient using gradient method.
			
			\begin{definition}[$L$-layer Neural network]
				Given $n_l,s_0,s_1,\ldots,s_{n_l}\in\mathbb{N}$, an $n_l$-layer neural network is defined as the following series of composition: Input layer: ${\bf a}^0 - {\bf x} = {\bf0}$, The $i$th layer: ${\bf a}^i - \sigma(\mathcal{W}^i{\bf a}^{i-1} + {\bf b}^i) = {\bf0}$, $i = 1,\ldots,n_l$, where ${\bf x}\in\mathbb{R}^{s_0},\mathcal{W}^i\in\mathbb{R}^{s_i}\times\mathbb{R}^{s_i - 1}$, ${\bf b}^i\in\mathbb{R}^{s_i}$, $i = 1,\ldots,n_l$, are weight matrix \& bias vector of the $i$th layer; ${\bf a}^i\in\mathbb{R}^{s_i}$ is the output of the $i$th layer; \& the activation function $\sigma$ acts componentwise when its argument is a vector.
			\end{definition}
			Define ${\bf u}\coloneqq[{\bf a}^0,\ldots,{\bf a}^{n_l}]^T,{\bf z}\coloneqq[\mathcal{W}^1,{\bf b}^1,\ldots,W^{n_l},{\bf b}^{n_l}]^T,{\bf c}({\bf u},{\bf z}) = {\bf0}$ as concatenation of all subequation. For concreteness, consider loss (objective) function $f({\bf u},{\bf z})\coloneqq\frac{1}{2}\|{\bf a}^{\rm obs} - {\bf a}^{n_l}\|^2$ where ${\bf a}^{\rm obs}$ is a given data (label). Neural network training problem is constrained optimization problem in Ex 3.59.
			
			Using adjoint equations, backpropagate to compute adjoint solution from output layer to $i$th layer, \& then compute gradients. From backpropagation point of view, ${\bf y}^i,i = 1,\ldots,n_l$ are simply temporary variables to help compute{\tt/}write chain rule in a succinct manner. The adjoint approach, however, reveals their precise role as adjoint solutions -- also known as Lagrangian multiliers -- of adjoint equations stemming from 1st-order optimality condition using reduced space approach. DNN training problem, from adjoint point of view, is a constrained optimization problem with forward pass as forward equations. The backpropagation is thus nothing more than a reduced space approach to compute gradient using adjoint method.
			
			\item {\sf Subsect. 3.7.} role of adjoint in establishing stability of autonomous ODEs. A brief view of role of adjoint in study of stability of equilibria of ODEs. Limit to autonomous systems of form $\dot{\bf x}\coloneqq\frac{d{\bf x}}{dt} = {\bf f}({\bf x})$ where ${\bf x}\in\mathbb{R}^d,{\bf f}:G\subset\mathbb{R}^d\to\mathbb{R}^d$ is assumed to be continuous \& locally Lipschitz.
			
			\begin{definition}[Lyapunov stability]
				The equilibrium point ${\bf0}$ is stable in the sense of Lyapunov if for any $\varepsilon > 0,\exists\delta > 0$ s.t. for every (maximal) solution ${\bf x}:I\to G$ s.t. ${\bf x}(0)\le\delta$, have ${\bf x}(t)\le\varepsilon$, $\forall t\in I\cap(0,\infty)$.
			\end{definition}
			
			\begin{theorem}[Lyapunov direct method]
				If there exists an open neighborhood $U$ of ${\bf0}$ \& a continuous differentiable function $V$ s.t.
				\item(i) $V({\bf0}) = 0$, $V({\bf z}) > 0$, $\forall{\bf z}\in U\backslash\{{\bf 0}\}$, \&
				\item(ii) $V_{\bf f}({\bf z})\coloneqq(\nabla V({\bf z}),{\bf f}({\bf z}))\coloneqq\sum_{i=1}^n \partial_{{\bf z}_i}V {\bf f}_i({\bf z})\le0$, $\forall{\bf z}\in U$.
				
				Then $\bf0$ is a stable equilibrium point of $\dot{\bf x}\coloneqq\frac{d{\bf x}}{dt} = {\bf f}({\bf x})$.
			\end{theorem}
			
			\begin{definition}[Asymptotic stability]
				The equilibrium $\bf0$ is \emph{attractive} if there exists $\delta > 0$ s.t. $\forall{\bf x}_0\in G$ s.t. $\|{\bf x}_0\|\le\delta$, then the solution ${\bf x}(t)\to{\bf0}$ as $t\to\infty$. Say $\bf0$ \emph{asymptotically stable} in the sense of Lyapunov if it is both stable \& attractive.
			\end{definition}
			
			\begin{theorem}[A sufficient condition for asymptotic stability]
				Assume that there exists a neighborhood $U$ of ${\bf x}_0$ \& a continuously differentiable function $V$ s.t.
				\item(i) $V({\bf0}) = 0$, $V({\bf z}) > 0$, $\forall{\bf z}\in U\backslash\{{\bf 0}\}$, \& $V_{\bf f}\le0$, $\forall{\bf z}\in U$, \&
				\item(ii) $\bf0$ is the inverse image of $V_{\bf f}({\bf z}) = 0$, i.e., $V_{\bf f}^{-1}(0) = {\bf0}$.
				
				Then $\bf0$ is asymptotically stable.
			\end{theorem}
			Study stability of systems of linear ODEs, \& this is where the adjoint comes into picture. Infer stability of nonlinear systems using stability of their linearizations. 
			
		\end{itemize}
		\item {\sf4. Part II: Adjoint operators in Infinite dimensional Hilbert spaces.}
		\item {\sf5. Conclusions.}
	\end{itemize}
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Algorithms for Optimization -- Thuật Toán Tối Ưu}

\subsection{\cite{Kochenderfer_Wheeler2019}. {\sc Mykel J. Kochenderfer, Tim A. Wheeler}. Algorithms for Optimization. 2019}
{\sf[118 Amazon ratings]}

{\sf Amazon review.} A comprehensive introduction to optimization with a focus on practical algorithms for design of engineering systems.

This book offers a comprehensive introduction to optimization with a focus on practical algorithms. Book approaches optimization from an engineering perspective, where objective: design a system that optimizes a set of metrics subject to constraints. Readers will learn about computational approaches for a range of challenges, including searching high-dimensional spaces, handling problems where there are multiple competing objectives, \& accommodating uncertainty in metrics. Figures, examples, \& exercises convey intuition behind mathematical approaches. Text provides concrete implementation in Julia programming language.

Topics covered include derivatives \& their generalization to multiple dimensions; local descent \& 1st- \& 2nd-order methods that inform local descent; stochastic methods, which introduce randomness into optimization process; linear constrained optimization, when both objective function \& constraints are linear; surrogate models, probabilistic surrogate models, \& using probabilistic surrogate models to guide optimization; expression optimization, \& multidisciplinary design optimization. Appendixes offer an introduction to Julia language, test functions for evaluating algorithm performance, \& mathematical concepts used in derivation \& analysis of optimization methods discussed in text. Book can be used by advanced undergraduates \& graduates students in mathematics, statistics, CS, any engineering field, (including electrical engineering \& aerospace engineering), \& operations research, \& as a reference for professionals.
\begin{itemize}
	\item {\sf Preface.} This book provides a broad introduction to optimization with a focus on practical algorithms for design of engineering systems. Cover a wide variety of optimization topics, introducing underlying mathematical problem formulations \& algorithms for solving them. Figures, examples, \& exercises are provided to convey intuition behind various approaches.
	
	This text is intended for advanced undergraduates \& graduate students as well as professionals. Book requires some mathematical maturity \& assumes prior exposure to multivariable calculus, linear algebra, \& probability concepts. Some review material is provided in appendix. Disciplines where book would be especially useful include mathematics, statistics, CS, aerospace, electrical engineering, \& operations research.
	
	Fundamental to this textbook are algorithms, which are all implemented in Julia programming language. Have found language to be ideal for specifying algorithms in human readable form. Permission is granted, free of charge, to use code snippets associated with this book, subject to condition: source of code is acknowledged. Anticipate (Dự đoán): others may want to contribute translations of these algorithms to other programming languages. As translations become available, link to them from book's webpage.
	\item {\sf1. Introduction.}
	\item {\sf2. Derivatives \& Gradients.}
	\item {\sf3. Bracketing.}
	\item {\sf4. Local Descent.}
	\item {\sf5. 1st-Order Methods.}
	\item {\sf6. 2nd-Order Methods.}
	\item {\sf7. Direct Methods.}
	\item {\sf8. Stochastic Methods.}
	\item {\sf9. Population Methods.}
	\item {\sf10. Constraints.}
	\item {\sf11. Linear Constrained Optimization.}
	\item {\sf12. Multiobjective Optimization.}
	\item {\sf13. Sampling Plans.}
	\item {\sf14. Surrogate Models.}
	\item {\sf15. Probabilistic Surrogate Models.}
	\item {\sf16. Surrogate Optimization.}
	\item {\sf17. Optimization under Uncertainty.}
	\item {\sf18. Uncertainty Propagation.}
	\item {\sf19. Discrete Optimization.}
	\item {\sf20. Expression Optimization.}
	\item {\sf21. Multidisciplinary Optimization.}
	\item {\sf A. Julia.}
	\item {\sf B. Test Functions.}
	\item {\sf C. Mathematical Concepts.}
\end{itemize}

%------------------------------------------------------------------------------%

\section{Mathematical Optimization}
\textbf{\textsf{Community -- Cộng đồng.}} {\sc Marc Quincampoix, C\'{e}dric Villani}.

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Villani2003}. {\sc C\'{e}dric Villani}. {\it Topics in Optimal Transportation}.
	
	\begin{itemize}
		\item {\sf Preface.} Optimal mass transportation was born in France in 1781, with a very famous paper by {\sc Gaspard Monge}, {\it M\'emoire sur la th\'eorie des d\'eblais et des remblais}. Since, become a classical subject in probability theory, economics, \& optimization. Gained extreme popularity, since many researches in different areas of mathematics understood that optimal mass transportation was strongly linked to their subject. A precise birthdate for this revival: 1987 note by {\sc Yann Brenier}, {\it D\'ecomposition polaire et r\'earrangement des champs de vecteurs} paved the way towards a beautiful interplay between PDEs, fluid mechanics, geometry, probability theory, \& function analysis, developed over last 10 years, through contributions of a number of authors, with optimal transportation problems as a common denominator. 2 volumes of {\it Mass transportation problems} by {\sc Rachev \& R\"uschendorf} depicting many applications of Monge-Kantorovich distance to various problems, together with classical theory of optimal transportation problem in a very abstract setting; survey by {\sc Evans}, which can also be considered as an introduction to subject, describes several applications of $L^1$ theory (i.e., when cost function is a distance), extremely clear lecture notes by {\sc Ambrosio}, centered on $L^1$ theory from point of view of calculus of variations, lecture notes by {\sc Urbas} -- a marvelous reference for regularity theory of Monge-Amp\`ere equation arising in mass transportation.
		
		\item {\sf Introduction.} {\it Formulation of optimal transportation problem.} Assume given a pile of sand, \& a hole having to be completely filled up with the sand. Obviously pile \& hole must have same volume. Normalize mass of pile $= 1$. Will model both pile \& hole by probability measures $\mu,\nu$, defined respectively on some measure spaces $X,Y$. Whenever $A,B$: measurable subsets of $X,Y$, resp., $\mu[A]$ gives a measure of how much sand is located inside $A$, $\nu[B]$ of how much sand can be piled in $B$. Moving sand around needs some effort, modeled by a measurable {\it cost function} defined on $X\times Y$. Informally, $c(x,y)$ tells how much it costs to transport 1 unit of mass from location $x$ to location $y$. Natural to assume at least: $c$ is measurable \& nonnegative. Should not a priori exclude the possibility that $c$ takes infinite values, \& so $c$ should be a measurable map from $X\times Y$ to $\mathbb{R}\cup\{+\infty\}$.
		
		\begin{problem}[Central question]
			How to realize transportation at minimal cost?
		\end{problem}
		Before studying this question, have to make clear what a way of transportation, or a {\it transference\footnote{the process of moving something from one place, person or use to another.} plan}, is. Will model transference plans by probability measures $\pi$ on the product space $X\times Y$. Informally, $d\pi(x,y)$ measures amount of mass transferred from location $x$ to location $y$. Do not a priori exclude the possibility that some mass located at point $x$ may be split into several parts (several possible destination $y$'s). For a transference plan $\pi\in P(X\times Y)$ to be admissible, of course necessary that all the mass taken from point $x$ coincide with $d\mu(x)$, \& all the mass transferred to $y$ coincide with $d\nu(y)$, i.e., $\int_Y d\pi(x,y) = d\mu(x)$, $\int_X d\pi(x,y) = d\nu(y)$. More rigorously, require $\pi[A\times Y] = \mu[A]$, $\pi[X\times B] = \nu[B]$ for all measurable subsets $A\subset X$ \& $B\subset Y$. Equivalent to stating: for all functions $\phi,\psi$ in a suitable class of test functions,
		\begin{equation}
			\int_{X\times Y} \phi(x) + \psi(y)\,{\rm d}\pi(x,y) = \int_X \phi(x)\,{\rm d}\mu(x) + \int_Y \psi(y)\,{\rm d}\nu(y).
		\end{equation}
		In general, the natural set of admissible test functions for $(\phi,\psi)$ is $L^1(d\mu)\times L^1(d\nu)$, or equivalently, $L^\infty(d\mu)\times L^\infty(d\nu)$. In most situations of interest, this class can be narrowed to just $C_b(X)\times C_b(Y)$, or $C_0(X)\times C_0(Y)$. Those probability measures $\pi$ satisfying $\pi[A\times Y] = \mu[A]$, $\pi[X\times B] = \nu[B]$ are said to have {\it marginals} $\mu,\nu$, \& will be the admissible transference plans. Denote the set of all such probability measures by $\Pi(\mu,\nu)\coloneqq\{\pi\in P(X\times Y);\pi[A\times Y] = \mu[A]$, $\pi[X\times B] = \nu[B]\mbox{ holds for all measurable } A,B\}\ne\emptyset$ since tensor product $\mu\otimes\nu\in\Pi(\mu,\nu)$, corresponding to the most stupid transportation plan that one may imagine: any piece of sand, regardless of its location, is distributed over the entire hole, proportionally to the depth. A clear mathematical def of basic problem:
		
		\begin{problem}[Kantorovich's optimal transportation problem]
			Minimize $I[\pi] = \int_{X\times Y} c(x,y)\,{\rm d}\pi(x,y)$ for $\pi\in\Pi(\mu,\nu)$.
		\end{problem}
		This minimization problem was studied in 40s by {\sc Kantorovich} awarded a Nobel prize for related work in economics. The optimal transference problem is related to basic questions in economics becomes clear if one thinks of $\mu$ as a density of production units, \& of $\nu$ as aa density of consumers. For a given transference plan $\pi$, the nonnegative (possibly infinite) quantity $I[\pi]$ is called the {\it total transportation cost} associated to $\pi$. The {\it optimal transportation cost} between $\mu,\nu$ is the value $\mathcal{T}_{\rm c}(\mu,\nu)\coloneqq\inf_{\pi\in\Pi(\mu,\nu)} I[\pi]$. The optimal $\pi$'s, i.e., those s.t. $I[\pi] = \mathcal{T}_{\rm c}(\mu,\nu)$, if they exist, will be called {\it optimal transference plans}.
		
		Translate Kantorovich problem into its probabilistic equivalent:
		
		\begin{problem}[Probabilistic interpretation]
			Given 2 probability measures $\mu,\nu$, minimize expectation $I(U,V) = \mathbb{E}[c(U,V)]$ over all pairs $(U,V)$ of random variables $U$ in $X$, $V$ in $Y$, s.t. ${\rm law}(U) = \mu$, ${\rm law}(V) = \nu$.
		\end{problem}
		{\sf Basic probabilistic theory.} a random variable $U$ in $X$ is a measurable map with values in $X$, defined on a probability space $\Omega$ equipped with a probability measure $\mathbb{P}$, that the {\rm law} of $U$ is the probability measure $\mu$ on $X$ defined by $\mu[A]\coloneqq\mathbb{P}[U^{-1}(A)]$, \& the expectation stands for the integral w.r.t. $\mathbb{P}$. Transference plans $\pi\in\Pi(\mu,\nu)$ are all possible laws of the couple $(U,V)$. Such a $\pi$ is often said to be the {\it joint law} of random variables $U,V$; also says that it constitutes a {\it coupling} of $U,V$.
		
		Kantorovich's problem is a relaxed version of the original mass transportation problem considered by {\sc Monge}. {\sc Monge}'s problem is just the same as {\sc Kantorovich}'s, except for 1 thing: additionally required: {\it no mass be split}. I.e., to each location $x$ is associated a unique destination $y$. In terms of random variables, this requirement means that we ask for $V$ to be a function of $U$ in $I(U,V) = \mathbb{E}[c(U,V)]$. In terms of transference plans, it means that we ask for $\pi$ in $I[\pi]$ to have special form $d\pi(x,y) = d\pi_T(x,y)\equiv d\mu(x)\delta[y = T(x)]$, where $T$ is a measurable map $X\to Y$. 
		
		\item {\sf Chap. 1: Kantorovich Duality.}
		\item {\sf Chap. 2: Geometry of Optimal Transportation.}
		\item {\sf Chap. 3: Brenier's Polar Factorization Theorem.}
		\item {\sf Chap. 4: Monge-Amp\`ere Equation.}
		\item {\sf Chap. 5: Displacement Interpolation \& Displacement Convexity.}
		\item {\sf Chap. 6: Geometric \& Gaussian Inequalities.}
		\item {\sf Chap. 7: Metric Side of Optimal Transportation.}
		\item {\sf Chap. 8: A Differential Point of View on Optimal Transportation.}
		\item {\sf Chap. 9: Entropy Production \& Transportation Inequalities.}
		\item {\sf Chap. 10: Problems.}
	\end{itemize}
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Dynamic Programming -- Quy Hoạch Động}



%------------------------------------------------------------------------------%

\section{Linear Programming -- Quy Hoạch Tuyến Tính}
\begin{definition}[Linear programming]
	``\emph{Linear programming (LP)}, also called \emph{linear optimization}, is a method to achieve the best outcome, e.g., maximum profit or lower cost, in a \href{https://en.wikipedia.org/wiki/Mathematical_model}{mathematical model} whose requirements \& objective are represented by \href{https://en.wikipedia.org/wiki/Linear_function#As_a_polynomial_function}{linear relationships}. Linear programming is a special case of mathematical programming $\equiv$ \href{https://en.wikipedia.org/wiki/Mathematical_optimization}{mathematical optimization}.'' -- \href{https://en.wikipedia.org/wiki/Linear_programming}{Wikipedia{\tt/}linear programming}
\end{definition}
More formally, linear programming is a technique for the \href{https://en.wikipedia.org/wiki/Mathematical_optimization}{optimization} of a linear \href{https://en.wikipedia.org/wiki/Objective_function}{linear objective function}, subject to \href{https://en.wikipedia.org/wiki/Linear_equality}{linear equality} \& \href{https://en.wikipedia.org/wiki/Linear_inequality}{linear inequality} \href{https://en.wikipedia.org/wiki/Constraint_(mathematics)}{constraints}. Its \href{https://en.wikipedia.org/wiki/Feasible_region}{feasible region} is a \href{https://en.wikipedia.org/wiki/Convex_polytope}{convex polytope}, which is a set defined as the \href{https://en.wikipedia.org/wiki/Intersection_(mathematics)}{intersection} of finitely many \href{https://en.wikipedia.org/wiki/Half-space_(geometry)}{half spaces}, each of which is defined by a linear inequality. Its objective function is a real-valued \href{https://en.wikipedia.org/wiki/Affine_function}{affine (linear) function} defined on this polytope. A linear programming \href{https://en.wikipedia.org/wiki/Algorithm}{algorithm} finds a point in the \href{https://en.wikipedia.org/wiki/Polytope}{polytope} where this function has the largest (or smallest) value if such a point exists.

Linear programs are problems that can be expressed in \href{https://en.wikipedia.org/wiki/Canonical_form}{standard form} as
\begin{equation}
	\label{linear program}
	\tag{lp}
	\mbox{Find a vector }{\bf x}\mbox{ that maximizes{\tt/}minimizes }{\bf c}^\top{\bf x}\mbox{ subject to } A{\bf x}\le{\bf b}\mbox{ \& }{\bf x}\ge{\bf 0}.
\end{equation}
Here the components of ${\bf x}$ are the variables to be determined, ${\bf b},{\bf c}$ are given vectors, \& $A$ is a given matrix. The function whose value is to be maximized (${\bf x}\mapsto{\bf c}^\top{\bf x}$ in this case) is called the \href{https://en.wikipedia.org/wiki/Objective_function}{objective function}. The constraint $A{\bf x}\le{\bf x}$ \& ${\bf x}\ge{\bf 0}$ specify a \href{https://en.wikipedia.org/wiki/Convex_polytope}{convex polytope} over which the objective function is to be optimized.

Linear programming can be applied to various fields of study, which is widely used in mathematics \&, to a lesser extent, in business, economics, \& to some engineering problems. There is a close connection between linear programs, eigenequations, \href{https://en.wikipedia.org/wiki/John_von_Neumann}{John von Neumann}'s general equilibrium model, \& structural equilibrium models (see \href{https://en.wikipedia.org/wiki/Dual_linear_program}{dual linear program}). Industries using linear programming models include transportation, energy, telecommunications, \& manufacturing. It has proven useful in modeling diverse types of problems in \href{https://en.wikipedia.org/wiki/Automated_planning_and_scheduling}{planning}, \href{https://en.wikipedia.org/wiki/Routing}{routing}, \href{https://en.wikipedia.org/wiki/Scheduling_(production_processes)}{scheduling}, \href{https://en.wikipedia.org/wiki/Assignment_problem}{assignment}, \& design.

\begin{dinhnghia}[Quy hoạch tuyến tính]
	Bài toán \emph{quy hoạch tuyến tính} là bài toán tìm {\rm GTLN{\tt/}GTNN} của \emph{hàm mục tiêu} trong điều kiện hàm mục tiêu là hàm bậc nhất đối với các biến \& mỗi 1 điều kiện ràng buộc là bất phương trình bậc nhất đối với các biến (không kể điều kiện ràng buộc biến thuộc tập số nào, e.g., $\mathbb{N},\mathbb{Q},\mathbb{R},\mathbb{C}$.
\end{dinhnghia}
Ta có thể viết bài toán quy hoạch tuyến tính 2 biến $x,y$ về dạng sau:
\begin{align}
	\max T&\coloneqq\alpha x + \beta y\mbox{ s.t } a_ix + b_iy\le c_i,\ \forall i = 1,2,\ldots,n,\label{linear programming 2 vars max}\tag{lp2max}\\
	\min T&\coloneqq\alpha x + \beta y\mbox{ s.t } a_ix + b_iy\le c_i,\ \forall i = 1,2,\ldots,n,\label{linear programming 2 vars min}\tag{lp2min}
\end{align}
trong đó các điều kiện ràng buộc đều là các bất phương trình bậc nhất đối với $x,y$. See also:
\begin{itemize}
	\item {\it Problem: Inequation \& Linear System of Inequations -- Bài Tập: Bất Phương Trình \& Hệ Bất Phương Trình}.
	
	Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 10{\tt/}linear system inequations{\tt/}problem}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/problem/NQBH_linear_system_inequations_problem.tex}.}].
	\begin{itemize}
		\item {\it Problem \& Solution: Inequation \& Linear System of Inequations -- Bài Tập \& Lời Giải: Bất Phương Trình \& Hệ Bất Phương Trình}.
		
		Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 10{\tt/}linear system inequations{\tt/}solution}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_10/linear_system_inequations/solution/NQBH_linear_system_inequations_solution.tex}.}].
	\end{itemize}
	\item {\it Problem: Mathematical Optimization -- Bài Tập: Ứng Dụng Toán Học Để Giải Quyết 1 Số Bài Toán Tối Ưu}.
	
	Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 12{\tt/}optimization{\tt/}problem}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/problem/NQBH_optimization_problem.tex}.}].
	\begin{itemize}
		\item {\it Problem \& Solution: Mathematical Optimization -- Bài Tập \& Lời Giải: Ứng Dụng Toán Học Để Giải Quyết 1 Số Bài Toán Tối Ưu}.
		
		Folder: {\sf Elementary STEM \& Beyond{\tt/}Elementary Mathematics{\tt/}grade 12{\tt/}optimization{\tt/}solution}: [\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.pdf}{pdf}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.pdf}.}][\href{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.tex}{\TeX}\footnote{{\sc url}: \url{https://github.com/NQBH/elementary_STEM_beyond/blob/main/elementary_mathematics/grade_12/optimization/solution/NQBH_optimization_solution.tex}.}].
	\end{itemize}
\end{itemize}

\subsection{How to solve some linear programmings -- Cách giải 1 số bài toán quy hoạch tuyến tính}
Có thể giải 1 số bài toán quy hoạch tuyến tính dạng \eqref{linear programming 2 vars max} hay \eqref{linear programming 2 vars min} theo 2 bước:
\begin{enumerate}
	\item Xác định miền nghiệm $S\subset\mathbb{R}^2$ của hệ bất phương trình $a_ix + b_iy\le c_i$, $\forall i = 1,\ldots,n$.
	\item Tìm điểm $(x,y)\in S$ sao cho biểu thức $T = T(x,y) = \alpha x + \beta y$ có {\rm GTLN} ở bài toán \eqref{linear programming 2 vars max} hoặc có {\rm GTNN} ở bài toán \eqref{linear programming 2 vars min}.
	
	Khi miền nghiệm $S$ là đa giác (polygon), biểu thức $T(x,y) = \alpha x + \beta y$ đạt GTLN{\tt/}GTNN (gộp chung gọi là {\it cực trị}) tại $(x,y)\in\mathbb{R}^2$ là tọa độ 1 trong các đỉnh của đa giác đó. Khi đó, bước 2 có thể được thực hiện như sau:
	\begin{enumerate}
		\item Xác định tọa độ các đỉnh của đa giác đó.
		\item Tính giá trị của biểu thức $T(x,y) = \alpha x + \beta y$ tại các đỉnh của đa giác đó.
		\item So sánh các giá trị \& kết luận.
	\end{enumerate}
\end{enumerate}
\cite[Chuyên đề II, \S1, LT1--3, 1., 2., 3., 4., 5., pp. 20--25]{CDHT_Toan_12_Canh_Dieu}.

%------------------------------------------------------------------------------%

\section{Control Theory -- Lý Thuyết Điều Khiển}
``{\it Control theory} is a field of \href{https://en.wikipedia.org/wiki/Control_engineering}{control engineering} \& \href{https://en.wikipedia.org/wiki/Applied_mathematics}{applied mathematics} that deals with the \href{https://en.wikipedia.org/wiki/Control_system}{control} of \href{https://en.wikipedia.org/wiki/Dynamical_system}{dynamical systems} in engineered processes \& machines. The objective is to develop a model or algorithm governing the application of system inputs to drive the system to a desired state, while minimizing any {\it delay, overshoot}, or {\it steady-state error} \& ensuring a level of control \href{https://en.wikipedia.org/wiki/Stability_theory}{stability}; often with the aim to achieve a degree of \href{https://en.wikipedia.org/wiki/Optimal_control}{optimality}.

To do this, a {\it controller} with the requisite corrective behavior is required. This controller monitors the controlled \href{https://en.wikipedia.org/wiki/Process_variable}{process variable} (PV), \& compares it with the reference or \href{https://en.wikipedia.org/wiki/Setpoint_(control_system)}{set point} (SP). The difference between actual \& desired value of the process variable, called the {\it error} signal, or SP-PV error, is applied as feedback to generate a control action to bring the controlled process variable to the same value as the set point. Other aspects which are also studied are \href{https://en.wikipedia.org/wiki/Controllability}{controllability} \& \href{https://en.wikipedia.org/wiki/Observability}{observability}. Control theory is used in \href{https://en.wikipedia.org/wiki/Control_system_engineering}{control system engineering} to design automation that have revolutionized manufacturing, aircraft, communications, \& other industries, \& created new fields such as \href{https://en.wikipedia.org/wiki/Robotics}{robotics}.

Extensive use is usually made of a diagrammatic style known as the \href{https://en.wikipedia.org/wiki/Block_diagram}{block diagram}. In it the \href{https://en.wikipedia.org/wiki/Transfer_function}{transfer function}, also known as the system function or network function, is a mathematical model of the relation between the input \& output based on the \href{https://en.wikipedia.org/wiki/Differential_equation}{differential equations} describing the system.

Control theory dates from the 19th century, when the theoretical basis for the operation of governors was 1st described by \href{https://en.wikipedia.org/wiki/James_Clerk_Maxwell}{James Clerk Maxwell}. Control theory was further advanced by \href{https://en.wikipedia.org/wiki/Edward_Routh}{Edward Routh} in 1874, \href{https://en.wikipedia.org/wiki/Jacques_Charles_Fran%C3%A7ois_Sturm}{Charles Sturm} \& in 1895, \href{https://en.wikipedia.org/wiki/Adolf_Hurwitz}{Adolf Hurwitz}, who all contributed to the establishment of control stability criteria; \& from 1922 onwards, the development of \href{https://en.wikipedia.org/wiki/PID_control}{PID control} theory by Nicolas Minorsky. Although a major application of mathematical control theory is in \href{https://en.wikipedia.org/wiki/Control_Systems_Engineering}{control systems engineering}, which deals with the design of \href{https://en.wikipedia.org/wiki/Process_control}{process control} systems for industry, other applications range far beyond this. As the general theory of feedback systems, control theory is useful wherever feedback occurs -- thus control theory also has applications in life sciences, computer engineering, sociology, \& \href{https://en.wikipedia.org/wiki/Operations_research}{operations research}.'' -- \href{https://en.wikipedia.org/wiki/Control_theory}{Wikipedia{\tt/}control theory}

\paragraph{Open-loop \& closed-loop (feedback) control.} ``Fundamentally, there are 2 types of control loop: \href{https://en.wikipedia.org/wiki/Open-loop_control}{open-loop control} (feedforward), \& \href{https://en.wikipedia.org/wiki/Closed-loop_control}{closed-loop control} (feedback).

In open-loop control, the control action from the controller is independent of the ``process output'' (or ``controlled process variable''). A good example of this is a central heating boiler controlled only by a timer, so that heat is applied for a constant time, regardless of the temperature of the building. The control action is the switching on{\tt/}off of the boiler, but the controlled variable should be the building temperature, but is not because this is open-loop control of the boiler, which does not give closed-loop control of the temperature.

In closed loop control, the control action from the controller is dependent on the process output. In the case of the boiler analogy this would include a thermostat to monitor the building temperature, \& thereby feed back a signal to ensure the controller maintains the building at the temperature set on the thermostat. A closed loop controller therefore has a feedback loop which ensures the controller exerts a control action to give a process output the same as the ``reference input'' or ``set point''. For this reason, closed loop controllers are also called {\it feedback controllers}.

The definition of a closed loop control system according to the \href{https://en.wikipedia.org/wiki/British_Standards_Institution}{British Standards Institutin} is ``a control system possessing monitoring feedback, the deviation signal formed as a result of this feedback being used to control the action of a final control element in such a way as to tend to reduce the deviation to 0.''

Likewise; ``A {\it Feedback Control System} is a system which tends to maintain a prescribed relationship of 1 system variable to another by comparing functions of these variables \& using the difference as a means of control.''

\paragraph{Classical control theory.}

\subsection{Application of $C_0$-Semigroup in Control Theory -- Ứng Dụng của Nửa Nhóm $C_0$ Trong Lý Thuyết Điều Khiển}

%------------------------------------------------------------------------------%

\section{$C_0$ Semigroup -- Nửa Nhóm $C_0$}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Anh_Ke_semigroup}. {\sc Cung Thế Anh, Trần Đình Kế}. {\it Nửa Nhóm Các Toán Tử Tuyến Tính \& Ứng Dụng}.
\end{enumerate}
``Ứng dụng của lý thuyết nửa nhóm các toán tử tuyến tính trong lý thuyết điều khiển toán học, bao gồm bài toán điều khiển được, bài toán quan sát, bài toán điều khiển tối ưu, \& bài toán ổn định hóa. Nhấn mạnh đến các hệ điều khiển vô hạn chiều, i.e., các hệ điều khiển sinh bởi các PDEs.'' -- \cite[Chap. IV: {\it Ứng Dụng Trong Lý Thuyết Điều Khiển}]{Anh_Ke_semigroup}

%------------------------------------------------------------------------------%

\section{Feedback Control -- Kiểm Soát Phản Hồi{\tt/}Điều Khiển Hồi Tiếp}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Raymond2007}. {\sc J.-P. Raymond}. {\it Feedback Boundary Stabilization of 3D Incompressible NSEs}.
	
	Study local stabilization of 3D NSEs around an unstable stationary solution ${\bf w}$, by means of a feedback boundary control. 1st determine a feedback law for the linearized system around ${\bf w}$. Show: this feedback provides a local stabilization of NSEs. To deal with the nonlinear term, the solutions to the closed loop system must be in $H^{\frac{3}{2} + \varepsilon,\frac{3}{4} + \frac{\varepsilon}{2}}(Q)$ with $\varepsilon > 0$. Such a regularity is achieved with a feedback obtained by minimizing a functional involving a norm of the state variable strong enough. The feedback controller cannot be determined by a well posed Riccati equation. Here choose a controller at $t = 0$, is achieved by choosing a time varying control operator in a neighborhood of $t = 0$.
	
	{\it Keywords}: Dirichlet control; Navier–Stokes equations; Feedback control; Stabilization; Riccati equation.
	
	An important issue in control theory is the controllability of systems. The local stability of 3D NSEs in a neighborhood of an unstable stationary solution may be deduced from this controllability result. Another important issue is the characterization of stabilizing feedback control laws, pointwise in time.
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Optimal Control -- Điều Khiển Tối Ưu}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Lasiecka_Triggiani2000}. {\sc Irena Lasiecka, Roberto Triggiani}. {\it Control Theory for PDEs: Continuous \& Approximation Theories II: Abstract Hyperbolic-Like Systems Over a Finite Time Horizon}.
	\item \cite{Lions1969}. {\sc Jacques-Louis Lions}. {\it Quelques m\'{e}thodes de r\'{e}solution des probl\`emes aux limites nonlin\'{e}aires -- Some Methods for Solving Nonlinear Boundary Problems}.
	\item \cite{Lions1971}. {\sc Jacques-Louis Lions}. {\it Optimal control of systems governed by PDEs}.
	\item \cite{Troltzsch2010}. {\sc Fredi Tr\"{o}ltzsch}. {\it Optimal Control of PDEs}.
\end{enumerate}

\begin{definition}[Optimal control theory]
	``\emph{Optimal control theory} is a branch of \href{https://en.wikipedia.org/wiki/Control_theory}{control theory} that deals with finding a \href{https://en.wikipedia.org/wiki/Control_(optimal_control_theory)}{control} for a \href{https://en.wikipedia.org/wiki/Dynamical_system}{dynamical system} over a period of time s.t. an \href{https://en.wikipedia.org/wiki/Objective_function}{objective function} is optimized. It has numerous applications in science, engineering, \& operational research. E.g., the dynamical system might be a \href{https://en.wikipedia.org/wiki/Spacecraft}{spacecraft} with controls corresponding to rocket thrusters, \& the objective might be to reach the Moon with minimum fuel expenditure. Or the dynamical system could be a nation's \href{https://en.wikipedia.org/wiki/Economy}{economy}, with the objective to minimize \href{https://en.wikipedia.org/wiki/Unemployment}{unemployment}; the controls in this case could be \href{https://en.wikipedia.org/wiki/Fiscal_policy}{fiscal} \& \href{https://en.wikipedia.org/wiki/Monetary_policy}{monetary policy}. A dynamical system may also be introduced to embed \href{https://en.wikipedia.org/wiki/Operations_research}{operations research problems} within the framework of optimal control theory.'' -- \href{https://en.wikipedia.org/wiki/Optimal_control}{Wikipedia{\tt/}optimal control}
\end{definition}
``Optimal control is an extension of the \href{https://en.wikipedia.org/wiki/Calculus_of_variations}{calculus of variations}, \& is a mathematical optimization method for deriving \href{https://en.wikipedia.org/wiki/Control_theory}{control policies}. The method is largely due to the work of \href{https://en.wikipedia.org/wiki/Lev_Pontryagin}{Lev Pontryagin} \& \href{https://en.wikipedia.org/wiki/Richard_Bellman}{Richard Bellman} in the 1950s, after contributions to calculus of variations by \href{https://en.wikipedia.org/wiki/Edward_J._McShane}{Edward J. McShane}. Optimal control can be seen as a \href{https://en.wikipedia.org/wiki/Control_strategy}{control strategy} in \href{https://en.wikipedia.org/wiki/Control_theory}{control theory}.'' -- \href{https://en.wikipedia.org/wiki/Optimal_control}{Wikipedia{\tt/}optimal control}

\paragraph{General method.} Optimal control deals with the problem of finding a control law for a given system s.t. a certain \href{https://en.wikipedia.org/wiki/Optimality_criterion}{optimality criterion} is achieved. A control problem includes a \href{https://en.wikipedia.org/wiki/Cost_functional}{cost functional} that is a function of state \& control variables. An {\it optimal control} is a set of \href{https://en.wikipedia.org/wiki/Differential_equation}{differential equations} describing the paths of the control variables that minimize the cost function. The optimal control can be derived using \href{https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle}{Pontryagin's maximum principle} (a \href{https://en.wikipedia.org/wiki/Necessary_condition}{necessary condition} also known as Pontryagin's minimum principle or simply Pontryagin's principle), or by solving the \href{https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation}{Hamilton--Jacobi--Bellman equation0} (a \href{https://en.wikipedia.org/wiki/Sufficient_condition}{sufficient condition}).

\begin{example}
	Consider a car traveling in a straight line on a hilly road. Question: How should the driver press the accelerator pedal in order to minimize the total traveling time? The term \emph{control law} refers specifically to the way in which the driver presses the accelerator \& shifts the gears. The \emph{system} consists of both the car \& the road, \& the \emph{optimality criterion} is the minimization of the total traveling time. Control problems usually include ancillary \href{https://en.wikipedia.org/wiki/Constraint_(mathematics)}{constraints}. E.g., the amount of available fuel might be limited, the accelerator pedal cannot be pushed through the floor of the car, speed limits, etc.
\end{example}
A proper cost function will be a mathematical expression giving the traveling time as a function of the speed, geometrical considerations, \& \href{https://en.wikipedia.org/wiki/Initial_condition}{initial conditions} of the system. \href{https://en.wikipedia.org/wiki/Constraint_(mathematics)}{Constraints} are often interchangeable with the cost function.

Another related optimal control problem may be to find the way to drive the car so as to minimize its fuel consumption, given that it must complete a given course in a time not exceeding some amount. Yet another related control problem may be to minimize the total monetary cost of completing the trip, given assumed monetary prices for time \& fuel.

\paragraph{An abstract framework.} Minimize the continuous-time cost functional
\begin{equation}
	J(t_0,t_{\rm f},{\bf x}(\cdot),{\bf u}(\cdot))\coloneqq E({\bf x}(t_0),t_0,{\bf x}(t_{\rm f}),t_{\rm f}) + \int_{t_0}^{t_{\rm f}} F(t,{\bf x}(t),{\bf u}(t))\,{\rm d}t,
\end{equation}
subject to the 1st-order dynamic constraints (the {\it state equation})
\begin{equation}
	\dot{\bf x}(t) = {\bf f}(t,{\bf x}(t),{\bf u}(t)),
\end{equation}
the algebraic {\it path constraints}
\begin{equation}
	{\bf h}(t,{\bf x}(t),{\bf u}(t))\le{\bf 0},
\end{equation}
\& the \href{https://en.wikipedia.org/wiki/Boundary_condition}{endpoint conditions}
\begin{equation}
	{\bf e}(t_0,{\bf x}(t_0),t_{\rm f},{\bf x}(t_{\rm f})) = {\bf 0},
\end{equation}
where ${\bf x}(t)$: the {\it state}, ${\bf u}(t)$ is the {\it control}, $t$: the independent variable (generally speaking, time), $t_0$: the initial time, \& $t_{\rm f}$: the terminal time. The terms $E,F$ are called the {\it endpoint cost} \& {\it running cost}, respectively. In the calculus of variations, $E,F$ are referred to as the Mayer term \& the \href{https://en.wikipedia.org/wiki/Lagrange_multiplier}{Lagrangian}, respectively. Furthermore, it is noted that the path constraints are in general {\it inequality} constraints \& thus may not be active (i.e., $= 0$) at the optimal solution. It is also noted that the optimal control problem as stated above may have multiple solutions (i.e., the solution may not be unique). Thus, it is most often the case that any solution $(t_0^\star,t_{\rm f}^\star,{\bf x}^\star(t),{\bf u}^\star(t))$ to the optimal control problem is {\it locally minimizing{\tt/}minimizer}.

\subsection{Linear Quadratic Control}
A special case of the general nonlinear optimal control problem is the \href{https://en.wikipedia.org/wiki/Linear-quadratic_regulator}{linear quadratic (LQ) optimal control problem}. The LQ problem is stated as follows. Minimize the {\it quadratic} continuous-time cost functional
\begin{equation}
	J = \frac{1}{2}{\bf x}^\top(t_{\rm f})S_{\rm f}{\bf x}(t_{\rm f}) + \frac{1}{2}\int_{t_0}^{t_{\rm f}} {\bf x}^\top(t)Q(t){\bf x}(t) + {\bf u}^\top(t)R(t){\bf u}(t)\,{\rm d}t,
\end{equation}
subject to the linear 1st-order dynamic constraints
\begin{equation}
	\left\{\begin{split}
		\dot{\bf x}(t) &= A(t){\bf x}(t) + B(t){\bf u}(t),\\
		{\bf x}(t_0) &= {\bf x}_0.
	\end{split}\right.	
\end{equation}
A particular form of the LQ problem arising in many control system problems is that of the {\it linear quadratic regulator} (LQR) where all of the matrices (i.e., $A,B,Q,R$) are constant, the initial time is arbitrarily set to 0, \& the terminal time is taken in the limit $t_{\rm f}\to0$ (this last assumption is what is known as {\it infinite horizon}). The LQR problem is stated as follows. Minimize the infinite horizon quadratic continuous-time cost functional
\begin{equation}
	J = \frac{1}{2}\int_0^\infty {\bf x}^\top(t)Q{\bf x}(t) + {\bf u}^\top(t)R{\bf u}(t)\,{\rm d}t,
\end{equation}
subject to the {\it linear time-invariant} 1st-order dynamic constraints
\begin{equation}
	\left\{\begin{split}
		\dot{\bf x}(t) &= A{\bf x}(t) + B{\bf u}(t),\\
		{\bf x}(t_0) &= {\bf x}_0.
	\end{split}\right.
\end{equation}
In the finite-horizon case the matrices are restricted in that $Q,R$ are positive semi-definite \& positive definite, respectively. In the infinite-horizon case, however, the matrices $Q,R$ are not only positive-semidefinite \& positive-definite, respectively, but are also constant. These additional restrictions on $Q,R$ in the infinite-horizon case are enforced to ensure that the cost functional remains positive. Furthermore, in order to ensure that the cost function is {\it bounded}, the additional restriction is imposed that the pair $(A,B)$ is \href{https://en.wikipedia.org/wiki/Controllability}{\it controllable}. Note that the LQ or LQR cost functional can be thought of physically as attempting to minimize the {\it control energy} (measured as a quadratic form).

The infinite horizon problem, i.e., LQR, may seem overly restrictive \& essentially useless because it assumes that the operator is driving the system to 0-state \& hence driving the output of the system to 0. This is indeed correct. However the problem of driving the output to a desired nonzero level can be solved {\it after} the zero output one is. In fact, can prove that this secondary LQR problem can be solved in a very straightforward manner. It has been shown in classical optimal control theory that the LQ (or LQR) optimal control has the feedback form
\begin{equation}
	{\bf u}(t) = -K(t){\bf x}(t),
\end{equation}
where $K(t)$ is a properly dimensioned matrix, given as
\begin{equation}
	K(t) = R^{-1}B^\top S(t),
\end{equation}
\& $S(t)$ is the solution of the differential \href{https://en.wikipedia.org/wiki/Riccati_equation}{Riccati equation}. The differential Riccati equation is given as
\begin{equation}
	\dot S(t) = -S(t)A - A^\top S(t) + S(t)BR^{-1}B^\top S(t) - Q.
\end{equation}
For the finite horizon LQ problem, the Riccati equation is integrated backward in time using the terminal boundary condition
\begin{equation}
	S(t_{\rm f}) = S_{\rm f}.
\end{equation}
For the infinite horizon LQR problem, the differential Riccati equation is replaced with the {\it algebraic} Riccati equation (ARE) given as
\begin{equation}
	-SA - A^\top S + SBR^{-1}B^\top S - Q = {\bf 0}.
\end{equation}
Understanding that the ARE arises from infinite horizon problem, the matrices $A,B,Q,R$ are all constant. It is noted that there are in general multiple solutions to the algebraic Riccati equation \& the {\it positive definite} (or positive semi-definite) solution is the one that is used to compute the feedback again. The LQ (LQR) problem was elegantly solved by \href{https://en.wikipedia.org/wiki/Rudolf_E._K%C3%A1lm%C3%A1n}{Rudolf E. K\'alm\'an}.

\subsection{Numerical Methods for Optimal Control}
``Optimal control problems are generally nonlinear \& therefore, generally do not have analytic solutions, e.g., like the linear-quadratic optimal control problem. As a result, it is necessary to employ numerical methods to solve optimal control problems. In the early years of optimal control (c. 1950s--1980s) the favored approach for solving optimal control problems was that of {\it indirect methods}. In an indirect method, the calculus of variations is employed to obtain the 1st-order optimality conditions. These conditions result in a 2-point (or, in the case of a complex problem, a multi-point) \href{https://en.wikipedia.org/wiki/Boundary-value_problem}{BVP}. This BVP actually has a special structure because it arises from taking the derivative of a \href{https://en.wikipedia.org/wiki/Hamiltonian_(control_theory)}{Hamiltonian}. Thus, the resulting \href{https://en.wikipedia.org/wiki/Dynamical_system}{dynamical system} is a \href{https://en.wikipedia.org/wiki/Hamiltonian_system}{Hamiltonian system} of the form
\begin{equation}
	\left\{\begin{split}
		\dot{\bf x} &= \partial_{\boldsymbol{\lambda}}H,\\
		\dot{\boldsymbol{\lambda}} &= -\partial_{\bf x}H,
	\end{split}\right.
\end{equation}
where $H = F + \boldsymbol{\lambda}^\top{\bf f} - \boldsymbol{\mu}^\top{\bf h}$ is the {\it augmented Hamiltonian} \& in an indirect method, the BVP is solved (using the appropriate boundary or {\it transversality} conditions). The beauty of using an indirect method is that the state \& adjoint, i.e., $\boldsymbol{\lambda}$, are solved for \& the resulting solution is readily verified to be an extremal trajectory. The disadvantage of indirect methods is that the BVP is often extremely difficult to solve (particularly for problems that span large time intervals or problems with interior point constraints). A well-known software program that implements direct methods is BNDSCO.

The approach that has risen to prominence in numerical optimal control since the 1980s is that of so-called {\it direct methods}. In a direct method, the state or the control, or both, are approximated using an appropriate function approximation (e.g., polynomial approximation or piecewise constant parameterization). Simultaneously, the cost functional is approximated as a {\it cost function}. Then, the coefficients of the function approximations are treated as optimization variables \& the problem is ``transcribed'' to a nonlinear optimization problem of the form
\begin{equation}
	\min F({\bf z})
\end{equation}
subject to the algebraic constraints
\begin{equation}
	{\bf g}({\bf z}) = {\bf 0},\ {\bf h}({\bf z})\le{\bf 0}.
\end{equation}
Depending upon the type of direct method employed, the size of the nonlinear optimization problem can be quite small (e.g., as in a direct shooting or \href{https://en.wikipedia.org/wiki/Quasilinearization}{quasilinearization} method), moderate (e.g., \href{https://en.wikipedia.org/wiki/Pseudospectral_optimal_control}{pseudospectral optimal control}) or may be quite large (e.g., a direct \href{https://en.wikipedia.org/wiki/Collocation_method}{collocation method}). In the latter case (i.e., a collocation method), the nonlinear optimization problem may be literally thousands $a\cdot10^3$ to tens of thousands $a\cdot10^4$ of variables \& constraints. Given the size of many NLPs arising from a direct method, it may appear somewhat counter-intuitive that solving the nonlinear optimization problem is easier than solving the BVP. It is, however, the fact that the NLP is easier to solve than the BVP. The reason for the relative ease of computation, particularly of a direct collocation method, is that the NLP is {\it sparse} \& many well-known software programs exist (e.g., \href{https://en.wikipedia.org/wiki/SNOPT}{SNOPT}) to solve large sparse NLPs. As a result, the range of problems that can be solved via direct methods (particularly direct {\it collocation methods} which are very popular these days) is significantly larger than the range of problems that can be solved via indirect methods. In fact, direct methods have become so popular these days that many people have written elaborate software programs employing these methods. In particular, many such programs include DIRCOL, SOCS, OTIS, GESOP{\tt/}\href{https://en.wikipedia.org/wiki/ASTOS}{ASTOS}, DITAN., \& PyGMO{\tt/}PyKEP. In recent years, due to the advent of the \href{https://en.wikipedia.org/wiki/MATLAB}{MATLAB} programming language, optimal control software in MATLAB has become more common. Examples of academically developed MATLAB software tools implementing direct methods include RIOTs, \href{https://en.wikipedia.org/wiki/DIDO_(optimal_control)}{DIDO}, DIRECT, FALCON.m, \& GPOPs, while an example of an industry developed MATLAB tool is \href{https://en.wikipedia.org/wiki/PROPT}{PROPT}. These software tools have increased significantly the opportunity for people to explore complex optimal control problems both for academic research \& industrial problems. Finally, it is noted that general-purpose MATLAB optimization environments such as \href{https://en.wikipedia.org/wiki/TOMLAB}{TOMLAB} have made coding complex optimal control problems significantly easier than was previously possible in languages such as C \& \href{https://en.wikipedia.org/wiki/FORTRAN}{FORTRAN}.

\subsection{Discrete-Time Optimal Control}
``The examples thus far have shown \href{https://en.wikipedia.org/wiki/Continuous_time}{continuous time} systems \& control solutions. In fact, as optimal control solutions are now often implemented \href{https://en.wikipedia.org/wiki/Digital_data}{digitally}, contemporary control theory is now primarily concerned with \href{https://en.wikipedia.org/wiki/Discrete_time}{discrete time} systems \& solutions. The theory of Consistent Approximations provides conditions under which solutions to a series of increasingly accurate discretized optimal control problem converge to the solution of the original, continuous-time problem. Not all discretization methods have this property, even seemingly obvious ones. E.g., using a variable step-size routine to integrate the problem's dynamic equations may generate a gradient which does not converge to 0 (or point in the right direction) as the solution is approached. The direct method \href{http://www.schwartz-home.com/RIOTS}{RIOTS} is based on the Theory of Consistent Approximation.

A common solution strategy in many optimal control problems is to solve for the costate (sometimes called the \href{https://en.wikipedia.org/wiki/Shadow_price}{shadow price}) $\lambda(t)$. The costate summaries in 1 number the marginal value of expanding or contracting the state variable next turn. The marginal value is not only the gains accruing to it next turn but associated with the duration of the program. It is nice when $\lambda(t)$ can be solved analytically, but usually, the most one can do is describe it sufficiently well that the intuition can grasp the character of the solution \& an equation solver can solve numerically for the values.

Having obtained $\lambda(t)$, the turn-$t$ optimal value for the control can usually be solved as a differential equation conditional on knowledge of $\lambda(t)$. Again it is infrequent, especially in continuous-time problems, that one obtains the value of the control or the state explicitly. Usually, the strategy is to solve for thresholds \& regions that characterize the optimal control \& use a numerical solver to isolate the actual choice values in time.

\begin{example}
	``Consider the problem of a mine owner who must decide at what rate to extract ore from their mine. They own rights to the ore from date $0$ to date $T$. At date $0$ there is $x_0$ ore in the ground, \& the time-dependent amount of ore $x(t)$ left in the ground declines at the rate of $u(t)$ that the mine owner extracts it. The mine owner extracts ore at cost $\frac{u(t)^2}{x(t)}$ (the cost of extraction increasing with the square of the extraction speed \& the inverse of the amount of ore left) \& sells ore at a constant price $p$. Any ore left in the ground at time $T$ cannot be sold \& has no value (there is no ``scrap value''). The owner chooses the rate of extraction varying with time $u(t)$ to maximize profits over the period of ownership with no time discounting. See discrete-time version vs. Continuous-time version.'' -- \href{https://en.wikipedia.org/wiki/Optimal_control#Finite_time}{Wikipedia{\tt/}optimal control{\tt/}finite time}
\end{example}

\subsection{Optimal Control for PDEs -- Điều Khiển Tối Ưu Cho Phương Trình Vi Phân Đạo Hàm Riêng}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Lions1971}. {\sc Jacques-Louis Lions}. {\it Optimal control of systems governed by PDEs}.
	\item \cite{Troltzsch2010}. {\sc Fredi Tr\"{o}ltzsch}. {\it Optimal Control of PDEs}.
\end{enumerate}

\subsubsection{Optimal Control for Navier--Stokes Equations -- Điều Khiển Tối Ưu Cho Phương Trình Navier--Stokes}

%------------------------------------------------------------------------------%

\section{Stochastic Control}
\textbf{\textsf{Community -- Cộng đồng.}} {\sc Michael Hinterm\"uller, Caroline Geiersbach}.

{\it Stochastic control} or {\it stochastic \href{https://en.wikipedia.org/wiki/Optimal_control}{optimal control}} is a subfield of \href{https://en.wikipedia.org/wiki/Control_theory}{control theory} that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system. The system designer assumes, in a \href{https://en.wikipedia.org/wiki/Bayesian_probability}{Bayesian probability}-driven fashion, that random noise with known \href{https://en.wikipedia.org/wiki/Probability_distribution}{probability distribution} affects the evolution \& observation of the state variables. Stochastic control aims to design the time path of the controlled variables that performs the desired control task with minimum cost, somehow defined, despite the presence of this noise. The context may be either \href{https://en.wikipedia.org/wiki/Discrete_time}{discrete time} or \href{https://en.wikipedia.org/wiki/Continuous_time}{continuous time}.

\paragraph{Certainty equivalence.} ``An extremely well-studied formulation in stochastic control is that of \href{https://en.wikipedia.org/wiki/Linear_quadratic_Gaussian_control}{linear quadratic Gaussian control}. Here the model is linear, the objective function is the expected value of a quadratic form, \& the disturbances are purely additive. A basic result for discrete-time centralized systems with only additive uncertainty is the {\it certainty equivalence property}: that the optimal control solution in this case is the same as would be obtained in the absence of the additive disturbances. This property is applicable to all centralized systems with linear equations of evolution, quadratic cost function, \& noise entering the model only additively; the quadratic assumption allows for the optimal control laws, which follow the certainty-equivalence property to be linear functions of the observations of the controllers.

Any deviation from the above assumptions -- a nonlinear stat equation, a non-quadratic objective function, \href{https://en.wikipedia.org/wiki/Multiplier_uncertainty}{noise in the multiplicative parameters} of the model, or decentralization of control -- causes the certainty equivalence property not to hold. E.g., its failure to hold for decentralized control was demonstrated in \href{https://en.wikipedia.org/wiki/Witsenhausen%27s_counterexample}{Witsenhausen's counterexample}.'' -- \href{https://en.wikipedia.org/wiki/Stochastic_control#Certainty_equivalence}{Wikipedia{\tt/}stochastic control{\tt}certainty equivalence}.

\paragraph{Discrete time.} ``In a discrete-time context, the decision-maker observes the state variable, possibly with observational noise, in each time period. The objective may be to optimize the sum of expected values of a nonlinear (possibly quadratic) objective function over all the time periods from the present to the final period of concern, or to optimize the value of the objective function as of the final period only. At each time period new observations are made, \& the control variables are to be adjusted optimally. Finding the optimal solution for the present time may involve iterating a \href{https://en.wikipedia.org/wiki/Linear-quadratic-Gaussian_control#Discrete_time}{matrix Riccati equation} backwards in time from the last period to the present period.

In the discrete-time case with uncertainty about the parameter values in the transition matrix (giving the effect of current values of the state variables on their own evolution) \&{\tt/}or the control response matrix of the state equation, but still with a linear state equation \& quadratic objective function, a Riccati equation can still be obtained for iterating backward to each period's solution even though certainty equivalence does not apply. The discrete-time case of a non-quadratic loss function but only additive disturbances can also be handled, albeit with more complications.

\begin{example}
	A typical specification of the discrete-time stochastic linear quadratic control problem is to minimize
	\begin{equation}
		{\rm E}_1\sum_{t=1}^S y_t^\top Qy_t + u_t^\top Ru_t,
	\end{equation}
	where ${\rm E}_1$ is the \href{https://en.wikipedia.org/wiki/Expected_value}{expected value} operator conditional on $y_0$, $S$: the time horizon, subject to the state equation $y_t = A_ty_{t-1} + B_tu_t$.
\end{example}

\paragraph{Continuous time.} ``If the model is in continuous time, the controller knows the state of the system at each instant of time. The objective is to maximize either an integral of, e.g., a concave function of a state variable over a horizon from time 0 (the present) to a terminal time $T$, or a concave function of a state variable at some future date $T$. As time evolves, new observations are continuously made \& the control variables are continuously adjusted in optimal fashion.''

\paragraph{Stochastic model predictive control.} ``In the literature, there are 2 types of MPCs for stochastic systems; Robust model predictive control \& Stochastic Model Predictive Control (SMPC). Robust model predictive control is a more conservative method which considers the worst scenario in the optimization procedure. However, this method, similar to other robust controls, deteriorates the overall controller's performance \& also is applicable only for systems with bounded uncertainties. The alternative method, SMPC, considers soft constraints which limit the risk of violation by a probabilistic inequality.

\begin{example}[Stochastic model predictive control in finance]
	``In a continuous time approach in a \href{https://en.wikipedia.org/wiki/Finance}{finance} context, the state variable in the stochastic differential equation is usually wealth or net worth, \& the controls are the shares placed at each time in the various assets. Given the \href{https://en.wikipedia.org/wiki/Asset_allocation}{asset allocation} chosen at any time, the determinants of the change in wealth are usually the stochastic returns to assets \& the interest rate on the risk-free asset. The field of stochastic control has developed greatly since the 1970s, particularly in its applications to finance. Robert Merton used stochastic control to study \href{https://en.wikipedia.org/wiki/Optimal_portfolio}{optimal portfolios} of safe \& risky assets. \href{https://en.wikipedia.org/wiki/Merton%27s_portfolio_problem}{Merton's portfolio problem} \& that of \href{https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model}{Black Scholes} changed the nature of the finance literature. Influential mathematical textbook treatments were by Fleming \& Rishel, \& by Fleming \& Soner. These techniques were applied by Stein to the \href{https://en.wikipedia.org/wiki/Financial_crisis_of_2007%E2%80%9308}{financial crisis of 2007--08}.
	
	The maximization, say of the expected logarithm of net work at a terminal date $T$, is subject to stochastic processes on the components of wealth. In this case, in continuous time \href{https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma}{It\^o's equation} is the main tool of analysis. In the case where the maximization is an integral of a concave function of utility over an horizon $(0,T)$, dynamic programming is used. There is no certainty equivalence as in the older literature, because the coefficients of the control variables -- i.e., the returns received by the chosen shares of assets -- are stochastic.
\end{example}

%------------------------------------------------------------------------------%

\section{Shape Optimization -- Tối Ưu Hình Dạng}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Allaire_Henrot2001}. {\sc Gr\'egoire Allaire, Antoine Henrot}. {\it On some recent advances in shape optimization}.
	\item \cite{Azegami2020}. {\sc Hideyuki Azegami}. {\it Shape Optimization Problems}.
	\item \cite{Bandle_Wagner2023}. {\sc Catherine Bandle, Alfred Wagner}. {\it Shape Optimization: Variations of Domains \& Applications}.
	\item \cite{Delfour_Zolesio2001,Delfour_Zolesio2011}. {\sc Michael C. Delfour, Jean-Paul Zol\'{e}sio}. {\it Shapes \& Geometries}.
	\item \cite{Haslinger_Makinen2003}. {\sc J. Haslinger, R. A. E. M\"{a}kinen}. {\it Introduction to Shape Optimization}.
	\item \cite{Mohammadi_Pironneau2010}. {\sc Bijan Mohammadi, Olivier Pironneau}. {\it Applied Shape Optimization for Fluids}.
	\item \cite{Moubachir_Zolesio2006}. {\sc Marwan Moubachir, Jean-Paul Zol\'{e}sio}. {\it Moving Shape Analysis \& Control}.
	\item {\sc Stephan Schmidt}. Master course: {\it Shape \& Geometry}. Humboldt University of Berlin. [written in German, taught in English \& German].
	\item \cite{Sokolowski_Zolesio1992}. {\sc Jan Soko\l owski, Jean-Paul Zol\'{e}sio}. {\it Introduction to Shape Optimization}.
	\item \cite{Walker2015}. {\sc Shawn W. Walker}. {\it The Shapes of Things}.
	
	{\bf Differential equations on surfaces.} Differential geometry is useful for understanding mathematical models containing geometric PDEs, e.g., surface{\tt/}manifold version of the standard Laplace equation, which requires the development of the surface gradient \& surface Laplacian operators -- the usual gradient $\nabla$ \& Laplacian $\Delta = \nabla\cdot\nabla$ operators defined on a surface (manifold) instead of standard Euclidean space $\mathbb{R}^n$. {\it Advantage}: provide alternative formulas for geometric quantities, e.g., the summed (mean) curvature, that are much clearer than the usual presentation of texts on differential geometry.
	
	{\bf Differentiating w.r.t. Shape.} The approach to differential geometry is advantageous for developing the framework of {\it shape differential calculus} -- the study of how quantities change w.r.t. changes of independent ``shape variable''.
	
	``The framework of shape differential calculus provides the tools for developing the equations of mean curvature flow \& Willmore flow, which are geometric flows occurring in many applications such as fluid dynamics \& biology.'' -- \cite[p. 2]{Walker2015}
	
	The shape perturbation $\delta J(\Omega;V)$ is similar to the gradient operator, which is a directional derivative, analogous to $V\cdot\nabla f$ where $V$ is a given direction, providing information about the local slope, or the sensitivity of a quantity w.r.t. some parameters.
	
	It takes only 2 or 3 numbers to specify a point $(x,y)$ in 2D \& a point $(x,y,z)$ in 3D, whereas an ``infinite'' number of coordinate pairs is needed to specify a domain $\Omega$. $V$ is a 2D{\tt/}3D vector in the scalar function setting; for a shape functional, $V$ is a full-blown function requiring definition at every point in $\Omega$. This ``infinite dimensionality'' is the reason for using the notation $\delta J(\Omega;V)$ to denote a shape perturbation. $\delta J(\Omega;V)$ indicates how we should change $\Omega$ to decrease $J$, similarly to how $\nabla f(x,y)$ indicates how the coordinate pair $(x,y)$ should change to decrease $f$, which opens up the world of shape optimization.
	
	{\bf3 schools of shape optimization.} Cf. engineering shape optimization vs. applied shape optimization \cite{Mohammadi_Pironneau2010} vs. theoretical shape optimization \cite{Sokolowski_Zolesio1992,Delfour_Zolesio2011}.
	
	Shape perturbations allow us to ``climb down the hill'' in the infinite dimensional setting of shape, which is a powerful tool for producing sophisticated engineering designs in an automatic way.
	
	{\bf Extrinsic vs. intrinsic point of views.} To make the discussion as clear as possible, we adopt the {\it extrinsic} point of view: curves \& surfaces are assumed to lie in a Euclidean space of higher dimension. The ambient space is 3D Euclidean space. Alternatively, there is the {\it intrinsic} point of view, i.e., the surface is not assumed to lie in an ambient space, i.e., one is not allowed to reference anything ``outside'' of the surface when defining it. Moreover, no mathematical structures ``outside'' of the surface can be utilized. Walker \cite{Walker2015} did not adopt the intrinsic view or consider higher dimensional manifolds, general embedding dimensions, etc. for the reasons: \cite{Walker2015} is meant as a {\it practical guide} to differential geometry \& shape differentiation that can be used by researchers in other fields.
	\begin{itemize}
		\item \cite{Walker2015} is meant to be used as background information for deriving physical models where geometry plays a critical role. Because most physical problems of interest take place in 3D Euclidean space, the extrinsic viewpoint is sufficient.
		\item Many of the proofs \& derivations of differential geometry relations simplify dramatically for 2D surfaces in 3D \& require only basic multivariable calculus \& linear algebra.
		\item The concepts of {\it normal vectors} \& {\it curvature} are harder to motivate with the intrinsic viewpoint. {\it What does it meant for a surface to ``curve through space'' if you cannot talk about the ambient space?}
		\item Walker wants to keep in mind applications of this machinery to geometric PDEs, fluid dynamics, numerical analysis, optimization, etc. An interesting application of this methodology is for the development of numerical methods for mean curvature flow \& surface tension driven fluid flow. Ergo ($=$ therefore), the extrinsic viewpoint is often more convenient for computational purposes.
		\item Walker wants his framework to be useful for analyzing \& solving {\it shape optimization} problems, i.e., optimization problems where geometry{\tt/}shape is the control variable.
	\end{itemize}
	{\bf Prerequisites.} ``When reading any mathematical text, the reader must have a certain level of mathematical ``maturity'' in order to efficiently learn what is in the text.
\end{enumerate}

\begin{example}[\cite{Walker2015}, Sect. 1.2.1, pp. 1--2]
	Let $f = f(r,\theta)$ be a smooth function defined on the disk $B_{R,2}(0,0)$ of radius $R$ in terms of polar coordinates. The integral of $f$ over $B_{2,R}(0,0)$ $J\coloneqq\int_{B_{2,R}(0,0)} f\,{\rm d}{\bf x} = \int_0^{2\pi}\int_0^R f(r,\theta)\,{\rm d}r\,{\rm d}\theta$ depends on $R$. Assume $f$ also depends on $R$, i.e., $f = f(r,\theta,R)$ with a physical example: $J$ is the \emph{net flow rate} of liquid through a pipe with cross-section $\Omega$, then $f$ is the flow rate per unit area \& could be the solution of a PDE defined on $\Omega$, e.g., a Navier--Stokes fluid flowing in a circular pipe. Advantageous to know the \emph{sensitivity} of $J$ w.r.t. $R$, e.g., for optimization purposes. Differentiate $J$ w.r.t. $R$:
	\begin{equation*}
		\frac{d}{dR}J = \int_0^{2\pi}\left(\frac{d}{dR}\int_0^R f(r,\theta;R)r\,{\rm d}r\right){\rm d}\theta = \int_0^{2\pi}\int_0^R f'(r,\theta;R)r\,{\rm d}r\,{\rm d}\theta + \int_0^{2\pi} f(R,\theta;R)\,{\rm d}\theta.
	\end{equation*}
	The dependence of $f$ on $R$ can more generally be viewed as dependence on $B_{R,2}(0,0)$, i.e., $f(\cdot;R)\equiv f(\cdot;B_{R,2}(0,0))$. Rewriting $d/dR J$ using Cartesian coordinates ${\bf x}$:
	\begin{equation}
		\frac{d}{dR}J = \int_{B_{R,2}(0,0)} f'({\bf x};\Omega)\,{\rm d}{\bf x} + \int_{S_{R,2}(0,0)} f({\bf x};\Omega)\,{\rm d}S({\bf x}),
	\end{equation}
	where ${\rm d}{\bf x}$ is the volume measure, ${\rm d}S({\bf x})$ is the surface area measure.
\end{example}

\begin{example}[Surface height function of a hill]
	Let $f = f(x,y)$ be a function describing the surface height of the hill, where $(x,y)$ are the coordinates of our position. Then, by using basic multivariate calculus, finding a direction that will move us downhill is equivalent to computing the gradient (vector) of $f$ \& moving in the opposite direction to the gradient. In this sense, we do not need to ``see'' the whole function. We just need to \emph{locally} compute the gradient $\nabla f$, analogous to feeling the ground beneath.
\end{example}

\begin{example}[Engineering shape optimization: minimizing drag with Navier--Stokes flow of fluid past a rigid body, \cite{Walker2015}, pp. 3--5]
	A shape functional representing the drag:
	\begin{equation}
		J_{\rm d}(\Omega)\coloneqq-{\bf u}_{\rm out}\cdot\int_{\Gamma_0} \boldsymbol{\sigma}({\bf u},p){\bf n}\,{\rm d}\Gamma = \frac{2}{{\rm Re}}\int_\Omega |\boldsymbol{\varepsilon}({\bf u})|^2\,{\rm d}{\bf x}\ge0,
	\end{equation}
	which physically represents the net force that must be applied to $\Omega_{\rm B}$ to keep it stationary while being acted upon by the imposed flow field \& represents the total amount of viscous dissipation of energy (per unit of time) in the fluid domain $\Omega$. Using the machinery of shape perturbations, $\delta J_{\rm d}(\Omega;V)$ indicates how $J_{\rm d}$ changes when we perturb $\Omega$ in the direction $V$. Hence, we can use this information to change $\Omega$ in small steps so as to slowly deform $\Omega$ into a shape that has better (lower) drag characteristics. A numerical computation: 2 large vortices appear behind the body, which indicate a large amount of viscous dissipation, i.e., large drag. The optimization process then computes $\delta J_{\rm d}(\Omega^0;V)$ for many different choices of $V$ \& chooses the one that drives down $J_{\rm d}$ the most. This choice of $V$ is used to deform $\Gamma_{\rm B}^0$ into a new shape $\Gamma_{\rm B}^1$ at iteration 1, with only a small difference between $\Gamma_{\rm B}^0$ \& $\Gamma_{\rm B}^1$. This process is repeated many times. Note how the vortices are eliminated by the more slender shape.
\end{example}

\begin{itemize}
	\item Condition (V)***
	\item Family of transformations $\{T_s:0\le s\le\tau\}$.
	\item Perturbed domain $\Omega_s = \Omega_s(V) = T_s(V)(\Omega)$.
	\item Assume that the velocity field $V$ satisfies $V_D$ and, in addition, $V\in C^0([0,\tau];C_{\rm loc}^1(\mathbb{R}^d,\mathbb{R}^d))$ and $\tau > 0$ is small enough such that the Jacobian $J_s$ is strictly positive: $J_s(X)\coloneqq\det DT_s(X) > 0$, $\forall s\in[0,\tau]$, where $DT_s(X)$ is the \textit{Jacobian matrix} of the transformation $T_s = T_s(V)$ associated with the velocity vector field $V$.
\end{itemize}

\subsection{Domain Integrals}

\begin{itemize}
	\item Given $\varphi\in W_{\rm loc}^{1,1}(\mathbb{R}^d)$, consider for $s\in[0,\tau]$ the volume integral
	\begin{equation}
		J(\Omega_s(V))\coloneqq\int_{\Omega_s(V)} \varphi\,{\rm d}{\bf x} = \int_\Omega \varphi\circ T_sJ_s\,{\rm d}{\bf x}.
	\end{equation}
	\begin{equation}
		dJ(\Omega;V) = \frac{d}{ds}J(\Omega_s(V))|_{s = 0} = \int_\Omega \nabla\varphi\cdot V(0) + \varphi\operatorname{div}V(0)\,{\rm d}{\bf x} = \int_\Omega \operatorname{div}(\varphi V(0))\,{\rm d}{\bf x}.
	\end{equation}
	If $\Omega$ has a Lipschitzian boundary, by Stokes's theorem:
	\begin{equation}
		dJ(\Omega;V) = \int_\Gamma \varphi V(0)\cdot{\bf n}\,{\rm d}\Gamma.
	\end{equation}
	
	\begin{theorem}[\cite{Delfour_Zolesio2011}, Thm. 4.1, pp. 482--483]
		Let $\varphi\in W_{\rm loc}^{1,1}(\mathbb{R}^d)$. Assume that the vector field $V = \{V(s):0\le s\le\tau\}$ satisfies condition (V).
		\item[(i)] For each $s\in[0,\tau]$ the map $\varphi\mapsto\varphi\circ T_s:W_{\rm loc}^{1,1}(\mathbb{R}^d)\to W_{\rm loc}^{1,1}(\mathbb{R}^d)$ and its inverse are both locally Lipschitzian and $\nabla(\varphi\circ T_s) = {}^*DT_s\nabla\varphi\circ T_s$.
		\item[(ii)] If $V\in C^0([0,\tau];C_{\rm loc}^1(\mathbb{R}^d,\mathbb{R}^d))$, then the map $s\mapsto J_s:[0,\tau]\to C_{\rm loc}^0(\mathbb{R}^d)$ is differentiable and
		\begin{equation}
			\frac{dJ_s}{ds} = [\nabla\cdot V(s)]\circ T_sJ_s\in C_{\rm loc}^0(\mathbb{R}^d).
		\end{equation}
		Hence the map $s\mapsto J_s$ belongs to $C^1([0,\tau];C_{\rm loc}^0(\mathbb{R}^d))$.
	\end{theorem}
	
	\begin{align}
		\frac{d}{ds}DT_s(X) &= DV(s,T_s(X))DT_s(X),\ DT_0(X) = I,\\
		\frac{d}{ds}\det DT_s(X) &= \operatorname{tr}DV(s,T_s(X))\det DT_s(X) = \nabla\cdot V(s,T_s(X))\det DT_s(X),\ \det DT_0(X) = 1.
	\end{align}
	
	\begin{theorem}[\cite{Delfour_Zolesio2011}, Thm. 4.2, pp. 483--484]
		Assume that there exists $\tau > 0$ such that the velocity field $V(t)$ satisfies conditions (V) and $V\in C^0([0,\tau];C_{\rm loc}^1(\mathbb{R}^d,\mathbb{R}^d))$. Given a function $\varphi\in C(0,\tau;W_{\rm loc}^{1,1}(\mathbb{R}^d))\cap C^1(0,\tau;L_{\rm loc}^1(\mathbb{R}^d))$ and a bounded measurable domain $\Omega$ with boundary $\Gamma$, the semiderivative of the function $J_V(s)\coloneqq\int_{\Omega_s(V)} \varphi(s)\,{\rm d}{\bf x}$ at $s = 0$ is given by
		\begin{equation}
			dJ_V(0) = \int_\Omega \varphi'(0) + \operatorname{div}(\varphi(0)V(0))\,{\rm d}{\bf x},
		\end{equation}
		where $\varphi(0)({\bf x})\coloneqq\varphi(0,{\bf x})$ and $\varphi'(0)({\bf x})\coloneqq\partial_t\varphi(0,{\bf x})$. In addition, $\Omega$ is an open domain with a Lipschitzian boundary $\Gamma$, then
		\begin{equation}
			dJ_V(0) = \int_\Omega \varphi'(0)\,{\rm d}{\bf x} + \int_\Gamma \varphi(0)V(0)\cdot{\bf n}\,{\rm d}{\bf x}.
		\end{equation}
	\end{theorem}
	
\end{itemize}

\subsection{Boundary Integrals}

\subsection{Material derivatives}
Let $\Omega\subset D$ be a bounded domain, 

%------------------------------------------------------------------------------%

\section{Topology Optimization -- Tối Ưu Tôpô}

%------------------------------------------------------------------------------%

\section{Wikipedia}

\subsection{Wikipedia{\tt/}adjoint}
``In mathematics, the term {\it adjoint} applies in several situations. Several of these share a similar formalism: if $A$ is adjoint to $B$, then there is typically some formula of the type $(Ax,y) = (x,By)$. Specifically, {\it adjoint} or {\it adjunction} may mean:
\begin{enumerate}
	\item \href{https://en.wikipedia.org/wiki/Transpose_of_a_linear_map}{Adjoint of a linear map}, also called its transpose in case of matrices
	\item \href{https://en.wikipedia.org/wiki/Hermitian_adjoint}{Hermitian adjoint} (adjoint of a linear operator) in functional analysis
	\item \href{https://en.wikipedia.org/wiki/Adjoint_endomorphism}{Adjoint endomorphism} of a Lie algebra
	\item \href{https://en.wikipedia.org/wiki/Adjoint_endomorphism}{Adjoint endomorphism} of a Lie algebra
	\item \href{https://en.wikipedia.org/wiki/Adjoint_representation_of_a_Lie_group}{Adjoint functors} in category theory
	\item \href{https://en.wikipedia.org/wiki/Adjunction_(field_theory)}{Adjunction (field theory)}
	\item \href{https://en.wikipedia.org/wiki/Adjunction_formula_(algebraic_geometry)}{Adjunction formula (algebraic geometry)}
	\item \href{https://en.wikipedia.org/wiki/Adjunction_space}{Adjunction space} in topology
	\item \href{https://en.wikipedia.org/wiki/Conjugate_transpose}{Conjuate transpose} of a matrix in linear algebra
	\item \href{https://en.wikipedia.org/wiki/Adjugate_matrix}{Adjugate matrix}, related to its inverse
	\item \href{https://en.wikipedia.org/wiki/Adjoint_equation}{Adjoint equation}
	\item The upper \& lower adjoints of a \href{https://en.wikipedia.org/wiki/Galois_connection}{Galois connection} in order theory
	\item The adjoint of a \href{https://en.wikipedia.org/wiki/Differential_operator}{differential operator} with general polynomial coefficients
	\item \href{https://en.wikipedia.org/wiki/Kleisli_adjunction}{Kleisli adjunction}
	\item \href{https://en.wikipedia.org/wiki/Monoidal_adjunction}{Monoidal adjunction}
	\item \href{https://en.wikipedia.org/wiki/Quillen_adjunction}{Qhillen adjunction}
	\item \href{https://en.wikipedia.org/wiki/Axiom_of_adjunction}{Axiom of adjunction} in set theory
	\item \href{https://en.wikipedia.org/wiki/Adjunction_(rule_of_inference)}{Adjunction (rule of inference)}'' -- \href{https://en.wikipedia.org/wiki/Adjoint}{Wikipedia{\tt/}adjoint}
\end{enumerate}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}dynamic programming}
``{\sf Finding shortest path in a graph using optimal substructure; a straight line indicates a single edge; a wavy line indicates a shortest path between 2 vertices it connects (among other paths, not shown, sharing same 2 vertices); bold line is overall shortest path from start to goal.} {\it Dynamic programming} is both a mathematical optimization method \& an \href{https://en.wikipedia.org/wiki/Algorithmic_paradigm}{algorithmic paradigm}. Method was developed by \href{https://en.wikipedia.org/wiki/Richard_Bellman}{\sc Richard Bellman} in 1950s \& has found applications in numerous fields, from aerospace engineering to economics.

In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a \href{https://en.wikipedia.org/wiki/Recursion}{recursive} manner. While some decision problems cannot be taken apart this way, decisions that span several points in  time do often break apart recursively. Likewise, in CS, if a problem, if a problem can be solved optimally by breaking it into sub-problems \& then recursively finding optimal solutions to sub-problems, then it is said to have \href{https://en.wikipedia.org/wiki/Optimal_substructure}{optimal substructure}.

If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between value of larger problem \& values of sub-problems. In optimization literature this relationship is called \href{https://en.wikipedia.org/wiki/Bellman_equation}{Bellmann equation}.

\subsubsection{Overview}

\begin{enumerate}
	\item {\bf Mathematical optimization.} In terms of mathematical optimization, dynamic programming usually refers to simplifying a decision by breaking it down into a sequence of decision steps over time.
	
	This is done by defining a sequence of {\it value functions} $V_1,\ldots,V_n$ taking $y$ as an argument representing \href{https://en.wikipedia.org/wiki/State_variable}{state} of system at times $i$ from 1 to $n$.
	
	Def of $V_n(y)$ is value obtained in state $y$ at last time $n$.
	
	Values $V_i$ at earlier times $i = n - 1,n - 2,\ldots,2,1$ can be found by working backwards, using a \href{https://en.wikipedia.org/wiki/Recursion}{recursive} relationship called \href{https://en.wikipedia.org/wiki/Bellman_equation}{Bellmann equation}.
	
	For $i = 2,\ldots,n$, $V_{i-1}$ at any state $y$ is calculated from $V_i$ by maximizing a simple function (usually sum) of gain from a decision at time $i - 1$ \& function $V_i$ at new state of system if this decision is made.
	
	Since $V_i$ has already been calculated for needed states, above operation yields $V_{i-1}$ for those states.
	
	Finally, $V_1$ at initial state of system is value of optimal solution. Optimal values of decision variables can be recovered, 1 by 1, by tracking back calculations already performed.
	\item {\bf Control theory.} In \href{https://en.wikipedia.org/wiki/Control_theory}{control theory}, a typical problem: find an admissible control ${\bf u}^*$ which causes system $\dot{\bf x}(t) = {\bf g}({\bf x}(t),{\bf u}(t),t)$ to follow an admissible trajectory ${\bf x}^*$ on a continuous time interval $t_0\le t\le t_1$ that minimizes a \href{https://en.wikipedia.org/wiki/Loss_function}{cost function}
	\begin{equation*}
		J = b({\bf x}(t_1),t_1) + \int_{t_0}^{t_1} f({\bf x}(t),{\bf u}(t),t)\,{\rm d}t.
	\end{equation*}
	Solution to this problem is an optimal control law or policy ${\bf u}^* = h({\bf x}(t),t)$, which produces an optimal trajectory ${\bf x}^*$ \& a \href{https://en.wikipedia.org/wiki/Cost-to-go_function}{cost-to-go function} $J^*$. Latter obeys fundamental equation of dynamic programming:
	\begin{equation*}
		-J_t^* = \min_{\bf u} \{f({\bf x}(t),{\bf u}(t),t) + {J_x^^{*\top}{\bf g}({\bf x}(t),{\bf u}(t),t)\}
	\end{equation*}
	a PDE known as \href{https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation}{Hamilton--Jacobi--Bellman equation}, in which
	\begin{equation*}
		J_x^* = \frac{\partial J^*}{\partial{\bf x}} = [\partial_{x_1}J^*,\ldots,\partial_{x_n}J^*]^\top,\ J_t^* = \partial_tJ^*.
	\end{equation*}
	One finds: minimizing ${\bf u}$ in terms of $t,{\bf x}$, \& unknown function $J_x^*$ \& then substitutes result into Hamilton--Jacobi--Bellman equation to get PDE to be solved with boundary condition $J(t_1) = b({\bf x}(t_1),t_1)$. In practice, this generally requires \href{https://en.wikipedia.org/wiki/Numerical_partial_differential_equations}{numerical techniques} for some discrete approximation to exact optimization relationship.
	
	Alternatively, continuous process can be approximated by a discrete system, which leads to a following recurrence relation analog to Hamilton--Jacobi--Bellman equation:
	\begin{equation*}
		J_k^*({\bf x}_{n-k}) = \min_{{\bf u}_{n-k}} \left\{\hat{f}({\bf x}_{n-k},{\bf u}_{n-k}) + J_{k-1}^*(\hat{\bf g}({\bf x}_{n-k},{\bf u}_{n-k}))\right\}
	\end{equation*}
	at $k$th state of $n$ equally spaced discrete time intervals, \& where $\hat{f},\hat{\bf g}$ denote discrete approximations to $f,{\bf g}$. This functional equation is known as Bellman equation, which can be solved for an exact solution of discrete approximation of optimization equation.
	\begin{itemize}
		\item {\bf Example from economics: Ramsey's problem of optimal saving.} See also: \href{https://en.wikipedia.org/wiki/Ramsey%E2%80%93Cass%E2%80%93Koopmans_model}{Ramsey--Cass--Koopmans model}. In economics, objective is generally to maximize (rather than minimize) some dynamic \href{https://en.wikipedia.org/wiki/Social_welfare_function}{social welfare function}. In Ramsey's problem, this function relates amounts of consumption to levels of \href{https://en.wikipedia.org/wiki/Utility}{utility}. Loosely speaking, planner faces trade-off between contemporaneous consumption \& future consumption (via investment in \href{https://en.wikipedia.org/wiki/Physical_capital}{capital stock} used in production), known as \href{https://en.wikipedia.org/wiki/Intertemporal_choice}{intertemporal choice}. Future consumption is discounted at a constant rate $\beta\in(0,1)$. A discrete approximation to transition equation of capital is given by
		\begin{equation*}
			k_{t+1} = \hat{g}(k_t,c_t) = f(k_t) - c_t,
		\end{equation*}
		where $c$ is consumption, $k$ is capital, $f$: a \href{https://en.wikipedia.org/wiki/Production_function}{production function} satisfying \href{https://en.wikipedia.org/wiki/Inada_conditions}{Inada conditions}. An initial capital stock $k_0 > 0$ is assumed.
		
		Let $c_t$ be consumption in period $t$, \& assume consumption yields utility $u(c_t) = \ln c_t$ as long as consumer lives. Assume consumer is impatient, so that he \href{https://en.wikipedia.org/wiki/Discounting}{discounts} future utility by a factor $b$ each period, where $0 < b < 1$. Let $k_t$ be \href{https://en.wikipedia.org/wiki/Capital_(economics)}{capital} in period $t$. Assume initial capital is a given amount $k_0 > 0$, \& suppose: this period's capital \& consumption determine next period's capital as $k_{t+1} = Ak_t^a - c_t$, where $A$: a positive constant \& $0 < a < 1$. Assume capital cannot be negative. Then consumer's decision problem can be written as follows:
		\begin{equation*}
			\max\sum_{t=0}^T b^t\ln c_t\mbox{ subject to } k_{t+1} = Ak_t^a - c_t\ge0,\ \forall t = 0,1,\ldots,T.
		\end{equation*}
		Written this way, problem looks complicated, because it involves solving for all choice variables $c_0,c_1,\ldots,c_T$. (Capital $k_0$ is not a choice variable -- consumer's initial capital is taken as given.)
		
		DP approach to solve this problem involves breaking it apart into a sequence of smaller decisions. To do so, define a sequence of {\it value functions} $V_t(k)$, for $t = 0,1,\ldots,T,T+1$ which represent value of having any amount of capital $k$ at each time $t$. There is (by assumption) no utility from having capital after death, $V_{T+1}(k) = 0$.
		
		Value of any quantity of capital at any previous time can be calculated by \href{https://en.wikipedia.org/wiki/Backward_induction}{backward induction} using Bellman equation. In this problem, for each $t = 0,1,\ldots,T$, Bellman equation is
		\begin{equation*}
			V_t(k_t) = \max(\ln c_t + bV_{t+1}(k_{t+1}))\mbox{ subject to } k_{t+1} = Ak_t^a - c_t\ge0.
		\end{equation*}
		This problem is much simpler than the once wrote down before, because it involves only 2 decision variables, $c_t$ \& $k_{t+1}$. Intuitively, instead of choosing his whole lifetime plan at birth, consumer can take things 1 step at a time. At time $t$, his current capital $k_t$ is given, \& he only needs to choose current consumption $c_t$ \& saving $k_{t+1}$.
		
		To actually solve this problem, work backwards. For simplicity, current level of capital is denoted as $k$. $V_{T+1}(k)$ is already known, so using Bellman equation once can calculate $V_T(k)$, \& so on until get to $V_0(k)$, which is {\it value} of initial decision problem for whole lifetime. I.e., once know $V_{T-j+1}(k)$, can calculate $V_{T_j}(k)$, which is maximum of $\ln c_{T-j} + bV_{T-j+1}(Ak^a - c_{T-j})$, where $c_{T-j}$ is choice variable \& $Ak^a - c_{T-j}\ge0$.
		
		Working backwards, it can be shown: value function at time $t = T - j$ is
		\begin{equation*}
			V_{T-j}(k) = a\sum_{i=0}^j a^ib^i\ln k + v_{T-j},
		\end{equation*}
		where each $v_{T-j}$ is a constant, \& optimal amount to consume at time $t = T - j$ is
		\begin{equation*}
			c_{T-j}(k) = \frac{1}{\sum_{i=0}^j a^ib^i}Ak^a.
		\end{equation*}
		See: optimal to consume a larger fraction of current wealth as one gets older, finally consuming all remaining wealth in period $T$, the last period of life.
	\end{itemize}
	\item {\bf CS.} There are 2 key attributes that a problem must have in order for DP to be applicable: optimal substructure \& \href{https://en.wikipedia.org/wiki/Overlapping_subproblem}{overlapping sub-problems}. If a problem can be solved by combining optimal solutions to {\it non-overlapping} sub-problems, strategy is called ``\href{https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm}{divide \& conquer}'' instead. This is why \href{https://en.wikipedia.org/wiki/Mergesort}{merge sort} \& \href{https://en.wikipedia.org/wiki/Quicksort}{quick sort} are not classified as DP problems.
	
	{\it Optimal substructure} means: solution to a given optimization problem can be obtained by combination of optimal solutions to its sub-problems. Such optimal substructures are usually described by means of \href{https://en.wikipedia.org/wiki/Recursion}{recursion}. E.g., given a graph $G = (V,E)$, shortest path $p$ from a vertex $u$ to a vertex $v$ exhibits optimal substructure: take any intermediate vertex $w$ on this shortest path $p$. If $p$ is truly shortest path, then it can be split into sub-paths $p_1$ from $u$ to $w$ \& $p_2$ from $w$ to $v$ s.t. these, in turn, are indeed shortest paths between corresponding vertices (by simple cut-\&-paste argument described in \href{https://en.wikipedia.org/wiki/Introduction_to_Algorithms}{\it Introduction to Algorithms}). Hence, one can easily formulate solution for finding shortest paths in a recursive manner, which is what \href{https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm}{Bellman--Ford algorithm} or \href{https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm}{Floyd--Warshall algorithm} does.
	
	{\it Overlapping} sub-problems means: space of sub-problems must be small, i.e., any recursive algorithm solving problem should solve same sub-problems over \& over, rather than generating new sub-problems. E.g., consider recursive formulation for generating Fibonacci sequence $F_i = F_{i-1} + F_{i-2}$, with base case $F_1 = F_2 = 1$. Then $F_{43} = F_{42} + F_{41}$, \& $F_{42} = F_{41} + F_{40}$. Now $F_{41}$ is being solved in recursive sub-trees of both $F_{43}$ as well as $F_{42}$. Even though total number of sub-problems is actually small (only 43 of them), end up solving same problems over \& over if adopt a naive recursive solution e.g. this. DP takes account of this fact \& solves each sub-problem only once.
	
	{\sf Fig. Subproblem graph for Fibonacci sequence. Fact: it is not a \href{https://en.wikipedia.org/wiki/Tree_structure}{tree} indicates overlapping subproblems.} This can be achieved in either of 2 ways:
	\begin{itemize}
		\item \href{https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design}{\it Top-down approach}: This is direct fall-out of recursive formulation of any problem. if solution to any problem can be formulated recursively using solution to its sub-problems, \& if its sub-problems are overlapping, then one can easily \href{https://en.wikipedia.org/wiki/Memoize}{memoize} or store solutions to sub-problems in a table (often an \href{https://en.wikipedia.org/wiki/Array_(data_structure)}{array} or \href{https://en.wikipedia.org/wiki/Hash_table}{hastable} in practice). Whenever attempt to solve a new sub-problem, 1st check table to see if already solved. If a solution has been recorded, can use it directly, otherwise solve sub-problem \& add its solution to table.
		\item \href{https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design}{\it Bottom-up approach}: Once formulate solution to a problem recursively as in terms of its sub-problems, can try reformulating problem in a bottom-up fashion: try solving sub-problems 1st \& use their solutions to build-on \& arrive at solutions to bigger sub-problems. This is also usually done in a tabular form by iteratively generating solutions to bigger \& bigger sub-problems by using solutions to small sub-problems. E.g., if already know values of $F_{41}$ \& $F_{40}$, can directly calculate value of $F_{42}$.
	\end{itemize}
	Some \href{https://en.wikipedia.org/wiki/Programming_language}{programming languagues} can automatically \href{https://en.wikipedia.org/wiki/Memoization}{memoize} result of a function call with a particular set of arguments, in order to speed up \href{https://en.wikipedia.org/wiki/Call-by-name}{call-by-name} evaluation (this mechanism is referred to as \href{https://en.wikipedia.org/wiki/Call-by-need}{\it call-by-need}). Some languages make it possible portably (e.g., \href{https://en.wikipedia.org/wiki/Scheme_(programming_language)}{Scheme}, \href{https://en.wikipedia.org/wiki/Common_Lisp}{Common Lisp}, \href{https://en.wikipedia.org/wiki/Perl}{Perl}, \& \href{https://en.wikipedia.org/wiki/D_(programming_language)}{D}). Some languages have automatic \href{https://en.wikipedia.org/wiki/Memoization}{memoization} built in, e.g. tabled \href{https://en.wikipedia.org/wiki/Prolog}{Prolog} \& \href{https://en.wikipedia.org/wiki/J_(programming_language)}{J}, which supports memoization with {\it M.} adverb. In any case, this is only possible for a \href{https://en.wikipedia.org/wiki/Referentially_transparent}{referentially transparent} function. Memoization is also encountered as an easily accessible design pattern within term-rewrite based languages e.g. \href{https://en.wikipedia.org/wiki/Wolfram_Language}{Wolfram Language}.
	\item {\bf Bioinformatics.} DP is widely used in bioinformatics for tasks e.g. \href{https://en.wikipedia.org/wiki/Sequence_alignment}{sequence alignment}, \href{https://en.wikipedia.org/wiki/Protein_folding}{protein folding}, RNA structure prediction \& protein-DNA binding. 1st DP algorithms for protein-DNA binding were developed in 1970s independently by \href{https://en.wikipedia.org/wiki/Charles_DeLisi}{\sc Charles DeLisi} in US \& by {\sc Georgii Gurskii \& Alexander Zasedatelev} in Soviet Union. Recently these algorithms have become very popular in bioinformatics \& \href{https://en.wikipedia.org/wiki/Computational_biology}{computational biology}, particularly in studies of \href{https://en.wikipedia.org/wiki/Nucleosome}{nucleosome} positioning \& \href{https://en.wikipedia.org/wiki/Transcription_factor}{transcription factor} binding.
\end{enumerate}

\subsubsection{Examples: computer algorithms}

\begin{enumerate}
	\item {\bf Dijkstra's algorithm for shortest path problem.} From a DP point of view, \href{https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm}{Dijkstra's algorithm} for \href{https://en.wikipedia.org/wiki/Shortest_path_problem}{shortest path problem} is a successive approximation scheme that solves DP functional equation for shortest path problem by {\it Reaching} method.
	
	In fact, Dijkstra's explanation of logic behind algorithm, namely
	\begin{problem}
		Find path of minimum total length between 2 given nodes $P,Q$.
	\end{problem}
	Use fact: if $R$ is a node on minimal path from $P$ to $Q$, knowledge of latter implies knowledge of minimal path from $P$ to $R$.
	
	is a paraphrasing of \href{https://en.wikipedia.org/wiki/Richard_Bellman}{\sc Bellman}'s famous \href{https://en.wikipedia.org/wiki/Principle_of_Optimality}{Principle of Optimality} in context of \href{https://en.wikipedia.org/wiki/Shortest_path_problem}{shortest path problem}.
	\item {\bf Fibonacci sequence.} Using DP in calculation of $n$th member of \href{https://en.wikipedia.org/wiki/Fibonacci_sequence}{Fibonacci sequence} improves its performance greatly. Here is a naive implementation, based directly on mathematical def:
	\begin{verbatim}
		function fib(n)
		    if n <= 1 return n
		    return fib(n - 1) + fib(n - 2)
	\end{verbatim}
	Notice if call, say, {\tt fib(5)}, produce a call tree that calls function on same value many different times [$\ldots$]. In particular, {\tt fib(2)} was calculated 3 times from scratch. In larger examples, many more values of {\tt fib}, or {\it subproblems}, are recalculated, leading to an exponential time algorithm.
	
	Now, suppose have a simple \href{https://en.wikipedia.org/wiki/Associative_array}{map} object $m$ which maps each value of {\tt fib} that has already been calculated to its result, \& modify our function to use it \& update it. Resulting function requires only $O(n)$ time instead of exponential time (but requires $O(n)$ space):
	\begin{verbatim}
		var m := map(0 -> 0, 1 -> 1)
		function fib(n)
		    if key n is not in map m
		        m[n] := fib(n - 1) + fib(n - 2)
		    return m[n]
	\end{verbatim}
	This technique of saving values that have already been calculated is called \href{https://en.wikipedia.org/wiki/Memoization}{memoization}; this is top-down approach, since 1st break problem into subproblems \& then calculate \& store values.
	
	In {\it bottom-up} approach, calculate smaller values of {\tt fib} 1st, then build larger values from them. This method also uses $O(n)$ time since it contains a loop that repeats $n - 1$ times, but it only takes constant $O(1)$ space, in contrast to top-down approach which requires $O(n)$ space to store map.
	\begin{verbatim}
		function fib(n)
		    if n = 0
		        return 0
		    else
		        var previousFib := 0, currentFib := 1
		        repeat n - 1 times // loop is skipped if n = 1
		            var newFib := previousFib + currentFib
		            previousFib := currentFib
		            currentFib := newFib
		        return currentFib
	\end{verbatim}
	In both examples, only calculate {\tt fib(2)} 1 time, \& then use it to calculate both {\tt fib(4)} \& {\tt fib(3)}, instead of computing it every time either of them is evaluated.	
	\item {\bf A type of balanced 0--1 matrix.} Consider problem of assigning values, either 0 or 1, to positions of an $n\times n$ matrix, with $n$ even, so that each row \& each column contains exactly $\frac{n}{2}$ 0s \& $\frac{n}{2}$ 1s. Ask how many different assignments there are for a given $n$. E.g., when $n = 4$, 5 possible solutions are [$\ldots$] There are at least 3 possible approaches: \href{https://en.wikipedia.org/wiki/Brute-force_search}{brute force}, \href{https://en.wikipedia.org/wiki/Backtracking}{backtracking}, \& DP.
	
	Brute force consists of checking all assignments of 0s \& 1s \& counting those that have balanced rows \& columns ($\frac{n}{2}$ 0s \& $\frac{n}{2}$ 1s). As there are $2^{n^2}$ possible assignments \& $\binom{n}{\frac{n}{2}}^2$ sensible assignments, this strategy is not practical except maybe up to $n = 6$.
	
	Backtracking for this problem consists of choosing some order of matrix elements \& recursively placing 1s or 0s, while checking: in every row \& column number of elements that have not been assigned plus number of 1s or 0s are both at least $\frac{n}{2}$. While more sophisticated than brute force, this approach will visit every solution once, making it impractical for $n > 6$, since number of solutions is already 116,963,796,250 for $n = 8$.
	
	DP makes it possible to count number of solutions without visiting them all. Imagine backtracking values for 1st row -- what information would we require about remaining rows, in order to be able to accurately count solutions obtained for each 1st row value? Consider $k\times n$ boards, where $1\le k\le n$, whose $k$ rows contain $\frac{n}{2}$ 0s \& $\frac{n}{2}$ 1s. Function $f$ to which memoization is applied maps vectors of $n$ pairs of integers to number of admissible boards (solutions). There is 1 pair for each column, \& its 2 components indicate respectively number of 0s \& 1s that have yet to be placed in that column. Seek value of $f\left(\left(\frac{n}{2},\frac{n}{2}\right),\left(\frac{n}{2},\frac{n}{2}\right),\ldots,\left(\frac{n}{2},\frac{n}{2}\right)\right)$ ($n$ arguments or 1 vector of $n$ elements). Process of subproblem creation involves iterating over every 1 of $\binom{n}{\frac{n}{2}}$ possible assignments for top row of board, \& going through every column, subtracting 1 from appropriate element of pair for that column, depending on whether assignment for top row contained a 0 or a 1 at that position. If any 1 of results is negative, then assignment is invalid \& does not contribute to set of solutions (recursion stops). Otherwise, have an assignment for top row of $k\times n$ board \& recursively compute number of solutions to remaining $(k - 1)\times n$ board, adding numbers of solutions for every admissible assignment of top row \& returning sum, which is being memoized. Base case is trivial subproblem, which occurs for a $1\times n$ board. Number of solutions for this board is either 0 or 1, depending on whether vector is a permutation of $\frac{n}{2}$ $(0,1)$ \& $\frac{n}{2}$ $(1,0)$ pairs or not.
	
	E.g., in 1st 2 boards shown above sequences of vectors would be [$\ldots$] The number of solutions (sequence) A058527 in \href{https://en.wikipedia.org/wiki/On-Line_Encyclopedia_of_Integer_Sequences}{OEIS}) is $1,2,90,297200,116963796250,6736218287430460752,\ldots$ Links to MAPLE implementation of DP approach may be found among external links.
	\item {\bf Checkerboard.} Consider a \href{https://en.wikipedia.org/wiki/Checkerboard}{checkerboard} with $n\times n$ squares \& a cost function {\tt c(i, j)} which returns a cost associated with square {\tt(i,j)} $i$: row, $j$: column. E.g., on a [$5\times5$ checkerboard]. Say there was a checker that could start at any square on 1st rank (i.e., row) \& wanted to know shortest path (sum of minimum costs at each visited rank) to get to last rank; assuming checker could move only diagonally left forward, diagonally right forward, or straight forward. I.e., a checker on $(1,3)$ can move to $(2,2),(2,3)$, or $(2,4)$. This problem exhibits {\it optimal substructure}. I.e., solution to entire problem relies on solutions to subproblems. Define a function $q(i,j)\coloneqq$ minimum cost to reach square $(i,j)$. Starting at rank $n$ \& descending to rank 1, compute value of this function for all squares at each successive rank. Picking square that holds minimum value at each rank gives shortest path between rank $n$ \& rank 1.
	
	Function $q(i,j) =$ minimum cost to get to any of 3 squares below it (since those are the only squares that can reach it) plus $c(i,j)$. E.g.: $q(A) = \min(q(B),q(C),q(D)) + c(A)$. Define $q(i,j)$ in somewhat more general terms:
	\begin{equation*}
		q(i,j) = \left\{\begin{split}
			&\infty&&\mbox{if } j < 1\mbox{ or } j > n,\\
			&c(i,j)&&\mbox{if } i = 1,\\
			&\min(q(i - 1,j - 1),q(i - 1,j),q(i - 1,j + 1)) + c(i,j)&&\mbox{otherwise}.
		\end{split}\right.
	\end{equation*}
	1st line of this equation deals with a board modeled as squares indexed on 1 at lowest bound \& $n$ at highest bound. 2nd line specifies what happens at 1st rank; providing a base case. 3rd line, recursion, is important part. It represents $A,B,C,D$ terms in example. From this def can derive straightforward recursive code for $q(i,j)$. In following pseudo code, $n$: size of board, $c(i,j)$: cost function, $\min()$ returns minimum of a number of values.
	\begin{verbatim}
		function minCost(i, j)
		     if j < 1 or j > n
		          return infinity
		     else if i = 1
		          return c(i, j)
		     else
		          return min(minCost(i - 1, j - 1), minCost(i - 1, j), minCost(i - 1, j + 1)) + c(i, j)
	\end{verbatim}
	This function only computes path cost, not actual path. Discuss actual path. This, like Fibonacci-numbers example, is horribly slow because it too exhibits {\it overlapping subproblems} attribute. I.e., it recomputes same path costs over \& over. However, can compute it much faster in a bottom-up fashion if store path costs in a 2D array {\tt q[i, j]} rather than using a function. This avoids recomputation; all values needed for array {\tt q[i, j]} are computed ahead of time only once. Precomputed values for $(i,j)$ are simply looked up whenever needed.
	
	Also need to know what actual shortest path is. To do this, use another array {\tt p[i, j]}; a {\it predecessor array}. This array records path to any square $s$. Predecessor of $s$ is modeled as an offset relative to index (in {\tt q[i, j]}) of precomputed path cost of $s$. To reconstruct complete path, lookup predecessor of $s$, then predecessor of that square, then predecessor of that square, \& so on recursively, until reach starting square. Consider following pseudocode:
	\begin{verbatim}
		function computeShortestPathArrays()
		    for x from 1 to n
		        q[1, x] := c(1, x)
		    for y from 1 to n
		        q[y, 0] := infinity
		        q[y, n + 1] := infinity
		    for y from 2 to n
		        for x from 1 to n
		            m := min(q[y-1, x-1], q[y-1, x], q[y-1, x+1])
		            q[y, x] := m + c(y, x)
		            if m = q[y-1, x-1]
		                p[y, x] := -1
		            else if m = q[y-1, x]
		                p[y, x] :=  0
		            else
		                p[y, x] :=  1
	\end{verbatim}
	Now the rest is a simple matter of finding minimum \& printing it.
	\begin{verbatim}
		function computeShortestPath()
		    computeShortestPathArrays()
		    minIndex := 1
		    min := q[n, 1]
		    for i from 2 to n
		        if q[n, i] < min
		            minIndex := i
		            min := q[n, i]
		    printPath(n, minIndex)
		
		function printPath(y, x)
		    print(x)
		    print("<-")
		    if y = 2
		        print(x + p[y, x])
		    else
		        printPath(y-1, x + p[y, x])
	\end{verbatim}
	\item {\bf Sequence alignment.} In \href{https://en.wikipedia.org/wiki/Genetics}{genetics}, \href{https://en.wikipedia.org/wiki/Sequence_alignment}{sequence alignment} is an important application where DP is essential. Typically, problem consists of transforming 1 sequence into another using edit operations that replace, insert, or remove an element. Each operation has an associated cost, \& goal: find \href{https://en.wikipedia.org/wiki/Edit_distance}{sequence of edits with lowest total cost}.
	
	Problem can be stated naturally as a recursion, a sequence $A$ is optimally edited into a sequence $B$ by either:
	\begin{enumerate}
		\item inserting 1st character of $B$, \& performing an optimal alignment of $A$ \& tail of $B$
		\item deleting 1st character of $A$, \& performing optimal alignment of tail of $A$ \& $B$
		\item replacing 1st character of $A$ with 1st character of $B$, \& performing optimal alignments of tails of $A$ \& $B$.
	\end{enumerate}
	Partial alignments can be tabulated in a matrix, where cell(i, j) contains cost of optimal alignment of A[1..i] to B[1..j]. Cost in cell(i, j) can be calculated by adding cost of relevant operations to cost of its neighboring cells, \& selecting optimum.
	
	Different variants exist, see \href{https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm}{Smith--Waterman algorithm} \& \href{https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm}{Needleman--Wunsch algorithm}.
	\item {\bf Tower of Hanoi puzzle.} \href{https://en.wikipedia.org/wiki/Tower_of_Hanoi}{Tower of Hanoi} or Towers of Hanoi is a \href{https://en.wikipedia.org/wiki/Mathematical_game}{mathematical game} or \href{https://en.wikipedia.org/wiki/Puzzle}{puzzle}. It consists of 3 rods, \& a number of disks of different sizes which can slide onto any rod. Puzzle starts with disks in a neat stack in ascending order of size on 1 rod, smallest at top, thus making a conical shape.
	
	Objective of puzzle: move entire stack to another rod, obey rules:
	\begin{enumerate}
		\item Only 1 disk may be moved at a time.
		\item Each move consists of taking upper disk from 1 of rods \& sliding it onto another rod, on top of other disks that may already be present on that rod.
		\item No disk may be placed on top of a smaller disk.
	\end{enumerate}
	DP solution consists of solving \href{https://en.wikipedia.org/wiki/Bellman_equation}{functional equation}
	\begin{equation*}
		S(n,h,t) = S(n - 1,h,not(h,t));S(1,h,t);S(n - 1,not(h,t),t)
	\end{equation*}
	where $n$ denotes number of disks to be moved, $h$ denotes home rod, $t$ denotes target rod, $not(h,t)$ denotes 3rd rod (neither $h$ nor $t$), ``;'' denotes concatenation, \& $S(n,h,t)\coloneqq$ solution to a problem consisting of $n$ disks that are to be moved from rod $h$ to rod $t$. For $n = 1$ problem is trivial, namely $S(1,h,t) =$ ``move a disk from rod $h$ to rod $t$'' (there is only 1 disk left).
	
	Number of moves required by this solution is $2^n - 1$. If objective is to {\it maximize} number of moves (without cycling) then DP \href{https://en.wikipedia.org/wiki/Bellman_equation}{functional equation} is slightly more complicated \& $3^n - 1$ moves are required.
	\item {\bf Egg dropping puzzle.} A description of instance of this famous puzzle involving $N = 2$ eggs \& a building with $H = 36$ floors:
	\begin{problem}
		Suppose wish to know which stories in a 36-story building are safe to drop eggs from, \& which will cause eggs to break on landing (using U.S. English terminology, in which 1st floor is at ground level). Make a few assumptions:
		\begin{itemize}
			\item An egg that survives a fall can be used again.
			\item A broken egg must be discarded.
			\item Effect of a fall is same for all eggs.
			\item If an egg breaks when dropped, then it would break if dropped from a higher window.
			\item If an egg survives a fall, then it would survive a shorter fall.
			\item It is not ruled out: 1st-floor windows break eggs, nor is it ruled out that eggs can survive 36th-floor windows.
		\end{itemize}
		If only 1 egg is available \& wish to be sure of obtaining right result, experiment can be carried out in only 1 way. Drop egg from 1st-floor window; if it survives, drop it from 2nd-floor window. Continue upward until it breaks. In worst case, this method may require 36 droppings. Suppose 2 eggs are available. What is lowest number of egg-droppings that is guaranteed to work in all cases?
	\end{problem}
	To derive a DP functional equation for this puzzle, let {\it state} of DP model be a pair $s = (n,k)$, where $n =$ number of test eggs available, $n = 0,1,,\ldots,N - 1$, $k =$ number of (consecutive) floors yet to be tested, $k = 0,1,\ldots,H - 1$. E.g., $s = (2,6)$ indicates: 2 test eggs are available \& 6 (consecutive) floors ar yet to be tested. Initial state of process is $s = (N,H)$ where $N$ denotes number of test eggs available at commencement of experiment. Process terminates either when there are no more test eggs $n = 0$ or when $k = 0$, whichever occurs 1st. If termination occurs at state $s = (0,k)$ \& $k > 0$, then test failed. Now let $W(n,k) =$  minimum number of trials required to identify value of critical floor under worst-case scenario given that process is in state $s = (n,k)$. Then it can be shown: $W(n,k) = 1 + \min\{\max(W(n - 1,x - 1), W(n,k - x)):x = 1,\ldots,k\}$ with $W(n,0) = 0$, $\forall n\in\mathbb{N}^\star$ \& $W(1,k) = k$ for all $k$. Easy to solve this equation iteratively by systematically increasing values of $n,k$.
	\begin{itemize}
		\item {\bf Faster DP solution using a different parametrization.} Notice above solution takes $O(nk^2)$ time with a DP solution. This can be improved to $O(nk\log k)$ time by binary searching on optiaml $x$ in above recurrence, since $W(n - 1,x - 1)$ is increasing in $x$ while $W(n,k - x)$ is decreasing in $x$, thus a local minimum of $\max(W(n - 1,x - 1),W(n,k - x))$ is a global minimum. Also, by storing optimal $x$ for each cell in DP table \& referring to its value for prev cell, optimal $x$ for each cell can be found in constant time, improving it to $O(nk)$ time. However, there is an even faster solution that involves a different parametrization of problem: Let $k$: total number of floors s.t. eggs break when dropped from $k$th floor (example above is equivalent to taking $k = 37$). Let $m$: minimum floor from which egg must be dropped to be broken. Let $f(t,n)$: maximum number of values of $m$ that are distinguishable using $t$ tries \& $n$ eggs. Then $f(t,0) = f(0,n) = 1$ $\forall t,n\ge0$. Let $a$: floor from which 1st egg is dropped in optimal strategy. If 1st egg broke, $m$ is from 1 to $a$ \& distinguishable using at most $t - 1$ tries \& $n - 1$ eggs. If 1st eggs did not break, $m$ is from $a + 1$ to $k$ \& distinguishable using $t - 1$ tries \& $n$ eggs. Therefore, $f(t,n) = f(t - 1,n - 1) + f(t - 1,n)$. Then problem is equivalent to finding minimum $x$ s.t. $f(x,n)\ge k$. To do so, could compute $\{f(t,i):0\le i\le n\}$ in order of increasing $t$, which would take $O(nx)$ time. Thus, if separately handle case of $n = 1$, algorithm would take $O(n\sqrt{k})$ time. But recurrence relation can in fact be solved, giving $f(t,n) = \sum_{i=0}^n \binom{t}{i}$, which can be computed in $O(n)$ time using identity $\binom{t}{i + 1} = \binom{t}{i}\dfrac{t - i}{i + 1}$, $\forall i\ge0$. Since $f(t,n)\le f(t + 1,n)$, $\forall t\ge0$, can binary search on $t$ to find $x$, giving an $O(n\log k)$ algorithm.
	\end{itemize}
	\item {\bf Matrix chain multiplication.} \href{https://en.wikipedia.org/wiki/Matrix_chain_multiplication}{Wikipedia{\tt/}matrix chain multiplication}. Matrix chain multiplication is a well-known example that demonstrates utility of DP. E.g., engineering applications often have to multiply a chain of matrices. It is not surprising to find matrices of large dimensions, e.g. $100\times100$. Therefore, our task: multiply matrices $A_1,\ldots,A_n$. Matrix multiplication is not commutative, but is associative; \& can multiply only 2 matrices at a time. So, can multiply this chain of matrices in many different ways, e.g.: [$\ldots$] There are numerous ways to multiply this chain of matrices. They will all produce same final result, however they will take more or less time to compute, based on which particular matrices are multiplied. If matrix $A$ has dimensions $m\times n$ \& matrix $B$ has dimension $n\times q$, then matrix $A = AB$ will have dimensions $m\times Q$, \& will require $mnq$ scalar multiplications (using a simplistic \href{https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm}{matrix multiplication algorithm} for purposes of illustration).
	
	E.g., multiply matrices $A,B,C$. Assume: their dimensions are $m\times n,n\times p,p\times s$, resp. Matrix $ABC$ will be of size $m\times s$ \& can be calculated in 2 ways shown:
	\begin{enumerate}
		\item $A(BC)$: this order of matrix multiplication will require $nps + mns$ scalar multiplications.
		\item $(AB)C$: this order of matrix multiplication will require $mnp + mps$ scalar calculations.
	\end{enumerate}
	Assume $m = 10,n = 100,p = 10,s = 1000$. So, 1st way to multiply chain will require $10^6 + 10^6$ calculations. 2nd way will require only $10000 + 100000$ calculations. Obviously, 2nd way is faster, \& should multiply matrices using that arrangement of parenthesis.
	
	Therefore, our conclusion: order of parenthesis matters, \& our task: find optimal order of parenthesis.
	
	At this point, have several choices, 1 of which: design a DP algorithm that will split problem into overlapping problems \& calculate optimal arrangement of parenthesis. DP solution is presented below.
	
	Call $m[i,j]$ minimum number of scalar multiplications needed to multiply a chain of matrices from matrix $i$ to matrix $j$, i.e., $A_i\cdots A_j$, i.e., $i\le j$. Split chain at some matrix $k$, s.t. $i\le k\le j$, \& try to find out which combination produces minimum $m[i,j]$. Formula is:
	\begin{verbatim}
		if i = j, m[i,j]= 0
		if i < j, m[i,j]= min over all possible values of k (m[i,k]+m[k+1,j] + p[i-1]*p[k]*p[j]
	\end{verbatim}
	where $k$ ranges from $i$ to $j - 1$, $p_{i - 1}$: row dimension of matrix $i$, $p_k$: column dimension of matrix $k$, $p_j$: column dimension of matrix $j$. This formula can be coded as shown below, where input parameter ``chain'' is chain of matrices, i.e., $A_1,\ldots,A_n$:
	\begin{verbatim}
		function OptimalMatrixChainParenthesis(chain)
		    n = length(chain)
		    for i = 1, n
		        m[i,i] = 0    // Since it takes no calculations to multiply one matrix
		    for len = 2, n
		        for i = 1, n - len + 1
		            j = i + len - 1
		            m[i,j] = infinity      // So that the first calculation updates
		            for k = i, j - 1
		                q = m[i, k] + m[k+1, j] + p[i-1]*p[k]*p[j]
		                if q < m[i, j]     // The new order of parentheses is better than what we had
		                    m[i, j] = q    // Update
		                    s[i, j] = k    // Record which k to split on, i.e. where to place the parenthesis
	\end{verbatim}
	So far, have calculated values for all possible $m[i,j]$, minimum number of calculations to multiply a chain from matrix $i$ to matrix $j$, \& have recorded corresponding ``split point'' $s[i,j]$. E.g., if multiplying chain $A_1A_2A_3A_4$, \& it turns out $m[1,3] = 100$ \& $s[1,3] = 2$, i.e. optimal placement of parenthesis for matrices 1 to 3 is $(A_1A_2)A_3$ \& to multiply those matrices will require 100 scalar calculations.
	
	This algorithm will produce ``tables'' $m[\cdot,\cdot],s[\cdot,\cdot]$ that will have entries for all possible values of $i,j$. Final solution for entire chain is $m[1,n]$, with corresponding split at $s[1,n]$. Unraveling solution will be recursive, starting from top \& continuing until reach base case, i.e., multiplication of single matrices.
	
	Therefore, next step: actually split chain, i.e., place parenthesis where they (optimally) belong. For this purpose could use algorithm:
	\begin{verbatim}
		function PrintOptimalParenthesis(s, i, j)
		    if i = j
		        print "A"i
		    else
		        print "(" 
		        PrintOptimalParenthesis(s, i, s[i, j]) 
		        PrintOptimalParenthesis(s, s[i, j] + 1, j) 
		        print ")"
	\end{verbatim}
	Of course, this algorithm is not useful for actual multiplication. This algorithm is just a user-friendly way to see what result looks like. To actually multiply matrices using proper splits, need algorithm:
	\begin{verbatim}
		function MatrixChainMultiply(chain from 1 to n)       // returns the final matrix, i.e. A1×A2×... ×An
		    OptimalMatrixChainParenthesis(chain from 1 to n)   // this will produce s[ . ] and m[ . ] "tables"
		    OptimalMatrixMultiplication(s, chain from 1 to n)  // actually multiply
		function OptimalMatrixMultiplication(s, i, j)   // returns the result of multiplying a chain of matrices from Ai to Aj in optimal way
		    if i < j
		        // keep on splitting the chain and multiplying the matrices in left and right sides
		        LeftSide = OptimalMatrixMultiplication(s, i, s[i, j])
		        RightSide = OptimalMatrixMultiplication(s, s[i, j] + 1, j)
		        return MatrixMultiply(LeftSide, RightSide)
		    else if i = j
		        return Ai   // matrix at position i
		    else
		        print "error, i <= j must hold"
	    function MatrixMultiply(A, B)    // function that multiplies two matrices
	        if columns(A) = rows(B)
	            for i = 1, rows(A)
	                for j = 1, columns(B)
	                    C[i, j] = 0
	                    for k = 1, columns(A)
	                        C[i, j] = C[i, j] + A[i, k]*B[k, j]
	                    return C
	        else
	            print "error, incompatible dimensions."
	\end{verbatim}
\end{enumerate}

\subsubsection{History of name}
Term {\it dynamic programming} was originally used in 1940s by {\sc Richard Bellman} to describe process of solving problems where one needs to find best decisions one after another. By 1953, he refined this to modern meaning, referring specifically to nesting smaller decision problems inside larger decisions, \& field was therefore recognized by IEEE as a \href{https://en.wikipedia.org/wiki/Systems_analysis}{system analysis} \& engineering topic. {\sc Bellman}'s contribution is remembered in name of Bellman equation, a central result of DP which restates an optimization problem in recursive form.

{\sc Bellman} explains reasoning behind term DP in his autobiography, {\it Eye of the Hurricane: An Autobiography}:
\begin{quote}
	``I spent Fall quarter (of 1950) at \href{https://en.wikipedia.org/wiki/RAND_Corporation}{RAND}. My 1st task was to find a name for multistage decision processes. An interesting question is, ``Where did name, DP, come from?'' 1950s were not good years for mathematical research. We had a very interesting gentleman in Washing named \href{https://en.wikipedia.org/wiki/Charles_Erwin_Wilson}{\sc Wilson}. He was Secretary of Defense, \& actually had a pathological fear \& hatred of word ``research''. I'm not using term lightly; I'm using it precisely. His face would suffuse, he would turn red, \& he would get violent if people used term research in his presence. Can imagine how he felt, then, about term mathematical. RAND Corporation was employed by Air Force, \& Air Force had {\sc Wilsn} as its boss, essentially. Hence, I felt I had to do sth to shield {\sc Wilson} \& Air Force from fact I was really doing mathematics inside RAND Corporation. What title, what name, could I choose? In 1st place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use word ``programming''. I wanted to get across idea that this was dynamic, this was multistage, this was time-varying. I thought, let's kill 2 birds with 1 stone. Let's take a word that has an absolutely precise meaning, namely dynamic, in classical physical sense. It also has a very interesting property as an adjective, \& i.e., impossible to use word dynamic in a pejorative sense (ý nghĩa miệt thị). Try thinking of some combination that will possibly give it a pejorative meaning. Impossible. Thus, I thought dynamic programming was a good name. It was sth not even a Congressman could object to. So I used it as an umbrella for my activities.'' -- {\sc Richard Bellman}, {\it Eye of the Hurricane: An Autobiography} (1984, p. 159)
\end{quote}
Word {\it dynamic} was chosen by {\sc Bellman} to capture time-varying aspect of problems, \& because it sounded impressive. Word {\it programming} referred to use of method to find an optimal {\it program}, in sense of a military schedule for training or logistics. This usage is same as that in phrases \href{https://en.wikipedia.org/wiki/Linear_programming}{linear programming} \& mathematical programming, a synonym for mathematical optimization.

Above explanation of origin of term may be inaccurate: According to {\sc Russell \& Norvig}, above story ``cannot be strictly true, because his 1st paper using the term (Bellman1952) appeared before {\sc Wilson} became Secretary of Defense in 1953.'' Also, \href{https://en.wikipedia.org/wiki/Harold_J._Kushner}{\sc Harold J. Kushner} stated in a speech: ``On other hand, when I asked {\sc Bellman} same question, he replied: he was trying to upstage \href{https://en.wikipedia.org/wiki/George_Dantzig}{\sc Dantzig}'s linear programming by adding dynamic. Perhaps both motivations were true.'''' -- \href{https://en.wikipedia.org/wiki/Dynamic_programming}{Wikipedia{\tt/}dynamic programming}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}optimal substructure}
``In CS, a problem is said to have {\it optimal substructure} if an optimal solution can be constructed from optimal solutions of its subproblems. This property is used to determine usefulness of greedy algorithms for a problem.

Typically, a \href{https://en.wikipedia.org/wiki/Greedy_algorithm}{greedy algorithm} is used to solve a problem with optimal substructure if it can be proven by induction that this is optimal at each step. Otherwise, provided problem exhibits \href{https://en.wikipedia.org/wiki/Overlapping_subproblems}{overlapping subproblems} as well, \href{https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm}{divide-\&-conquer} methods or DP may be used. If there are no appropriate greedy algorithms \& problem fails to exhibit overlapping subproblems, often a lengthy but straightforward search of solution space is best alternative.

In application of DP to mathematical optimization, \href{https://en.wikipedia.org/wiki/Richard_Bellman}{\sc Richard Bellman}'s \href{https://en.wikipedia.org/wiki/Bellman_equation#Bellman's_principle_of_optimality}{Principle of Optimality} is based on idea that in order to solve a dynamic optimization problem from some starting period $t$ to some ending period $T$, one implicitly has to solve subproblems starting from later dates $s$, where $t < s < T$. This is an example of optimal substructure. Principle of Optimality is used to derive \href{https://en.wikipedia.org/wiki/Bellman_equation}{Bellman equation}, which shows how value of problem starting from $t$ is related to value of problem starting from $s$.

\subsubsection{Example}
Consider finding a \href{https://en.wikipedia.org/wiki/Shortest_path_problem}{shortest path} for traveling between 2 cities by car, as illustrated in Fig. 1. Such an example is likely to exhibit optimal substructure. I.e., if shortest route from Seattle to Los Angeles passes through Portland \& then Sacramento, then shortest route from Portland to Los Angeles must pass through Sacramento too. I.e., problem of how to get from Portland to Los Angeles is nested inside problem of how to get from Seattle to Los Angeles. (Wavy lines in graph represent solutions to subproblems.)

As an example of a problem that is unlikely to exhibit optimal substructure, consider problem of finding cheapest airline ticket from Buenos Aires to Moscow. Even if that ticket involves stops in Miami \& then London, can't conclude: cheapest ticket from Miami to Moscow stops in London, because price at which an airline sells a multi-flight trip is usually not sum of prices at which it would sell individual flights in trip.

\subsubsection{Def}
A slightly more formal def of optimal substructure can be given. Let a ``problem'' be a collection of ``alternatives'', \& let each alternative have an associated cost $c(a)$. Task: find a set of alternatives that minimizes $c(a)$. Suppose: alternatives can be \href{https://en.wikipedia.org/wiki/Partition_of_a_set}{partitioned} into subsets, i.e., each alternative belongs to only 1 subset. Suppose each subset has its own cost function. Minima of each of these cost functions can be found, as can minima of global cost function, {\it restricted to same subsets}. If these minima match for each subsets, then it's almost obvious that a global minimum can be picked  not out of full set of alternatives, but out of only set that consists of minima of smaller, local cost functions we have defined. If minimizing local functions is a problem of ``lower order'', \& (specifically) if, after a finite number of these reductions, problem becomes trivial, then problem has an optimal substructure.

\subsubsection{Problems with optimal substructure}

\begin{itemize}
	\item \href{https://en.wikipedia.org/wiki/Longest_common_subsequence_problem}{Longest common subsequence problem}
	\item \href{https://en.wikipedia.org/wiki/Longest_increasing_subsequence}{Longest increasing subsequence}
	\item \href{https://en.wikipedia.org/wiki/Longest_palindromic_substring}{Longest palindromic substring}
	\item \href{https://en.wikipedia.org/wiki/Shortest_path_problem#All-pairs_shortest_paths}{All-Pairs Shortest Path}
	\item Any problem that can be solved by DP.
\end{itemize}

\subsubsection{Problems without optimal substructure}

\begin{itemize}
	\item \href{https://en.wikipedia.org/wiki/Longest_path_problem}{Longest path problem}
	\item \href{https://en.wikipedia.org/wiki/Addition-chain_exponentiation}{Addition-chain exponentiation}
	\item {\it Least-cost airline fare.} Using online flight search, will frequently find: cheapest flight from airport A to airport B involves a single connection through airport C, but cheapest flight from airport A to airport C involves a connection through some other airport D. However, if problem takes maximum number of layovers as a parameter, then problem has optimal substructure. Cheapest flight from A to B that involves at most $k$ layovers is either direct flight; or cheapest flight from A to some airport C that involves at most $t$ layovers for some integer $t$ with $0\le t < k$, plus cheapest flight from C to B that involves at most $k - 1 - t$ layovers.'' -- \href{https://en.wikipedia.org/wiki/Optimal_substructure}{Wikipedia{\tt/}optimal substructure}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}overlapping subproblems}
``In CS, a \href{https://en.wikipedia.org/wiki/Computational_problem}{computational problem} is said to have {\it overlapping subproblems} if problem can be broken down into subproblems which are reused several times or a recursive algorithm for problem solves same subproblem over \& over rather than always generating new subproblems.

-- Trong CS, 1 vấn đề tính toán được gọi là có {\it các vấn đề con chồng lấn} nếu vấn đề có thể được chia nhỏ thành các vấn đề con được sử dụng lại nhiều lần hoặc một thuật toán đệ quy để giải quyết cùng 1 vấn đề con nhiều lần thay vì luôn tạo ra các vấn đề con mới.

E.g., problem of computing \href{https://en.wikipedia.org/wiki/Fibonacci_sequence}{Fibonacci sequence} exhibits overlapping subproblems. Problem of computing $n$th \href{https://en.wikipedia.org/wiki/Fibonacci_number}{Fibonacci number} $F(n)$, can be broken down into subproblems of computing $F(n - 1),F(n - 2)$, \& then adding the 2. Subproblem of computing $F(n - 1)$ can itself be broken down into a subproblem that involves computing $F(n - 2)$. Therefore, computation of $F(n - 2)$ is reused, \& Fibonacci sequence thus exhibits overlapping subproblems.

A naive recursive approach to such a problem generally fails due to an \href{https://en.wikipedia.org/wiki/Exponential_time}{exponential complexity}. If problem also shares an \href{https://en.wikipedia.org/wiki/Optimal_substructure}{optimal substructure} property, DP is a good way to work it out.

\subsubsection{Fibonacci sequence example}
In following 2 implementations for calculating \href{https://en.wikipedia.org/wiki/Fibonacci_sequence}{Fibonacci sequence} {\tt fibonacci} uses regular recursion \& \verb|fibonacci_mem| uses \href{https://en.wikipedia.org/wiki/Memoization}{memoization}. \verb|fibonacci_mem| is much more efficient as value for any particular $n$ is computed only once.
\begin{verbatim}
	def fibonacci(n):
	    if n <= 1:
	        return n
	    return fibonacci(n - 1) + fibonacci(n - 2)
	def fibonacci_mem(n, cache):
	    if n <= 1:
	        return n
	    if n in cache:
	        return cache[n]
	    cache[n] = fibonacci_mem(n - 1, cache) + fibonacci_mem(n - 2, cache)
	    return cache[n]
	print(fibonacci_mem(5, {}))  # 5
	print(fibonacci(5))  # 5
\end{verbatim}
When executed, {\tt fibonacci} function computes value of some of numbers in sequence many times over, whereas \verb|fibonacci_mem| reuses value of $n$ which was computed previously [Figs.] Difference in performance may appear minimal with an $n$ value of 5; however, as $n$ increases, \href{https://en.wikipedia.org/wiki/Computational_complexity}{computational complexity} of original {\tt fibonacci} function grows exponentially. In contrast, \verb|fibonacci_mem| version exhibits a more linear increase in complexity.'' -- \href{https://en.wikipedia.org/wiki/Overlapping_subproblems}{Wikipedia{\tt/}overlapping subproblems}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}value function}
``{\it Value function} of an \href{https://en.wikipedia.org/wiki/Optimization_problem}{optimization problem} gives value attained by \href{https://en.wikipedia.org/wiki/Objective_function}{objective function} at a solution, while only depending on \href{https://en.wikipedia.org/wiki/Parameter}{parameters} of problem. In a \href{https://en.wikipedia.org/wiki/Control_theory}{controlled} \href{https://en.wikipedia.org/wiki/Dynamical_system}{dynamical system}, value function represents optimal payoff of system over interval $[t,t_1]$ when started at time-$t$ \href{https://en.wikipedia.org/wiki/State_variable}{state variable} $x(t) = x$. If objective function represents some cost that is to be minimized, value function can be interpreted as cost to finish optimal program, \& is thus referred to as ``cost-to-go function''. In an economic context, where objective function usually represents \href{https://en.wikipedia.org/wiki/Utility}{utility}, value function is conceptually equivalent to \href{https://en.wikipedia.org/wiki/Indirect_utility_function}{indirect utility function}.

In a problem of \href{https://en.wikipedia.org/wiki/Optimal_control}{optimal control}, value function is defined as supremum of objective function taken over set of admissible controls. Given $(t_0,x_0)\in[0,t_1]\times\mathbb{R}^d$, a typical optimal control problem is to
\begin{equation*}
	\mbox{maximize } J(t_0,x_0;u) = \int_{t_0}^{t_1} I(t,x(t),u(t))\,{\rm d}t + \phi(x(t_1))\mbox{ subject to }\frac{dx(t)}{dt} = f(t,x(t),u(t))
\end{equation*}
with initial state variable $x(t_0) = x_0$. Objective function $J(t_0,x_0;u)$ is to be maximized over all admissible controls $u\in U[t_0,t_1]$, where $u$ is a \href{https://en.wikipedia.org/wiki/Measurable_function}{Lebesgue measurable function} from $[t_0,t_1]$ to some prescribed arbitrary set in $\mathbb{R}^m$. Value function is then defined as
\begin{equation*}
	V(t,x(t)) = \max_{u\in U} \int_t^{t_1} I(\tau,x(\tau),u(\tau))\,{\rm d}\tau + \phi(x(t_1)),
\end{equation*}
with $V(t_1,x(t_1)) = \phi(x(t_1))$, where $\phi(x(t_1))$ is ``scrap value''. If optimal pair of control \& state trajectories is $(x^*,u^*)$, then $V(t_0,x_0) = J(t_0,x_0;u^*)$. Function $h$ that gives a optimal control $u^*$ based on current state $x$ is called a {\it feedback control policy}, or simply a {\it policy function}.

{\sc Bellman}'s principle of optimality roughly states: any optimal policy at time $t$, $t_0\le t\le t_1$ taking current state $x(t)$ as ``new'' initial condition must be optimal for remaining problem. If value function happens to be continuous differentiable, this gives rise to an important PDE known as \href{https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation}{Hamilton--Jacobi--Bellman equation}
\begin{equation*}
	-\frac{\partial V(t,x)}{\partial t} = \max_u \left\{I(t,x,u) + \frac{\partial V(t,x)}{\partial x}f(t,xu)\right\},
\end{equation*}
where \href{https://en.wiktionary.org/wiki/maximand}{maximand} on RHS can also be rewritten as \href{https://en.wikipedia.org/wiki/Hamiltonian_(control_theory)}{Hamiltonian} $H(t,x,u,\lambda) = I(t,x,u) + \lambda(t)f(t,x,u)$, as
\begin{equation*}
	-\frac{\partial V(t,x)}{\partial t} = \max_u H(t,x,u,\lambda)
\end{equation*}
with $\partial_xV(t,x) = \lambda(t)$ playing role of \href{https://en.wikipedia.org/wiki/Costate_variable}{costate variables}. Given this def, further have $\frac{d\lambda(t)}{dt} = \frac{\partial^2V(t,x)}{\partial x\partial t} + \frac{\partial^2V(t,x)}{\partial x^2}\cdot f(x)$, \& after differentiating both sides of HJB equation w.r.t. $x$
\begin{equation*}
	-\frac{\partial^2V(t,x)}{\partial t\partial x} = \frac{\partial I}{\partial x} + \frac{\partial^2V(t,x)}{\partial x^2}f(x) + \frac{\partial V(t,x)}{\partial x}\frac{\partial f(x)}{\partial x},
\end{equation*}
which after replacing appropriate terms recovers \href{https://en.wikipedia.org/wiki/Costate_equation}{costate equation}
\begin{equation*}
	-\dot{\lambda}(t) = \partial_xH = \partial_xI + \lambda(t)\partial_xf(x),
\end{equation*}
where $\dot{\lambda}(t)$ is \href{https://en.wikipedia.org/wiki/Newton_notation}{Newton notation} for derivative w.r.t. time.

Value function is unique \href{https://en.wikipedia.org/wiki/Viscosity_solution}{viscosity solution} to Hamilton--Jacobi--Bellman equation. In an \href{https://en.wikipedia.org/wiki/Online_algorithm}{online} closed-loop appropriate optimal control, value function is also a \href{https://en.wikipedia.org/wiki/Lyapunov_function}{Lyapunov function} that establishes global asymptotic stability of closed-loop system.'' -- \href{https://en.wikipedia.org/wiki/Value_function}{Wikipedia{\tt/}value function}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}