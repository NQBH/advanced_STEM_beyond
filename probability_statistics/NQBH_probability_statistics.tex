\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Probability {\it\&} Statistics -- Xác Suất {\it\&} Thống Kê}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Probability \& Statistics -- Xác Suất \& Thống Kê}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/probability_statistics/NQBH_probability_statistics.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/probability_statistics/NQBH_probability_statistics.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Wikipedia}
{\sf Relationship among AL, ML, \& DL}: $\rm DL\subset ML\subset AI$.

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}probability}
``{\it Probability} is the branch of mathematics concerning \href{https://en.wikipedia.org/wiki/Event_(probability_theory)}{events} \& numerical descriptions of how likely they are to occur. The probability of an event is a number $\in[0,1]$; the larger the probability, the more likely an event is to occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the 2 outcomes (``heads'' \& ``tails'') are both equally probable; the probability of ``heads'' equals the probability of ``tails''; \& since no other outcomes are possible, the probability of either ``heads'' or ``tails'' is $\frac{1}{2}$ (which could also be written as $0.5$ or 50\%).

These concepts have been given an \href{https://en.wikipedia.org/wiki/Probability_axioms}{axiomatic} mathematical formalization in \href{https://en.wikipedia.org/wiki/Probability_theory}{\it probability theory}, which is used widely in \href{https://en.wikipedia.org/wiki/Areas_of_study}{areas of study} e.g. statistics, mathematics, science, finance, \href{https://en.wikipedia.org/wiki/Gambling}{gambling}, AI, ML, computer science, \href{https://en.wikipedia.org/wiki/Game_theory}{game theory}, \& philosophy to, e.g., draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics \& regularities of \href{https://en.wikipedia.org/wiki/Complex_systems}{complex systems}.

\subsubsection{Interpretations}
Main article: \href{https://en.wikipedia.org/wiki/Probability_interpretations}{Wikipedia{\tt/}probability interpretations}. When dealing with \href{https://en.wikipedia.org/wiki/Experiment_(probability_theory)}{random experiments} -- i.e., \href{https://en.wikipedia.org/wiki/Experiment}{experiments} that are \href{https://en.wikipedia.org/wiki/Randomness}{random} \& \href{https://en.wikipedia.org/wiki/Well-defined_expression}{well-defined} -- in a purely theoretical setting (like tossing a coin), probabilities can be numerically described by the number of desired outcomes, divided by the total number of all outcomes. This is referred to as {\it theoretical probability} (in contrast to \href{https://en.wikipedia.org/wiki/Empirical_probability}{empirical probability}, dealing with probabilities in the context of real experiments). E.g., tossing a coin twice will yield ``head-head'', ``head-tail'', ``tail-head'', \& ``tail-tail'' outcomes. The probability of getting an outcome of ``head-head'' is 1 out of 4 outcomes, or, in numerical terms, $\frac{1}{4},0.25,25\%$. However, when it comes to practical application, there are 2 major competing categories of probability interpretations, whose adherents hold different views about the fundamental nature of probability:
\begin{itemize}
	\item \href{https://en.wikipedia.org/wiki/Objectivity_(philosophy)}{Objectivists} assign numbers to describe some objective or physical state of affairs. The most popular version of objective probability is \href{https://en.wikipedia.org/wiki/Frequentist_probability}{frequentist probability}, which claims that the probability of a random event denotes the {\it\href{https://en.wikipedia.org/wiki/Frequency_(statistics)}{relative frequency} of occurrence} of an experiment's outcome when the experiment is repeated indefinitely. This interpretation considers probability to be the relative frequency ``in the long run'' of outcomes. A modification of this is \href{https://en.wikipedia.org/wiki/Propensity_probability}{propensity probability}, which interprets probability as the tendency of some experiment to yield a certain outcome, even if it is performed only once.
	\item \href{https://en.wikipedia.org/wiki/Subjective_probability#Objective_and_subjective_Bayesian_probabilities}{Subjectivists} assign numbers per subjective probability, i.e., as a \href{https://en.wikipedia.org/wiki/Credence_(statistics)}{degree of belief}. The degree of belief has been interpreted as ``the price at which you would buy or sell a bet that pays 1 unit of utility of E, 0 if not E'', although that interpretation is not universally agreed upon. The most popular version of subjective probability is \href{https://en.wikipedia.org/wiki/Bayesian_probability}{Bayesian probability}, which includes expert knowledge as well as experimental data to produce probabilities. The expert knowledge is represented by some (subjective) \href{https://en.wikipedia.org/wiki/Prior_probability_distribution}{prior probability distribution}. These data are incorporated in a \href{https://en.wikipedia.org/wiki/Likelihood_function}{likelihood function}. The product of the prior \& the likelihood, when normalized, results in a \href{https://en.wikipedia.org/wiki/Posterior_probability_distribution}{posterior probability distribution} that incorporates all the information known to date. By \href{https://en.wikipedia.org/wiki/Aumann%27s_agreement_theorem}{Aumann's agreement theorem}, Bayesian agents whose prior beliefs are similar will end up with similar posterior beliefs. However, sufficiently different priors can lead to different conclusions, regardless of how much information the agents share.
\end{itemize}

\subsubsection{Etymology}
The word {\it probability} \href{https://en.wikipedia.org/wiki/Etymology}{derives} from the Latin {\it probabilitas}, which can also mean ``\href{https://en.wiktionary.org/wiki/probity}{probity}'', a measure of the \href{https://en.wikipedia.org/wiki/Authority}{authority} of a \href{https://en.wikipedia.org/wiki/Witness}{witness} in a \href{https://en.wikipedia.org/wiki/Legal_case}{legal case} in Europe, \& often correlated with the witness's \href{https://en.wikipedia.org/wiki/Nobility}{nobility}. In a sense, this differs much from the modern meaning of {\it probability}, which in contrast is a measure of the weight of \href{https://en.wikipedia.org/wiki/Empirical_evidence}{empirical evidence}, \& is arrived at from \href{https://en.wikipedia.org/wiki/Inductive_reasoning}{inductive reasoning} \& \href{https://en.wikipedia.org/wiki/Statistical_inference}{statistical inference}.

\subsubsection{History}
Main article: \href{https://en.wikipedia.org/wiki/History_of_probability}{Wikipedia{\tt/}history of probability}, \href{https://en.wikipedia.org/wiki/History_of_statistics}{Wikipedia{\tt/}history of statistics}. The scientific study of probability is a modern development of mathematics. \href{https://en.wikipedia.org/wiki/Gambling}{Gambling} shows that there has been an interest in quantifying the ideas of probability throughout history, but exact mathematical descriptions arose much later. There are reasons for the slow development of the mathematics of probability. Whereas games of chance provided the impetus for the mathematical study of probability, fundamental issues are still obscured by superstitions. [$\ldots$]

\subsubsection{Theory}
Main article: \href{https://en.wikipedia.org/wiki/Probability_theory}{Wikipedia{\tt/}probability theory}. Like other \href{https://en.wikipedia.org/wiki/Theory}{theories}, the \href{https://en.wikipedia.org/wiki/Probability_theory}{theory of probability} is a representation of its concepts in formal terms -- i.e., in terms that can be considered separately from the meaning. These formal terms are manipulated by the rules of mathematics \& logic, \& any results are interpreted or translated back into the problem domain.

There have been at least 2 successful attempts to formalize probability, namely the \href{https://en.wikipedia.org/wiki/Kolmogorov}{\sc Kolmogorov} formulation \& the \href{https://en.wikipedia.org/wiki/Richard_Threlkeld_Cox}{\sc Cox} formulation. In {\sc Kolmogorov}'s formulation (see also \href{https://en.wikipedia.org/wiki/Probability_space}{probability space}), sets are interpreted as \href{https://en.wikipedia.org/wiki/Event_(probability_theory)}{events} \& probability as a \href{https://en.wikipedia.org/wiki/Measure_(mathematics)}{measure} on a class of sets. In \href{https://en.wikipedia.org/wiki/Cox%27s_theorem}{{\sc Cox}'s theorem}, probability is taken as a primitive (i.e., not further analyzed), \& the emphasis is on constructing a consistent assignment of probability values to propositions. In both cases, the \href{https://en.wikipedia.org/wiki/Probability_axioms}{laws of probability} are the same, except for technical details.

There are other methods for quantifying uncertainty, e.g., the \href{https://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory}{Dempster--Shafer theory} or \href{https://en.wikipedia.org/wiki/Possibility_theory}{possibility theory}, but those are essentially different \& not compatible with the usually-understood laws of probability.

\subsubsection{Applications}
Probability theory is applied in everyday life in \href{https://en.wikipedia.org/wiki/Risk}{risk} assessment \& \href{https://en.wikipedia.org/wiki/Statistical_model}{modeling}. The insurance industry \& \href{https://en.wikipedia.org/wiki/Market_(economics)}{markets} use \href{https://en.wikipedia.org/wiki/Actuarial_science}{actuarial science} to determine pricing \& make trading decisions. Governments apply probabilistic methods in \href{https://en.wikipedia.org/wiki/Environmental_regulation}{environmental regulation}, entitlement analysis, \& \href{https://en.wikipedia.org/wiki/Financial_regulation}{financial regulation}.

An example of the use of probability theory in equity trading is the effect of the perceived probability of any widespread Middle East conflict on oil prices, which have ripple effects in the economy as a whole. An assessment by a commodity trader that a war is more likely can send that commodity's prices up or down, \& signals other traders of that opinion. Accordingly, the probabilities are neither assessed independently nor necessarily rationally. The theory of \href{https://en.wikipedia.org/wiki/Behavioral_finance}{behavioral finance} emerged to describe the effect of such \href{https://en.wikipedia.org/wiki/Groupthink}{groupthink} on pricing, on policy, \& on peace \& conflict.

In addition to financial assessment, probability can be used to analyze trends in biology (e.g., disease spread) as well as ecology (e.g., biological \href{https://en.wikipedia.org/wiki/Punnett_squares}{Punnett squares}). As with finance, risk assessment can be used as a statistical tool to calculate the likelihood of undesirable events occurring, \& can assist with implementing protocols to avoid encountering such circumstances. Probability is used to design \href{https://en.wikipedia.org/wiki/Games_of_chance}{games of chance} so that casinos can make a guaranteed profit, yet provide payouts to players that are frequent enough to encourage continued play.

Another significant application of probability theory in everyday life is \href{https://en.wikipedia.org/wiki/Reliability_(statistics)}{reliability}. Many consumer products, e.g., \href{https://en.wikipedia.org/wiki/Automobiles}{automobiles} \& consumer electronics, use reliability theory in product design to reduce the probability of failure. Failure probability may influence a manufacturer's decisions on a product's \href{https://en.wikipedia.org/wiki/Warranty}{warranty}.

The \href{https://en.wikipedia.org/wiki/Cache_language_model}{cache language model} \& other \href{https://en.wikipedia.org/wiki/Statistical_Language_Model}{statistical language models} that are used in \href{https://en.wikipedia.org/wiki/Natural_language_processing}{natural language processing} are also examples of applications of probability theory.

\subsubsection{Mathematical treatment}
[$\ldots$]
\begin{enumerate}
	\item {\bf Independent events.}
	\item {\bf Mutually exclusive events.}
	\item {\bf Not (necessarily) mutually exclusive events.}
	\item {\bf Conditional probability.}
	\item {\bf Inverse probability.}
	\item {\bf Summary of probabilities.}
\end{enumerate}

\subsubsection{Relation to randomness \& probability in quantum mechanics}
Main article: \href{https://en.wikipedia.org/wiki/Randomness}{Wikipedia{\tt/}randomness}. [$\ldots$]'' -- \href{https://en.wikipedia.org/wiki/Probability}{Wikipedia{\tt/}probability}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}universal approximation theorem}
``In the mathematical theory of \href{https://en.wikipedia.org/wiki/Artificial_neural_networks}{artificial neural networks}, {\it universal approximation theorems} are theorems of the following form:

\begin{theorem}[Universal approximation]
	Given a family of neural networks, for each function $f$ from a certain \href{https://en.wikipedia.org/wiki/Function_space}{function space}, there exists a sequence of neural networks $\phi_1,\phi_2,\ldots$ from the family, s.t. $\phi_n\to f$ as $n\to\infty$ according to some criterion, i.e., the family of neural networks is \href{https://en.wikipedia.org/wiki/Dense_set}{dense} in the function space.
\end{theorem}
The most popular version states that \href{https://en.wikipedia.org/wiki/Feedforward_neural_network}{feedforward networks} with non-polynomial \href{https://en.wikipedia.org/wiki/Activation_function}{activation functions} are dense in the space of continuous functions between 2 \href{https://en.wikipedia.org/wiki/Euclidean_space}{Euclidean spaces}, w.r.t. the \href{https://en.wikipedia.org/wiki/Compact_convergence}{compact convergence} topology.

Universal approximation theorems are existence theorems: They simply state that there {\it exists} such a sequence $\phi_n\to f$, \& do not provide any way to actually find such a sequence. They also do not guarantee any method, e.g., \href{https://en.wikipedia.org/wiki/Backpropagation}{backpropagation}, might actually find such a sequence. Any method for searching the space of neural networks, including backpropagation, might find a converging sequence, or not (i.e., the backpropagation might get stuck in a local optimum).

Universal approximation theorems are limit theorems: They simply state that for any $f$ \& a criteria of closeness $\epsilon > 0$, if there are {\it enough} neurons in a neural network, then there exists a neural network with that many neurons that does approximate $f$ to within $\epsilon$. There is no guarantee that any finite size, say, 10000 neurons, is enough.

\subsubsection{Setup}
\href{https://en.wikipedia.org/wiki/Artificial_neural_networks}{Artificial neural networks} are combinations of multiple simple mathematical functions that implement more complicated functions from (typically) real-valued vectors to real-valued vectors. The spaces of multivariate functions that can be implemented by a network are determined by the structure of the network, the set of simple functions, \& its multiplicative parameters. A great deal of theoretical work has gone into characterizing these function spaces.

Most universal approximation theorems are in 1 of 2 classes. The 1st quantifies the approximation capabilities of neural networks with an arbitrary number of artificial neurons (``{\it arbitrary width}'' case) \& the 2nd focuses on the case with an arbitrary number of hidden layers, each containing a limited number of artificial neurons (``{\it arbitrary depth}'' case). In addition to these 2 classes, there are also universal approximation theorems for neural networks with bounded number of hidden layers \& a limited number of neurons in each layer (``{\it bounded depth \& bounded width}'' case).

\subsubsection{History}

\paragraph{Arbitrary width.}

\paragraph{Arbitrary depth.}

\paragraph{Bounded depth \& bounded width.}

\paragraph{Quantitative bounds.}

\paragraph{Kolmogorov network.}

\paragraph{Variants.}

\subsubsection{Arbitrary-width case}
A spate of papers in the 1980s--1990s, from \href{https://en.wikipedia.org/wiki/George_Cybenko}{George Cybenko} \& Kurt Hornik etc., established several universal approximation theorems for arbitrary width \& bounded depth. Most often quoted:

\begin{theorem}[Universal approximation theorem]
	Let $C(X,\mathbb{R}^m)$ denote the set of \href{https://en.wikipedia.org/wiki/Continuous_functions}{continuous functions} from a subset $X$ of a Euclidean $\mathbb{R}^n$ space to a Euclidean space $\mathbb{R}^m$. Let $\sigma\in C(\mathbb{R},\mathbb{R})$. Note that $(\sigma\circ{\bf x})_i = \sigma(x_i)$, so $\sigma\circ{\bf x}$ denotes $\sigma$ applied to each component of ${\bf x}$. Then $\sigma$ is not polynomial iff $\forall m,n\in\mathbb{N}$, \href{https://en.wikipedia.org/wiki/Compact_subspace}{compact} $K\subseteq\mathbb{R}^n$, $f\in C(K,\mathbb{R}^m)$, $\varepsilon > 0$ there exist $k\in\mathbb{N}$, $A\in\mathbb{R}^{k\times n},b\in\mathbb{R}^k,C\in\mathbb{R}^{m\times k}$ s.t. $\sup_{{\bf x}\in K} \|f({\bf x}) - g({\bf x})\| < \varepsilon$ where $g({\bf x})\coloneqq C\cdot(\sigma\circ(A\cdot{\bf x} + b))$.
\end{theorem}
Also, certain non-continuous activation functions can be used to approximate a sigmoid function, which then allows the above theorem to apply to those functions. E.g., the \href{https://en.wikipedia.org/wiki/Step_function}{step function} works. In particular, this shows that a \href{https://en.wikipedia.org/wiki/Perceptron}{perceptron} network with a single infinitely wide hidden layer can approximate arbitrary functions.

Such an $f$ can also be approximated by a network of greater depth by using the same construction for the 1st layer \& approximating the identity function with later layers. {\sf Proof sketch} has not specified how one might use a ramp function to approximate arbitrary functions in $C_0(\mathbb{R}^n,\mathbb{R})$. A sketch of the proof is that one can 1st construct flat bump functions, intersect them to obtain spherical bump functions that approximate the \href{https://en.wikipedia.org/wiki/Dirac_delta_function}{Dirac delta function} $\delta$, then use those to approximate arbitrary functions in $C_0(\mathbb{R}^n,\mathbb{R})$. The original proofs, e.g., the one by Cybenko, use methods from functional analysis, including the \href{https://en.wikipedia.org/wiki/Hahn%E2%80%93Banach_theorem}{Hahn--Banach} \& \href{https://en.wikipedia.org/wiki/Riesz%E2%80%93Markov%E2%80%93Kakutani_representation_theorem}{Riesz --Markov--Kakutani representation} theorems.

Notice also that the neural network is only required to approximate within a compact set $K$. The proof does not describe how the function would be extrapolated outside of the region.

The problem with polynomials may be removed by allowing the outputs of the hidden layers to be multiplied together (the ``pi-sigma $\pi\sigma$ networks''), yielding the generalization:

\begin{theorem}[Universal approximation theorem for pi-sigma $\pi\sigma$ networks]
	With any nonconstant activation function, a 1-hidden-layer pi-sigma network is a universal approximator.
\end{theorem}

\subsubsection{Arbitrary-depth case}
The ``dual'' versions of the theorem consider networks of bounded width \& arbitrary depth. A variant of the universal approximation theorem was proved for the arbitrary depth case by Zhou Lu et al. in 2017. They showed that networks of width $n + 4$ with \href{https://en.wikipedia.org/wiki/ReLU}{ReLU} activation functions can approximate any \href{https://en.wikipedia.org/wiki/Lebesgue_integration}{Lebesgue-integrable function} on $n$-dimensional input space w.r.t. \href{https://en.wikipedia.org/wiki/L1_distance}{$L^1$ distance} if network depth is allowed to grow. It was also shown that if the width was $\le n$, this general expressive power to approximate any Lebesgue integrable function was lost. ReLU networks with width $n + 1$ were sufficient to approximate any continuous function of $n$-dimensional input variables. The following refinement, specifies the optimal minimum width for which such an approximation is possible \& is due to.

\begin{theorem}[Universal approximation theorem ($L_1$ distance, ReLU activation, arbitrary depth, minimal width)]
	For any \href{https://en.wikipedia.org/wiki/Bochner_integral}{Bochner--Lebesgue $p$-integrable} function $f:\mathbb{R}^n\to\mathbb{R}^m$, \& any $\varepsilon > 0$, there exists a \href{https://en.wikipedia.org/wiki/Fully_connected_network}{fully connected ReLU} network $F$ of width exactly $d_m\coloneqq\max\{n + 1,m\}$ satisfying $\int_{\mathbb{R}^n} \|f({\bf x}) - F({\bf x})\|^p\,{\rm d}{\bf x} < \varepsilon$. Moreover, there exists a function $f\in L^p(\mathbb{R}^n,\mathbb{R}^m)$ \& some $\varepsilon > 0$, for which there is no \href{https://en.wikipedia.org/wiki/Fully_connected_network}{fully connected ReLU} network of width $< d_m\coloneqq\max\{n + 1,m\}$ satisfying the above approximation bound.
\end{theorem}

\begin{remark}
	If the activation is replaced by leaky-ReLU, \& the input is restricted in a compact domain, then the exact minimum width is $d_m\coloneqq\max\{m,n,2\}$.
\end{remark}
{\sf Quantitative refinement}: In the case where $f:[0,1]^n\to\mathbb{R}$, i.e., $m = 1$, \& $\sigma$ is the \href{https://en.wikipedia.org/wiki/Rectifier_(neural_networks)}{ReLU activation function}, the exact depth \& width for a ReLU network to achieve $\varepsilon$ error is also known. If, moreover, the target function $f$ is smooth, then the required number of layer \& their width can be exponentially smaller. Even if $f$ is not smooth, the curse of dimensionality can be broken if $f$ admits additional ``compositional structure''.

Together, the central result of
\begin{itemize}
	\item {\sc Kidger, Patrick; Lyons, Terry} (July 2020). {\it Universal Approximation with Deep Narrow Networks}. Conference on Learning Theory. arXiv:1905.08539
\end{itemize}
yields the following universal approximation theorem for networks with bounded width:

\begin{theorem}[Universal approximation theorem (uniform non-\href{https://en.wikipedia.org/wiki/Affine_transformation}{affine} activation, arbitrary \href{https://en.wikipedia.org/wiki/Deep_learning}{depth}, constrained width)]
	Let $\mathcal{X}$ be a \href{https://en.wikipedia.org/wiki/Compact_set}{compact subset} of $\mathbb{R}^d$. Let $\sigma:\mathbb{R}\to\mathbb{R}$ be any non-\href{https://en.wikipedia.org/wiki/Affine_transformation}{affine} continuous function which is \href{https://en.wikipedia.org/wiki/Differentiable_function#Differentiability_classes}{continuously differentiable} at at least 1 point, with nonzero derivative at that point. Let $\mathcal{N}_{d,D:d + D + 2}^\sigma$ denote the space of feed-forward neural networks with $d$ input neurons, $D$ output neurons, \& an arbitrary number of hidden layers each with $d + D + 2$ neurons, s.t. every hidden neuron has activation function $\sigma$ \& every output neuron has the \href{https://en.wikipedia.org/wiki/Identity_function}{identity} as its activation function, with input layer $\phi$ \& output layer $\rho$. Then given any $\varepsilon > 0$ \& any $f\in C(\mathcal{X},\mathbb{R}^D)$, there exists $\hat{f}\in\mathcal{N}_{d,D:d + D + 2}^\sigma$ s.t. $\sup_{{\bf x}\in\mathcal{X}} \|\hat{f}({\bf x}) - f({\bf x})\| < \varepsilon$, i.e., $\mathcal{N}$ is \href{https://en.wikipedia.org/wiki/Dense_set}{dense} in $C(\mathcal{X};\mathbb{R}^D)$ w.r.t. the topology of \href{https://en.wikipedia.org/wiki/Uniform_convergence}{uniform convergence}.
\end{theorem}
{\sf Quantitative refinement}: The number of layers \& width of each layer required to approximate $f$ to $\varepsilon$ precision known; moreover, the result hold true when $\mathcal{X}$ \& $\mathbb{R}^D$ are replaced with any non-positively curved \href{https://en.wikipedia.org/wiki/Riemannian_manifold}{Riemannian manifold}.

Certain necessary conditions for the bounded width, arbitrary depth case have been established, but there is still a gap between the known sufficient \& necessary conditions.

\subsubsection{Bounded depth \& bounded width case}
The 1st result on approximation capabilities of neural networks with bounded number of layers, each containing a limited number of artificial neurons was obtained by Maiorov \& Pinkus. Their remarkable result revealed that such networks can be universal approximators \& for achieving this property 2 hidden layers are enough.

\begin{theorem}[Universal approximation theorem]
	There exists an activation $\sigma$ which is analytic, strictly increasing \& sigmoidal \& has the following property: For any $f\in C([0,1]^d)$ \& $\varepsilon > 0$ there exist constants $d_i,c_{ij},\theta_{ij},\gamma_i$, \& vectors ${\bf w}^{ij}\in\mathbb{R}^d$ for which
	\begin{equation*}
		\left|f({\bf x}) - \sum_{i=1}^{6d + 3} d_{i\sigma}\left(\sum_{j=1}^{3d} c_{ij}\sigma({\bf w}^{ij}\cdot{\bf x} - \theta_{ij}) - \gamma_i\right)\right| < \varepsilon,\ \forall{\bf x} = (x_1,\ldots,x_d)\in[0,1]^d.
	\end{equation*}
\end{theorem}
This is an existence result: activation functions providing universal approximation property for bounded depth bounded width networks exist. Using certain algorithmic \& computer programming techniques, Guliyev \& Ismailov efficiently constructed such activation functions depending on a numerical parameter. The developed algorithm allows one to compute the activation functions at any point of the real axis instantly. For the algorithm \& the corresponding computer code see
\begin{itemize}
	\item {\sc Guliyev, Namig; Ismailov, Vugar} (November 2018). {\it``Approximation capability of two hidden layer feedforward neural networks with fixed weights''}. Neurocomputing. 316: 262--269. arXiv:2101.09181. doi:10.1016/j.neucom.2018.07.075. S2CID 52285996.
\end{itemize}
The theoretical result can be formulated as follows.

\begin{theorem}[Universal approximation theorem]
	$[\ldots]$
\end{theorem}
'' -- \href{https://en.wikipedia.org/wiki/Universal_approximation_theorem}{Wikipedia{\tt/}universal approximation theorem}

%------------------------------------------------------------------------------%

\section{Probability -- Xác Suất}
\textbf{\textsf{Community -- Cộng đồng.}} {\sc Andrey Nikolaevich Kolmogorov}.

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item {\sc Simon J. D. Prince}. {\it Computer Vision: Models, Learning, \& Inference}.
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Statistics -- Thống Kê}
\textbf{\textsf{Community -- Cộng đồng.}} 

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item 
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Stochastic -- Ngẫu Nhiên}
\textbf{\textsf{Community -- Cộng đồng.}} {\sc Caroline Geiersbach}, {\sc Michael Hinterm\"uller}.

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item 
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Data Science (DS)}
\textbf{\textsf{Community -- Cộng đồng.}} 

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item 
\end{enumerate}

\subsection{Wikipedia{\tt/}Data Science}
``{\it Data science} is an \href{https://en.wikipedia.org/wiki/Interdisciplinary}{interdisciplinary} academic field that uses \href{https://en.wikipedia.org/wiki/Statistics}{statistics}, \href{https://en.wikipedia.org/wiki/Scientific_computing}{scientific computing}, \href{https://en.wikipedia.org/wiki/Scientific_method}{scientific methods}, processing, \href{https://en.wikipedia.org/wiki/Scientific_visualization}{scientific visualization}, \href{https://en.wikipedia.org/wiki/Algorithm}{algorithms} \& systems to extract or extrapolate \href{https://en.wikipedia.org/wiki/Knowledge}{knowledge} \& insights from potentially noisy, structured, or \href{https://en.wikipedia.org/wiki/Unstructured_data}{unstructured data}.

Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, \& medicine). Data science is multifaceted \& can be described as a science, a research paradigm, a research method, a discipline, a workflow, \& a profession.

Data science is ``a concept to unify statistics, \href{https://en.wikipedia.org/wiki/Data_analysis}{data analysis}, \href{https://en.wikipedia.org/wiki/Informatics}{informatics}, \& their related \href{https://en.wikipedia.org/wiki/Scientific_method}{methods}'' to ``understand \& analyze actual \href{https://en.wikipedia.org/wiki/Phenomena}{phenomena}'' with \href{https://en.wikipedia.org/wiki/Data}{data}. It uses techniques \& theories drawn from many fields within the context of mathematics, statistics, \href{https://en.wikipedia.org/wiki/Computer_science}{computer science}, \href{https://en.wikipedia.org/wiki/Information_science}{information science}, \& \href{https://en.wikipedia.org/wiki/Domain_knowledge}{domain knowledge}. However, data science is different from \href{https://en.wikipedia.org/wiki/Computer_science}{computer science} \& \href{https://en.wikipedia.org/wiki/Information_science}{information science}. \href{https://en.wikipedia.org/wiki/Turing_Award}{Turing Award} winner \href{https://en.wikipedia.org/wiki/Jim_Gray_(computer_scientist)}{Jim Gray} imagined data science as a ``4th paradigm'' of science (\href{https://en.wikipedia.org/wiki/Empirical_research}{empirical}, \href{https://en.wikipedia.org/wiki/Basic_research}{theoretical}, \href{https://en.wikipedia.org/wiki/Computational_science}{computational}, \& now data-driven) \& asserted that ``everything about science is changing because of the impact of \href{https://en.wikipedia.org/wiki/Information_technology}{information technology}'' \& the \href{https://en.wikipedia.org/wiki/Information_explosion}{data deluge}.

A {\it data scientist} is a professional who creates programming code \& combines it with statistical knowledge to create insights from data.

\subsubsection{Foundations}
Data science is an \href{https://en.wikipedia.org/wiki/Interdisciplinarity}{interdisciplinary} \href{https://en.wikipedia.org/wiki/Academic_discipline}{field} focused on \href{https://en.wikipedia.org/wiki/Knowledge_extraction}{extracting knowledge} from typically \href{https://en.wikipedia.org/wiki/Big_data}{large} \href{https://en.wikipedia.org/wiki/Data_set}{data sets} \& applying the knowledge \& insights from that data to \href{https://en.wikipedia.org/wiki/Problem-solving}{solve problems} in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, \href{https://en.wikipedia.org/wiki/Analysis}{analyzing} data, developing data-driven solutions, \& presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, \href{https://en.wikipedia.org/wiki/Data_visualization}{data visualization}, \href{https://en.wikipedia.org/wiki/Information_visualization}{information visualization}, \href{https://en.wikipedia.org/wiki/Data_sonification}{data sonification}, data \href{https://en.wikipedia.org/wiki/Data_integration}{integration}, \href{https://en.wikipedia.org/wiki/Graphic_design}{graphic design}, \href{https://en.wikipedia.org/wiki/Complex_systems}{complex systems}, \href{https://en.wikipedia.org/wiki/Communication}{communication} \& \href{https://en.wikipedia.org/wiki/Business}{business}. Statistician \href{https://en.wikipedia.org/wiki/Nathan_Yau}{Nathan Yau}, drawing on \href{https://en.wikipedia.org/wiki/Ben_Fry}{Ben Fry}, also links data science to \href{https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction}{human-computer interaction}: users should be able to intuitively control \& \href{https://en.wikipedia.org/wiki/Exploration}{explore} data. In 2015, the \href{https://en.wikipedia.org/wiki/American_Statistical_Association}{American Statistical Association} identified \href{https://en.wikipedia.org/wiki/Database}{database} management, statistics \& \href{https://en.wikipedia.org/wiki/Machine_learning}{machine learning}, \& \href{https://en.wikipedia.org/wiki/Distributed_computing}{distributed \& parallel systems} as the 3 emerging foundational professional communities.

\paragraph{Relationship to statistics.} Many statisticians, including \href{https://en.wikipedia.org/wiki/Nate_Silver}{Nate Silver}, have argued that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems \& techniques unique to digital data. \href{https://en.wikipedia.org/wiki/Vasant_Dhar}{Vasant Dhar} writes that statistics emphasizes quantitative data \& description. In contrast, data science deals with quantitative \& qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) \& emphasizes prediction \& action. \href{https://en.wikipedia.org/wiki/Andrew_Gelman}{Andrew Gelman} of Columbia University has described statistics as a non-essential part of data science.

Stanford professor \href{https://en.wikipedia.org/wiki/David_Donoho}{David Donoho} writes that data science is not distinguished from statistics by the size of datasets or use of computing \& that many graduate programs misleadingly advertise their analytics \& statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.

\subsubsection{Etymology}

\subsubsection{Data science \& data analysis}
Data science \& data analysis are both important disciplines in the field of \href{https://en.wikipedia.org/wiki/Data_management}{data management} \& analysis, but they differ in several key ways. While both fields involve working with data, data science is more of an \href{https://en.wikipedia.org/wiki/Interdisciplinary_field}{interdisciplinary field} that involves the application of statistical, computational, \& \href{https://en.wikipedia.org/wiki/Machine_learning}{machine learning} methods to extract insights from data \& make predictions, while data analysis is more focused on the examination \& interpretation of data to identify patterns \& trends.

Data analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. This can involve tasks such as \href{https://en.wikipedia.org/wiki/Data_cleaning}{data cleaning}, \href{https://en.wikipedia.org/wiki/Data_visualization}{data visualization}, \& exploratory data analysis to gain insights into the data \& develop hypotheses about relationships between \href{https://en.wikipedia.org/wiki/Variable_(research)}{variables}. Data analysts typically use statistical methods to test these hypotheses \& draw conclusions from the data. E.g., a \href{https://en.wikipedia.org/wiki/Data_analyst}{data analyst} might analyze sales data to identify trends in customer behavior \& make recommendations for marketing strategies.

Data science, on the other hand, is a more complex \& \href{https://en.wikipedia.org/wiki/Iterative}{iterative} process that involves working with larger, more complex datasets that often require advanced computational \& statistical methods to analyze. Data scientists often work with \href{https://en.wikipedia.org/wiki/Unstructured_data}{unstructured data} such as text or images \& use machine learning algorithms to build predictive models \& make data-driven decisions. In addition to \href{https://en.wikipedia.org/wiki/Statistical_analysis}{statistical analysis}, data science often involves tasks such as \href{https://en.wikipedia.org/wiki/Data_preprocessing}{data preprocessing}, \href{https://en.wikipedia.org/wiki/Feature_engineering}{feature engineering}, \& model selection. E.g., a data scientist might develop a recommendation system for an e-commerce platform by analyzing user behavior patterns \& using \href{https://en.wikipedia.org/wiki/Machine_learning_algorithms}{machine learning algorithms} to predict user preferences.

While data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development \& implementation of predictive models to make informed decisions. Data scientists are often responsible for collecting \& cleaning data, selecting appropriate analytical techniques, \& deploying models in real-world scenarios. They work at the intersection fo mathematics, computer science, \& \href{https://en.wikipedia.org/wiki/Domain_expertise}{domain expertise} to solve complex problems \& uncover hidden patterns in large datasets.

Despite these differences, data science \& data analysis are closely related fields \& often require similar skills sets. Both fields require a solid foundation in statistics, \href{https://en.wikipedia.org/wiki/Computer_programming}{programming}, \& \href{https://en.wikipedia.org/wiki/Data_visualization}{data visualization}, as well as the ability to communicate findings effectively to both technical \& non-technical audiences. Both fields benefit from \href{https://en.wikipedia.org/wiki/Critical_thinking}{critical thinking} \& \href{https://en.wikipedia.org/wiki/Domain_knowledge}{domain knowledge}, as understanding the context \& nuances of the data is essential for accurate analysis \& modeling.

In summary, data analysis \& data science are distinct yet interconnected disciplines within the broader field of \href{https://en.wikipedia.org/wiki/Data_management}{data management} \& analysis. Data analysis focuses on extracting insights \& drawing conclusions from \href{https://en.wikipedia.org/wiki/Structured_data}{structured data}, while data science involves a more comprehensive approach that combines \href{https://en.wikipedia.org/wiki/Statistical_analysis}{statistical analysis}, computational methods, \& machine learning to extract insights, build predictive models, \& drive data-driven \href{https://en.wikipedia.org/wiki/Decision-making}{decision-making}. Both fields use data to understand patterns, make informed decisions, \& solve complex problems across various domains.

\subsubsection{Cloud computing for data science}
\href{https://en.wikipedia.org/wiki/Cloud_computing}{Cloud computing} can offer access to large amounts of computational power \& \href{https://en.wikipedia.org/wiki/Data_storage}{storage}. In \href{https://en.wikipedia.org/wiki/Big_data}{big data}, where volumes of information are continually generated \& processed, these platforms can be used to handle complex \& resource-intensive analytical tasks.

Some distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process \& analyze large datasets in parallel, which can reducing processing times.

\subsubsection{Ethical consideration in data science}
Data science involve collecting, processing, \& analyzing data which often including personal \& sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, \& negative societal impacts.

Machine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.'' -- \href{https://en.wikipedia.org/wiki/Data_science}{Wikipedia{\tt/}data science}

%------------------------------------------------------------------------------%

\section{Deep Learning (DL)}
\textbf{\textsf{Community -- Cộng đồng.}} {\sc Yann LeCun, Yoshua Bengio, Geoffrey Hinton}.

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{LeCun_Bengio_Hinton2015}. {\sc Yann LeCun, Yoshua Bengio, Geoffrey Hinton}. {\it Deep Learning}.
	\item \cite{Nguyen_Pham2023}. {\sc Phan-Minh Nguyen, Huy Tuan Pham}. {\it A rigorous framework for the mean field limit of multilayer neural networks}.
\end{enumerate}
Những năm gần đây, sự phát triển của các hệ thống tính toán cùng lượng dữ liệu khổng lồ được thu thập bởi các hãng công nghệ lớn đã giúp machine learning tiến thêm 1 bước dài. 1 lĩnh vực mới được ra đời được gọi là {\it học sâu} (deep learning, DL). Deep learning đã giúp máy tính thực thi những việc vào 10 năm trước tưởng chừng là không thể: phân loại cả ngàn vật thể khác nhau trong các bức ảnh, tự tạo chú thích cho ảnh, bắt chước giọng nói \& chữ viết, giao tiếp với con người, chuyển đổi ngôn ngữ, hay thậm chí cả sáng tác văn thơ \& âm nhạc.'' -- \cite[p. 15]{Tiep_ML_co_ban}

{\bf$\Phi$: Start with simple things -- Luôn bắt đầu từ những điều đơn giản.} Khi bắt tay vào giải quyết 1 bài toán ML hay bất cứ bài toán nào, nên bắt đầu từ các thuật toán đơn giản. Không phải chỉ có các thuật toán phức tạp mới có thể giải quyết được vấn đề. Các thuật toán phức tạp thường có yêu cầu cao về khả năng tính toán \& đôi khi nhạy cảm với cách chọn tham số. Ngược lại, các thuật toán đơn giản giúp ta nhanh chóng có 1 bộ khung cho mỗi bài toán. Kết quả của các thuật toán đơn giản cũng mang lại cái nhìn sơ bộ về sự phức tạp của mỗi bài toán. Việc cải thiện kết quả sẽ được thực hiện dần ở các bước sau. '' -- \cite[p. 17]{Tiep_ML_co_ban}

{\bf Approach.} Để giải quyết mỗi bài toán ML, cần chọn 1 mô hình phù hợp. Mô hình này được mô tả bởi bộ các tham số ta cần đi tìm. Thông thường, lượng tham số có thể lên tới hàng triệu \& được tìm bằng cách giải 1 bài toán tối ưu. Khi viết về các thuật toán ML, VKTiệp sẽ bắt đầu từ các ý tưởng trực quan. Các ý tưởng này được mô hình hóa dưới dạng 1 bài toán tối ưu. Các suy luận toán học \& ví dụ mẫu trên Python sẽ giúp hiểu rõ hơn về nguồn gốc, ý nghĩa, \& cách sử dụng mỗi thuật toán. Xen kẽ giữa các thuật toán ML, trình bày các kỹ thuật tối ưu cơ bản, với hy vọng giúp hiểu rõ hơn bản chất của vấn đề.

{\bf Audiences.} Cuốn sách được thực hiện hướng tới nhiều nhóm độc giả khác nhau. Nếu không thực sự muốn đi sâu vào phần toán, vẫn có thể tham khảo mã nguồn \& cách sử dụng các thư viện. Nhưng để sử dụng các thư viện 1 cách hiệu quả, cũng cần hiểu nguồn gốc của mô hình \& ý nghĩa của các tham số. Còn nếu thực sự muốn tìm hiểu nguồn gốc, ý nghĩa của các thuật toán, có thể học được nhiều điều từ cách xây dựng \& tối ưu các mô hình.

{\bf Python.} Python là 1 ngôn ngữ lập trình miễn phí, có thể được cài đặt dễ dàng trên các nền tảng hệ điều hành khác nhau. Có rất nhiều thư viện hỗ trợ ML cũng như DL trên Python. Có 2 thư viện Python chính thường được sử dụng là {\tt numpy, scikit-learn}.
\begin{itemize}
	\item {\tt numpy} \url{www.numpy.org} là 1 thư viện phổ biến giúp xử lý các phép toán liên quan đến các mảng nhiều chiều, hỗ trợ các hàm gần gũi với đại số tuyến tính. Cách xử lý các mảng nhiều chiều.
	\item {\tt scikit-learn/sklearn} \url{scikit-learn.org}: 1 thư viện chứa đầy đủ các thuật toán ML cơ bản \& rất dễ sử dụng. Tài liệu của scikit-learn cũng là 1 nguồn tham khảo chất lượng cho MLer. Scikit-learn được dùng để kiểm chứng các suy luận toán học \& các mô hình được xây dựng thông qua {\tt numpy}.
\end{itemize}
{\bf Inevitability of mathematics in ML.} Có rất nhiều thư viện giúp tạo ra các sản phẩm ML{\tt/}DL mà không yêu cầu nhiều kiến thức toán. Hướng tới việc giúp hiểu bản chất toán học đằng sau mỗi mô hình trước khi áp dụng các thư viện sẵn có. Việc sử dụng thư viện $+$ yêu cầu kiến thức nhất định về việc lựa chọn mô hình \& điều chỉnh các tham số.

%------------------------------------------------------------------------------%

\section{Machine Learning (ML)}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item {\sc Andrew Ng}. {\it Machine Learning Course} on Coursera.
	\item Machine Learning Mastery: Making Developers Awesome at Machine Learning: \url{https://machinelearningmastery.com}.
	\begin{itemize}
		\item \href{https://machinelearningmastery.com/inspirational-applications-deep-learning/}{Machine Learning Mastery{\tt/}8 Inspirational Applications of Deep Learning}.
	\end{itemize}
	\item Machine Learning cơ bản:
	 \url{https://machinelearningcoban.com/}.
	\item \cite{Tiep_ML_co_ban}. {\sc Vũ Hữu Tiệp}. {\it Machine Learning Cơ Bản}.
	
	Mã nguồn cuốn ebook ``Machine Learning Cơ Bản'': \url{https://github.com/tiepvupsu/ebookMLCB}.
\end{enumerate}

\begin{definition}
	``\emph{Machine learning (ML)} is a field of study in \href{https://en.wikipedia.org/wiki/Artificial_intelligence}{AI} concerned with the development \& study of \href{https://en.wikipedia.org/wiki/Computational_statistics}{statistical algorithms} that can learn from \href{https://en.wikipedia.org/wiki/Data}{data} \& generalize to unseen data, \& thus perform \href{https://en.wikipedia.org/wiki/Task_(computing)}{tasks} without explicit \href{https://en.wikipedia.org/wiki/Machine_code}{instructions}. Quick progress in the fields of \href{https://en.wikipedia.org/wiki/Deep_learning}{deep learning}, beginning in 2010s, allowed neural networks to surpass many previous approaches in performance.'' -- \href{https://en.wikipedia.org/wiki/Machine_learning}{Wikipedia{\tt/}machine learning}
\end{definition}

\begin{dinhnghia}
	``\emph{Học máy} (machine learning, ML) là 1 tập con của trí tuệ nhân tạo. Machine learning là 1 lĩnh vực nhỏ trong Khoa học Máy tính, có khả năng tự học hỏi dựa trên dữ liệu được đưa vào mà không cần phải được lập trình cụ thể: ``Machine Learning is the subfield of computer science, that ``gives computers the ability to learn without being explicitly programmed'' -- Wikipedia.'' -- \cite[p. 15]{Tiep_ML_co_ban}
\end{dinhnghia}
``ML finds application in many fields, including \href{https://en.wikipedia.org/wiki/Natural_language_processing}{natural language processing}, \href{https://en.wikipedia.org/wiki/Computer_vision}{computer vision}, \href{https://en.wikipedia.org/wiki/Speech_recognition}{speech recognition}, \href{https://en.wikipedia.org/wiki/Email_filtering}{email filtering}, \href{https://en.wikipedia.org/wiki/Agriculture}{agriculture}, \& \href{https://en.wikipedia.org/wiki/Medicine}{medicine}. The application of ML to business problems is known as \href{https://en.wikipedia.org/wiki/Predictive_analytics}{predictive analysis}.

Statistics \& mathematical optimization{\tt/}mathematical programming methods comprise the foundations of machine learning. \href{https://en.wikipedia.org/wiki/Data_mining}{Data mining} is related field of study, focusing on \href{https://en.wikipedia.org/wiki/Exploratory_data_analysis}{exploratory data analysis} (EDA) via \href{https://en.wikipedia.org/wiki/Unsupervised_learning}{unsupervised learning}.

From a theoretical viewpoint, \href{https://en.wikipedia.org/wiki/Probably_approximately_correct_learning}{probably approximately correct (PAC) learning} provides a framework for describing machine learning.'' -- \href{https://en.wikipedia.org/wiki/Machine_learning}{Wikipedia{\tt/}machine learning}

{\bf Relationships of ML to AI.} As a scientific endeavor, machine learning grew out of the quest for AI. In the early days of AI as an \href{https://en.wikipedia.org/wiki/Discipline_(academia)}{academic discipline}, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ``\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{neural networks}''; these were mostly \href{https://en.wikipedia.org/wiki/Perceptron}{perceptrons} \& other models e.g. \href{https://en.wikipedia.org/wiki/ADALINE}{ADALINE} that were later found to be reinventions of the \href{https://en.wikipedia.org/wiki/Generalized_linear_model}{generalized linear models} of statistics. \href{https://en.wikipedia.org/wiki/Probabilistic_reasoning}{Probabilistic reasoning} was also employed, especially in \href{https://en.wikipedia.org/wiki/Automated_medical_diagnosis}{automated medical diagnosis}. However, an increasing emphasis on the \href{https://en.wikipedia.org/wiki/Symbolic_AI}{logical, knowledge-based approach} caused a rift between AI \& machine learning. Probabilistic systems were plagued by theoretical \& practical problems of data acquisition \& representation.

\subsection{Wikipedia{\tt/}Machine Learning}
``{\it Machine learning} (ML) is a \href{https://en.wikipedia.org/wiki/Field_of_study}{field of study} in AI concerned with the development \& study of \href{https://en.wikipedia.org/wiki/Computational_statistics}{statistical algorithms} that can learn from \href{https://en.wikipedia.org/wiki/Data}{data} \& \href{https://en.wikipedia.org/wiki/Generalize}{generalize} to unseen data, \& thus perform \href{https://en.wikipedia.org/wiki/Task_(computing)}{tasks} without explicit \href{https://en.wikipedia.org/wiki/Machine_code}{instructions}. Quick progress in the field of \href{https://en.wikipedia.org/wiki/Deep_learning}{deep learning}, beginning in 2010s, allowed neutral networks to surpass many previous approaches in performance.

ML finds application in many fields, including \href{https://en.wikipedia.org/wiki/Natural_language_processing}{natural language processing}, \href{https://en.wikipedia.org/wiki/Computer_vision}{computer vision}, \href{https://en.wikipedia.org/wiki/Speech_recognition}{speed recognition}, \href{https://en.wikipedia.org/wiki/Email_filtering}{email filtering}, \href{https://en.wikipedia.org/wiki/Agriculture}{agriculture}, \& \href{https://en.wikipedia.org/wiki/Medicine}{medicine}. The application of ML to business problems is known as \href{https://en.wikipedia.org/wiki/Predictive_analytics}{predictive analytics}.

Statistics \& \href{https://en.wikipedia.org/wiki/Mathematical_optimization}{mathematical optimization{\tt/}programming} methods comprise the foundations of machine learning. \href{https://en.wikipedia.org/wiki/Data_mining}{Data mining} is a related field of study, focusing on \href{https://en.wikipedia.org/wiki/Exploratory_data_analysis}{exploratory data analysis} (EDA) via \href{https://en.wikipedia.org/wiki/Unsupervised_learning}{unsupervised learning}.

From a theoretical viewpoint, \href{https://en.wikipedia.org/wiki/Probably_approximately_correct_learning}{probably approximately correct (PAC) learning} provides a framework for describing machine learning.

\subsubsection{History}

\subsubsection{Relationships to other fields}

\paragraph{AI.} As a scientific endeavor, machine learning grew out of the quest for AI. In the early days of AI as an \href{https://en.wikipedia.org/wiki/Discipline_(academia)}{academic discipline}, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ``\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{neural networks}''; these were mostly \href{https://en.wikipedia.org/wiki/Perceptron}{perceptrons} \& \href{https://en.wikipedia.org/wiki/ADALINE}{other models ADALINE} that were later found to be reinventions of the \href{https://en.wikipedia.org/wiki/Generalized_linear_model}{generalized linear models} of statistics. \href{https://en.wikipedia.org/wiki/Probabilistic_reasoning}{Probablistic reasoning} was also employed, especially in \href{https://en.wikipedia.org/wiki/Automated_medical_diagnosis}{automated medical diagnosis}.

However, an increasing emphasis on the \href{https://en.wikipedia.org/wiki/Symbolic_AI}{logical, knowledge-based approach} caused a rift between AI \& machine learning. Probabilistic systems were plagued by theoretical \& practical problems of data acquisition \& representation. By 1980, \href{https://en.wikipedia.org/wiki/Expert_system}{expert systems} had come to dominate AI, \& statistics was out of favor. Work on symbolic{\tt/}knowledge-based learning did continue within AI, leading to \href{https://en.wikipedia.org/wiki/Inductive_logic_programming}{inductive logic programming} (ILP), but the more statistical line of research was now outside the field of AI proper, in \href{https://en.wikipedia.org/wiki/Pattern_recognition}{pattern recognition} \& \href{https://en.wikipedia.org/wiki/Information_retrieval}{information retrieval}. Neural networks research had been abandoned by AI \& computer science around the same time. This line, too, was continued outside the AI{\tt/}CS field, as ``\href{https://en.wikipedia.org/wiki/Connectionism}{connectionism}'', by researchers from other disciplines including \href{https://en.wikipedia.org/wiki/John_Hopfield}{John Hopfield}, \href{https://en.wikipedia.org/wiki/David_Rumelhart}{David Rumelhart}, \& \href{https://en.wikipedia.org/wiki/Geoffrey_Hinton}{Geoffrey Hinton}. Their main success came in the mid-1980s with the reinvention of \href{https://en.wikipedia.org/wiki/Backpropagation}{backpropagation}.

Machine learning (ML), reorganized \& recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving AI to tackling solvable problems of a practical nature. It shifted focus away from the \href{https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence}{symbolic approaches} it had inherited from AI, \& toward methods \& models borrowed from statistics, \href{https://en.wikipedia.org/wiki/Fuzzy_logic}{fuzzy logic}, \& \href{https://en.wikipedia.org/wiki/Probability_theory}{probability theory}.

\paragraph{Data compression.} Main: \href{https://en.wikipedia.org/wiki/Data_compression#Machine_learning}{Wikipedia{\tt/}data compression{\tt/}ML}. There is a close connection between ML \& compression. A system that predicts the \href{https://en.wikipedia.org/wiki/Posterior_probabilities}{posterior probabilities} of a sequence given its entire history can be used for optimal data compression (by using \href{https://en.wikipedia.org/wiki/Arithmetic_coding}{arithmetic coding} on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for ``general intelligence''.

An alternative view can show compression algorithms implicitly map strings into implicit \href{https://en.wikipedia.org/wiki/Feature_space_vector}{feature space vectors}, \& compression-based similarity measures compute similarity within these feature spaces. For each compressor $C(\cdot)$ we define an associated vector space $\mathfrak{N}$ s.t. $C(\cdot)$ maps an input string $x$, corresponding to the vector norm $\|\widetilde{x}\|$. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine 3 representative lossless compression methods, LZW, LZ77, \& PPM.

According to \href{https://en.wikipedia.org/wiki/AIXI}{AIXI} theory, a connection more directly explained in \href{https://en.wikipedia.org/wiki/Hutter_Prize}{Hutter Prize}, the best possible compression of $x$ is the smallest possible software that generates $x$. E.g., in that model, a zip file's compressed size includes both the zip file \& the unzipping software, since you cannot unzip it without both, but there may be an even smaller combined form.

Examples of AI-powered audio{\tt/}video compression software include \href{https://en.wikipedia.org/wiki/NVIDIA_Maxine}{NVIDIA Maxine}, AIVC. Examples of software that can perform AI-powered image compression include \href{https://en.wikipedia.org/wiki/OpenCV}{OpenCV}, \href{https://en.wikipedia.org/wiki/TensorFlow}{TensorFlow}, \href{https://en.wikipedia.org/wiki/MATLAB}{MATLAB}'s Imaging Processing Toolbox (IPT) \& High-Fidelity Generative Image Compression.

In \href{https://en.wikipedia.org/wiki/Unsupervised_machine_learning}{unsupervised machine learning}, \href{https://en.wikipedia.org/wiki/K-means_clustering}{$k$-mean clustering} can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels \& finds widespread use in fields such as \href{https://en.wikipedia.org/wiki/Image_compression}{image compression}.

Data compression aims to reduce the size of data files, enhancing storage efficiency \& speeding up data transmission. $k$-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, $k$, each represented by the \href{https://en.wikipedia.org/wiki/Centroid}{centroid} of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in \href{https://en.wikipedia.org/wiki/Image_processing}{image} \& \href{https://en.wikipedia.org/wiki/Signal_processing}{signal processing}, $k$-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.

\href{https://en.wikipedia.org/wiki/Large_language_model}{Large language models} (LLMs) are also capable of lossless data compression, as demonstrated by \href{https://en.wikipedia.org/wiki/DeepMind}{DeepMind}'s research with the Chinchilla 70B model. Developed by DeepMind, Chinchilla 70B effectively compressed data, outperforming conventional methods such as \href{https://en.wikipedia.org/wiki/Portable_Network_Graphics}{Portable Network Graphics} (PNG) for images \& \href{https://en.wikipedia.org/wiki/Free_Lossless_Audio_Codec}{Free Lossless Audio Codec} (FLAC) for audio. It achieved compression of image \& audio data to 43.4\% \& 16.4\% of their original sizes, respectively.

\paragraph{Data mining.} ML \& \href{https://en.wikipedia.org/wiki/Data_mining}{data mining} often employ the same methods \& overlap significantly, but while ML focuses on prediction, based on {\it known} properties learned from the training data, data mining focuses on the \href{https://en.wikipedia.org/wiki/Discovery_(observation)}{discovery} of (previously) {\it unknown} properties in the data (this is the analysis step of \href{https://en.wikipedia.org/wiki/Knowledge_discovery}{knowledge discovery} in databases). Data mining uses many ML methods, but with different goals; on the other hand, ML also employs data mining methods as ``\href{https://en.wikipedia.org/wiki/Unsupervised_learning}{unsupervised learning}'' or as a preprocessing step to improve learner accuracy. Much of the confusion between these 2 research communities (which do often have separate conferences \& separate journals, \href{https://en.wikipedia.org/wiki/ECML_PKDD}{ECML PKDD} being a major exception) comes from the basic assumptions they work with: in ML, performance is usually evaluated w.r.t. the ability to {\it reproduce known} knowledge, while in knowledge discovery \& data mining (KDD) the key task is the discovery of previously {\it unknown} knowledge. Evaluated w.r.t. known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.

ML also has intimate ties to optimization: Many learning problems are formulated as minimization of some \href{https://en.wikipedia.org/wiki/Loss_function}{loss function} on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained \& the actual problem instances (e.g., in classification, one wants to assign a \href{https://en.wikipedia.org/wiki/Labeled_data}{label} to instances, \& models are trained to correctly predict the preassigned labels of a set of examples).

\paragraph{Generalization.} Characterizing the generalization of various learning algorithms is an active topic of current research, especially for \href{https://en.wikipedia.org/wiki/Deep_learning}{deep learning} algorithms.

\paragraph{Statistics.} ML \& statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population \href{https://en.wikipedia.org/wiki/Statistical_inference}{inferences} from a \href{https://en.wikipedia.org/wiki/Sample_(statistics)}{sample}, while ML finds generalizable predictive patterns. According to \href{https://en.wikipedia.org/wiki/Michael_I._Jordan}{Michael I. Jordan}, the ideas of ML, from methodological principles to theoretical tools, have had a long pre-history in statistics. he also suggested the term data science as a placeholder to call the overall field.

Conventional statistical analyzes require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, ML is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.

\href{https://en.wikipedia.org/wiki/Leo_Breiman}{Leo Breiman} distinguished 2 statistical modeling paradigms: data model \& algorithmic model, wherein ``algorithm model'' means more or less the ML algorithms like \href{https://en.wikipedia.org/wiki/Random_forest}{Random Forest}.

Some statisticians have adopted methods from ML, leading to a combined field that they call {\it statistical learning}.

\paragraph{Statistical physics.} Analytical \& computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including ML, e.g., to analyze the weight space of \href{https://en.wikipedia.org/wiki/Deep_neural_network}{deep neural networks}. Statistical physics is thus finding applications in the area of \href{https://en.wikipedia.org/wiki/Medical_diagnostics}{medical diagnostics}.

\subsubsection{Theory}
Main: \href{https://en.wikipedia.org/wiki/Computational_learning_theory}{Wikipedia{\tt/}computational learning theory} \& \href{https://en.wikipedia.org/wiki/Statistical_learning_theory}{Wikipedia{\tt/}statistical learning theory}. A core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples{\tt/}tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) \& the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.

The computational analysis of ML algorithms \& their performance is a branch of \href{https://en.wikipedia.org/wiki/Theoretical_computer_science}{theoretical computer science} known as \href{https://en.wikipedia.org/wiki/Computational_learning_theory}{computational learning theory} via the \href{https://en.wikipedia.org/wiki/Probably_approximately_correct_learning}{Probably Approximately Correct Learning} (PAC) model. Because training sets are finite \& the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The \href{https://en.wikipedia.org/wiki/Bias%E2%80%93variance_decomposition}{bias-variance decompositon} is 1 way to quantify generalization \href{https://en.wikipedia.org/wiki/Errors_and_residuals}{error}.

For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of he function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to \href{https://en.wikipedia.org/wiki/Overfitting}{overfitting} \& generalization will be poorer.

In addition to performance bounds, learning theorists study the time complexity \& feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in \href{https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time}{polynomial time}. There are 2 kinds of \href{https://en.wikipedia.org/wiki/Time_complexity}{time complexity} results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time. 

\subsubsection{Approaches}
ML approaches are traditionally divided into 3 broad categories, which corresponding to learning paradigms, depending on the nature of the ``signal'' or ``feedback'' available to the learning system:
\begin{itemize}
	\item \href{https://en.wikipedia.org/wiki/Supervised_learning}{Supervised learning}: The computer is presented with example inputs \& their desired outputs, given by a ``teacher'', \& the goal to learn a general rule that maps inputs to outputs.
	\item \href{https://en.wikipedia.org/wiki/Unsupervised_learning}{Unsupervised learning}: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means toward an end (\href{https://en.wikipedia.org/wiki/Feature_learning}{feature learning}).
	\item \href{https://en.wikipedia.org/wiki/Reinforcement_learning}{Reinforcement learning}: A computer program interacts with a dynamic environment in which it must perform a certain goal (e.g. \href{https://en.wikipedia.org/wiki/Autonomous_car}{driving a vehicle} or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize.
\end{itemize}
Although each algorithm has advantages \& limitations, no single algorithm works for all problems.

\paragraph{Supervised learning.}

\paragraph{Semi-supervised learning.}

\paragraph{Reinforcement learning.}

\paragraph{Dimensionality reduction.}

\paragraph{Other types.}

'' -- \href{https://en.wikipedia.org/wiki/Machine_learning}{Wikipedia{\tt/}machine learning}

%------------------------------------------------------------------------------%

\section{Artificial Intelligence (AI)}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Bac_Viet_AI}. {\sc Lê Hoài Bắc, Tô Hoài Việt}. {\it Cơ Sở Trí Tuệ Nhân Tạo}.
	\item \cite{Aoun_robot-proof}. {\sc Joseph E. Aoun}. {\it Robot-Proof: Higher Education in the Age of Artificial Intelligence}.
	\item \cite{Aoun_robot-proof_VN}. {\sc Joseph E. Aoun}. {\it Robot-Proof: Higher Education in the Age of Artificial Intelligence -- Chạy Đua Với Robot: Học Tập Thời Trí Tuệ Nhân Tạo}.
\end{enumerate}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}