\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{Bài toán}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Lecture Note: Introduction to Artificial Intelligence\\Bài Giảng: Nhập Môn Trí Tuệ Nhân Tạo}
\author{Nguyễn Quản Bá Hồng\footnote{A scientist- {\it\&} creative artist wannabe, a mathematics {\it\&} computer science lecturer of Department of Artificial Intelligence {\it\&} Data Science (AIDS), School of Technology (SOT), UMT Trường Đại học Quản lý {\it\&} Công nghệ TP.HCM, Hồ Chí Minh City, Việt Nam.\\E-mail: {\sf nguyenquanbahong@gmail.com} {\it\&} {\sf hong.nguyenquanba@umt.edu.vn}. Website: \url{https://nqbh.github.io/}. GitHub: \url{https://github.com/NQBH}.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Lecture Note: Introduction to Artificial Intelligence -- Bài Giảng: Nhập Môn Trí Tuệ Nhân Tạo}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/AI/lecture/NQBH_introduction_AI_lecture.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/AI/lecture/NQBH_introduction_AI_lecture.tex}.
		\item {\it Codes}:
		\begin{itemize}
			\item C{\tt/}C++:
			\item Python: \url{https://github.com/NQBH/advanced_STEM_beyond/tree/main/AI/Python}.
		\end{itemize}
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basic}

\subsection{Gradient -- Độ dốc}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{enumerate}
	\item \cite{Tiep_ML_co_ban}. {\sc Vũ Hữu Tiệp}. {\it Machine Learning Cơ Bản}. Chap. 12: Gradient Descent.
\end{enumerate}

\begin{vidu}[\cite{Tiep_ML_co_ban}, p. 160]
	Xét hàm số $f(x) = x^2 + 5\sin x$, $f\in C(\mathbb{R}$ có đạo hàm $f'(x) = 2x + 5\cos x$. Giả sử xuất phát từ 1 điểm $x_0$, quy tắc cập nhật tại vòng lặp thứ $t$ là
	\begin{equation*}
		x_{t+1} = x_t - \eta f'(x_t) = x_t - \eta(2x_t + 5\cos x_t).
	\end{equation*}
	Codes:
	\begin{itemize}
		\item Python:
		\begin{verbatim}
import math
import numpy as np

# f(x) = x^2 + 5sin x
def f(x):
    return x**2 + 5*np.sin(x)

def df(x): # derivative f'(x) of f(x)
    return 2*x + 5 * np.cos(x)

x = float(input("x = "))
print("f(x) = ", f(x))
print("df(x) = ", df(x))

tol = 1e-3 # tolerance: just a small number

def gradient_descent(x0, eta): # x0: starting point, eta: learning rate
    x = [x0]
    for i in range(100):
        x_new = x[-1] - eta*df(x[-1]) # x_new: x_{t+1}, x[-1]: x_t
        if abs(df(x_new)) < tol:
            break
        x.append(x_new)
    return(x, i)

x0 = float(input("x0 = "))
eta = float(input("eta = "))
if eta <= 0:
    print("error: eta must be positive!")
else:
    print(gradient_descent(x0, eta))
		\end{verbatim}
	\end{itemize}
\end{vidu}

\begin{baitoan}
	Xét hàm số $f(x) = x^3 + 3x^2 + 5\sin x - 7\cos x + \sqrt{2}e^{-2x}$. Viết chương trình {\sf C{\tt/}C++, Python} để: (a) Tính hàm $f(x),f'(x)$ với $x\in\mathbb{R}$ được nhập từ bàn phím. (b) Viết hàm gradient descent theo công thức
	\begin{equation*}
		x_{t+1} = x_t - \eta f'(x_t),
	\end{equation*}
	với $\eta\in(0,\infty)$ được gọi là {\rm tốc độ học (learning rate)}.
\end{baitoan}

\begin{proof}
	Dễ thấy $f(x)$ là 1 hàm liên tục trên $\mathbb{R}$, i.e., $f\in C(\mathbb{R}$ , \& có đạo hàm $f'(x) = 3x^2 + 6x + 5\cos x + 7\sin x - 2\sqrt{2}e^{-2x}$.
	
	Code Python:
	\begin{verbatim}
# f1(x) = x^3 + 3x^2  + 5sin x - 7cos x + sqrt{2}e^{-2x}
def f1(x):
    return x**3 + 3*x**2 + 5*np.sin(x) - 7*np.cos(x) + np.sqrt(2)*np.exp(-2*x)

def df1(x):
    return 3*x**2 + 6*x + 5*np.cos(x) + 7*np.sin(x) - 2*np.sqrt(2)*np.exp(-2*x)

x = float(input("x = "))
print("f(x) = ", f(x))
print("df(x) = ", df(x))

tol = 1e-3 # tolerance: just a small number

def gradient_descent_f1(x0, eta): # x0: starting point, eta: learning rate
    x = [x0]
    for i in range(100):
        x_new = x[-1] - eta*df1(x[-1]) # x_new: x_{t+1}, x[-1]: x_t
        if abs(df1(x_new)) < tol:
            break
        x.append(x_new)
    return(x, i)

x0 = float(input("x0 = "))
eta = float(input("eta = "))
if eta <= 0:
    print("error: eta must be positive!")
else:
    print(gradient_descent_f1(x0, eta))
	\end{verbatim}
\end{proof}

\begin{remark}
	Có thể tham khảo các công thức tính đạo hàm ở \href{https://en.wikibooks.org/wiki/Calculus/Tables_of_Derivatives}{Wikipedia{\tt/}tables of derivatives}.
\end{remark}

\begin{baitoan}
	Xét hàm số $f(x,y) = 2x^3y^2 + \dfrac{\sqrt{x^3}}{y} + \sin(x^2y) + e^{\cos(xy^2)}$. Viết chương trình {\sf C{\tt/}C++, Python} để: (a) Tính hàm $f(x,y),\nabla f(x,y)$ với $x,y\in\mathbb{R}$ được nhập từ bàn phím. (b) Viết hàm gradient descent cho 2 trường hợp:
	\begin{equation*}
		(x_{t+1},y_{t+1}) = (x_t,y_t) - \eta\nabla f(x_t,y_t),
	\end{equation*}
	or
	\begin{equation*}
		\left\{\begin{split}
			x_{t+1} = x_t - \boldsymbol{\alpha}\cdot\nabla f(x_t,y_t) = x_t - \alpha_1\partial_xf(x_t,y_t) - \alpha_2\partial_xf(x_t,y_t),\\
			y_{t+1} = y_t - \boldsymbol{\beta}\cdot\nabla f(x_t,y_t) = x_t - \beta_1\partial_xf(x_t,y_t) - \beta_2\partial_xf(x_t,y_t),
		\end{split}\right.
	\end{equation*}
	Python:
	\begin{verbatim}
# f(x,y) = 2x^3y^2 + sqrt(x^3)/y + sin(x^2y) + e^{cos(xy^2)}

def f(x, y):
    return 2*x**3*y**2 + np.sqrt(x**3)/y + np.sin(x**2 * y) + np.exp(np.cos(x * y**2))

def grad_f(x, y):
    df_dx = 6*x**2 * y**2 + (3/2) * x**0.5 / y + 2*x*y * np.cos(x**2 * y) - y**2 * np.sin(x * y**2) * np.exp(np.cos(x * y**2))
    df_dy = 4*x**3 * y - np.sqrt(x**3) / y**2 + x**2 * np.cos(x**2 * y) - 2*x*y * np.sin(x * y**2) * np.exp(np.cos(x * y**2))
    return np.array([df_dx, df_dy])

x = float(input("x = "))
y = float(input("y = "))
print("f(x,y) = ", f(x,y))
print("grad f(x,y) = ", grad_f(x,y))
	\end{verbatim}
\end{baitoan}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}