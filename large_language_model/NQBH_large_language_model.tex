\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Large Language Models -- Mô Hình Ngôn Ngữ Lớn}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Large Language Models -- Mô Hình Ngôn Ngữ Lớn}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/large_language_model/NQBH_large_language_model.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/large_language_model/NQBH_large_language_model.tex}.
		\item {\it }.
		
		PDF: {\sc url}: \url{.pdf}.
		
		\TeX: {\sc url}: \url{.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basics LLMs}

\subsection{\cite{Raschka2024}. {\sc Sebastian Raschka}. Build A Large Language Model (From Scratch)}
{\sf[125 Amazon ratings]}
\begin{itemize}
	\item {\sf Amazon review.} Learn how to create, train, \& tweak LLMs by building 1 from ground up! In {\it Build a Large Language Model (from Scratch)} bestselling author {\sc Sebastian Raschka} guides you step by step through creating your own LLM. Each stage is explained with clear text, diagrams, \& examples. Go from initial design \& creation, to pretraining on a general corpus, \& on to fine-tuning for specific tasks.
	
	{\it Build a Large Language Model (from Scratch)} teaches how to:
	\begin{itemize}
		\item Plan \& code all parts of an LLM
		\item Prepare a dataset suitable for LLM training
		\item Fine-tune LLMs for text classification \& with your own data
		\item Use human feedback to ensure your LLM follows instructions
		\item Load pretrained weights into an LLM
	\end{itemize}
	{\it Build a Large Language Model (from Scratch)} takes you inside AI black box to tinker with internal systems that power generative AI. As work through each key stage of LLM creation, develop an in-depth understanding of how LLMs work, their limitations, \& their customization methods. Your LLM can be developed on an ordinary laptop, \& used as your own personal assistant.
	
	{\bf About technology.} Physicist {\sc Richard P. Feynman} reportedly said, ``I don't understand anything I can't build.'' Based on this same powerful principle, bestselling author {\sc Sebastian Raschka} guides you step by step as build a GPT-style LLM that can run on laptop. This is an engaging book that covers each stage of process, from planning \& coding to training \& fine-tuning.
	
	{\bf About book.} {\it Build a Large Language Model (From Scratch)} is a practical \& eminently-satisfying hands-on journey into foundations of generative AI. Without relying on any existing LLM libraries, code a base model, evolve it into a text classifier, \& ultimately create a chatbot that can follow your conversational instructions. \& really understand it because built it yourself!
	
	{\bf What's inside.}
	\begin{itemize}
		\item Plan \& code an LLM comparable to GPT-2
		\item Load pretrained weights
		\item Construct a complete training pipeline
		\item Fine-tune your LLM for text classification
		\item Develop LLMs that follow human instructions
	\end{itemize}
	\item {\sf About reader.} Readers need intermediate Python skills \& some knowledge of ML. LLM you create will run on any modern laptop \& can optionally utilize GPUs.
	\item {\sf About author.} {\sc Sebastian Raschka} is a Staff Research Engineer at Lighting AI, where he works on LLM research \& develops an open-source software. The technical editor on this book was  {\sc David Caswell}.
	\item {\sf Editorial Reviews.}
	\begin{itemize}
		\item ``The most comprehensive book I've seen on building LLMs. Highly recommended!'' -- {\sc Raul Ciotescu}, CTO, Netzinkubator Software
		\item ``A clear, hands-on guide that empowers readers to build their own models \& explore cutting edge of AI.'' -- {\sc Guillermo Alcántara}, Project manager, PepsiCo Global
		\item ``Must-have resource for quickly getting up to speed on LLMs. Whether you're new to field or looking to deepen your knowledge, it's perfect guide.'' -- {\sc Walter Reade},  Staff Developer Relations Engineer, Kaggle{\tt/}Google
		\item ``A fantastic resource for diving into LLMs -- a must-read for anyone eager to get hands-on!'' -- Dr. {\sc Vahid Mirjalili}, Senior Data Scientist, FM Global
	\end{itemize}
	{\sf Pipeline: 3 main stages of coding a LLM are implementing LLM architecture \& data preparation process (stage 1), pretraining an LLM to create a foundation model (stage 2), \& fine-tuning foundation model to become a personal assistant or text classifier (stage 3). Each of these stages is explored \& implemented in this book.}
	\item {\sf Preface.} Always been fascinated with language models. $> 1$ decade ago, journey into AI began with a statistical pattern classification class, which led to 1st independent project: developing a model \& web application to detect mood of a song based on its lyrics.
	
	Fast forward to 2022, with release of ChatGPT, LLMs have taken world by storm \& have revolutionized how many of us work. These models are incredibly versatile, aiding in tasks e.g. checking grammar, composing emails, summarizing lengthy documents, \& much more. This is owed to their ability to parse \& generate human-like text, which is important in various fields, from customer service to content creation, \& even in more technical domains like coding \& data analysis.
	
	As their name implies, a hallmark of LLMs: they are ``large'' -- very large -- encompassing millions to billions of parameters. (For comparison, using more traditional ML or statistical methods, Iris flower dataset can be classified with $>$ 90\% accuracy using a small model with only 2 parameters.) However, despite large size of LLMs compared to more traditional methods, LLMs don't have to be a black box.
	
	In this book, will learn how to build an LLM 1 step at a time. By end, will have a solid understanding of how an LLM, like ones used in ChatGPT, works on a fundamental level. Believe: developing confidence with each part of fundamental concepts \& underlying code is crucial for success. This not only helps in fixing bugs \& improving performance but also enables experimentation with new ideas.
	
	Several years ago, when I started working with LLMs, had to learn how to implement them hard way, sifting through many research papers \& incomplete code repositories to develop a general understanding. With this book, hope to make LLMs more accessible by developing \& sharing a step-by-step implementation tutorial detailing all major components \& developments phases of an LLM.
	
	Strongly believe: best way to understand LLMs: code one from scratch -- \& will see that this can be fun too! Happy reading \& coding!
	\item {\sf Acknowledgments.} Writing a book is a significant undertaking -- Viết 1 cuốn sách là một công việc quan trọng.
	\item {\sf About this Book.} {\it Build a Large Language Model (From Scratch)} was written to help you understand \& create your own GPT-like LLMs from ground up. It begins by focusing on fundamentals of working with text data \& coding attention mechanisms \& then guides through implementing a complete GPT model from scratch. Book then covers pretraining mechanism as well as fine-tuning for specific tasks e.g. text classification \& following instructions. By end of this book, have a deep understanding of how LLMs work \& skills to build your own models. While models you'll create are smaller in scale compared to large foundational models, they use same concepts \& serve as powerful educational tools to grasp core mechanisms \& techniques used in building state-of-art LLMs.
	\begin{itemize}
		\item {\sf Who should read this book.} {\it Build a Large Language Model (From Scratch)} is for ML enthusiasts, engineers, researchers, students, \& practitioners who want to gain a deep understanding of how LLMs work \& learn to build their own models from scratch. Both beginners \& experienced developers will be able to use their existing skills \& knowledge to grasp concepts \& techniques used in creating LLMs.
		
		What sets this book apart is its comprehensive coverage of entire process of building LLMs, from working with datasets to implementing model architecture, pretraining on unlabeled data, \& fine-tuning for specific tasks. As of this writing, no other resource provides such a complete \& hands-on approach to building LLMs from ground up.
		
		To understand code examples in this book, should have a solid grasp of Python programming. While some familiarity with ML, DL, \& AI can be beneficial, an extensive background in these areas is not required. LLMs are a unique subset of AI, so even if relatively new to field, will be able to follow along.
		
		If have some experience with deep neural networks, may find certain concepts more familiar, as LLMs are built upon these architectures. However, proficiency in PyTorch is not a prerequisite. Appendix A provides a concise introduction to PyTorch, equipping with necessary skills to comprehend code examples throughout book.
		
		A high school-level understanding of mathematics, particularly working with vectors \& matrices, can be helpful as explore inner workings of LLMs. However, advanced mathematical knowledge is not necessary to grasp key concepts \& ideas presented in this book.
		
		Most important prerequisite is a strong foundation in Python programming. With this knowledge, will be well prepared to explore fascinating world of LLMs \& understand concepts \& code examples presented in this book.
		\item {\sf How this book is organized: A roadmap.} This book is designed to be read sequentially, as each chap builds upon concepts \& techniques introduced in prev ones. Book is divided into 7 chaps that cover essential aspects of LLMs \& their implementation.
		\begin{itemize}
			\item Chap. 1 provides a high-level introduction to fundamental concepts behind LLMs. It explores transformer architecture, which forms basis for LLMs e.g. those used on ChatGPT platform.
			\item Chap. 2 lays out a plan for building an LLM from scratch. It covers process of preparing text for LLM training, including splitting text into word \& subword tokens, using byte pair encoding for advanced tokenization, sampling training examples with a sliding window approach, \& converting tokens into vectors that feed into LLM.
			\item Chap. 3 focuses on attention mechanisms used in LLMs. It introduces a basic self-attention framework \& progresses to an enhanced self-attention mechanism. Chap also covers implementation of a causal attention module that enables LLMs to generate 1 token at a time, masking randomly selected attention weights with dropout to reduce overfitting \& stacking multiple causal attention modules into a multihead attention module.
			\item Chap. 4 focuses on coding a GPT-like LLM that can be trained to generate human-like text. It covers techniques e.g. normalizing layer activations to stabilize neural network training, adding shortcut connections in deep neural networks to train models more effectively, implementing transformer blocks to create GPT models of various sizes, \& computing number of parameters \& storage requirements of GPT models.
			\item Chap. 5 implements pretraining process of LLMs. It covers computing training \& validation set losses to assess quality of LLM-generated text, implementing a training function \& pretraining LLM, saving \& loading model weights to continue training an LLM, \& loading pretrained weights form OpenAI.
			\item Chap. 6 introduces different LLM fine-tuning approaches. It covers preparing a dataset for text classification, modifying a pretrained LLM for fine-tuning, fine-tuning an LLM to identify spam messages, \& evaluating accuracy of a fine-tuned LLM classifier.
			\item Chap. 7 explores instruction fine-tuning process of LLMs. It covers preparing a dataset for supervised instruction fine-tuning, organizing instruction data in training batches, loading a pretrained LLM \& fine-tuning it to follow human instructions, extracting LLM-generated instruction responses for evaluation, \& evaluating an instruction-fine-tuned LLM.
		\end{itemize}
		\item {\sf About code.}
	\end{itemize}
	\item {\sf1. Understanding LLMs.}
	\begin{itemize}
		\item {\sf1.1. What is an LLM?}
		\item {\sf1.2. Applications of LLMs.}
		\item {\sf1.3. Stages of building \& using LLMs.}
		\item {\sf1.4. Introducing transformer architecture.}
		\item {\sf1.5. utilizing large datasets.}
		\item {\sf1.6. A closer look at GPT architecture.}
		\item {\sf1.7. Building a large language model.}
	\end{itemize}
	\item {\sf2. Working with text data.}
	\begin{itemize}
		\item {\sf2.1. Understanding word embeddings.}
		\item {\sf2.2. Tokenizing text.}
		\item {\sf2.3. Converting tokens into token IDs.}
		\item {\sf2.4. Adding special context tokens.}
		\item {\sf2.5. Byte pair encoding.}
		\item {\sf2.6. Data sampling with a sliding window.}
		\item {\sf2.7. Creating token embeddings.}
		\item {\sf2.8. Encoding word positions.}
	\end{itemize}
	\item {\sf3. Coding attention mechanisms.}
	\begin{itemize}
		\item {\sf3.1. Problem with modeling long sequences.}
		\item {\sf3.2. Capturing data dependencies with attention mechanisms.}
		\item {\sf3.3. Attending to different parts of input with self-attention.}
		\begin{itemize}
			\item {\sf A simple self-attention mechanism without trainable weights.}
			\item {\sf Computing attention weights for all input tokens.}
		\end{itemize}
		\item {\sf3.4. Implementing self-attention with trainable weights.}
		\begin{itemize}
			\item {\sf Computing attention weights step by step.}
			\item {\sf Implementing a compact self-attention Python class.}
		\end{itemize}
		\item {\sf3.5. Hiding future words with causal attention.}
		\begin{itemize}
			\item {\sf Applying a causal attention mask.}
			\item {\sf Masking additional attention weights with dropout.}
			\item {\sf Implementing a compact causal attention class.}
		\end{itemize}
		\item {\sf3.6. Extending single-head attention to multi-head.}
		\begin{itemize}
			\item {\sf Stacking multiple single-head attention layers.}
			\item {\sf Implementing multi-head attention with weight splits.}
		\end{itemize}
	\end{itemize}
	\item {\sf4. Implementing a GPT model from scratch to generate text.}
	\begin{itemize}
		\item {\sf4.1. Coding an LLM architecture.}
		\item {\sf4.2. Normalizing activations with layer normalization.}
		\item {\sf4.3. Implementing a feed forward network with GELU activations.}
		\item {\sf4.4. Adding shortcut connections.}
		\item {\sf4.5. Connecting attention \& linear layers in a transformer block.}
		\item {\sf4.6. Coding GPT model.}
		\item {\sf4.7. Generating text.}
	\end{itemize}
	\item {\sf5. Pretraining on unlabeled data.}
	\begin{itemize}
		\item {\sf5.1. Evaluating generative text models.}
		\begin{itemize}
			\item {\sf Using GPT to generate text.}
			\item {\sf Calculating text generation loss.}
			\item {\sf Calculating training \& validation set losses.}
		\end{itemize}
		\item {\sf5.2. Training an LLM.}
		\item {\sf5.3. Decoding strategies to control randomness.}
		\begin{itemize}
			\item {\sf Temperature scaling.}
			\item {\sf Top-$k$ sampling.}
			\item {\sf Modifying text generation function.}
		\end{itemize}
		\item {\sf5.4. Loading \& saving model weights in PyTorch.}
		\item {\sf5.5. Loading pretrained weights from OpenAI.}
	\end{itemize}
	\item {\sf6. Fine-tuning for classification.}
	\begin{itemize}
		\item {\sf6.1. Different categories of fine-tuning.}
		\item {\sf6.2. Preparing dataset.}
		\item {\sf6.3. Creating data loaders.}
		\item {\sf6.4. Initializing a model with pretrained weights.}
		\item {\sf6.5. Adding a classification head.}
		\item {\sf6.6. Calculating classification loss \& accuracy.}
		\item {\sf6.7. Fine-tuning model on supervised data.}
		\item {\sf6.8. Using LLM as a spam classifier.}
	\end{itemize}
	\item {\sf7. Fine-tuning to follow instructions.}
	\begin{itemize}
		\item {\sf7.1. Introduction to instruction fine-tuning.}
		\item {\sf7.2. Preparing a dataset for supervised instruction fine-tuning.}
		\item {\sf7.3. Organizing data into training batches.}
		\item {\sf7.4. Creating data loaders for an instruction dataset.}
		\item {\sf7.5. Loading a pretrained LLM.}
		\item {\sf7.6. Fine-tuning LLM on instruction data.}
		\item {\sf7.7. Extracting \& saving responses.}
		\item {\sf7.8. Evaluating fine-tuned LLM.}
		\item {\sf7.9. Conclusions.}
		\begin{itemize}
			\item {\sf What's next?}
			\item {\sf Staying up to data in a fast-moving field.}
			\item {\sf Final words.}
		\end{itemize}
	\end{itemize}
	\item {\sf A. Introduction to PyTorch.}
	\item {\sf B. References \& further reading.}
	\item {\sf C. Exercise solutions.}
	\item {\sf D. Adding bells \& whistles to training loop.}
	\item {\sf E. Parameter-efficient fine-tuning with LoRA.}
\end{itemize}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}