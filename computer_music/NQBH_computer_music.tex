\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Computer Music -- Âm Nhạc Máy Tính}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Computer Music -- Âm Nhạc Máy Tính}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/computer_music/NQBH_computer_music.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/computer_music/NQBH_computer_music.tex}.
		\item {\it }.
		
		PDF: {\sc url}: \url{.pdf}.
		
		\TeX: {\sc url}: \url{.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basic Computer Music}

\subsection{{\sc Renato Fabbri, Vilson Vieira de Silva Junior, Ant\^onio Carlos Silvano Pessotti, D\'ebora Cristina Corr\^ea, Osvaldo N. Oliveira Jr.} Musical Elements in Discrete-Time Representation of Sound}
{\sf[2 citations]}
\begin{itemize}
	\item {\sf Abstract.} Representation of basic elements of music in terms of discrete audio signals is often used in software for musical creation \& design. Nevertheless, there is no unified approach that relates these elements to discrete samples of digitized sound. In this article, each musical element is related by equations \& algorithms to discrete-time samples of sounds, \& each of these relations are implemented in scripts within a software toolbox, referred to as MASS (Music \& Audio in Sample Sequences). Fundamental element, musical note with duration, volume, pitch, \& timbre, is related quantitatively to characteristics of digital signal. Internal variations of a note, e.g. tremolos, vibratos, \& spectral fluctuations, are also considered, which enables synthesis of notes inspired by real instruments \& new sonorities. With this representation of notes, resources are provided for generation of higher scale musical structures, e.g. rhythmic meter, pitch intervals \& cycles. This framework enables precise \& trustful scientific experiments, data sonification \& is useful for education \& art. Efficacy of MASS is confirmed by synthesis of small musical pieces using basic notes, elaborated notes \& notes in music, which reflects organization of toolbox \& thus of this article. Possible to synthesize whole albums through collage of scripts \& settings specified by user. With open source paradigm, toolbox can promptly scrutinized, expanded in co-authorship processes \& used with freedom by musicians, engineers, \& other interested parties. In fact, MASS has already been employed for diverse purposes which include music production, artistic presentations, psychoacoustic experiments \& computer language diffusion where appeal of audiovisual artifacts is exploited for education.
	\item CCS Concepts: Applied computing $\to$ Sound \& music computing. Computing methodologies $\to$ Modeling methodologies. General \& reference $\to$ Surveys \& overviews, Reference works.
	\item Additional Key Words \& Phrases: music, acoustics, psychophysics, digital audio, signal processing.
	\item {\sf1. Introduction.} Music is usually defined as art whose medium is sound. Definition might also state: medium includes silences \& temporal organization of structures, or music is also a cultural activity or product. In physics \& in this document, sounds are longitudinal waves of mechanical pressure. Human auditory system perceives sounds in frequency bandwidth between 20Hz \& 20kHz, with actual boundaries depending on person, climate conditions \& sonic characteristics themselves. Since speed of sound $\approx343.2$ m{\tt/}s, such frequency limits corresponds to wavelengths of $\frac{343.2}{20}\approx17.6$ m \& $\frac{343.2}{20000}\approx17.16$ mm. Hearing involves stimuli in bones, stomach, ears, transfer functions of head \& torso (thân mình), \& processing by nervous system. Ear is a dedicated organ or appreciation of these waves, which decomposes them into their sinusoidal spectra \& delivers to nervous system. Sinusoidal components are crucial to musical phenomena, as one can recognize in constitution of sounds of musical interest (e.g. harmonic sounds \& noises, discussed in Sects. 2--3), \& higher level musical structures (e.g. tunings, scales, \& chords, Sect. 4) [55]
	
	Representation of sound can take many forms, from musical scores \& texts in a phonetic language to electric analog signals \& binary data. It includes sets of features e.g. wavelet or sinusoidal components. Although terms `audio' \& `sound' are often used without distinction \& `audio' has many definitions which depend on context \& author, audio most often means a representation of amplitude through time. In this sense, audio expresses sonic waves yield by synthesis or input by microphones, although these sources are not always neatly distinguishable e.g. as captured sounds are processed to generate new sonorities (âm thanh). Digital audio protocols often imply in quality loss (to achieve smaller files, ease storage \& transfer) \& are called {\it lossy} [47]. This is case e.g. of MP3 \& Ogg Vorbis. Non-lossy representations of digital audio, called {\it lossless} protocols or formats, on other hand, assures perfect reconstruction of analog wave within any convenient precision. Standard paradigm of lossless audio consists of representing sound with samples equally spaced by a duration $\delta_s$, \& specifying amplitude of each sample by a fixed number of bits. This is linear Pulse Code Modulation (LPCM) representation of sound, herein referred to as PCM. A PCM audio format has 2 essential attributes: a sampling frequency $f_s = \frac{1}{\delta_s}$ (also called e.g. sampling rate or sample rate), which is number of samples used for representing a second of sound; \& a bit depth, which is number of bits used for specifying amplitude of each sample. {\sf Fig. 1. Example of PCM audio: a sound wave is represented by 25 samples equally spaced in time where each sample has an amplitude specified with 4 bits.} shows 25 samples of a PCM audio with a bit depth of 4, which yields $2^4 = 16$ possible values for amplitude of each sample \& a total of $4\cdot25 = 100$ bits for representing whole sound.
	
	Fixed sampling frequency \& bit depth yield quantization error or quantization noise. This noise diminishes as bit depth increases while greater sampling frequency allows higher frequencies to be represented. Nyquist theorem asserts: sampling frequency is twice maximum frequency: represented signal can contain [49]. Thus, for general musical purposes, suitable to use a sample rate of at least twice highest frequency heard by humans, i.e., $f_s\ge2\cdot20$ kHz $= 40$ kHz. This is basic reason for adoption of sampling frequencies e.g. 44.1 kHz \& 48 kHz, which are standards in Compact Disks (CD) \& broadcast systems (radio \& television), resp.
	
	Within this framework for representing sounds, musical notes can be characterized. Note often stands as `fundamental unit' of musical structures (e.g. atoms in matter or cells in macroscopic organisms) \&, in practice, it can unfold into sounds that uphold other approaches to music. This is of capital importance because science \& scholastic artists widened traditional comprehension of music in 20th century to encompass discourse without explicit rhythm, melody or harmony. This is evident, e.g., in concrete, electronic, electroacoustic, \& spectral musical styles. In 1990s, it became evident: popular (commercial) music had also incorporated sound amalgrams \& abstract discursive arcs. [There are well known incidences of such characteristics in ethnic music, e.g. in Pygmy music, but western theory assimilated them only in last century [74].] Notes are also convenient for another reason: average listener -- \& a considerable part of specialists -- presupposes rhythmic \& pitch organization (made explicit in Sect. 4) as fundamental musical properties, \& these are developed in traditional musical theory in terms of notes. Thereafter, in this article describe musical notes in PCM audio through equations \& then indicate mechanisms for deriving higher level musical structures. Understand: this is not unique approach to mathematically express music in digital audio, but musical theory \& practice suggest: this is a proper framework for understanding \& making computer music, as should become patent in reminder of this text \& is verifiable by usage of MASS toolbox. Hopefully, interested reader or programmer will be able to use this framework to synthesize music beyond traditional conceptualizations when intended.
	
	This document provides a fundamental description of musical structures in discrete-time audio. Results include mathematical relations, usually in terms of musical characteristics \& PCM samples, concise musical theory considerations, \& their implementation as software routines both as very raw \& straightforward algorithms \& in context of rendering musical pieces. Despite general interests involved, there are only a few books \& computer implementations that tackle subject directly. These mainly focus on computer implementations \& way to mimic traditional instruments, with scattered mathematical formalisms for basic notations. Articles on topic appear to be lacking, to best of our knowledge, although advanced \& specialized developments are often reported. A compilation of such works \& their contributions is in Appendix G of [21]. Although current music software uses analytical descriptions presented here, there is no concise mathematical description of them, \& far from trivial to achieve equations by analyzing available software implementations.
	
	Accordingly, objectives of this paper:
	\begin{enumerate}
		\item Present a concise set of mathematical \& algorithmic relations between basic musical elements \& sequences of PCM audio samples.
		\item Introduce a framework for sound \& musical synthesis with control at sample level which entails potential uses in psychoacoustic experiments, data sonification \& synthesis with extreme precision (recap in Sect. 5).
		\item Provide a powerful theoretical framework which can be used to synthesize musical pieces \& albums.
		\item Provide approachability to developed framework [All analytic relations presented in this article are implemented as small scripts in public domain. They constitute MASS toolbox, available in an open source Git repository [9]. These routines are written in Python \& make use of Numpy, which performs numerical routines efficiently (e.g. through LAPACK), but language \& packages are by no means mandatory. Part of scripts has been ported to JavaScript (which favors their use in Web browsers e.g. Firefox \& Chromium) \& native Python [48, 56, 70]. These are all open technologies, published using licenses that grant permission for copying, distributing, modifying \& usage in research, development, art \& education. Hence, work presented here aims at being compliant with recommended practices for availability \& validation \& should ease co-authorship processes [43, 52].]
		\item Provide a didactic (mang tính giáo huấn) presentation of content, which is highly multidisciplinary, involving signal processing, music, psychoacoustics \& programming.
	\end{enumerate}
	Reminder of this article is organized as follows: Sect. 2 characterizes basic musical note; Sect. 3 develops internal dynamics of musical notes; Sect. 4 tackles organization of musical notes into higher level musical structures [14, 41, 42, 54, 62, 72, 74, 76]. As these descriptions require knowledge on topics e.g. psychoacoustics, cultural traditions, \& mathematical formalisms, text points to external complements as needed \& presents methods, results, \& discussions altogether. Sect. 5 is dedicated to final considerations \& further work.	
	\begin{itemize}
		\item {\sf1.1. Additional material.} 1 Supporting Information document [27] holds commented listings of all equations, figures, tables, \& sects in this document \& scripts in MASS toolbox. Another Supporting Information document [28] is a PDF version of code that implements equations \& concepts in each sect [Toolbox contains a collection of Python scripts which
		\begin{itemize}
			\item implements each of equations
			\item render music \& illustrate concepts
			\item render each of figures used in this article.
		\end{itemize}
		Documentation of toolbox consists of this article, Supporting Information documents \& scripts themselves.]. Git repository [26] holds all PDF documents \& Python scripts. Rendered musical pieces are referenced when convenient \& linked directly through URLs, \& constitute another component of framework. They are not very traditional, which facilitates understanding of specific techniques \& extrapolation of note concept. There are MASS-based software packages [23, 25] \& further musical pieces that are linked in Git repository.
		\item {\sf1.2. Synonymy, polysemy \& theoretical frames (disclaimer).} Given: main topic of this article (expression of musical elements in PCM audio) is multidisciplinary \& involves art, reader should be aware: much of vocabulary admits different choices of terms \& defs. More specifically, often case where many words can express same concept \& where 1 word can carry different meanings. This is a very deep issue which might receive a dedicated manuscript. Reader might need to read rest of this document to understand this small selection of synonymy \& polysemy (đa nghĩa) in literature, but important to illustrate point before more dense sects:
		\begin{itemize}
			\item a ``note'' can mean a pitch or an abstract construct with pitch \& duration or a sound emitted from a musical instrument or a specific note in a score or a music.
			\item Sampling rate is also called {\it sampling frequency} or {\it sample rate}.
			\item A harmonic in a sound is most often a sinusoidal component which is in harmonic series of fundamental frequency. Many times, however, terms harmonic \& component are not distinguished. A harmonic can also be a note performed in an instrument by preventing certain overtones (components).
			\item Harmony can refer to chords or to note sets related to chords or even to ``harmony'' in a more general sense, as a kind of balance \& consistency.
			\item A ``tremolo'' can mean different things: e.g. in a piano score, a tremolo is a fast alternation of 2 notes (pitches) while in computer music theory it is (most often) an oscillation of loudness.
		\end{itemize}
		Strived to avoid nomenclature clashes \& use of more terms than needed. Also, there are many theoretical standpoints for understanding musical phenomena, which is an evidence: most often there is not a single way to express or characterize musical structures. Therefore, in this article, adjectives e.g. ``often'', ``commonly'', \& ``frequently'' are abundant \& they would probably be even more numerous if wanted to be pedantically precise. Some of these issues are exposed when content is convenient, e.g. in 1st considerations of timbre.
		
		-- Cố gắng tránh xung đột danh pháp \& sử dụng nhiều thuật ngữ hơn mức cần thiết. Ngoài ra, có nhiều quan điểm lý thuyết để hiểu các hiện tượng âm nhạc, đây là bằng chứng: thường không có một cách duy nhất để diễn đạt hoặc mô tả các cấu trúc âm nhạc. Do đó, trong bài viết này, các tính từ như ``thường xuyên'', ``thường xuyên'', \& ``thường xuyên'' rất nhiều \& chúng có thể còn nhiều hơn nữa nếu muốn chính xác về mặt học thuật. Một số vấn đề này được nêu ra khi nội dung thuận tiện, ví dụ như trong những cân nhắc đầu tiên về âm sắc.
	\end{itemize}	
	\item {\sf2. Characterization of musical note in discrete-time audio.} In diverse artistic \& theoretical contexts, music is conceived as constituted by fundamental units referred to as notes, ``atoms'' that constitute music itself [44, 72, 74]. In a cognitive perspective, notes are understood as discernible elements that facilitate \& enrich transmission of information through music [41, 55]. Canonically, basic characteristics of a musical note are duration, loudness, pitch, \& timbre (âm sắc) [41]. All relations described in this sect are implemented in file {\tt src/sections/eqs2.1.py}. Musical pieces {\it5 sonic portraits \& reduced-fi} are also available online to corroborate \& illustrate concepts.
	\begin{itemize}
		\item {\sf2.1. Duration.} Sample frequency $f_s$ is defined as number of samples in each sec of discrete-time signal. Let $T = \{t_i\}$ be an ordered set of real samples separated by $\delta_s = \frac{1}{f_s}$ secs ($f_s = 44.1$ kHz $\Rightarrow\delta_s = \frac{1}{44100}\approx0.023$ ms). A musical note of duration $\Delta$ secs can be expressed as a sequence $T^\Delta$ with $\Lambda = \lfloor\Delta\cdot f_s\rfloor$ samples. I.e., integer part of multiplication is considered, \& an error of $\le\delta_s$ missing secs is admitted, which is usually fine for musical purposes. Thus
		\begin{equation*}
			T^\Delta = \{t_i\}_{i=0}^{\lfloor\Delta f_s\rfloor - 1} = \{t_i\}_0^{\Lambda - 1}.
		\end{equation*}
		\item {\sf2.2. Loudness.} Loudness [Loudness \& ``volume'' are often used indistinctly. In technical contexts, loudness is used for subjective perception of sound intensity while volume might be used for some measurement of loudness or to a change in intensity of signal by equipment. Accordingly, one can perceive a sound as loud or soft \& change volume by turning a knob. Will use term loudness \& avoid more ambiguous term volume.] is a perception of sonic intensity that depends on reverberation, spectrum, \& other characteristics described in Sect. 3 [11]. One can achieve loudness variations through power of wave [11]:
		\begin{equation*}
			{\rm pow}(T) = \frac{\sum_{i=0}^{\Lambda - 1} t_i^2}{\Lambda}.
		\end{equation*}
		Final loudness is dependent on amplification of signal by speakers. Thus, what matters: relative power of a note in relation to the others around it, or power of a musical sect in relation to the rest. Differences in loudness are result of complex psychophysical phenomena but can often be reasoned about in terms of decibels, calculated directly from amplitudes through energy or power:
		\begin{equation*}
			V_{\rm dB} = 10\log_{10} \frac{{\rm pow}(T')}{{\rm pow}(T)}.
		\end{equation*}
		\item {\sf2.3. Pitch.}
		\item {\sf2.4. Timbre.}
		\item {\sf2.5. Spectra of sampled sounds.}
		\item {\sf2.6. Basic note.}
		\item {\sf2.7. Spatialization: localization \& reverberation.}
		\item {\sf2.8. Musical usages.}
	\end{itemize}
	\item {\sf3. Variation in Basic Note.}
	\begin{itemize}
		\item {\sf3.1. Lookup table.}
		\item {\sf3.2. Incremental variations of frequency \& intensity.}
		\item {\sf3.3. Application of digital filters.}
		\item {\sf3.4. Noise.}
		\item {\sf3.5. Tremolo \& vibrato, AM \& FM.}
		\item {\sf3.6. Musical usages.}
	\end{itemize}
	\item {\sf4. Organization of notes in music.}
	\begin{itemize}
		\item {\sf4.1. Tuning, intervals, scales, \& chords.}
		\item {\sf4.2. Atonal \& tonal harmonies, harmonic expansion \& modulation.}
		\item {\sf4.3. Counterpoint.}
		\item {\sf4.4. Rhythm.}
		\item {\sf4.5. Repetition \& variation: motifs \& larger units.}
		\item {\sf4.6. Directional structures.}
		\item {\sf4.7. Cyclic structures.}
		\item {\sf4.8. Serialism \& post-serial techniques.}
		\item {\sf4.9. Musical idiom?}
		\item {\sf4.10. Musical usages.}
	\end{itemize}
	\item {\sf5. Conclusions \& Further Developments.}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{\cite{Horn_West_Roberts2022}. {\sc Michael S. Horn, Melanie West, Cameron Roberts}. Introduction to Digital Music with Python Programming: Learning Music with Code}
{\sf[4 Amazon ratings]}
\begin{itemize}
	\item {\sf Amazon reviews.} {\it Introduction to Digital Music with Python Programming} provides a foundation in music \& code for beginner. It shows how coding empowers new forms of creative expression while simplifying \& automating many of tedious aspects of production \& composition.
	
	With help of online, interactive examples, this book covers fundamentals of rhythm, chord structure, \& melodic composition alongside basics of digital production. Each new concept is anchored in a real-world musical example that will have you making beats in a matter of minutes.
	
	Music is also a great way to learn core programming concepts e.g. loops, variables, lists, \& functions, {\it Introduction to Digital Music with Python Programming} is designed for beginners of all backgrounds, including high school students, undergraduates, \& aspiring professionals, \& requires no prev experience with music or code.
	
	A beginner's approach to digital music production focuses on key concepts, ensuring ease \& progress in learning.
	
	Streamline your programming education by incorporating music, making complex core concepts easier to grasp \& apply.
	
	Amplify your music creativity by generating unique beats with code in minutes, without needing advanced technical skills.
	
	A great book for learning Python programming \& exploring digital music.
	
	This broad manual combines music theory \& programming basics, providing interactive examples \& real-world applications to help you compose \& produce music from scratch.
	
	Perfect for aspiring musicians \& programmers exploring music-code fusion.
	\item {\sf About Author.} {\sc Michael S. Horn} is Associate Prof of CS \& Learning Sciences at Northwestern University in Evanston, Illinois, where he directs Tangible Interaction Design \& Learning (TIDAL) Lab.
	
	{\sc Melanie West} is a PhD student in Learning Sciences at Northwestern University \& co-founder of Tiz Media Foundation, a nonprofit dedicated to empowering underrepresented youth through science, technology, engineering, \& mathematics (STEM) programs.
	
	{\sc Cameron Roberts} is a software developer \& musician living in Chicago. He holds degrees from Northwestern University in Music Performance \& CS.
	\item {\sf Foreword.} When I was a kid growing up in Texas, I ``learned'' how to play viola. I put {\it learned} in quotes because it was really just a process of rote memorization -- hours \& hours of playing same songs over \& over again. I learned how to read sheet music, bu only to extent that I knew note names \& could translate them into grossest of physical movements. I never learned to read music as literature, to understand its deeper meaning, structure, or historical context. I never understood anything about music theory beyond being annoyed that I had to pay attention to accidentals in different keys. I never composed {\it anything}, not even informally scratching out a tune. I never developed habits of deep listening, of taking songs apart in my head \& puzzling over how they were put together in 1st place. I never played just for fun. \&, despite best intentions of parents \& teachers, I never fell in love with music.
	
	Learning how to code was complete opposite experience for me. I was largely self-taught. Courses I took in school were electives (môn tự chọn) that I chose for myself. Teachers gave me important scaffolding at just right times, but it never felt forced. I spent hours working on games or other projects (probably when I should have been practicing viola). I drew artwork, planned out algorithms, \& even synthesized my own rudimentary sound effects (hiệu ứng âm thanh thô sơ). I had no idea what I was doing, but that was liberating. No one was around to point out my mistakes or to show me ow to do things ``right'' way (at least, not until college). I learned how to figure things out for myself, \& skills I picked up from those experiences are still relevant today. I fell in love with coding. [I was also fortunate to have grown up in a time \& place where these activities were seen as socially acceptable for a person of my background \& identity.]
	
	But I know many people whose stories are flipped 180 degrees. For them, music was so personally, socially, \& culturally motivating that they couldn't get enough. They'd practice for hours \& hours, not just for fun but for something \fbox{much deeper}. For some it was an instrument like guitar that got them started. For others it was an app like GarageBand that gave them a playful entry point into musical ideas. To extent that they had coding experiences, those experiences ranged from uninspiring to off-putting (từ không hấp dẫn đến khó chịu). It's not that they necessarily hated coding, but it was something they saw as not being for them.
	
	In foreword of his book, {\it Mindstorms: Children, Computers, \& Powerful Ideas}, {\sc Seymour Papert} wrote: ``fell in love with gears'' as a way of helping us imagine a future in which children (like me) would fall in love with computer programming, not for its own sake, but for creative worlds \& powerful ideas that programming could open up. Part of what he was saying was: love \& learning go hand in hand, \& that computers could be an entry point into many creative \& artistic domains e.g. mathematics \& music. Coding can revitalize subjects that have become painfully rote in schools.
	
	Process of developing TunePad over past several years has been a fascinating rediscovery of musical ideas for me. Code has given me a different kind of language for thinking about things like rhythm, chords, \& harmony. I can experiment with composition unencumbered by my maladroit hands. Music has become something creative \& alive in a way that it never was for me before. Music theory is no longer a thicket of confusing terminology \& instead has become a fascinating world of mathematical beauty that structures creative process.
	
	-- Quá trình phát triển TunePad trong nhiều năm qua là một sự khám phá lại đầy hấp dẫn đối với tôi về các ý tưởng âm nhạc. Mã đã cho tôi một loại ngôn ngữ khác để suy nghĩ về những thứ như nhịp điệu, hợp âm, \& sự hòa âm. Tôi có thể thử nghiệm sáng tác mà không bị cản trở bởi đôi tay vụng về của mình. Âm nhạc đã trở thành một thứ gì đó sáng tạo \& sống động theo cách mà trước đây tôi chưa từng có. Lý thuyết âm nhạc không còn là một mớ thuật ngữ khó hiểu \& thay vào đó đã trở thành một thế giới hấp dẫn của vẻ đẹp toán học cấu trúc nên quá trình sáng tạo.
	
	{\sc Melanie, Cameron}, \& I hope: this book gives a similarly joyful learning experience with music \& code. Hope: feel empowered to explore algorithmic \& mathematical beauty of music. Hope: discover, as we have: music \& code reinforce one another in surprising \& powerful ways that open new creative opportunities for you. Hope, regardless of your starting point -- as a coder, as a musician, as neither, as both -- will discover something new about yourself \& what you can become.
	\item {\sf1. Why music \& coding?} This book is designed for people who {\it love} music \& are interested in intersection of music \& coding. Maybe you're an aspiring musician or music producer who wants to know more about coding \& what it can do. Or maybe already know a little about coding, \& want to expand your creative musical horizon. Or maybe a total beginner in both. Regardless of your starting point, this book is designed for you to learn about music \& coding as mutually reinforcing skills. Code gives us an elegant language to think about musical ideas, \& music gives us a context within which code makes sense \& is immediately useful. Together they form a powerful new way to create music that will be interconnected with digital production tools of future.
	
	More \& more code will be used to produce music, to compose music, \& even to perform music for live audiences. Digital production tools e.g. Logic, Reason, Pro Tools, FL Studio, \& Ableton Live are complex software applications created with {\it millions} of lines of code written by huge teams of software engineers. With all of these tools can write code to create custom plugins \& effects. Beyond production tools, live coding is an emerging from of musical performance art in which Information Age DJs write computer code to generate music in real time for live audiences.
	
	In other ways, still on cusp of a radical transformation in way use code to create music. History of innovation in music has always been entwined with innovations in technology. Whether talking about {\sc Franz Liszt} in 19th century, who pioneered persona of modern music virtuoso based on technological breakthroughs of piano [Fans were so infatuated with {\sc Liszt}'s piano ``rockstar'' status that they fought over his silk handkerchiefs \& velvet gloves at his performances.], or DJ {\sc Kool Herc} in 20th century, who pioneered hip-hop with 2 turntables \& a crate full of funk records in Bronx, technologies have created new opportunities for musical expression that have challenged status quo \& given birth to new genres. Don't have {\sc Franz Liszt} or DJ {\sc Kool Herc} of coding yet, but it's only a matter of time before coding virtuosos of tomorrow expand boundaries of what's possible in musical composition, production, \& performance.
	\begin{itemize}
		\item {\sf1.1. What is Python?} In this book learn how to create your own digital music using a computer programming language called {\it Python}. If not familiar with programming languages, Python is a general-purpose language 1st released in 1990s that is now 1 of most widely used languages in world. Python is designed to be easy to read \& write, which makes it a popular choice for beginners. Also fully featured \& powerful, making it a good choice for professionals working in fields as diverse as DS, web development, arts, \& video game development. Because Python has been around for decades, it runs on every major computer OS. Examples in this book even use a version of Python that runs directly inside of your web browser without need for any special software installation.
		
		Unlike many other common beginner programming languages, Python is ``text-based'', i.e., type code into an editor instead of dragging code blocks on computer screen. This makes Python a little harder to learn than other beginner languages, but it also greatly expands what you can do. By time yet through this book should feel comfortable writing short Python programs \& have conceptual tools need to explore more on your own.
		\item {\sf1.2. What this book is {\it not}.} Before get into a concrete example of what you can do with a little bit of code, just a quick note about what this book is {\it not}. This book is not a comprehensive guide to Python programming. There are many excellent books \& tutorials designed for beginners, several of which are free. [Recommend \url{https://www.w3schools.com/python/}.]
		
		This book is also not a comprehensive guide to music theory or Western music notation. Get into core ideas behind rhythm, harmony, melody, \& composition, but there are, again, many other resources available for beginners who want to go deeper. What offering is a different approach that combines learning music with learning code in equal measure.
		\item {\sf1.3. What this book {\it is}.} What will do is give an intuitive understanding of fundamental concepts behind both music \& coding. Code \& music are highly technical skills, full of arcane symbols \& terminology, seem almost designed to intimidate beginners. In this book put core concepts to use immediately to start making music. Get to play with ideas at your own pace \& get instant feedback as bring ideas to life. Skip most of technical jargon \& minutiae for now -- can come later. Instead, focus on developing your confidence \& understanding. Importantly, skills, tools, \& ways of thinking introduce in this book will be broadly applicable in many other areas as well. Working in Python code, but core structures of variables, functions, loops, conditional logical, \& classes are same across many programming languages including JavaScript, Java, C, C++, \& C\#. After learn 1 programming language, each additional language is that much easier to pick up.
		\item {\sf1.4. TunePad \& EarSketch.} This book uses 2 free online platforms that combine music \& Python coding. 1st, called TunePad \url{https://tunepad.com}, was developed by a team of researchers at Northwestern University in Chicago. TunePad lets create short musical loops that you can layer together using a simple digital audio workstation (DAW) interface. 2nd platform, called EarSketch \url{https://earsketch.gatech.edu}, was created by researchers at Georgia Tech in Atlanta. EarSketch uses Python code to arrange samples \& loops into full-length compositions. Both platforms are browser-based apps, so all need to get started is a computer (tablets or Chromebooks are fine), an internet connection, \& a web browser like Chrome or Firefox. External speakers or headphones are also nice but not required. Both platforms have been around for years \& have been used by many thousands of students from middle school all way up to college \& beyond. TunePad \& EarSketch are designed primarily as learning platforms, but there are easy ways to export your work to professional production software if want to go further.
		\item {\sf1.5. A quick example.} A quick example of what coding in Python looks like. This program runs in TunePad to create a simple beat pattern, variants of which have been used in literally thousands of songs e.g. {\it Blinding Lights} by The Weeknd \& {\it Roses} by SAINt JHN.
		\begin{verbatim}
			playNote(1) # play a kick drum sound
			playNote(2) # play a snare drum sound
			playNote(1)
			playNote(2)
			rewind(4)   # rewind 4 beats
			for i in range(4):
			    rest(0.5)
			    playNote(4, beats = 0.5) # play hat for a half beat
		\end{verbatim}
		These 8 lines of Python code tell TunePad to play a pattern of kick drums, snare drums, \& high-hats. Most of lines are {\it playNote} instructions, \& those instructions tell TunePad to play musical sounds indicated by numbers inside of parentheses. This example also includes sth called a {\it loop}. Loop is an easy way to repeat a set of actions over \& over again. In this case, loop tells Python to repeat lines 7 \& 8 4 times in a row. Screenshot {\sf Fig. 1.1: A TunePad program to play a simple rock beat.} shows what this looks like in TunePad. Can try out example for yourself with this link: \url{https://tunepad.com/examples/roses}.
		\item {\sf1.6. 5 reasons to learn code.} Now seen a brief example of what can do with a few lines of Python code, here are top 5 reasons to get started with programming \& music if still in doubts.
		\begin{itemize}
			\item {\sf1.6.1. Reason 1: Like it or note, music is already defined by code.} Looking across modern musical landscape, clear: music is already defined by code. 1 of biggest common factors of almost all modern music from any popular genre: {\it everything} is edited, if not created entirely, with sophisticated computer software. Hard to overstate how profoundly such software has shaped sound of music in 21st century. Relatively inexpensive DAW applications \& myriad ubiquitous plugins that work across platforms have had a disruptive \& democratizing effect across music industry. Think about effects plugins like autotune, reverb, or ability to change pitch of a sample without changing tempo. These effects are all generated with sophisticated software. Production studios size of small offices containing hundreds of thousands of dollars' worth of equipment now fit on screen of a laptop computer available to any aspiring producer with passion, a WiFi connection, \& a small budget. Reasons behind shift to digital production tools are obvious. Computers have gotten to a point where they are cheap enough, fast enough, \& capacious enough to do real-time audio editing. Can convert sound waves into editable digital information with microsecond precision \& then hear effects of our changes in real time. These DAWs didn't just appear out of nowhere. They were constructed by huge teams of software engineers writing code -- millions of lines of it. E.g., TunePad was created with $> 1.5$ million lines of code written in over a dozen computer languages e.g. Python, HTML, JavaScript, CSS, \& Dart. Regardless of how feel about digital nature of modern music, not going away. Learning to code will help understand a little more about how all of this works under hood. More to point, it's increasingly common for producers to write their own code to manipulate sound. E.g., in Logic, can write JavaScript code to process incoming MIDI (Musical Instrument Digital Interface) data to do things like create custom arpeggiators. Learning to code can give you more control \& help expand your creative potential {\sf Fig. 1.2: Typical DAW software}.
			\item {\sf1.6.2. Reason 2: Code is a powerful way to make music.} Don't always think about it this way, but music is {\it algorithmic} in nature -- it's full of mathematical relationships, logical structure, \& recursive patterns. Beauty of Baroque fugue is in part a reflection of beauty of mathematical \& computational ideas behind music. Call Bach a genius not just because his music is so compelling, but also because he was able to hold complex algorithms in his mind \& then transcribe them to paper using representation system called Western music notation. I.e., music notation is a language for recording output of composition process, but not a language for capturing algorithmic nature of composition process itself.
			
			Code, on other hand, is a language specifically designed to capture mathematical relationships, logical structure, \& recursive patterns. E.g., take stuttered hi-hat patterns that are 1 of defining characteristics of trap music. Here are a few lines of Python code that generate randomized hi-hat stutters that can bring an otherwise conventional beat to life with sparking energy.
			\begin{verbatim}
				for _ in range(16):
				    if randint(6) > 1: # roll die for a random number
				        playNote(4, beats=0.5) # play an 8th note
				    else:
				        playNote(4, beats=0.25) # or play 16th notes
				        playNote(4, beats=0.25)
			\end{verbatim}
			Or, as another example, here's a 2-line Python program that plays a snare drum riser effect common in house, EDM, or pop music. Often hear this technique right before beat drops. This code uses a decay function so that each successive note is a little shorter resulting in a gradual acceleration effect.
			\begin{verbatim}
				for i in range(50): # play 50 snares
				    playNote(2, beats = pow(2, -0.09 * i))
			\end{verbatim}
			What's cool about these effects: they're {\it parametrized}. Because code describes algorithms to generate music, \& not music itself, i.e., can create infinite variation by adjusting numbers involved. E.g., in trap hi-hat code, can easily play around with how frequently stuttered hats are inserted into pattern by increasing or decreasing 1 number. Can think of code as sth like a power drill; can swap out different bits to make holes of different sizes. Drill bits are like parameters that change what tool does in each specific instance. In same way, algorithms are vastly more general-purpose tools that can accomplish myriad tasks by changing input parameters.
			
			Creating a snare drum riser with code is obviously a very different kind of thing than picking up 2 drumsticks \& banging out a pattern on a real drum. \&, to be clear, not advocating for code to replace learning how to perform with live musical instruments. But, code can be another tool in your musical repertoire for generating repetitive patterns, exploring mathematical ideas, or playing sequences that are too fast or intricate to play by hand.
			
			-- Tạo 1 bộ phận nâng cao trống snare bằng mã rõ ràng là một việc rất khác so với việc nhặt 2 dùi trống \& đánh một mẫu trên một chiếc trống thật. \&, nói rõ hơn, không ủng hộ việc sử dụng mã để thay thế việc học cách biểu diễn với các nhạc cụ sống. Nhưng mã có thể là một công cụ khác trong tiết mục âm nhạc của bạn để tạo ra các mẫu lặp lại, khám phá các ý tưởng toán học hoặc chơi các chuỗi quá nhanh hoặc phức tạp để chơi bằng tay.
			\item {\sf1.6.3. Reason 3: Code lets you build your own musical toolkit.} Becoming a professional in any field is about developing expertise with tools -- acquiring equipment \& knowing how to use it. Clearly, this is true in music industry, but also true in software. Professional software engineers acquire specialized equipment \& software packages. Develop expertise in a range of programming languages \& technical frameworks. But, they also build their own specialized tools that they use across projects. In this book, show how to build up your own library of Python functions. Can think of functions as specialized tools that you create to perform different musical tasks. In addition to examples described above, might write a function to generate a chord progression or play an arpeggio, \& can use functions again \& again across many musical projects.
			\item {\sf1.6.4. Reason 4: Code is useful for a thousand \& 1 other things.} Python is 1 of most powerful, multi-purpose languages in world. Used to create web servers \& social media platforms as much as video games, animation, \& music. Used for research \& DS, politics \& journalism. Knowing a little Python gives access to powerful ML \& AI (AI{\tt/}ML) techniques that are poised to transform most aspects of human work, including in creative domains e.g. music. Python is both a scripting language \& a software engineering platform -- equal parts duct tape \& table saw -- \& it's capable of everything from quick fixes to durable software applications. Learning a little Python won't make you a software engineer, just like learning a few guitar chords won't make you a performance musician. But it's a start down a path. An open door that was previously closed, \& a new way of using your mind \& a new way of thinking about music.
			\item {\sf1.6.5. Reason 5: Coding makes us more human.} When think about learning to code, tend to think about economic payoff. Hear arguments that learning to code is a resume builder \& a path to a high-paying job. Not that this perspective is wrong, but it might be wrong reason for you to learn how to code.
			
			Just like people who are good at music {\it love} music, people who are good at coding tend to {\it love} coding. Craft of building software can be tedious \& frustrating, but it can also be rewarding. A way to express oneself creatively \& to engage in craftwork. People don't learn to knit, cook, or play an instrument for lucrative (có lợi nhuận) career paths that these pursuits open up -- although by all means those pursuits can lead to remarkable careers. People learn these things because they have a {\it passion} for them. Because they are personally fulfilling. These passions connect us to centuries of tradition; they connect us to communities of teachers, learners, \& practitioners; \&, in end, they make us more {\it human}. So when things get a little frustrating -- \& things always get a little frustrating when learning any worthwhile skill -- remember: just like poetry, literature, or music, code is an arts as much as it is a science. \&, just like woodworking, knitting, or cooking, code is a craft as much as it is an engineering discipline. Be patient \& give yourself a chance to fall in love with coding.
		\end{itemize}
		\item {\sf1.7. Future of music \& code.} Before get on with book, wanted to leave you with a brief thought about future of technology, music, \& code. For as long as there have been people on this planet there has been music. \&, as long as there has been music, people have created technology to expand \& enhance their creative potential. A drum is a kind of technology -- a piece of animal hide stretched across a hollow log \& tied in place. It's a polylithic accomplishment, an assembly of parts that requires skill \& craft to make. One must know how to prepare animal hide, to make rope from plant fiber, \& to craft \& sharpen tools. More than that, one must know how to perform with drum, to connect with an audience, to enchant them to move their bodies through an emotional \& rhythmic connection to beat. Technology brings together materials \& tools with knowledge. People must have knowledge both to craft an artifact \& to wield it. \&, over time -- over generations -- that knowledge is refined as it gets passed down from teacher to student. It becomes stylized \& diversified. Tools, artifacts, knowledge, \& practice all become sth greater. Sth we call culture.
		
		Again \& again world of music has been disrupted, democratized, \& redefined by new technologies. Hip-hop was a rebellion against musical status quo fueled by low-cost technologies like recordable cassette tapes, turntables, \& 808 drum machines. Early innovators shattered norms of artistic expression, redefining music, poetry, visual art, \& dance in process. Inexpensive access to technology coupled with a need for new forms of authentic self-expression was a match to dry tinder of racial \& economic oppression.
		
		Hard to overstate how quickly world is still changing as a result of technological advancements. Digital artifacts \& infrastructures are so ubiquitous that they have reconfigured social, economic, legal, \& political systems; revolutionized scientific research; upended arts \& culture; \& even wormed their way into most intimate aspects of our personal \& romantic lives. Already talked about transformative impact that digital tools have had on world of music in 21st century, but exhilarating (\& scary) part: we're on precipice of another wave of transformation in which human creativity will be redefined by AI \& ML. Imagine AI accompanists that can improvise harmonies or melodies in real time with human musicians. Or DL algorithms that can listen to millions of songs \& innovate music in same genre. Or silicon poets that grasp human language well enough to compose intricate rap lyrics. Or machines with trillions of transistor synapses so complex that they begin to ``dream'' -- inverted ML algorithms that ooze imagery unhinged enough to disturb absinthe slumber of surrealist painters. Now, imagine: this is not speculative science fiction, but reality of our world today. These things are here now \& already challenging what we mean by human creativity. What are implications of a society of digital creative cyborgs?
		
		But here's trick: we've always been cyborgs. Western music notation is as much a technology as Python code. Becoming literate in any sufficiently advanced representation system profoundly shapes how think about \& perceive world around us. Classical music notation, theory, \& practice shaped mind of Beethoven as much as he shaped music with it -- so much so that he was still able to compose many of his most famous works while almost totally deaf. {\sc Beethoven} was a creative cyborg enhanced by technology of Western music notation \& theory. Difference: now we've externalized many of cognitive processes into machines that think alongside us. \&, increasingly, these tools are available to everyone. How that changes what it means to be a creative human being is anyone's guess.
		\item {\sf1.8. Book overview.} Excited to have you with us on this journey through music \& code. A short guide for where go from here. Chaps. 2--3 cover foundations of rhythm, pitch, \& harmony. These chaps are designed to move quickly \& get you coding in Python early on. Cover Python variables, loops, which both connect directly to musical concepts. Chaps. 4--6 cover foundations of chords, scales, \& keys using Python lists, functions, \& data structures. Chaps. 7, 8, 10 shift from music composition to music production covering topics e.g. frequency domain, modular synthesis, \& other production effects. In Chap. 9, switch to EarSketch platform to talk about how various musical elements are combined to compose full-length songs. Finally, Chap. 11 provides a short overview of history of music \& code along with a glimpse of what future might hold. Between each chap, provide a series of short {\it interludes} that are like step-by-step tutorials to introduce new music \& coding concepts.
		
		A few notes about how to read this book. Any time include Python code, it will be shown in a programming font. Sometimes write code in a table with line numbers so that can refer to specific lines. When introduce new terms, bold word. If get confused by any of programming or music terminology, check out appendices, which contain quick overviews of all of important concepts. Often invite to follow along with online examples. Best way to learn is by doing it yourself, so strongly encourage to try coding in Python online as go through chaps.
	\end{itemize}
	\item {\sf Interlude 1: Basic Pop Beat.} In this interlude, get familiar with TunePad interface by creating a basic rock beat in style of songs like {\it Roses} by SAINt JHN. Can follow along online by visiting \url{https://tunepad.com/interlude/pop-beat}
	\begin{enumerate}
		\item {\bf Step 1: Deep listening.} Good to get in habit of deep listening. Deep listening is practice of trying every possible way of listening to sounds. Start by loading a favorite song in a streaming service \& listening -- really listening -- to it. Take song apart element by element. What sounds do you hear? How are they layered together? When do different parts come into track \& how do they change over time? Think about how producer balances sounds across frequency spectrum or opens up space for transitions in lyrics. Try focusing on just drums. Can start to recognize individual percussion sounds \& their rhythmic patterns?
		\item {\bf Step 2: Create a new TunePad project.} Visit \url{https://tunepad.com} on a laptop or Chromebook \& set up an account. [Recommend using free Google Chrome browser for best overall experience.] After signing in, click on {\tt New Project} button to create an empty project workspace. Your project will look sth like this {\sf Fig. 1.3: TunePad project workspace}.
		\item {\bf Step 3: Kick drums.} In your project window, click on {\tt ADD CELL} button \& then select {\tt Drums} {\sf Fig. 1.4: Selecting instruments in TunePad.} In TunePad can think of a ``cell'' as an instrument that you can program to play music. Name new instrument ``Kicks'' \& then add this Python code.
		\begin{verbatim}
			# play four kick drums
			playNote(1)
			playNote(1)
			playNote(1)
			playNote(1)
		\end{verbatim}
		When done, your project should look sth like {\sf Fig. 1.5: Parts of a TunePad cell}.
		
		Go ahead \& press Play button at top left to hear how this sounds.
		
		{\it Syntax errors.} Occasionally your code won't work right \& get a red error message box that looks sth like {\sf Fig. 1.6: Python syntax error in TunePad}. This kind of error message is called a ``syntax'' error. In this case, code was written as {\tt playnote} as a lowecase ``n'' instead of an uppercase ``N''. Can fix this error by changing code to read {\tt playNote} on line 2.
		\item {\bf Step 4: Snare drums.} In your project window, click on {\tt ADD CELL} button again \& select {\tt Drums}. Now should have 2 drum cells one appearing above the other in your project. Name 2nd instrument ``Snare Drums'' \& then add this Python code.
		\begin{verbatim}
			# play 2 snare drums on the up beats only
			rest(1) # skip a beat
			playNote(2) # play a snare drum sound
			rest(1)
			playNote(2)
		\end{verbatim}
		Might start to notice text that comes after hashtag symbol \# is a special part of your program. This text is called a {\it comment}, \& it's for human coders to help organize \& document their code. Anything that comes after hashtag on a line is ignored by Python. Try playing this snare drum cell to hear how it sounds. Can also play kick drum cell at same time to see how they sound together.
		\item {\bf Step 5: Hi-hats.} Click on {\tt ADD CELL} button again to add a 3rd drum cell. Change title of this cell to be ``Hats'' \& add code:
		\begin{verbatim}
			# play four hats between the kicks and snares
			rest(0.5) # rest for half a beat
			playNote(4, beats=0.5) # play a hat for half a beat
			rest(0.5)
			playNote(4, beats=0.5)
			rest(0.5)
			playNote(4, beats=0.5)
			rest(0.5)
			playNote(4, beats=0.5)
		\end{verbatim}
		When play all 3 of drum cells together, should hear a basic rock beat pattern: {\tt kick - hat - snare - hat - kick - hat - snare - hat}.
		\item {\bf Step 6: Fix your kicks.} Might notice kick drums feel a little heavy in this mix. Can make some space in pattern by resting on up beats (beats 2 \& 4) when snare drums are playing. Scroll back up to your {\tt Kick drum cell} \& change code to look like this:
		\begin{verbatim}
			# play kicks on the down beats only
			playNote(1)
			rest(1)
			playNote(1)
			rest(1)
			playNote(1)
			rest(1)
			playNote(1)
			rest(0.5) # rest a half beat
			playNote(1, beats = 0.5) # half beat pickup kick
		\end{verbatim}
		\item {\bf Step 7: Adding a bass line.} Add a new cell to your project, but this time select {\tt Bass} instead of {\tt Drums}. Once cell is loaded up, change voice to {\tt Plucked Bass} {\sf Fig. 1.7: Selecting an instrument's voice in TunePad.}
		
		Entering this code to create a simplified bass line in style of {\it Roses} by SAINt JHN. When done, try playing everything together to get full sound.
		\begin{verbatim}
			playNote(5, beats=0.5) # start on low F
			playNote(17, beats=0.5) # up an octave
			rest(1)
			
			playNote(10, beats=0.5) # A sharp
			playNote(22, beats=0.5) # up an octave
			rest(1)
			
			playNote(8, beats=0.5) # G sharp
			playNote(20, beats=0.5) # up an octave
			rest(0.5)
			
			playNote(8, beats=0.5) # G sharp - G - G
			playNote(12, beats=0.5)
			playNote(24, beats=0.5)
			
			playNote(10, beats=0.75) # C sharp
			playNote(22, beats=0.25) # D sharp
		\end{verbatim}
	\end{enumerate}	
	\item {\sf2. Rhythm \& tempo.} This chap dives into fundamentals of {\it rhythm} in music. Start with beat -- what it is, how it's measured, \& how can visualize beat to compose, edit, \& play music. From there provide examples of some common rhythmic motifs from different genres of music \& how to code them with Python. Main programming concepts for this chap include loops, variables, calling function, \& passing parameter values. This chap covers a lot of ground, but it will give you a solid start on making music with code.
	\begin{itemize}
		\item {\sf2.1. Beat \& tempo.} {\it Beat} is foundation of rhythm in music. Term {\it beat} has a number of different meanings in music, [Term beat can also refer to main groove in a dance track (``drop the beat'') or instrumental music that accompanies vocals in a hip-hop track (``she produced a beat for a new artist'') in addition to other meanings.] but this chap uses it to mean a unit of time, or how long an individual note is played -- e.g., ``rest for 2 beats'' or ``play a note for half a beat''. Based on beat, musical notes are combined in repeated patterns that move through time to make rhythmic sense to our ears.
		
		{\it Tempo} refers to speed at which rhythm moves, or how quickly 1 beat follows another in a piece of music. As a listener, can feel tempo by tapping your foot to rhythmic pulse. Standard way to measure tempo is in beats per minute (BPM or bpm), meaning total number of beats played in 1 minute's time. This is almost always a whole number like 60, 120, or 78. At a tempo of 60 bpm, your foot taps 60 times each minute (or 1 beat per sec). At 120 bpm, get 2 beats every sec; \&, at 90 bpm, get 1.5 beats every sec. Later in this chap when start working with TunePad, can set tempo by clicking on bpm indicator in top bar of a project, see {\sf Fig. 2.1: TunePad project information bar. Can click on tempo, time signature, or key to change settings for your project.}
		
		Different genres of music have their own typical tempo ranges (although every song \& every artist is different). E.g., hip-hop usually falls in 60--110 bpm range, while rock is faster in 100--140 bpm range. House{\tt/}techno{\tt/}trance is faster still, with tempos between 120--140 bpm. {\sf[Table: Genre: Tempo Range (BPM)]}.
		
		It takes practice for musicians to perform at a steady tempo, \& they sometimes use a device called a {\it metronome} to help keep their playing constant with pulse of music. Can create a simple metronome in TunePad using 4 lines of code in a drum cell. This works best if switch instrument to {\tt Drums $\to$ Percussion Sounds}.
		\begin{verbatim}
			playNote(3, velocity = 100) # louder 1st note
			playNote(3, velocity = 60)
			playNote(3, velocity = 60)
			playNote(3, velocity = 60)
		\end{verbatim}
		Can adjust tempo of your metronome with bpm indicator {\sf Fig. 2.1: TunePad project information bar. Can click on tempo, time signature, or key to change settings for your project}. As this example illustrates, computers excel at keeping a perfectly steady tempo. This is great if want precision, but there's also a risk that resulting music will sound too rigid \& machine-like. When real people play music they \fbox{often speed up or slow down, either for dramatic effect or just as a result of being a human}. Depending on genre, performers might add slight variations in rhythm called swing or shuffle, that's a kind of back \& forth rocking of beat that you can feel almost more than you can hear. Show how to add a more human feel to computer generated music later in book.
		\item {\sf2.2. Rhythmic notations.} Over centuries, musicians \& composers have developed many different written systems to record \& share music. With invention of digital production software, a number of other interactive representations for mixing \& editing have become common as well. Here are 4 common visual representations of same rhythmic pattern. Pattern has a total duration of 4 beats \& can be counted as ``1 \& 2, 3 \& 4''. 1st 2 notes are $\frac{1}{2}$ beats long followed by a note that is 1 beat long. Then pattern repeats.
		\begin{itemize}
			\item {\sf2.2.1. Representation 1: Standard Western music notation.} 1st representation shows standard music notation (or Western notation), a system of recording notes that has been developed over many hundreds of years. 2 thick vertical lines on left side of illustration indicate: this is rhythmic notation, i.e., there is no information about musical pitch, only rhythmic timing. Dots on long horizontal lines are notes whose shapes indicate duration of each to be played. Sometimes different percussion instruments will have their notes drawn on different lines. Describe what various note symbols mean in more detail in {\sf Fig. 2.2: Standard notation example}.
			\item {\sf2.2.2. Representation 2: Audio waveforms.} 2nd representation shows a visualization of actual audio waveform that gets sent to speakers when play music. Waveform shows amplitude (or volume) of audio signal over time. Next chap talks more about audio waveforms, but for now can think of a waveform as a graph that shows literal intensity of vibration of your speakers over time. When compose a beat in TunePad, can switch to waveform view by clicking on small dropdown arrow at top-left side of timeline {\sf Fig. 2.3: Waveform representation of Fig. 2.2}.
			\item {\sf2.2.3. Representation 3: Piano (MIDI) roll.} 3rd representation shows a piano roll (or MIDI (Musical Instrument Digital Interface) roll). This uses solid lines to show individual notes. Length of lines represents length of individual notes, \& vertical position of lines represents percussion sound being played (kick drums \& snare drums in this case). This representation is increasingly common in music production software. Many tools even allow for drag \& drop interaction with individual notes to compose \& edit music {\sf Fig. 2.4: Piano or MIDI roll representation of Fig. 2.2}.
			\item {\sf2.2.4. Representation 4: Python code.} A final representation for now shows Python code in TunePad. In this representation, duration of each note is set using {\tt beats} parameter of {\tt playNote} function calls.
			\begin{verbatim}
				playNote(2, beats = 0.5)
				playNote(2, beats = 0.5)
				playNote(6, beats = 1)
				
				playNote(2, beats = 0.5)
				playNote(2, beats = 0.5)
				playNote(6, beats = 1)
			\end{verbatim}
		\end{itemize}
		Each of these representation has advantages \& disadvantages; they are good for conveying some kinds of information \& less good at conveying others. E.g., standard rhythm notation has been refined over centuries \& is accessible to an enormous, worldwide community of musicians. On other hand, it can be confusing for people who haven't learn how to read sheet music. Timing of individual notes is communicated using tails \& flags attached to notes, but there's no consistent mapping between horizontal space \& timing.
		
		Audio waveform is good at showing what sound {\it actually} looks like -- how long each note rings out (``release'') \& how sharp its onset is (``attack''). Helpful for music production, mixing, \& mastering. On other hand, waveforms don't really tell you much about pitch of a note or its intended timing as recorded by composer.
		
		Python code is easier for computers to read than humans -- it's definitely not sth you would hand to a musician to sight read. On other hand, it has advantage that it can be incorporated into computer {\it algorithms} \& manipulated \& transformed in endless ways.
		
		There are many, many other notation systems designed to transcribe a musical performance -- what hear at a live performance -- onto a sheet of paper or a computer screen. Each of these representations was invented for a specific purpose \&{\tt/}or genre of music. Might pick a representation based on context \& whether you're in role of a musician (\& what kind of instrument you play), a singer, a composer, a sound engineer, or a producer. Music notation systems are as rich \& varied as cultures \& musical traditions that invented them. 1 nice thing about working with software: easy to switch between multiple representations of music depending on task trying to accomplish.
		\item {\sf2.3. Standard rhythmic notation.} This sect will review a standard musical notation system that has roots in European musical traditions. This system is versatile \& has been refined \& adapted over a long period of time across many countries \& continents to work with an increasingly diverse range of instruments \& musical genres. Start with percussive rhythmic note values in this chap, \& move on to working with pitched instruments in Chap. 3.
		
		{\sf Fig. 2.5: Common note symbols starting with a whole note (4 beats) on top down to 16th notes on bottom. Notes on each new row are half length of row above.} shows most common symbols used in rhythmic music notation. Notes are represented with oval-shaped dots that are either open or closed. All notes except for whole note have tails attached to them that can point either up or down. It doesn't matter which direction (up or down) tail points. Notes that are faster than a quarter note also have horizontal flags or beams connected to tails. Each additional flag or beam indicates note is twice as fast.
		
		Symbol: Name: Beats: TunePad code:
		\begin{enumerate}
			\item Whole Note: Larger open circle with no tail \& no flag: 4: {\tt playNote(1, beats = 4)}
			\item Half Note: Open circle with a tail \& no flag: 2: {\tt playNote(1, beats = 2)}
			\item Quarter Note: Solid circle with a tail \& no flag: 1: {\tt playNote(1, beats = 1)}
			\item 8th Note: Solid circle with a tail \& 1 flag or bar: 0.5 or $\frac{1}{2}$: {\tt playNote(1, beats = 0.5)}
			\item 16th Note: Solid circle with a tail \& 2 flags or bars: 0.25 or $\frac{1}{4}$: {\tt playNote(1, beats = 0.25)}
			\item Dotted Half Note: Open circle with a tail. Dot adds an extra beat to half note: 3: {\tt playNote(1, beats = 3)}
			\item Dotted Quarter Note: Solid circle with a tail. Dot adds an extra half-beat: 1.5: {\tt playNote(1, beats = 1.5)}
			\item Dotted 8th Note: Solid circle with tail \& 1 flag. Dot adds an extra quarter beat: 0.75. {\tt playNote(1, beats = 0.75)}
		\end{enumerate}
		Standard notation also includes {\it dotted notes}, where a small dot follows note symbol. With a dotted note, take original note's duration \& add half of its value to it. So, a dotted quarter note is 1.5 beats long, a dotted half note is 3 beats long, etc.
		
		There are also symbols representing different durations of silence or ``rests''.
		
		Symbol: Name: Beats: TunePad code
		\begin{enumerate}
			\item Whole Rest: 4: {\tt rest(beats = 4)}
			\item Half Rest: 2: {\tt rest(beats = 2)}
			\item Quarter Rest: 1: {\tt rest(beats = 1)}
			\item 8th Rest: 0.5 or $\frac{1}{2}$: {\tt rest(beats = 0.5)}
			\item 16th Rest: 0.25 or $\frac{1}{4}$: {\tt rest(beats = 0.25)}
		\end{enumerate}
		\item {\sf2.4. Time signatures.} In standard notation, notes are grouped into segments called {\it measures} (or bars). Each measure contains a fixed number of beats, \& duration of all notes in a measure should add up to that amount. Relationship between measures \& beats is represented by a fraction called a {\it time signature}. Numerator (or top number) indicates number of beats in measure, \& denominator (bottom number) indicates beat duration.
		
		-- Trong ký hiệu chuẩn, các nốt nhạc được nhóm thành các đoạn gọi là {\it nhịp} (hoặc ô nhịp). Mỗi ô nhịp chứa một số phách cố định, \& thời lượng của tất cả các nốt nhạc trong một ô nhịp phải cộng lại bằng số lượng đó. Mối quan hệ giữa các ô nhịp \& nhịp được biểu diễn bằng một phân số gọi là {\it nhịp điệu}. Tử số (hoặc số trên cùng) biểu thị số phách trong ô nhịp, \& mẫu số (số dưới cùng) biểu thị thời lượng của phách.
		\begin{enumerate}
			\item $\frac{4}{4}$: 4-4 Time or ``Common tTime'': There are 4 beats in each measure, \& each beat is a quarter note. This time signature is sometimes indicated using a special symbol
			\item $\frac{2}{2}$: 2-2 Time or ``Cut Time'': There are 2 beats in each measure, \& beat value is a half note. Cut time is sometimes indicated with a `C' with a line through it.
			\item $\frac{2}{4}$: 2-4 Time: There are 2 beats in each measure, \& quarter note gets beat.
			\item $\frac{3}{4}$: 3-4 Time: There are 3 beats in each measure, \& quarter note gets beat.
			\item $\frac{3}{8}$: 3-8 Time: There are 3 beats in each measure, \& 8th note gets beat.
		\end{enumerate}
		Most common time signature is $\frac{4}{4}$. So common, in fact, referred to as {\it common time}. Often denoted by a C symbol shown in table. In common time, there are 4 beats to each measure, \& quarter note ``gets beat'' meaning: 1 beat is same as 1 quarter note.
		
		Vertical lines separate measures in standard notation. In example, there are 2 measures in 4/4 time (4 beats in each measure, \& each beat is a quarter note).
		
		If have a time signature of 3/4, then there are 3 beats per measure, \& each beat's duration is a quarter note. Some examples of songs is 3/4 time are {\it My Favorite Things} from {\it The Sound of Music, My 1st Song} by Jay Z, {\it Manic Depression} by {\sc Jimi Hendrix}, \& {\it Kiss from a Rose} by {\sc Seal}.
		
		If those notes were 8th notes, it would look like {\sf Fig.}
		
		Other common time signatures include 2/4 time (with 2 quarter note beats per measure) \& 2/2 time (with 2 {\it half note} beats in each measure). With 2/2 there are actually 4 quarter notes in each measure because 1 half note has same duration as 2 quarter notes. For this reason, 2/2 time is performed similarly to common time, but is generally faster. It is referred to as {\it cut time} \& is denoted by a C symbol with a line through it.
		
		Can adjust time signature of your TunePad project by clicking on time indicator in top bar (see {\sf Fig. 2.1}).		
		\item {\sf2.5. Percussion sounds \& instruments.} Working with rhythm, come across lots of terminology for different percussion instruments \& sounds. A quick rundown on some of most common drum sounds that you'll work with in digital music ({\sf Fig. 2.6: Drums in a typical drum kit.})
		
		{\sf Drum names: Description: TunePad note number}
		\begin{enumerate}
			\item Kick or bass drum: Kick drum (or bass drum) makes a loud, low thumping sound. Kicks are commonly placed on beats 1 \& 3 in rock, pop, house, \& electronic dance music. In other genres like hip-hop \& funk, kick drums are very prominent, but their placement is more varied: 0 \& 1
			\item Snare: Snare drums make a recognizable sharp staccato sound that cuts across frequency spectrum. They are built with special wires called snares that give drums its unique snapping sound. Snare drums are commonly used on beats 2 \& 4: 2 \& 3
			\item Hi-hat: Hi-hat is a combination of 2 cymbals sandwiched together on a metal rod. A foot pedal opens or closes cymbals together. In closed position hi-hat makes a bright tapping sound. In open position cymbal is allowed to ring out. Hi-hats have become an integral part of rhythm across almost all genres of popular music.: 4 (closed), 5 (open)
			\item Low, mid, high tom: Tom drums (tom-toms) are cylindrical drums that have a less snappy sound than snare drum. Drum kits typically have multiple tom drums with slightly different pitches (e.g. low, mid, \& high).: 6, 7, 8
			\item Crash cymbal: A large cymbal that makes a loud crash sound, often used as a percussion accent: 9
			\item Claps \& shakers: Different TunePad drum kits include a range of other percussion sounds common in popular music including various claps, shakers, \& other sounds.: 10 \& 11
		\end{enumerate}
		\begin{itemize}
			\item {\sf2.5.1. 808 Drum kit.} Released in early 1980s, Roland 808 drum machine was a hugely influential sound in early hip-hop music (\& other genres as well). 8.8 used electronic synthesis techniques to create synthesis replicas of drum sounds like kicks, snares, hats, toms, cowbells, \& rim shots. Tinkerers would also open up 808s \& hack circuits to create entirely new sounds. Today 808s usually refers to low, booming bass lines that were 1st generated using tweaked versions of 808s' kick drums. TunePad's default drum kit uses samples that sound like original electronically synthesized 808s ({\sf Fig. 2.7: Roland 808 drum sequencer.}).
			\item {\sf2.5.2. Selecting TunePad instruments.} When coding in Tunepad, sound that your code makes will depend on instrument you have selected. If coding a rhythm, can choose from several different drum kits including an 808 \& rock kits. Can change instrument by clicking on selector shown below {\sf Fig. 2.8: Changing an instrument's voice in TunePad.}
		\end{itemize}
		\item {\sf2.6. Coding rhythm in Python.}
		\begin{itemize}
			\item {\sf2.6.1. Syntax errors.} Python is a text-based language, i.e., you're going to be typing code that has to follow strict grammatical rules. When speak a natural language like English, grammar is important, but can usually bend or break rules \& still get your message across. When say something ambiguous it can be ironic, humorous, or poetic. This isn't case in Python. Python has no sense of humor \& no appreciation for poetry. If make a grammatical mistake in coding, Python gives a message called a {\it syntax error}. These messages can be confusing, but they're there to help you fix your code in same way that a spell checker helps you fix typos. Here's what a syntax error looks like in TunePad ({\sf Fig. 2.9: Example of a Python syntax error in TunePad. This line of code was missing a parenthesis symbol.})
			
			This line of code was missing a parenthesis symbol, which generated error message ``bad input on line 5''. Notice Python is giving hints about where problems are \& how to fix them, but those hints aren't always that helpful \& can be frustrating for beginners.
			\item {\sf2.6.2. Flow of control.} A Python program is made up of a list of statements. For most part, each statement goes on its own line in your program. Python will read \& perform each line of code from top to bottom in order that you write them. In programming this is called {\it flow of control}. This is similar to way you would read words in a book or notes on a line of sheet music. Difference: programming languages also have special rules that let you change flow of control. Those rules include {\it loops} (which repeat some part of your code multiple times), {\it conditional logic} (which runs some part of your code only if some condition is met), \& {\it user-defined functions} (which lets you create your own functions that can be called). Talk about these special ``control structures'' later in book.
		\end{itemize}
		\item {\sf2.7. Calling functions.} Almost everything you do in Python involves {\it calling} functions. A function (sometimes called a command or an instruction) tells Python to do something or to compute a value. E.g., {\tt playNote} function tells TunePad to make a sound. There are 3 things you have to do to call a function:
		
		1st, have to write name of function. Functions have 1-word names (with no spaces) that can consist of letters, numbers, \& underscore \verb|_| character. Multi-word functions will either use underscore character between words as in \verb|my_multi_word_function()| or each new word will be capitalized as in {\tt playNote()}.
		
		2nd, after type name of function, have to include parentheses. Parentheses tell Python that you're calling a function.
		
		3rd, include any {\it parameters} that you want to {\it pass} to function in between left \& right parentheses. A parameter provides extra information or tells function how to behave. E.g., {\tt playNote} statement needs at least 1 parameter to tell it which note or sound to play. Sometimes functions accept multiple parameters (some of which can be optional). {\tt playNote} function accepts several optional parameters described in next sect. Each additional parameter is separated with a comma ({\sf Fig. 2.10: Calling {\tt playNote} function in TunePad with 2 parameters inside parentheses.})
		\item {\sf2.8. playNote functions.} {\tt playNote} function tells TunePad to play a percussion sound or a musical note. {\tt playNote} function accepts up to 4 parameters contained within parentheses.
		\begin{verbatim}
			playNote(1, beats = 1, velocity = 100, sustain = 0)
		\end{verbatim}
		{\sf Name: Description}
		\begin{itemize}
			\item {\tt note}: This is a {\it required} parameter that says which note or percussion sound to play. Kind of sound depends on which instrument you have selected in TunePad for this code. Can play more than 1 note at same time by enclosing notes in square brackets.
			\item {\tt beats}: An {\it optional} parameter that says how long to play note. TunePad {\it playhead} will be moved forward by duration given. This parameter can be a whole number (like 1 or 2), a decimal number (like 1.5 or 0.25), or a fraction (like 1/2).
			\item {\tt velocity}: An {\it optional} parameter that says how loud to play note or sound. A value of 100 is full volume, \& a value of 0 is no volume (muted). Velocity is a technical term in digital music that means how fast or how hard you hit instrument. You might imagine it as how loud a drum sounds based on how hard it gets hit.
			\item {\tt sustain}: An {\it optional} parameter that allows a note to ring out for an additional number of beats without advancing playhead.
		\end{itemize}
		\begin{itemize}
			\item {\sf2.8.1. Optional parameters.} Sometimes parameters are {\it optional}, i.e., they have a value that gets provided by default if you don't specify one. For {\tt playNote}, only note parameter is required. If don't pass other parameters, it provides values for you by default. Can also include {\it names} of parameters in a function call. E.g., all 4 of lines below do exactly same thing; they play a note for 1 beat. 1st 2 use parameters without their names. 2nd 2 include names of parameter, followed by equals sign $=$, followed by parameter value.
			\begin{verbatim}
				playNote(60) # the beats parameter is optional
				playNote(60, 1) # with the beats parameter set to 1
				playNote(60, beats = 1) # with a parameter name for beats
				playNote(note = 60, beats = 1) # with a parameter name for note and beats
			\end{verbatim}
			\item {\sf2.8.2. Comments.} In code above, some of text appears after hashtag \# symbols on each line. This text is called a {\it comment}. A comment is a freedom note that programmers add to make their code easier to understand. Comment text is ignored by Python, so you can write anything you want after hashtag symbol on a line. Can also use a hashtag at beginning of a line to temporarily disable code. This is called ``commenting out'' code.
		\end{itemize}
		\item {\sf2.9. {\tt rest} function.} Silence is an important element of music. {\tt rest} function generates silence, or a break in sound. It only takes 1 parameter, which is length of time the rest is held. So {\tt rest(beats = 2)} will trigger a rest for a length of 2 beats. If don't provide a parameter, {\tt rest} uses a value of 1.0 by default.
		\begin{verbatim}
			rest() # rest for one beat
			rest(1.0) # rest for one beat
			rest(0.25) # rest for one quarter beat
			rest(beats = 0.25) # rest for one quarter beat
		\end{verbatim}
		\item {\sf2.10. Examples of {\tt playNote, rest}.} Try a few examples of {\tt playNote, rest} to get warmed up. This rhythm plays 2 8 notes (beats = 0.5) followed by a quarter note (beats = 1). Pattern then repeats a 2nd time. Here's how would code this in TunePad with a kick drum \& snare:
		\begin{verbatim}
			playNote(1, beats = 0.5) # play a kick drum (1) for half a beat
			playNote(1, beats = 0.5)
			playNote(2, beats = 1) # play snare (2) for one beat
			playNote(1, beats = 0.5) # play kick (1) for half a beat
			playNote(1, beats = 0.5)
			playNote(2, beats = 1) # play snare (2) for one beat
		\end{verbatim}
		Here's another example that plays a quarter note followed by a rest of 0.5 beats followed by an 8 note (beats = 0.5). Pattern is repeated 2 times in a row:
		\begin{verbatim}
			playNote(2, beats = 1) # play a snare drum (2) for one beat
			rest(beats = 0.5) # rest for half a beat
			playNote(1, beats = 0.5) # play a kick drum (1) for half a beat
			playNote(2, beats = 1) # play a snare drum (2) for one beat
			rest(beats = 0.5) # rest for half a beat
			playNote(1, beats = 0.5)
		\end{verbatim}
		A 3rd example that plays 8 notes in a row, each an 8 note (beats = 0.5).
		\item {\sf2.11. Loops.} All of examples in prev sect included repeated elements. \&, if listen closely, can hear repeated elements at all levels of music. There are repeated rhythmic patterns, recurring melodic motifs, \& storylines defined by song sects that get repeated \& elaborated. It turns out: there are many circumstances in both music \& computer programming where we want to repeat something over \& over again.
		
		To show how can take advantage of some of capabilities of Python, start with last example from prev sec where we wanted to tap out a run of 8th notes (0.5 beats) on hi-hat. 1 way to program that rhythm would be to just type in 8 {\tt playNotes} in a row.
		\begin{verbatim}
			for i in range(8):
			    playNote(4, beats = 0.5)
			    print(i)
		\end{verbatim}
		This will get job done, but there are a few problems with this style of code. 1 problem: it violates 1 of most important character traits of a computer programmer -- laziness! A lazy programmer is someone who works smart, not hard. A lazy programmer avoids doing repetitive, error-prone work. A lazy programmer knows that there are some things that computer can do better than a human can.
		
		In Python (\& just about any other programming language), when want to do something multiple times, can use a loop. Python has a number of different kinds of loops, but, in this case, our best option is sth called a {\tt for} loop. Version of code on right repeats 8 times in a row. For each iteration of loop, TunePad {\tt playNote} function gets called.
		
		With original code on left, had to do a lot of tying (or, more likely, copying \& pasting) to enter our program -- a warning sign that we're not being lazy enough. We generated a lot of repetitive code, which makes program harder to read (not as legible), error prone, \& not as elegant as it could be. Right-side code accomplishes same thing with just 3 lines of code instead of 8.
		
		Finally, code on left is harder to change \& reuse. What if wanted to use a different drum sound (like a snare instead of a hat)? Or, what if wanted to tap out a run of 16 16th notes instead of 8 8th notes? Would have to go through code line by line making same change over \& over again. This is a slow, error-prone process that is definitely not lazy or elegant.
		
		To see why this is better, try changing code on right so that it plays 16 16th notes instead of 8 8th notes. Or try changing drum sound from a hat to sth else. {\tt print} statement on line 3 is just there to help you see what's going on with your code. If click on {\tt Show Python Output} option, can see how variable called {\tt i} (that gets created on line 1) counts up from 0 to 7 {\sf Fig. 2.11: How to show print output of your code in a TunePad cell.} More detail about anatomy of a {\tt for} loop ({\sf Fig. 2.12: Anatomy of a {\tt for} loop in Python.}) A {\tt for} loop with range function:
		\begin{itemize}
			\item begins with {\tt for} keyword
			\item includes a loop {\it variable} name; this can be anything you want (above it is {\tt i}). Each time loop goes around, loop variable is incremented by 1.
			\item includes {\tt in} keyword
			\item includes {\tt range} function that says how many times to repeat
			\item includes a colon :
			\item includes a block of code indented by 4 spaces
		\end{itemize}
		Python uses {\it indentation} to determine what's {\it inside} loop, meaning it's sect of code that gets repeated multiple times. Intended block of code is repeated total number of times specified by {\tt range}. Try adding a few extras to prev example. In version below, add a run of 16th notes for last beat.
		\begin{verbatim}
			for i in range(6):
			    playNote(4, beats = 0.5)
			for i in range(4):
			    playNote(4, beats = 0.25)
		\end{verbatim}
		But there are lots of other things we could do as well. If wanted to play an even faster run, could use code like:
		\begin{verbatim}
			for i in range(8):
			    playNote(4, beats = 0.125)
		\end{verbatim}
		Or, if wanted to play a triplet that divides a half-beat into 3 equal parts, could do sth like this:
		\begin{verbatim}
			for i in range(3):
			playNote(4, beats = 0.25 / 3) # divide into 3 parts
		\end{verbatim}
		If open this example in TunePad, can experiment with different combinations of numbers to get different effects: \url{https://tunepad.com/examples/loops-and-hats}
		\item {\sf2.12. Variables.} A {\it variable} is a name you give to some piece of information in a Python program. Can think of a variable as a kind of nickname or alias. Similar to loops, variables help make your code more elegant, easier to read, \& easier to change in future. E.g., code on left plays a drum pattern without variables, \& code on right plays same thing with variables. Notice how variables help make code easier to understand because they give us descriptive names for various drum sounds instead of just numbers.
		\begin{verbatim}
			playNote(0)
			playNote(4)
			playNote(2)
			playNote(4)
			
			kick = 0
			hat = 4
			snare = 2
			
			playNote(kick)
			playNote(hat)
			playNote(snare)
			playNote(hat)
		\end{verbatim}
		In version on right defined a variable called {\tt kick} on line 1, a variable called {\tt hat} on line 2, \& a variable called {\tt snare} on line 3. Each variable is {\it initialized} to a different number for corresponding drum sound. Also possible to change value of a variable later in program by assigning it a different number.
		\begin{verbatim}
			kick = 0
			playNote(kick) # plays sound 0
			kick = 1       # set kick to a different value
			playNote(kick) # plays sound 1
		\end{verbatim}
		Variable names can be anything you want as long as they're 1 word long (no spaces) \& consist only of letters, numbers, \& underscore character \verb|_|. Variable names cannot start with a number, \& they can't be same as any existing Python keyword.
		
		As begin to get comfortable with code \& to exercise your creativity, find yourself wanting to experiment with sounds. Might want to try different sounds for same rhythmic pattern, maybe change a high-hat sound to a shaker to get a more organic feel. Using variables makes it easy to experiment by changing values around.
		
		Another example with a hi-hat pattern. Imagine really like this pattern, but wondering how it would sound with a different percussion instrument. Maybe you want to change 4 sound to a shaker sound (like 11). Nice thing about variables: can give them just about any name you want as long as Python is not already using that name for something else. This way you can make name meaningful to you. So, for our shaker example could create a variable with a meaningful name like {\tt shake} \& set it equal to 11. When use variable {\tt shake} you are inserting whatever number is currently assigned to it.
		\begin{verbatim}
			for i in range(8):
			    playNote(4,
			    beats = 0.5)
			for i in range(8):
			    playNote(4,
			    beats = 0.25)
			for i in range(4):
			    playNote(4,
			    beats = 0.5)
			    
			shake = 11
			for i in range(8):
			    playNote(shake,
			    beats = 0.5)
			for i in range(8):
			    playNote(shake,
			    beats = 0.25)
			for i in range(4):
			    playNote(shake,
			    beats = 0.5)
		\end{verbatim}
		As progress with coding, find that loops \& variables help create a smoother workflow that gives you more flexibility, freedom, \& creative power. Try out using variables with exercise \url{https://tunepad.com/examples/variables}.		
		\item {\sf2.13. More on syntax errors.} Python code is like a language with strict grammatical rules called syntax. When make a mistake in coding -- \& everyone makes coding mistakes all time -- Python will give feedback about what error is \& approximately what line it's on. E.g., if been trying to code exercises in this chap, may have seen a message like {\sf Fig. 2.13: Example of a Python syntax error. Command `ployNote' should instead say `playNote'.}
		
		[p. 38 +++]
	\end{itemize}
	\item {\sf3. Pitch, harmony, \& dissonance.} Chap. 2 introduced basics of rhythm \& how to use Python programming language to code beats with percussion sounds. In this chap, explore topics of pitch, harmony, \& dissonance -- or what happens when you bring tonal instruments \& human voice into music. Start with physical properties of sound (including frequency, amplitude, \& wavelength) \& why different musical notes sound harmonious or dissonant when played together. Also talk about different ways to represent pitch, including frequency value, musical note names, \& MIDI (Musical Instrument Digital Interface) note numbers that we can use with Python code \& TunePad.
	
	-- Chương 2 giới thiệu những điều cơ bản về nhịp điệu \& cách sử dụng ngôn ngữ lập trình Python để mã hóa nhịp điệu với âm thanh bộ gõ. Trong chương này, hãy khám phá các chủ đề về cao độ, sự hòa hợp, \& sự bất hòa -- hoặc điều gì xảy ra khi bạn đưa nhạc cụ có âm \& giọng nói của con người vào âm nhạc. Bắt đầu với các đặc tính vật lý của âm thanh (bao gồm tần số, biên độ, \& bước sóng) \& lý do tại sao các nốt nhạc khác nhau nghe có vẻ hòa hợp hay bất hòa khi chơi cùng nhau. Ngoài ra, hãy nói về các cách khác nhau để biểu diễn cao độ, bao gồm giá trị tần số, tên nốt nhạc, \& số nốt MIDI (Giao diện kỹ thuật số nhạc cụ) mà chúng ta có thể sử dụng với mã Python \& TunePad.
	\begin{itemize}
		\item {\sf3.1. Sound Waves.} All sound, no matter how simple or complex, is made up of waves of energy that travel through air, water, or some other physical medium. If could see a sound wave, it might look sth like ripples of water from a pebble dropped in a still pound. Pebble is like source of sound, \& ripples are sound waves that expand outward in all directions. Any source of sound (car horns, cell phone rings, chirping birds, or a plucked guitar string) sends vibrating waves of air pressure out at around 343 meters per sec (speed of sound) from source. It's not that air molecules themselves travel from source of sound to our ears; it's that small localized movements in molecules create fluctuations in air pressure that propagate outward over long distances.
		
		-- Mọi âm thanh, dù đơn giản hay phức tạp, đều được tạo thành từ các sóng năng lượng truyền qua không khí, nước hoặc một số môi trường vật lý khác. Nếu có thể nhìn thấy sóng âm, nó có thể trông giống như gợn sóng nước từ một viên sỏi thả vào một pound đứng yên. Viên sỏi giống như nguồn âm thanh, \& gợn sóng là sóng âm lan ra ngoài theo mọi hướng. Bất kỳ nguồn âm thanh nào (tiếng còi xe, chuông điện thoại di động, tiếng chim hót hoặc dây đàn guitar gảy) đều phát ra sóng rung động của áp suất không khí với tốc độ khoảng 343 mét một giây (tốc độ âm thanh) từ nguồn. Không phải bản thân các phân tử không khí di chuyển từ nguồn âm thanh đến tai chúng ta; mà là các chuyển động cục bộ nhỏ trong các phân tử tạo ra sự dao động trong áp suất không khí lan truyền ra ngoài trên những khoảng cách xa.
		
		Once those waves reach human ear, they are captured by outer ear \& funneled to a seashell-shaped muscle in inner ear called {\it cochlea}. This muscles has tiny hairs that resonate at different frequencies causing messages to get sent to brain that we interpret as sound.
		
		\begin{remark}[Protect your hearing]
			As musicians or music producers, your sense of hearing is 1 of your most precious assets. Always wear ear protection when you're exposed to loud sustained sounds! Loud sounds can damage your inner ear permanently, meaning you can start to close your ability to hear.
		\end{remark}
		All sound waves have following properties: {\it frequency, wavelength, \& amplitude.}
		\item {\sf3.2. Frequency.} Frequency refers to number of times a complete waveform passes through a single point over a period of time or how fast wave is vibrating. It is measured by cycles per sec in a unit called {\it hertz} (Hz). 1 cycle per sec is equivalent to 1 Hz, \& 1000 cycles are equivalent to 1000 Hz, or 1 kHz (pronounecd kilohertz). \fbox{Higher frequency, higher pitch of sound.} {\sf Fig. 3.1: Sound is made up of compression waves of air molecules that expand outward at a speed of around $343$ {\rm m{\tt/}s}. Frequency of a sound wave refers to how fast it vibrates: amplitude refers to intensity of sound; \& wavelength refers to length of 1 complete cycle of waveform.}
		\item {\sf3.3. Wavelength.} Wavelength refers to length of 1 complete cycle of a wave in physical space. This is distance from 1 peak or zero crossing to next. Can't actually see sound waves, but wavelength can be calculated by dividing speed of sound ($\approx343$ m{\tt/}s) by its frequency. So, for pitch of a {\it Concert A} note (440 Hz), length of waveform would be $\approx78$ cm or 2.6 ft.
		\begin{equation*}
			\frac{343\mbox{ m{\tt/}s}}{440\mbox{ Hz}} = 0.78\mbox{ m} = 78\mbox{ cm} = 2.56\mbox{ ft}.
		\end{equation*}
		On most pianos, wavelength of lowest bass note is almost 40 feet long! In contrast, wavelength of highest note is only around 3 inches. \fbox{Longer wavelength, lower the note.}
		
		Lower frequency sound also tends to travel longer distances. Think of a car playing loud music. As it approaches, you can hear fat sound of a bass guitar or a kick drum long before you can hear other instruments. Using this property, people in West Africa were able to transmit detailed messages over long distances using a language of deep drum sounds. A drummer called a ``carrier'' would drum out a rhythmic pattern on a huge log drum that carried messages like ``all people should gather at market place tomorrow morning''. All those within hearing range, which under ideal conditions could be as far as 7 miles, would receive message.
		
		-- Âm thanh tần số thấp hơn cũng có xu hướng truyền đi xa hơn. Hãy nghĩ đến một chiếc ô tô đang phát nhạc lớn. Khi nó đến gần, bạn có thể nghe thấy âm thanh to của một cây đàn ghi-ta bass hoặc trống đá rất lâu trước khi bạn có thể nghe thấy các nhạc cụ khác. Sử dụng đặc tính này, người dân ở Tây Phi có thể truyền tải các thông điệp chi tiết trên những khoảng cách xa bằng ngôn ngữ của âm thanh trống sâu. Một tay trống được gọi là ``người mang'' sẽ đánh một mẫu nhịp điệu trên một chiếc trống gỗ lớn mang theo các thông điệp như ``tất cả mọi người nên tập trung tại chợ vào sáng mai''. Tất cả những người trong phạm vi nghe được, trong điều kiện lý tưởng có thể cách xa tới 7 dặm, sẽ nhận được thông điệp.
		\item {\sf3.4. Amplitude.} Amplitude is related to volume of a sound, or how high peaks of waveform are Fig. 3.1. You can think of this as how much energy passes through a fixed amount of space over a fixed amount of time. Human ear perceives a vast range of sound levels, from sounds that are softer than a whisper to sounds that are louder than a pain-inducing jackhammer. In order to communicate volume of sound in a manageable way, music producers \& engineers use a unit of loudness called {\it decibels} (dB). Whispered voice level might be 30 dB, while jackhammer sound would be about 110 dB. Loud noises $> 120$ dB can cause immediate harm to ears.
		
		-- Biên độ liên quan đến âm lượng của âm thanh hoặc độ cao của các đỉnh sóng Hình 3.1. Bạn có thể nghĩ về điều này như lượng năng lượng đi qua một lượng không gian cố định trong một khoảng thời gian cố định. Tai người cảm nhận được một phạm vi rộng lớn các mức âm thanh, từ âm thanh nhẹ hơn tiếng thì thầm đến âm thanh to hơn tiếng búa khoan gây đau đớn. Để truyền đạt âm lượng âm thanh theo cách dễ quản lý, các nhà sản xuất âm nhạc \& kỹ sư sử dụng một đơn vị độ lớn gọi là {\it decibel} (dB). Mức giọng nói thì thầm có thể là 30 dB, trong khi âm thanh của búa khoan sẽ là khoảng 110 dB. Tiếng ồn lớn $> 120$ dB có thể gây hại ngay lập tức cho tai.
		\item {\sf3.5. Dynamics.} Variation of amplitude levels from low to high within a musical composition is referred to as dynamics. Difference between softest sound to loudest sound is called {\it dynamic range} of music. You can look at {\it waveform} of an audio signal to get a quick sense for its dynamic range. In general, lower heights mean lower amplitude \& higher heights mean higher amplitude. Loudness of a sound is also dependent on frequency. So, looking at a waveform alone won't tell you how loud sth will sound to listeners ({\sf Fig. 3.3: A waveform with varying amplitude}).
		
		-- {\sf Động lực học.} Sự thay đổi mức biên độ từ thấp đến cao trong một bản nhạc được gọi là động lực học. Sự khác biệt giữa âm thanh nhỏ nhất đến âm thanh to nhất được gọi là {\it dynamic range} của âm nhạc. Bạn có thể xem {\it waveform} của tín hiệu âm thanh để có cảm nhận nhanh về dải động của nó. Nhìn chung, độ cao thấp hơn có nghĩa là biên độ thấp hơn \& độ cao cao hơn có nghĩa là biên độ cao hơn. Độ to của âm thanh cũng phụ thuộc vào tần số. Vì vậy, chỉ xem dạng sóng sẽ không cho bạn biết âm thanh nào đó sẽ to như thế nào đối với người nghe ({\sf Hình 3.3: Dạng sóng có biên độ thay đổi}).
		\item {\sf3.6. Bandwidth.} Bandwidth refers to range of frequencies present in audio. As in case of dynamic range, can think of this as difference between highest \& lowest frequencies. Humans with good hearing can distinguish sounds between 20 Hz \& 20000 Hz. Most audio formats designed for music support frequencies up to 22 kHz (pronounced 22 kilohertz or 22000 hertz) so that they can capture full range of human hearing.
		
		-- Băng thông đề cập đến phạm vi tần số có trong âm thanh. Giống như trường hợp của phạm vi động, có thể coi đây là sự khác biệt giữa tần số cao nhất \& thấp nhất. Con người có thính giác tốt có thể phân biệt âm thanh giữa 20 Hz \& 20000 Hz. Hầu hết các định dạng âm thanh được thiết kế cho âm nhạc đều hỗ trợ tần số lên đến 22 kHz (phát âm là 22 kilohertz hoặc 220000 hertz) để chúng có thể thu được toàn bộ phạm vi thính giác của con người.
		
		Musical instruments naturally fall within range of human hearing at different places on frequency spectrum; this is referred to as instrument's bandwidth. {\it Instrument bandwidth} is important to music producers as they arrange a musical composition. In addition to quality of sound of instruments, those in different bandwidths can complement each other. Like a cello \& a flute, or a bass \& a saxophone. Music producers are keenly aware of influence of low- \& high-frequency instruments on their listeners. Musical instruments in bass register are often foundation of composition, holding everything together.
		
		-- Nhạc cụ tự nhiên nằm trong phạm vi nghe của con người ở các vị trí khác nhau trên phổ tần số; điều này được gọi là băng thông của nhạc cụ. {\it Băng thông nhạc cụ} rất quan trọng đối với các nhà sản xuất âm nhạc khi họ sắp xếp một bản nhạc. Ngoài chất lượng âm thanh của các nhạc cụ, những nhạc cụ có băng thông khác nhau có thể bổ sung cho nhau. Giống như đàn cello \& sáo, hoặc đàn bass \& saxophone. Các nhà sản xuất âm nhạc nhận thức sâu sắc về ảnh hưởng của các nhạc cụ có tần số thấp \& cao đến người nghe của họ. Nhạc cụ có âm trầm thường là nền tảng của bản nhạc, giữ mọi thứ lại với nhau.
		\item {\sf3.7. Pitch.} Within spectrum of human hearing, specific frequencies, ranges of frequencies, \& combinations of frequencies are essential for creating music. This sect covers some combinations of musical tones common in Western music culture. Then work in TunePad to try out different combinations \& explore those relationships through well-known musical compositions.
		
		-- Trong phổ thính giác của con người, các tần số cụ thể, phạm vi tần số, \& sự kết hợp của các tần số là điều cần thiết để tạo ra âm nhạc. Giáo phái này bao gồm một số sự kết hợp của các giai điệu âm nhạc phổ biến trong văn hóa âm nhạc phương Tây. Sau đó, hãy làm việc trong TunePad để thử các sự kết hợp khác nhau \& khám phá các mối quan hệ đó thông qua các tác phẩm âm nhạc nổi tiếng.
		
		While music producers \& engineers often think in terms of frequencies (hertz), musicians use pitch \& intervals to describe musical tones \& relationships between them. Pitches are individual notes like F, G, A, B, C, D, E as seen on piano keyboard. Interval between each adjacent note on a traditional keyboard is called a half step or a semitone. These base pitches can also have {\it accidentals}. Accidentals are like modifiers to notes that raise or lower base pitch. A note with a sharp \# applied has its pitch raised by a semitone, which a note with a flat $\flat$ applied is lowered by a semitone. Black notes on a piano are notes with accidentals. E.g., moving a C\# (black key) is a half step. Moving directly from a C to a D (both white keys) is called a whole step. Moving from a B to a C or an E to an F is also a half step because there's no black key in between ({\sf Fig. 3.4: A half step is distance between 2 adjacent piano keys, measured in semitones.})
		
		-- Trong khi các nhà sản xuất âm nhạc \& kỹ sư thường nghĩ theo tần số (hertz), thì các nhạc sĩ sử dụng cao độ \& khoảng cách để mô tả các cung bậc âm nhạc \& mối quan hệ giữa chúng. Cao độ là các nốt riêng lẻ như F, G, A, B, C, D, E như thấy trên bàn phím piano. Khoảng cách giữa mỗi nốt liền kề trên bàn phím truyền thống được gọi là nửa cung hoặc nửa cung. Các cao độ cơ bản này cũng có thể có {\it dấu hóa ngẫu nhiên}. Dấu hóa ngẫu nhiên giống như các dấu hiệu bổ nghĩa cho các nốt làm tăng hoặc giảm cao độ cơ bản. Một nốt có dấu thăng \# được áp dụng có cao độ được tăng lên một nửa cung, trong khi một nốt có dấu giáng $\flat$ được áp dụng sẽ hạ xuống một nửa cung. Các nốt đen trên đàn piano là các nốt có dấu hóa ngẫu nhiên. Ví dụ, di chuyển một C\# (phím đen) là nửa cung. Di chuyển trực tiếp từ C sang D (cả hai đều là phím trắng) được gọi là một cung trọn vẹn. Di chuyển từ B sang C hoặc từ E sang F cũng là nửa cung vì không có phím đen nào ở giữa ({\sf Hình 3.4: Nửa cung là khoảng cách giữa 2 phím đàn piano liền kề, được đo bằng nửa cung.})
		\item {\sf3.8. Musical Instrument Digital Interface.} 1 takeaway from prev sect: note names are confusing. There are multiple names for same pitch (G\# is same as A$\flat$), \& note names are repeated every octave. To help make things less ambiguous, computers \& digital musical instruments use a standardized format called {\it MIDI}, which stands for Musical Instrument Digital Interface. MIDI is a protocol, or set of rules, for how digital musical instruments communicate. Digital musical instruments send message to your computer or to other musical instruments. Typical MIDI controllers look like piano keyboards or drum pads but can take many other forms as well. When play a MIDI instrument, it sends information about a note's pitch, timing, \& volume along with other messages about vibrato, pitch bend, pressure, panning, \& clock signals. This table show 2 octaves of notes with their typical frequency values {\sf[Table]}. Appendix contains a complete table with note names, frequency values, \& MIDI numbers.
		
		TunePad uses MIDI numbers to designate pitch. To play a C0, lowest pitch on TunePad keyboard, use code {\tt playNote(12)}. To play a C4, a middle C in center of an 880key piano, use code {\tt playNote(60)}. MIDI notes go all way up to note G9 with note value 127.
		
		Now experiment with pitch in TunePad. Try creating a new piano instrument in TunePad \& adding this code:
		\begin{verbatim}
			# code for first piano cell
			playNote(48)
			playNote(55)
			playNote(60)
			playNote(55)
		\end{verbatim}
		This program plays 4 notes: 48 is a C, 55 is a G, \& 60 is a middle C. Now add a 2nd piano instrument to same project so that you have 2 cells. Add this code to 2nd cell:
		\begin{verbatim}
			# code for second piano cell
			playNote(72, beats = 4) # C5
			playNote(79, beats = 4) # G5
			playNote(76, beats = 4) # E5
			playNote(79, beats = 4) # G5
		\end{verbatim}
		This Python program looks similar to 1st one, but we've changed length of each note using {\it beats} parameter. In this case, asking TunePad to play 4 notes, each 4 beats long. Try playing both piano parts at same time. Can also make our notes shorter instead of longer. Add a 3rd piano instrument with notes that are each 1 half beat long. Try playing all 3 pianos together.
		\begin{verbatim}
			# code for third piano cell
			playNote(36, beats = 0.5)
			playNote(36, beats = 0.5)
			playNote(43, beats = 0.5)
			playNote(43, beats = 0.5)
			playNote(48, beats = 0.5)
			playNote(48, beats = 0.5)
			playNote(43, beats = 0.5)
			playNote(43, beats = 0.5)
		\end{verbatim}
		\item {\sf3.9. Harmony.} {\it Harmony} in music can be defined as a combination of notes that, when played together, have a pleasing sound. Although opinions about what sounds good in music are highly subjective, certain combinations of notes played together can \fbox{elicit predictable psychological responses} -- some combinations of notes sound {\it harmonious} while others some {\it discordant}. Musicians use this phenomenon to create an emotional tone for their compositions.
		
		-- {\it Harmony} trong âm nhạc có thể được định nghĩa là sự kết hợp các nốt nhạc, khi chơi cùng nhau, tạo ra âm thanh dễ chịu. Mặc dù ý kiến về những gì nghe hay trong âm nhạc là rất chủ quan, nhưng một số sự kết hợp các nốt nhạc chơi cùng nhau có thể \fbox{gợi ra những phản ứng tâm lý có thể dự đoán được} -- một số sự kết hợp các nốt nhạc nghe {\it hài hòa} trong khi một số khác lại {\it không hài hòa}. Các nhạc sĩ sử dụng hiện tượng này để tạo ra giai điệu cảm xúc cho các sáng tác của họ.
		
		In Western music, much of our conception of pitch is built on different mathematical ratios. Consider string of an instrument like a guitar or violin. Plucking open A (2nd lowest) string plays an A, which has a frequency of 110 Hz. Now if touch string at its midpoint, dividing it in half, still hear an A an octave above previous one -- twice frequency of 1st note, or 220 Hz. If touch string $\frac{1}{3}$ of way down \& pluck it, result is an E above higher A. This E is exactly 3 times our original frequency, or 330 Hz. Likewise, dividing string into 4ths multiplies original frequency by 4. Can continue this division on string as follows {\sf Fig. 3.5: harmonic series.}
		
		-- Trong âm nhạc phương Tây, phần lớn quan niệm của chúng ta về cao độ được xây dựng dựa trên các tỷ lệ toán học khác nhau. Hãy xem xét dây của một nhạc cụ như đàn ghi-ta hoặc đàn violin. Gảy mở dây A (dây thấp thứ 2) sẽ tạo ra nốt A, có tần số 110 Hz. Bây giờ nếu chạm vào dây ở điểm giữa của nó, chia nó thành hai nửa, vẫn nghe thấy nốt A cao hơn một quãng tám so với nốt trước đó -- gấp đôi tần số của nốt đầu tiên, hoặc 220 Hz. Nếu chạm vào dây $\frac{1}{3}$ xuống một khoảng \& gảy nó, kết quả là nốt E cao hơn nốt A cao hơn. Nốt E này chính xác gấp 3 lần tần số ban đầu của chúng ta, hoặc 330 Hz. Tương tự như vậy, chia dây thành 4 quãng sẽ nhân tần số ban đầu với 4. Có thể tiếp tục phép chia này trên dây như sau {\sf Hình 3.5: chuỗi điều hòa.}
		
		Resulting sequence of ascending pitches this produces is known as {\it harmonic series}. If 2 notes have a harmonic relationship, i.e., 2 frequencies s.t. result is a whole number. Harmonic series is simply set of frequencies that have a harmonic relationship to a {\it fundamental pitch} (initial note). Our initial experiment with string illustrates this relationship.
		
		To find frequencies that make up harmonic series for a given pitch, multiply its frequency by set of whole numbers. For A1, which is 55 Hz, 1st 8 harmonics would be {\sf[Table]}.
		
		In table, MIDI value is given for each harmonic of A1. Notice these values are given in decimal format. In TunePad, {\tt playNote} accepts both whole \& decimal values. Whole numbers are a data type referred to as {\it integer} values, or just as {\it ints}. Decimals are a separate data type referred to as {\it floating point} values, or just {\it floats}.
		
		Listen to an example \url{https://tunepad.com/examples/harmonic-series}. Notes with frequencies that form simple ratios, e.g. 2:1, 3:2, 4:3, or 5:4, tend to sound good together. E.g., can take note A4 (440 Hz) \& add a frequency that is 1.5 times its value, giving us an E4 (660 Hz). This results in a ratio of 3:2 \& a pleasant sound. However, if add a frequency that is 1.3 times value of 440 Hz, end up with 572 Hz, which creates a not-so-pleasant combination of tones. It's not an accident that there is no corresponding musical note to 572 Hz on piano keyboard.
		\item {\sf3.10. Intervals.} In music, an {\it interval} is distance between 2 notes. These notes can either be played simultaneously or not. If they are played simultaneously, pitches are called a {\it dyad} or a {\it chord}. Otherwise, they are a {\it melodic} interval. An interval is always measured from lowest note. Intervals have 2 different components: {\it generic interval} \& quality. Generic interval is distance from 1 note of a scale to another; this can also be described as number of letter names between 2 notes, including both notes in question. E.g., generic interval of C4 \& E4 has C4, D4, \& E4 in between. That's 3 notes, so we have a 3rd. Generic interval between F\#3 \& G3 is a second. Generic interval between G2 \& G3 in an 8th -- also known as an octave. Quality can be 1 of 5 options: Perfect, Major, Minor, Augmented, or Diminished. Each quality has a distinct sound \& can generate different emotional responses. Some common intervals in music along with their frequency ratios \& half steps. {\sf[Table]}
		
		These intervals are based on harmonic series, but this isn't exactly how most instruments are tuned. Talk more about this below. Also, naming of ratios (5th, 4th, Major 3rd, etc.) will make more sense in Chap. 5 where talk about scales \& keys. Notice that there's 1 particularly nasty-looking ratio called {\it Tritone interval} (45:32). This interval has historically been referred to as {\it Devil in Music} \& was frequently avoided in music composition for its dissonant qualities.
		
		-- Các khoảng này dựa trên chuỗi hài hòa, nhưng đây không phải là cách chính xác mà hầu hết các nhạc cụ được lên dây. Hãy nói thêm về điều này bên dưới. Ngoài ra, việc đặt tên cho các tỷ lệ (5, 4, 3 trưởng, v.v.) sẽ hợp lý hơn trong Chương 5, nơi nói về các thang âm \& cung. Lưu ý rằng có 1 tỷ lệ trông đặc biệt khó chịu được gọi là {\it Quãng ba cung} (45:32). Khoảng này trước đây được gọi là {\it Devil in Music} \& thường bị tránh trong sáng tác nhạc vì tính chất bất hòa của nó.
		
		1 of simplest \& most common intervals is octave, which has a frequency ratio of 2 to 1 (2:1) -- i.e., higher pitch completes 2 cycles in same amount of time that lower pitch completes 1 full cycle. Notes that are octave intervals from 1 another have same letter name \& are grouped together on a piano keyboard. Notice repeating patterns where C is highlighted note, C3, C4, C5 ({\sf Fig. 3.4: A half step is distance between 2 adjacent piano keys, measured in semitones.}). To illustrate, can begin with {\it middle C} (C4), which is $\approx262$ Hz, \& then move to a C5, which is an octave above it at 524 Hz. Can see C5 is double C4 frequency forming octave ratio, 2:1. Waveforms representing 2 notes forming this octave are plotted in {\sf Fig. 3.6: 2 waves at an interval of an octave.} Can see that for every single complete cycle of 262 Hz wave, C4, there are 2 full cycles of waveform for octave above it, 524 Hz C5. Easier to count cycles if look at 0-crossings.
		
		What does an octave look like in code? As have seen, each note on piano keyboard is a half step, \& there are 12 half steps between octaves. Try counting notes between C4 \& C5. Remember, black keys count!
		
		TunePad tracks notes on keyboard by half steps, so can easily play any octave interval without having to figure out exact note number. E.g., this code plays a middle C (60) \& a C 1 octave higher.
		\begin{verbatim}
			note = 60
			playNote(note)
			playNote(note + 12)
		\end{verbatim}
		This code assigned number 60 to variable {\tt note} on 1st line. 3rd line played a note 1 octave higher by adding 12 to original {\tt note} variable (not 72 is played). Expanding on this, can substitute any number you want for {\tt note} \& generate an octave above it by adding 12.
		
		Octaves sound good together in music \& are used in many popular songs. E.g., in song {\it Over the Rainbow} composed by {\sc Harold Arlen} from {\it Wizard of Oz}, beginning 2 notes are an octave apart.
		\begin{verbatim}
			# First two bars of "Over the Rainbow"
			# Composed by Henry Arlen
			playNote(60, beats = 2) # note C4
			playNote(60 + 12, beats = 2) # note C5
			playNote(71, beats = 1)
			playNote(67, beats = 0.5)
			playNote(69, beats = 0.5)
			playNote(71, beats = 0.5)
			rest(0.5)
			playNote(72, beats = 1)
		\end{verbatim}
		Try this example at \url{https://tunepad.com/examples/rainbow}.
		
		Another interval relationship important to Western music is ratio of 3:2, also known as perfect 5th, which has 7 half steps between notes. With this interval ratio, there are 3 complete cycles of higher frequency for every 2 periods of lower frequency ({\sf Fig. 3.7: Ratio between note C 262 Hz \& note G 393 Hz is considered a perfect 5th.})
		
		{\sc Henry Mancini} uses a perfect 5th (G3 392 Hz \& D4 587 Hz) in 1st 2 notes in melody for song {\it Moon River}. Code 1st few bars of {\it Moon River} using a variable called \verb|root_note| to set starting note. This allows us flexibility to easily play song beginning from any note on keyboard \& relationship between notes stays same no matter which note you start with. Try changing value of variable \verb|root_note| to another MIDI note. This can come in handy when you are composing for a singer who would rather have song in another key or octave.
		\begin{verbatim}
			# First bars of "Moon River"
			# Composed by Henry Mancini
			root note = 55
			playNote(root note, beats = 3)
			playNote(root note + 7, beats = 1)
			playNote(root note + 5, beats = 2)
			playNote(root note + 4, beats = 1.5)
			playNote(root note + 2, beats = 0.5)
			playNote(root note, beats = 0.5)
			playNote(root note - 2, beats = 0.5)
			playNote(root note, beats = 2)
		\end{verbatim}
		See this example at \url{https://tunepad.com/examples/moon}.
		\item {\sf3.11. Dissonance.} Dissonance refers to combinations of notes which when combined have an unpleasant sound that creates tension. Interval of a minor second (or 1 half step) is a complex frequency ratio of about 9.5:1. This combination gives you a sense of suspense. Can hear effect of dissonance used in composition by {\sc John Willimas} for movie {\it Jaws}. Use {\tt for} loop for this example, as 2 notes are repeated.
		\begin{verbatim}
			# bass line for the theme from Jaws
			# composed by John Williams
			for i in range(8):
			    playNote(40, beats = 0.5) # E2
			    playNote(41, beats = 0.5) # F2
		\end{verbatim}
		Intervals that are dissonant are unstable, leaving listener with impression that notes {\it want} to move elsewhere to resolve to more stable or {\it consonant} intervals.
		
		Can try this example in TunePad to hear how notes that are 1 half step apart crunch when played together.
		\begin{verbatim}
			# half step - the notes are just 1 number away
			playNote(41, beats=1, sustain=3)
			playNote(42, beats=1, sustain=2)
			rest(2)
			playNote([41, 42], beats=4)
			rest(2)
			
			# whole step - these notes are 2 numbers away
			playNote(41, beats=1, sustain=3)
			playNote(43, beats=1, sustain=2)
			rest(2)
			playNote([41, 43], beats=4)
			rest(2)
		\end{verbatim}
		Listen to this example \url{https://tunepad.com/examples/dissonance}.
		
		Another example of use of dissonant intervals comes from horror movie {\it Halloween} (1978). Theme song by {\sc John Carpenter} creates a sense of suspense \& deep unease with use of dissonant intervals e.g. Tritone (ratio 45:32).
		\item {\sf3.12. Temperaments \& Tuning.} Follow along at \url{https://tunepad.com/examples/temperaments} .
		
		Intervals in prev sects were based on ratios called perfect or pure intervals. Waves of so-called perfect intervals align at a simple integer ratio. If 2 tones form a perfect interval, it will result in a louder sound, as amplitudes are added. If 1 of tones is out of tune, then there will be interference between 2 waves. This interference manifests as an audible rhythmic swelling or ``wah-wah'' between waves, which we call {\it beating}. Farther 2 tones are from being perfect, faster beating. If tones are apart far enough, might even hear this beating as a 3rd tone, called a {\it combination tone}. Pure \& impure intervals are not a value judgment but a description of natural phenomena.
		
		Using notes based on these simple ratios seems to make a lot of sense -- it's based on simple mathematical relationships that we know sound good to human ear. But, it turns out: quickly run into problems using this system when start trying to tune an instrument like a piano. E.g., say trying to tune an A4 against a fixed lower tone on a keyboard using pure ratios. If tuning this A4 against an F4 at $\approx349$ Hz, our intervals form a major 3rd at a 5:4 ratio of frequencies. This results in our A4 being $\approx436.26$ Hz. But, if tune our A4 against an F\#4 at 370 Hz, this produces a minor 3rd, which is at a 6:5 ratio of frequencies. Now our A4 is 444 Hz instead of 436.25 Hz! How can it be that same note maps to different frequencies?
		
		Question of how to map frequency -- of which there are endless possible values -- to a fine set of notes means that we have to both arbitrarily choose a starting point \& also decide at what intervals to increment. This is basis for what are called {\it temperaments}. Temperaments are systems that define sizes of different intervals -- how tones relate to 1 another. In choosing tones in an octave, must compromise between our melodic intervals \& our harmony. Ideally, want a system with as consistent melodic intervals \& that is as close to perfect harmonic intervals as possible. In a system based on perfect ratios -- also referred to {\it Just Intonation} -- divisions, or semitones, of an octave are not evenly distributed. I.e., there are unique sets of tunings for every note we choose as base note of our octave. Just Intonation also does not form a closed loop of an octave. This is getting into weeds a bit, but if derive each note's frequency by tuning ratio of a perfect 5th (3:2) from prev note, do not end up at same place 1 octave higher. In fact, tuning ratio of 3:2 12 times brings us back to our exact starting note only after 7 octaves. Because Just Intonation has too many mathematical snares to be represented by 12 notes of keyboard, it's not a stable tuning system \& not a temperament. By definition, a temperament is a calculated deviation from Just Intonation that maps each note to exactly 1 frequency while still getting as close as possible to pure intervals.
		
		Most contemporary music, including TunePad, is based on a system called Equal Temperament. Octave at a pure 2:1 ratio serves as foundation, which is then divided into 12 equal half steps. Most often, Western harmony is built primarily from 3rds, 5ths, \& octaves. Every octave (\& unison) is a pure interval in Equal Temperament. Perfect 4ths \& 5ths are {\it nearly} pure intervals. Major \& minor 3rds are quite far from perfect, but because we have grown so accustomed to hearing these intervals, they do not sound off to our ears. Because Equal Temperament is, well, equal, every chord will have same sound in every key. Each semitone is equally sized, \& every note maps to exactly 1 frequency. Furthermore, each semitone is divided into 100 cents, which we can use to further specify intonation. With our intervals decided, now only have to choose a starting pitch from which to tune others. Most of time in North America, system is aligned to A440, i.e. A4 is equal to exactly 440 Hz.
		
		Keyboard instruments have fixed pitch, while singers \& instruments e.g. violin or flute have flexible tuning. In acoustic performance, pitch can vary due to many factors. No instrument is perfectly in tune. Tuning can be affected by factors e.g. air pressure \& temperature. Even a performer's physiology can affect tuning. Often, performers will tune harmonies using Just Intonation s.t. a chord uses pure intervals \& is more pleasing. Many musicians will do this without even being aware that they are doing it -- Just Intonation just {\it feels} in tune.
		
		Important to remember that decision to tune to A440 \& to divide octave into 12 equal semitones is only 1 possibility in response to debates about musical tuning that date back thousands of years, \& it's only 1 of a myriad of ways that music can be tuned. There are many alternate tuning systems, both historical \& contemporary from both Western \& non-Western cultures, which are still in use today.
		
		-- Điều quan trọng cần nhớ là quyết định lên dây A440 \& chia quãng tám thành 12 nửa cung bằng nhau chỉ là 1 khả năng để đáp lại các cuộc tranh luận về cách lên dây nhạc có từ hàng ngàn năm trước, \& đó chỉ là 1 trong vô số cách để lên dây nhạc. Có nhiều hệ thống lên dây thay thế, cả lịch sử \& đương đại từ cả nền văn hóa phương Tây \& không phải phương Tây, vẫn được sử dụng cho đến ngày nay.
	\end{itemize}
	\item {\sf Interlude 3: Melodies \& Lists.} [p. 68 +++] 
	\item {\sf4. Chords.}
	\item {\sf5. Scales, keys, \& melody.}
	\item {\sf6. Diatonic chords \& chord progressions.}
	\item {\sf7. Frequency, fourier, \& filters.}
	\item {\sf8. Note-based production effects.}
	\item {\sf9. Song composition \& EarSketch.}
	\item {\sf10. Modular synthesis.}
	\item {\sf11. History of music \& computing.}
	\item {\sf Appendix A: Python ref.}
	\item {\sf Appendix B: TunePad programming ref.}
	\item {\sf Appendix C: Music ref.}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{\cite{Valimaki_Pakarinen_Erkut_Karjalainen2006}. {\sc Vesa Välimäki, Jyri Pakarinen, Cumhur Erkut, Matti Karjalainen}. Discrete-Time Modeling of Musical Instruments}
{\sf[283 citations]}
\begin{question}[Cf. Continuous-time modeling vs. Discrete-time modeling]
	How about continuous-time modeling of musical instruments?
\end{question}

\begin{question}[Cf. Mathematical modeling technique vs. Physical modeling technique]
	Compare Mathematical modeling technique vs. Physical modeling technique.
\end{question}

\begin{itemize}
	\item {\bf Abstract.} This article describes physical modeling techniques that can be used for simulating musical instruments. Methods are closely related to digital signal processing. They discretize system w.r.t. time, because aim: run simulation using a computer. Physics-based modeling methods can be classified as mass-spring, modal, wave digital, finite difference, digital waveguide \& source-filter models. Present basic theory \& a discussion on possible extensions for each modeling technique. For some methods, a simple model example is chosen from existing literature demonstrating a typical use of method. E.g., in case of digital waveguide modeling technique a vibrating string model is discussed, \& in case of wave digital filter technique, present a classical piano hammer model. Tackle some nonlinear \& time-varying models \& include new results on digital waveguide modeling of a nonlinear string. Discuss current trends \& future directions in physical modeling of musical instruments.
	\item {\sf1. Introduction.} Musical instruments have historically been among most complicated mechanical systems made by humans. They have been a topic of interest for physicists \& acousticians for over a century. Modeling of musical instruments using computers is newest approach to understanding how these instruments work.
	
	This paper presents an overview of physics-based modeling of musical instruments. Specifically, this paper focuses on sound synthesis methods derived using physical modeling approach. Several previously published tutorial \& review papers discussed physical modeling synthesis techniques for musical instruments sounds [73, 129, 251, 255, 256, 274, 284, 294]. Purpose of this paper: give a unified introduction to 6 main classes of discrete-time physical modeling methods, namely mass--spring, modal, wave digital, finite difference, digital waveguide \& source--filter models. This review also tackles mixed \& hybrid models in which usually 2 different modeling techniques are combined.
	
	Physical models of musical instruments have been developed for 2 main purposes: research of acoustical properties \& sound synthesis. Methods discussed in this paper can be applied to both purpose, but here main focus is sound synthesis. Basic idea of physics-based sound synthesis: build a simulation model of sound production mechanism of a musical instrument \& to generate sound with a computer program or signal processing hardware that implements that model. Motto of physical modeling synthesis: when a model has been designed properly, so that it behaves much like actual acoustic instrument, synthetic sound will automatically be natural in response to performance. In practice, various simplifications of model cause sound output to be similar to, but still clearly different from, original sound. Simplifications may be caused by intentional approximations that reduce computational cost or by inadequate knowledge of what is actually happening in acoustic instrument. A typical \& desirable simplification: linearization of slightly nonlinear phenomena, which may avert unnecessary complexities, \& hence may improve computational efficiency.
	
	In speech technology, idea of accounting for physics of sound source, human voice production organs, is an old tradition, which has led to useful results in speech coding \& synthesis. While 1st experiments on physics-based musical sound synthesis were documented several decades ago, 1st commercial products based on physical modeling synthesis were introduced in 1990s. Thus, topic is still relatively young. Research in field has been very active in recent years.
	
	1 of motivations for developing a physically based sound synthesis: musicians, composers, \& other users of electronic musical instruments have a constant hunger for better digital instruments \& for new tools for organizing sonic events. A major problem in digital musical instruments has always been how to control them. For some time, researchers of physical models have hoped: these models would offer more intuitive, \& in some ways better, controllability than previous sound synthesis methods. In addition to its practical applications, physical modeling of musical instruments is an interesting research topic for other reasons. It helps to solve old open questions, e.g. which specific features in a musical instrument's sound make it recognizable to human listeners or why some musical instruments sound sophisticated while others sound cheap. Yet another fascinating aspect of this field: when physical principles are converted into computational methods, possible to discover new algorithms. This way, possible to learn new signal processing methods from nature.
	\item {\sf2. Brief history.} Modeling of musical instruments is fundamentally based on understanding of their sound production principles. 1st person attempting to understand how musical instruments work might have been Pythagoras, who lived in ancient Greece around 500 BC. At that time, understanding of musical acoustics was very limited \& investigations focused on tuning of string instruments. Only after late 18th century, when rigorous mathematical methods e.g. PDEs were developed, was it possible to build formal models of vibrating strings \& plates.
	
	Earliest work on physics-based discrete-time sound synthesis was probably conducted by {\sc Kelly \& Lochbaum} in context of vocal-tract modeling [145]. A famous early musical example is `Bicycle Built for 2' (1961), where singing voice was produced using a discrete-time model of human vocal tract. This was result of collaboration between {\sc Mathews, Kelly \& Lochbaum} [43]. 1st vibrating string simulations were conducted in early 1970s by {\sc Hiller \& Ruiz} [113, 114], who discretized wave equation to calculate waveform of a single point of a vibrating string. Computing 1 s of sampled waveform took minutes. A few years later, {\sc Cadoz} \& his colleagues developed discrete-time mass-spring models \& built dedicated computing hardware to run real-time simulations [38].
	
	In late 1970s \& early 1980s, {\sc McIntyre, Woodhouse, \& Schumacher} made important contributions by introducing simplified discrete-time models of bowed strings, clarinet \& flute [173, 174, 235], \& {\sc Karplus \& Strong} [144] invented a simple algorithm that produces string-instrument-like sounds with few arithmetic operations. Based on these ideas \& their generalizations, {\sc Smith \& Jaffe} introduced a signal-processing oriented simulation technique for vibrating strings [120, 244]. Soon thereafter, {\sc Smith} proposed term `digital waveguide' \& developed general theory [247, 249, 253].
	
	1st commercial product based on physical modeling synthesis, an electronic keyboard instrument by Yamaha, was introduced in 1994 [168]; it used digital waveguide techniques. More recently, digital waveguide techniques have been also employed in MIDI synthesizers on personal computer soundcards. Currently, much of practical sound synthesis is based on software, \& there are many commercial \& freely available pieces of synthesis software that apply 1 or more physical modeling methods.
	\item {\sf3. General concepts of physics-based modeling.} In this sect, discuss a number of physical \& signal processing concepts \& terminology that are important in understanding modeling paradigms discussed in subsequent sects. Each paradigm is also characterized briefly in end of this sect. A  reader familiar with basic concepts in context of physical modeling \& sound synthesis may go directly to Sect. 4.
	\begin{itemize}
		\item {\sf3.1. Physical domains, variables, \& parameters.} Physical phenomena can be categorized as belonging to different `physical domains'. Most important ones for sound sources e.g. musical instruments are acoustical \& mechanical domains. In addition, electrical domain is needed for electroacoustic instruments \& as a domain to which phenomena from other domains are often mapped. Domains may interact with one another, or they can be used as analogies (equivalent models) of each other. Electrical circuits \& networks are often applied as analogies to describe phenomena of other physical domains.
		
		Quantitative description of a physical system is obtained through measurable quantities that typically come in pairs of variables, e.g. force \& velocity in mechanical domain, pressure \& volume velocity in acoustical domain or voltage \& current in electrical domain. Members of such dual variable pairs are categorized generically as `across variable' or `potential variable', e.g. voltage, force or pressure, \& `through variable' or `kinetic variable', e.g. current, velocity or volume velocity. If there is a linear relationship between dual variables, this relation can be expressed as a parameter, e.g. impedance $Z = \frac{U}{I}$ being ratio of voltage $U$ \& current $I$, or by its inverse, admittance $Y = \frac{I}{U}$. An example from mechanical domain is mobility (mechanical admittance) defined as ratio of velocity \& force. When using such parameters, only 1 of dual variables is needed explicitly, because the other one is achieved through constraint rule.
		
		Modeling methods discussed in this paper use 2 types of variables for computation, `K-variables' \& `wave variables' (also denoted as `W-variables'). `K' comes from Kirchhoff \& refers to Kirchhoff continuity rules of quantities in electric circuits \& networks [185]. `W' is shortform for wave, referring to wave components of physical variables. Instead of pairs of across \& through as with K-variables, wave variables come in pairs of incident \& reflected wave components. Details of wave modeling are discussed in Sects. 7--8, while K-modeling is discussed particularly in Sects. 4 \& 10. It will become obvious: these are different formulations of same phenomenon, \& possibility to combine both approaches in hybrid modeling will be discussed in Sect. 10.
		
		Decomposition into wave components is prominent in such wave propagation phenomena where opposite-traveling waves add up to actual observable K-quantities. A wave quantity is directly observable only when there is no other counterpart. It is, however, a highly useful abstraction to apply wave components to any physical case, since this helps in solving computability (causality) problems in discrete-time modeling.
		\item {\sf3.2. Modeling of physical structure \& interaction.} Physical phenomena are observed as structures \& processes in space \& time. In sound source modeling, interested in dynamic behavior that is modeled by variables, while slowly varying or constant properties are parameters. Physical interaction between entities in space always propagates with a finite velocity, which may differ by orders of magnitude in different physical domains, speed of light being upper limit.
		
		`Causality' is a fundamental physical property that follows from finite velocity of interaction from a cause to corresponding effect. In many mathematical relations used in physical models causality is not directly observable. E.g., relation of voltage across \& current through an impedance is only a constraint, \& variables can be solved only within context of whole circuit. Requirement of causality (more precisely temporal order of cause preceding effect) introduces special computability problems in discrete-time simulation, because 2-way interaction with a delay shorter than a unit delay (sampling period) leads to `delay-free loop problem'. Use of wave variables is advantageous, since incident \& reflected waves have a causal relationship. In particular, wave digital filter (WDF) theory, discussed in Sect. 8, carefully treats this problem through use of wave variables \& specific scheduling of computation operations.
		
		Taking finite propagation speed into account requires using a spatially distributed model. Depending on case at hand, this can be a full 3D model e.g. used for room acoustics, a 2D model e.g. for a drum membrane (discarding air loading) or a 1D model e.g. for a vibrating sting. If object to be modeled behaves homogeneously enough as a whole, e.g. due to its small size compared with wavelength of wave propagation, it can be considered a lumped entity that does not need a description of spatial dimensions.
		
		-- Việc tính đến tốc độ lan truyền hữu hạn đòi hỏi phải sử dụng một mô hình phân bố không gian. Tùy thuộc vào trường hợp cụ thể, đây có thể là mô hình 3D đầy đủ, ví dụ như được sử dụng cho âm học phòng, mô hình 2D, ví dụ như cho màng trống (loại bỏ tải trọng không khí) hoặc mô hình 1D, ví dụ như cho một cú chích rung. Nếu vật thể được mô hình hóa hoạt động đủ đồng nhất như một tổng thể, ví dụ như do kích thước nhỏ so với bước sóng truyền sóng, thì nó có thể được coi là một thực thể tập trung không cần mô tả về kích thước không gian.
		\item {\sf3.3. Signals, signal processing, \& discrete-time modeling.} In signal processing, signal relationships are typically represented as 1-directional cause-effect chains. Contrary to this, bi-directional interaction is common in (passive) physical systems, e.g. in systems where reciprocity principle is valid. In true physics-based modeling, 2-way interaction must be taken into account. I.e., from signal processing viewpoint, such models are full of feedback loops, which further implicates: concepts of computability (causality) \& stability become crucial.
		
		In this paper, apply digital signal processing (DSP) approach to physics-based modeling whenever possible. Motivation for this: DSP is an advanced theory \& tool that emphasizes computational issues, particularly maximal efficiency. This efficiency is crucial for real-time simulation \& sound synthesis. Signal flow diagrams are also a good graphical means to illustrate algorithms underlying simulations. Assume: reader is fmailiar with fundamentals of DSP, e.g. sampling theorem [242] to avoid aliasing (also spatial aliasing) due to sampling in time \& space as well as quantization effects due to finite numerical precision.
		
		An important class of systems is those that are linear \& time invariant (LTI). They can be modeled \& simulated efficiently by digital filters. They can be analyzed \& processed in frequency domain through linear transforms, particularly by Z-transform \& discrete Fourier transform (DFT) in discrete-time case. While DFT processing through fast Fourier transform (FFT) is a powerful tool, it introduces a block delay \& does not easily fit to sample-by-sample simulation, particularly when bi-directional physical interaction is modeled.
		
		Nonlinear \& time-varying systems bring several complications to modeling. Nonlinearities create new signal frequencies that easily spread beyond Nyquist limit, thus causing aliasing, which is perceived as very disturbing distortion. In addition to aliasing, delay-free loop problem \& stability problems can become worse than they are in linear systems. If nonlinearities in a system to be modeled are spatially distributed, modeling task is even more difficult than with a localized nonlinearity. Nonlinearities will be discussed in several sects of this paper, most completely in Sect. 11.
		\item {\sf3.4. Energetic behavior \& stability.} Product of dual variables e.g. voltage \& current gives power, which, when integrated in time, yields energy. Conservation of energy in a closed system is a fundamental law of physics that should also be obeyed in true physics-based  modeling. In musical instruments, resonators are typically passive, i.e. they do not produce energy, while excitation (plucking, bowing, blowing, etc.) is an active process that injects energy to passive resonators.
		
		Stability of a physical system is closely related to its energetic behavior. Stability can be defined so that energy of system remains finite for finite energy excitations. From a signal processing viewpoint, stability may also be defined so that variables, e.g. voltages, remain within a linear operating range for possible inputs in order to avoid signal clipping \& distortion.
		
		In signal processing systems with 1-directional input--output connections between stable subblocks, an instability can appear only if there are feedback loops. In general, impossible to analyze such a system's stability without knowing its whole feedback structure. Contrary to this, in models with physical 2-way interaction, if each element is passive, then any arbitrary network of such elements remains stable.
		\item {\sf3.5. Modularity \& locality of computation.} For a computational realization, desirable to decompose a model systematically into blocks \& their interconnections. Such an object-based approach helps manage complex models through use of modularity principle. Abstractions to macro blocks on basis of more elementary ones helps hiding details when building excessively complex models.
		
		For 1-directional interactions used in signal processing, enough to provide input \& output terminals for connecting blocks. For physical interaction, connections need to be done through ports, with each port having a pair of K- or wave variables depending on modeling method used. This allows mathematical principles used for electrical networks [185]. Details on block-wise construction of models will be discussed in following sects for each modeling paradigm.
		
		Locality of interaction is a desirable modeling feature, which is also related to concept of causality. For a physical system with a finite propagation speed of waves, enough: a block interacts only with its nearest neighbors; it does not need global connections to compute its task \& effect automatically propagates throughout system.
		
		In a discrete-time simulation with bi-directional interactions, delays shorter than a unit delay (including 0 delay) introduce delay-free loop problem that we face several times in this paper. While possible to realize fractional delays [154], delayers shorter than unit delay contain a delay-free component. There are ways to make such `implicit' system computable, but cost in time (or accuracy) may become prohibitive for real-time processing.
		\item {\sf3.6. Physics-based discrete-time modeling paradigms.} This paper presents an overview of physics-based methods \& techniques for modeling \& synthesizing musical instruments. Have excluded some methods often used in acoustics, because they do not easily solve task of efficient discrete-time modeling \& synthesis. E.g., finite element \& boundary element methods (FEM \& BEM) are generic \& powerful for solving system behavior numerically, particularly for linear systems, but focus on inherently time-domain methods for sample-by-sample computation.
		
		Main paradigms in discrete-time modeling of musical instruments can be briefly characterized as follows.
		\begin{itemize}
			\item {\sf3.6.1. Finite difference models.} In Sect. 4, finite difference models are numerical replacement for solving PDEs. Differentials are approximated by finite differences so that time \& position will be discretized. Through proper selection of discretization to regular meshes, computational algorithms become simple \& relatively efficient. Finite difference time domain (FDTD) schemes are K-modeling methods, since wave components are not explicitly utilized in computation. FDTD schemes have been applied successfully to 1D, 2D, \& 3D systems, although in linear 1D cases digital waveguides are typically superior in computational efficiency \& robustness. In multidimensional mesh structures, FDTD approach is more efficient. It also shows potential to deal systematically with nonlinearities (Sect. 11). FDTD algorithms can be problematic due to lack of numerical robustness \& stability, unless carefully designed.
			\item {\sf3.6.2. Mass--spring networks.} In Sect. 5, mass--spring networks are a modeling approach, where intuitive basic elements in mechanics -- masses, springs, \& damping elements -- are used to construct vibrating structures. It is inherently a K-modeling methodology, which has been used to construct small- \& large-scale mesh-like \& other structures. It has resemblance to FDTD schemes in mesh structures \& to WDFs for lumped element modeling. Mass--spring networks can be realized systematically also by WDFs using wave variables (Sect. 8).
			\item {\sf3.6.3. Modal decomposition methods.} In Sect. 6 modal decomposition methods represent another approach to look at vibrating systems, conceptually from a frequency-domain viewpoint. Eigenmodes of a linear system are exponentially decaying sinusoids at eigenfrequencies in response of a system to impulse excitation. Although thinking by modes is normally related to frequency domain, time-domain simulation by modal methods can be relatively efficient, \& therefore suitable to discrete-time computation. Modal decomposition methods are inherently based on use of K-variables. Modal synthesis has been applied to make convincing sound synthesis of different musical instruments. Functional transform method (FTM) is a recent development of systematically exploiting idea of spatially distributed modal behavior, \& it has also been extended to nonlinear system modeling.
			\item {\sf3.6.4. Digital waveguides.} Digital waveguides (DWGs) in Sect. 7 are most popular physics-based method of modeling \& synthesizing musical instruments that are based on 1D resonators, e.g. strings \& wind instruments. Reason for this is their extreme computational efficiency in their basic formulations. DWGs have been used also in 2D \& 3D mesh structures, but in such cases wave-based DWGs are not superior in efficiency. Digital waveguides are based on use of traveling wave components; thus, they form a wave modeling (W-modeling) paradigm [Term digital waveguide is used also to denote K-modeling, e.g. FDTD mesh-structures, \& source-filter models derived from traveling wave solutions, which may cause methodological confusion.]. Therefore, they are also compatible with WDFs (Sect. 8), but in order to be compatible with K-modeling techniques, special conversion algorithms must be applied to construct hybrid models, as discussed in Sect. 10.
			\item {\sf3.6.5. Wave digital filters.} WDFs in Sect. 8 are another wave-based modeling technique, originally developed for discrete-time simulation of analog electric circuits \& networks. In their original form, WDFs are best suited for lumped element modeling; thus, they can be easily applied to wave-based mass--spring modeling. Due to their compatibility with digital waveguides, these methods complement each other. WDFs have also been extended to multidimensional networks \& to systematic \& energetically consistent modeling of nonlinearities. They have been applied particularly to deal with lumped \& nonlinear elements in models, where wave propagation parts are typically realized by digital waveguides.
			\item {\sf3.6.6. Source--filter models.} In Sect. 9 source--filter models form a paradigm between physics-based modeling \& signal processing models. True spatial structure \& bi-directional interactions are not visible, but are transformed into a transfer function that can be realized as a digital filter. Approach is attractive in sound synthesis because digital filters are optimized to implement transfer functions efficiently. Source part of a source--filter model is often a wavetable, consolidating different physical or synthetic signal components needed to feed filter part. Source--filter paradigm is frequently used in combination with other modeling paradigms in more or less ad hoc ways.
		\end{itemize}
	\end{itemize}
	\item {\sf4. Finite difference models.} Finite difference schemes can be used for solving PDEs, e.g. those describing vibration of a string, a membrane or an air column inside a tube [264]. Key idea in finite difference scheme: replace derivatives with finite difference approximations. An early example of this approach in physical modeling of musical instruments is work done by {\sc Hiller \& Ruiz} in early 1970s [113, 114]. This line of research has been continued \& extended by {\sc Chaigne} \& colleagues [45, 46, 48] \& recently by others [25, 26, 29, 30, 81, 103, 131].
	
	Finite difference approach leads to a simulation algorithm that is based on a difference equation, which can be easily programmed with a computer. E.g., how basic wave equation, which describes small-amplitude vibration of a lossless, ideally flexible string, is discretized using this principle. Here present a formulation after Smith [253] using an ideal string as a starting point for discrete-time modeling. A more thorough continuous-time analysis of physics of strings can be found in [96].
	\begin{itemize}
		\item {\sf4.1. Finite difference models for an ideal vibrating string.} {\sf Fig. 1: Part of an ideal vibrating string.} depicts a snapshot of an ideal (lossless, linear, flexible) vibrating string by showing displacement as a function of position. Wave equation for string is given by \fbox{$Ky'' = \epsilon\ddot{y}$}
		\item {\sf4.2. Boundary conditions \& string excitation.}
		\item {\sf4.3. Finite difference approximation of a lossy string.}
		\item {\sf4.4. Stiffness in finite difference strings.}
	\end{itemize}
	\item {\sf5. Mass-spring networks.}
	\begin{itemize}
		\item {\sf5.1. Basic theory.}
		\item {\sf5.2. CORDIS-ANIMA.}
		\item {\sf5.3. Other mass-spring systems.}
	\end{itemize}
	\item {\sf6. Modal decomposition methods.}
	\begin{itemize}
		\item {\sf6.1. Modal synthesis.}
		\item {\sf6.2. Filter-based modal methods.}
		\item {\sf6.3. Functional transform method.}
	\end{itemize}
	\item {\sf7. Digital waveguides.}
	\begin{itemize}
		\item {\sf7.1. From wave propagation to digital waveguides.}
		\item {\sf7.2. Modeling of losses \& dispersion.}
		\item {\sf7.3. Modeling of waveguide termination \& scattering.}
		\item {\sf7.4. Digital waveguide meshes \& networks.}
		\item {\sf7.5. Reduction of a DWG model to a single delay loop structure.}
		\item {\sf7.6. Commuted DWG synthesis.}
		\item {\sf7.7. Case study: modeling \& synthesis of acoustic guitar.} Acoustic guitar is an example of a musical instruments for which DWG modeling is found to be an efficient method, especially for real-time sound synthesis [134, 137, 142, 160, 286, 295]. DWG principle in {\sf Fig. 14: A DWG block diagram of 2 strings coupled through a common bridge impedance $Z_{\rm b}$ \& terminated at other end by nut impedances $Z_{\rm t1},Z_{\rm t2}$. Plucking points are for force insertion from wavetables ${\rm WT}_i$ into junctions in delay-lines ${\rm DL}_{ij}$. Output is taken as bridge velocity.} allows for true physically distributed modeling of strings \& their interaction, while SDL commuted synthesis ({\sf Fig. 17: Reduction of bi-directional delay-line waveguide model (top) to a single delay line loop structure (bottom).} \& {\sf Fig. 18: Principles of commuted DWG synthesis: (a) cascaded excitation, string \& body, (b) body \& string blocks commuted \& (c) excitation \& body blocks consolidated into a wavetable for feeding string model.}) allows for more efficient computation. In this subsect discuss principles of commuted waveguide synthesis as applied to high-quality synthesis of acoustic guitar.
		
		There are several features that must be added to simple commuted SDL structure in order to achieve natural sound \& control of playing features. {\sf Fig. 19: Degrees of freedom for string vibration in guitar: Torsional, Longitudinal, Vertical, Horizontal.} depicts degrees of freedom for vibration of strings in guitar. Transversal directions, i.e. vertical \& horizontal polarizations of vibration, are most prominent ones. Vertical vibration connects strongly to bridge, resulting in stronger initial sound \& faster decay than horizontal vibrations that start more weakly \& decay more slowly. Effect of longitudinal vibration is weak but can be observed in generation of some partials of sound [320]. Longitudinal effects are more prominent in piano [16, 58], but are particularly important in such instruments as kantele [82] through nonlinear effect of tension modulation (Sect. 11). Torsional vibration of strings in guitar is not shown to have a remarkable effect on sound. In violin it has a more prominent physical role, although it makes virtually no contribution to sound.
		
		In commuted waveguide synthesis, 2 transversal polarizations can be realized by 2 separate SDL string models, $S_{\rm v}(z)$ for vertical \& $S_{\rm h}(z)$ for horizontal polarization in {\sf Fig. 20: Dual-polarization string model with sympathetic vibration coupling between strings. Multiple wavetables are used for varying plucking styles. Filter $E(z)$ can control detailed timbre of plucking \& $P(z)$ is a plucking point comb filter.}, each one with slightly different delay \& decay parameters. Coefficient $m_{\rm p}$ is used to control relative excitation amplitudes of each polarization, depending on initial direction of string movement after plucking. Coefficients $m_{\rm o}$ can be used to mix vibration signal components at bridge.
		
		{\sf Fig. 20} also shows another inherent feature of guitar, sympathetic coupling between strings at bridge, which causes an undamped string to gain energy from another string set in vibration. While principle shown in {\sf Fig. 14} implements this automatically if string \& bridge admittances are correctly set, model in {\sf Fig. 20} requires special signal connections from point C to vertical polarization model of other strings. This is just a rough approximation of physical phenomenon that guarantees stability of model. There is also a connection through $g_{\rm c}$ that allows for simple coupling from horizontal polarization to excite vertical vibration, with a final result of a coupling between polarizations.
		
		Dual-polarization model in {\sf Fig. 20} is excited by wavetables containing commuted waveguide excitations for different plucking styles. Filter $E(z)$ can be used to control timbre details of selected excitation, \& filter $E(z)$ can be used to control timbre details of selected excitation, \& filter $P(z)$ is a plucking point comb filter, as prev discussed.
		
		-- Mô hình phân cực kép trong {\sf Hình 20} được kích thích bằng các bảng sóng chứa các kích thích ống dẫn sóng chuyển mạch cho các kiểu gảy khác nhau. Bộ lọc $E(z)$ có thể được sử dụng để kiểm soát các chi tiết âm sắc của kích thích đã chọn, \& bộ lọc $E(z)$ có thể được sử dụng để kiểm soát các chi tiết âm sắc của kích thích đã chọn, \& bộ lọc $P(z)$ là bộ lọc lược điểm gảy, như đã thảo luận trước đó.
		
		For solid body electric guitars, a magnetic pickup model is needed, but body effect can be neglected. Magnetic pickup can be modeled as a lowpass filter [124,137] in series with a comb filter similar to plucking point filter, but in this case corresponding to pickup position.
		
		Calibration of model parameters is an important task when simulating a particular instrument. Methods for calibrating a string instrument model are presented, e.g., in [8, 14, 24, 27, 137, 142, 211, 244, 286, 295, 320].
		
		-- Hiệu chuẩn các tham số mô hình là 1 nhiệm vụ quan trọng khi mô phỏng một nhạc cụ cụ thể. Các phương pháp hiệu chuẩn mô hình nhạc cụ dây được trình bày $\ldots$
		
		A typical procedure: apply time-frequency analysis to recorded sound of plucked or struck string, in order to estimate decay rate of each harmonic. Parametric models e.g. FZ-ARMA analysis [133, 138] may yield more complete information of modal components in string behavior. This information is used to design a low-order loop filter which approximates frequency-dependent losses in SDL loop structure [14,17,79,244,286]. A recent novel idea has been to design a sparse FIR loop filter, which is of high order but has few nonzero coefficients [163, 209, 293]. This approach offers a computationally efficient way to imitate large deviations in decay rates of harmonic components. Through implementing a slight difference in delays \& decay rates of 2 polarizations, beating or 2-stage decay of signal envelope can be approximated. for plucking point comb filter: required to estimate plucking point from a recorded tone [199, 276, 277, 286].
		
		{\sf Fig. 21: Detailed SDL loop structure for string instrument sound synthesis.} depicts a detailed structure used in practice to realize SDL loop. Fundamental frequency of string sound is inversely proportional to total delay of loop blocks. Accurate tuning requires application of a fractional delay, because an integral number of unit delays is not accurate enough when a fixed sampling rate is used. Fractional delays are typically approximated by 1st-order allpass filters or 1st- to 5th-order Lagrange interpolators as discussed in [154].
		
		When loop filter properties are estimated properly, excitation wavetable signal is obtained by inverse filtering (deconvolution) of recorded sound by SDL response. For practical synthesis, only initial transient part of inverse-filtered excitation is used, typically covering several 10s of milliseconds.
		
		After careful calibration of model, a highly realistic sounding synthesis can be obtained by parametric control \& modification of sound features. Synthesis is possible even in cases which are not achievable in practice in real acoustic instruments.		
		\item {\sf7.8. DWG modeling of various musical instruments.} Digital waveguide modeling has been applied to a variety of musical instruments other than acoustic guitar. In this subsect, present a brief overview of such models \& features that need special attention to each case. For an in-depth presentation on DWG modeling techniques applied to different instrument families, see [254].
		\begin{itemize}
			\item {\sf7.8.1. Other plucked string instruments.}
			\item {\sf7.8.2. Struck string instruments.}
			\item {\sf7.8.3. Bowed string instruments.}
			\item {\sf7.8.4. Wind instruments.}
			\item {\sf7.8.5. Percussion instruments.}
			\item {\sf7.8.6. Speech \& singing voice.}
			\item {\sf7.8.7. Inharmonic SDL type of DWG models.}
		\end{itemize}
	\end{itemize}
	\item {\sf8. Wave digital filters.} Purpose of this sect: provide a general overview of physical modeling using WDFs in context of musical instruments. Only essential basics of topic will be discussed in detail; the rest will be glossed over. For more information about project, reader is encouraged to refer to [254]. Also, another definitive work can be found in [94].
	\begin{itemize}
		\item {\sf8.1. What are wave digital filters?} WDFs were developed in late 1960s by {\sc Alfred Fettweis} [93] for digitizing lumped analog electrical circuits. Traveling-wave formulation of lumped electrical elements, where WDF approach is based, was introduced earlier by {\sc Belevitch} [21, 254].
		
		WDFs are certain types of digital filters with valid interpretations in physical world. I.e., can simulate behavior of a lumped physical system (hệ thống vật lý tập trung) using a digital filter whose coefficients depend on parameters of this physical system. Alternatively, WDFs can be seen as a particular type of finite difference schemes with excellent numerical properties [254]. As discussed in Sect. 4, task of finite difference schemes in general: provide discrete versions of PDEs for simulation \& analysis purposes.
		
		WDFs are useful for physical modeling in many respects. 1stly, they are modular: same building blocks can be used for modeling very different systems; all that needs to be changed: \fbox{topology of wave digital network}. 2ndly, preservation of energy \& hence also stability is usually addressed, since elementary blocks can be made passive, \& energy preservation between blocks are evaluated using Kirchhoff's laws. Finally, WDFs have good numerical properties, i.e., they do not experience artificial damping at high frequencies.
		
		Physical systems were originally considered to be lumped in basic wave digital formalism. I.e., system to be modeled, say a drum, will become a point-like black box, which has functionality of drum. However, its inner representation, as well as its spatial dimensions, is lost. Must bear in mind, however: question of whether a physical system can be considered lumped depends naturally not only on which of its aspects wish to model but also on frequency scale want to use in modeling (Sect. 3).
		\item {\sf8.2. Analog circuit theory.}
		\item {\sf8.3. Wave digital building blocks.}
		\item {\sf8.4. Interconnection \& adaptors.}
		\item {\sf8.5. Physical modeling using WDFs.}
		\item {\sf8.6. Current research.}
	\end{itemize}
	\item {\sf9. Source-filter models.}
	\begin{itemize}
		\item {\sf9.1. Subtractive synthesis in computer music.}
		\item {\sf9.2. Source-filter models in speech synthesis.}
		\item {\sf9.3. Instrument body modeling by digital filters.}
		\item {\sf9.4. Karplus--Strong algorithm.}
		\item {\sf9.5. Virtual analog synthesis.}
	\end{itemize}
	\item {\sf10. Hybrid models.}
	\begin{itemize}
		\item {\sf10.1. KW-hybrids.}
		\item {\sf10.2. KW-hybrids modeling examples.}
	\end{itemize}
	\item {\sf11. Modeling of nonlinear \& time-varying phenomena.}
	\begin{itemize}
		\item {\sf11.1. Modeling of nonlinearities in musical instruments.}
		\item {\sf11.2. Case study: nonlinear string model using generalized time-varying allpass filters.}
		\item {\sf11.3. Modeling of time-varying phenomena.}
	\end{itemize}
	\item {\sf12. Current trends \& further research.}
	\item {\sf13. Conclusions.}
\end{itemize}

%------------------------------------------------------------------------------%

\section{Wikipedia}

\subsection{Wikipedia{\tt/}computer music}
``{\it Computer music} is application of \href{https://en.wikipedia.org/wiki/Computing_technology}{computing technology} in \href{https://en.wikipedia.org/wiki/Musical_composition}{music composition}, to help human composers create new music or to have computers independently create music, e.g. with \href{https://en.wikipedia.org/wiki/Algorithmic_composition}{algorithmic composition} programs. it includes theory \& application of new \& existing computer software technologies \& basic aspects of music, e.g. \href{https://en.wikipedia.org/wiki/Sound_synthesis}{sound synthesis}, \href{https://en.wikipedia.org/wiki/Digital_signal_processing}{digital signal processing}, \href{https://en.wikipedia.org/wiki/Sound_design}{sound design}, sonic diffusion, \href{https://en.wikipedia.org/wiki/Acoustics}{acoustics}, \href{https://en.wikipedia.org/wiki/Electrical_engineering}{electrical engineering}, \& \href{https://en.wikipedia.org/wiki/Psychoacoustics}{psychoacoustics}. Field of computer music can trace its roots back to origins of \href{https://en.wikipedia.org/wiki/Electronic_music}{electric music}, \& 1st experiments \& innovations with electronic instruments at turn of 20th century.

\subsubsection{History}

\subsubsection{Advances}

\subsubsection{Research}

\subsubsection{Machine improvisation}

\subsubsection{Live coding}

'' -- \href{https://en.wikipedia.org/wiki/Computer_music}{Wikipedia{\tt/}computer music}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}transcription (music)}
``{\sf A {\sc J. S. Bach} keyboard piece transcribed for guitar.} In music, {\it transcription} is practice of \href{https://en.wikipedia.org/wiki/Musical_notation}{notating} a piece or a sound which was previously unnotated \&{\tt/}or unpopular as a written music, e.g., a \href{https://en.wikipedia.org/wiki/Jazz_improvisation}{jazz improvisation} or a \href{https://en.wikipedia.org/wiki/Video_game_soundtrack}{video game soundtrack}. When a musician is tasked with creating \href{https://en.wikipedia.org/wiki/Sheet_music}{sheet music} from a recording \& they write down notes that make up piece in \href{https://en.wikipedia.org/wiki/Music_notation}{music notation}, it is said: they created a {\it musical transcription} of that recording. Transcription may also mean rewriting a piece of music, either solo or \href{https://en.wikipedia.org/wiki/Musical_ensemble}{ensemble}, for another instrument or other instruments than which it was originally intended. \href{https://en.wikipedia.org/wiki/Beethoven_Symphonies_(Liszt)}{Beethoven Symphonies} transcribed for solo piano by \href{https://en.wikipedia.org/wiki/Franz_Liszt}{Franz Liszt} are an example. Transcription in this sense is sometimes called \href{https://en.wikipedia.org/wiki/Arrangement}{\it arrangement}, although strictly speaking transcriptions are faithful adaptations, whereas arrangements change significant aspects of original piece.

Further examples of music transcription include \href{https://en.wikipedia.org/wiki/Ethnomusicology}{ethnomusicological} notation of \href{https://en.wikipedia.org/wiki/Oral_tradition}{oral traditions} of folk music, e.g. Béla Bartók's \& Ralph Vaughan Williams' collections of national folk music of Hungary \& England resp. French composer Olivier Messiaen transcribed \href{https://en.wikipedia.org/wiki/Bird_song}{birdsong} in wild, \& incorporated it into many of his compositions, e.g. his \href{https://en.wikipedia.org/wiki/Catalogue_d%27oiseaux}{Catalogue d'oiseaux} for solo piano. Transcription of this nature involves scale degree recognition \& harmonic analysis, both of which transcriber will need \href{https://en.wikipedia.org/wiki/Relative_pitch}{relative} or \href{https://en.wikipedia.org/wiki/Perfect_pitch}{perfect pitch} to perform. 

In popular music \& rock, there are 2 forms of transcription. Individual performers copy a note-for-note guitar solo or other melodic line. As well, music publishers transcribe entire recordings of guitar solos \& bass lines \& sell sheet music in bound books. Music publishers also publish PVG (piano{\tt/}vocal{\tt/}guitar) transcriptions of popular music, where melody line is transcribed, \& then accompaniment on recording is arranged as a piano part. Guitar aspect of PVG label is achieved through guitar chords written above melody. Lyrics are also included below melody.

\subsubsection{Adaptation}
Some composers have rendered homage to other composers by creating ``identical'' versions of earlier composers' pieces while adding their own creativity through use of completely new sounds arising from difference in instrumentation. Most widely known example of this is {\sc Ravel}'s arrangement for orchestra of {\sc Mussorgsky}'s piano piece \href{https://en.wikipedia.org/wiki/Pictures_at_an_Exhibition}{\it Pictures at an Exhibition}. {\sc Webern} used his transcription for orchestra of 6-part \href{https://en.wikipedia.org/wiki/Ricercar}{ricercar} from {\sc Bach}'s \href{https://en.wikipedia.org/wiki/The_Musical_Offering}{\it The Musical Offering} to analyze structure of Bach piece, by using different instruments to play different subordinate \href{https://en.wikipedia.org/wiki/Motif_(music)}{motifs} of Bach's themes \& melodies.

In transcription of this form, new piece can simultaneously imitate original sounds while recomposing them with all technical skills of an expert composer in such a way that it seems: piece was originally written for new medium. But some transcriptions \& arrangements have been done for purely pragmatic or contextual reasons. E.g., in Mozart's time, overtures \& songs from this popular operas were transcribed for small \href{https://en.wikipedia.org/wiki/Wind_ensemble}{wind ensemble} simply because such ensembles were common ways of providing popular entertainment in public places. {\sc Mozart} himself did this in his own opera \href{https://en.wikipedia.org/wiki/The_Marriage_of_Figaro}{\it The Marriage of Figaro}. A more contemporary example is {\sc Stravinsky}'s transcription for 4 hands piano of \href{https://en.wikipedia.org/wiki/The_Rite_of_Spring}{\it The Rite of Spring}, to be used on ballet's rehearsals. Today musicians who play in cafes or restaurants will sometimes play transcriptions or arrangements of pieces written for a larger group of instruments.

Other examples of this type of transcription include {\sc Bach}'s arrangement of {\sc Vivaldi}'s 4-violin concerti for 4 keyboard instruments \& orchestra; {\sc Mozart}'s arrangement of some Bach \href{https://en.wikipedia.org/wiki/Fugue}{fugues} from \href{https://en.wikipedia.org/wiki/The_Well-Tempered_Clavier}{\it The Well-Tempered Clavier} for string \href{https://en.wikipedia.org/wiki/Trio_(music)}{trio}; {\sc Beethoven}'s arrangement of his \href{https://en.wikipedia.org/wiki/Gro%C3%9Fe_Fuge}{\it Gro$\beta$e Fuge}, originally written for \href{https://en.wikipedia.org/wiki/String_quartet}{string quartet}, for \href{https://en.wikipedia.org/wiki/Piano}{piano} duet, \& his arrangement of his \href{https://en.wikipedia.org/wiki/Violin_Concerto_(Beethoven)}{Violin Concerto} as a \href{https://en.wikipedia.org/wiki/Piano_concerto}{piano concerto}; Franz Liszt's piano arrangements of works of many composers, including \href{https://en.wikipedia.org/wiki/Beethoven_Symphonies_(Liszt)}{symphonies of Beethoven}; {\sc Tchaikovsky}'s arrangement of 4 Mozart piano pieces into an \href{https://en.wikipedia.org/wiki/Orchestral_suite}{orchestral suite} called ``\href{https://en.wikipedia.org/wiki/Orchestral_Suite_No._4_Mozartiana_(Tchaikovsky)}{Mozartiana}''; {\sc Mahler}'s re-orchestration of {\sc Schumann} symphonies; \& {\sc Schoenberg}'s arrangement for orchestra of {\sc Brahms}'s piano quintet \& {\sc Bach}'s ``St. Anne'' Prelude \& Fugue for organ.

Since piano became a popular instrument, a large literature has sprung up of transcriptions \& arrangements for piano of works for orchestra or chamber music ensemble. These are sometimes called ``\href{https://en.wikipedia.org/wiki/Reduction_(music)}{piano reductions}'', because multiplicity of orchestral parts -- in an orchestral piece there may be as many as 2 dozen separate instrumental parts being played simultaneously -- has to be reduced to what a single pianist (or occasionally 2 pianists, or 1 or 2 pianos, e.g. different arrangements for {\sc George Gershwin}'s \href{https://en.wikipedia.org/wiki/Rhapsody_in_Blue}{\it Rhapsody in Blue}) can manage to play.

Piano reductions are frequently made of orchestral accompaniments to choral works, for purposes of rehearsal or of performance with keyboard alone.

Many orchestral pieces have been transcribed for \href{https://en.wikipedia.org/wiki/Concert_band}{concert band}.

\subsubsection{Transcription aids}

\begin{itemize}
	\item {\bf Notation software.} Since advent of desktop publishing, musicians can acquire \href{https://en.wikipedia.org/wiki/Music_notation_software}{music notation software}, which can receive user's mental analysis of notes \& then store \& format those notes into standard music notation for personal printing or professional publishing of sheet music. Some notation software can accept a Standard \href{https://en.wikipedia.org/wiki/MIDI}{MIDI} File (SMF) or MIDI performance as input instead of manual note entry. These notation applications can export their scores in a variety of formats like \href{https://en.wikipedia.org/wiki/Encapsulated_PostScript}{EPS}, \href{https://en.wikipedia.org/wiki/Portable_Network_Graphics}{PNG}, \& \href{https://en.wikipedia.org/wiki/Scalable_Vector_Graphics}{SVG}. Often software contains a sound library that allows user's score to be played aloud by application for verification.
	\item {\bf Slow-down software.} Prior to invention of digital transcription aids, musicians would slow down a record or a tape recording to be able to hear melodic lines \& chords at a slower, more digestible pace. Problem with this approach was: it also changed pitches, so once a piece was transcribed, it would then have to be transposed into correct key. Software designed to slow down tempo of music without changing pitch of music can be very helpful for recognizing pitches, melodies, chords, rhythms, \& lyrics when transcribing music. However, unlike slow-down effect of a record player, pitch \& original octave of notes will stay same, \& not descend in pitch. This technology is simple enough that it is available in many free software applications.
	
	Software generally goes through a 2-step process to accomplish this. 1st, audio file is played back at a lower sample rate than that of original file. This has same effect as playing a tape or vinyl record at slower speed -- pitch is lowered meaning music can sound like it is in a different key. 2nd step: use \href{https://en.wikipedia.org/wiki/Digital_signal_processing}{Digital Signal Processing} (or DSP) to shift pitch back up to original pitch level or musical key.
	\item {\bf Pitch tracking software.} Main article: \href{https://en.wikipedia.org/wiki/Pitch_tracker}{Wikipedia{\tt/}pitch tracker}. As mentioned in the Automatic music transcription sect, some commercial software can roughly track pitch of dominant melodies in polyphonic musical recordings. Note scans are not exact, \& often need to be manually edited by user before saving to file in either a proprietary file format or in Standard MIDI File Format. Some pitch tracking software also allows scanned note lists to be animated during audio playback.
\end{itemize}

\subsubsection{Automatic music transcription (AMT)}
Term ``automatic music transcription'' was 1st used by audio researchers {\sc James A. Moorer,  Martin Piszczalski, \& Bernard Galler} in 1977. With their knowledge of digital audio engineering, these researchers believed: a computer could be programmed to analyze a \href{https://en.wikipedia.org/wiki/Digital_recording}{digital recording} of music s.t. pitches of melody lines \& chord patterns could be detected, along with rhythmic accents of percussion instruments. Task of AMT concerns 2 separate activities: making an analysis of a musical piece, \& printing out a score from that analysis.

This was not a simple goal, but one that would encourage academic research for at least another 3 decades. Because of close scientific relationship of speech to music, much academic \& commercial research that was directed toward more financially resourced \href{https://en.wikipedia.org/wiki/Speech_recognition}{speech recognitions} technology would be recycled into research about music recognition technology. While many musicians \& educators insist that manually doing transcriptions is a valuable exercise for developing musicians, motivation for AMT remains same as motivation for sheet music: musicians who do not have intuitive transcription skills will search for sheet music or a chord chart, so that they may quickly learn how to play a song. A collection of tools created by this ongoing research could be of great aid to musicians. Since much recorded music does not have available sheet music, an automatic transcription device could also offer transcriptions that are otherwise unavailable in sheet music. To date, no software application can yet completely fulfill {\sc James Moorer};s definition of AMT. However, pursuit of AMT has spawned creation of many software applications that can aid in manual transcription. Some can slow down music while maintaining original pitch \& octave, some can track pitch of melodies, some can track chord changes, \& others can track beat of music.

Automatic transcription most fundamentally involves identifying pitch \& duration of performed notes. This entails tracking pitch \& identifying note onsets. After capturing those physical measurements, this information is mapped into traditional music notation, i.e., sheet music.

\href{https://en.wikipedia.org/wiki/Digital_signal_processing}{Digital Signal Processing} is branch of engineering that provides software engineers with tools \& algorithms needed to analyze a digital recording in terms of pitch (note detection of melodic instruments), \& energy content of un-pitched sounds (detection of percussion instruments). Musical recordings are sampled at a given recording rate \& its frequency data is stored in any digital wave format in computer. Such format represents sound by \href{https://en.wikipedia.org/wiki/Sampling_(signal_processing)}{digital sampling}.
\begin{itemize}
	\item {\bf Pitch detection.} \href{https://en.wikipedia.org/wiki/Pitch_detection}{Pitch detection} is often detection of individual \href{https://en.wikipedia.org/wiki/Musical_note}{notes} that might make up a \href{https://en.wikipedia.org/wiki/Melody}{melody} in music, or notes in a \href{https://en.wikipedia.org/wiki/Chord_(music)}{chord}. When a single key is pressed upon a piano, what we hear is not just {\it1} \href{https://en.wikipedia.org/wiki/Frequency}{frequency} of sound vibration, but a {\it composite} of multiple sound vibrations occurring at different mathematically related frequencies. Elements of this composite of vibrations at differing frequencies are referred to as \href{https://en.wikipedia.org/wiki/Harmonic}{harmonics} or partials.
	
	E.g., if note $A_3$ (220 Hz) is played, individual \href{https://en.wikipedia.org/wiki/Frequency}{frequencies} of composite's \href{https://en.wikipedia.org/wiki/Harmonic_series_(music)}{harmonic series} will start at 220 Hz as \href{https://en.wikipedia.org/wiki/Fundamental_frequency}{fundamental frequency}: 440 Hz would be 2nd harmonic, 660 Hz would be 3rd harmonic, 880 Hz would be 4th harmonic, etc. These are integer multiples of fundamental frequency (e.g., $2\cdot220 = 440$, 2nd harmonic). While only about 8 harmonics are really needed to audibly recreate note, total number of harmonics in this mathematical series can be large, although higher harmonic's numerical weaker magnitude \& contribution of that harmonic. Contrary to intuition, a musical recording at its lowest physical level is not a collection of individual \href{https://en.wikipedia.org/wiki/Musical_note}{notes}, but is really a collection of individual harmonics. That is why very similar-sounding recordings can be created with differing collections of instruments \& their assigned notes. As long as total harmonics of recording are recreated to some degree, it does not really matter which instruments or which notes were used.
	\item {\bf Beat detection.}
	\item {\bf How ATM works.}
	\item {\bf Detailed computer steps behind AMT.}
\end{itemize}

'' -- \href{https://en.wikipedia.org/wiki/Transcription_(music)}{Wikipedia{\tt/}transcription (music)}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}