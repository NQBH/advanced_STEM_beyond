\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Computer Music -- Âm Nhạc Máy Tính}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Computer Music -- Âm Nhạc Máy Tính}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/computer_music/NQBH_computer_music.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/computer_music/NQBH_computer_music.tex}.
		\item {\it }.
		
		PDF: {\sc url}: \url{.pdf}.
		
		\TeX: {\sc url}: \url{.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basic Computer Music}

\subsection{{\sc Renato Fabbri, Vilson Vieira de Silva Junior, Ant\^onio Carlos Silvano Pessotti, D\'ebora Cristina Corr\^ea, Osvaldo N. Oliveira Jr.} Musical Elements in Discrete-Time Representation of Sound}
{\sf[2 citations]}
\begin{itemize}
	\item {\sf Abstract.} Representation of basic elements of music in terms of discrete audio signals is often used in software for musical creation \& design. Nevertheless, there is no unified approach that relates these elements to discrete samples of digitized sound. In this article, each musical element is related by equations \& algorithms to discrete-time samples of sounds, \& each of these relations are implemented in scripts within a software toolbox, referred to as MASS (Music \& Audio in Sample Sequences). Fundamental element, musical note with duration, volume, pitch, \& timbre, is related quantitatively to characteristics of digital signal. Internal variations of a note, e.g. tremolos, vibratos, \& spectral fluctuations, are also considered, which enables synthesis of notes inspired by real instruments \& new sonorities. With this representation of notes, resources are provided for generation of higher scale musical structures, e.g. rhythmic meter, pitch intervals \& cycles. This framework enables precise \& trustful scientific experiments, data sonification \& is useful for education \& art. Efficacy of MASS is confirmed by synthesis of small musical pieces using basic notes, elaborated notes \& notes in music, which reflects organization of toolbox \& thus of this article. Possible to synthesize whole albums through collage of scripts \& settings specified by user. With open source paradigm, toolbox can promptly scrutinized, expanded in co-authorship processes \& used with freedom by musicians, engineers, \& other interested parties. In fact, MASS has already been employed for diverse purposes which include music production, artistic presentations, psychoacoustic experiments \& computer language diffusion where appeal of audiovisual artifacts is exploited for education.
	\item CCS Concepts: Applied computing $\to$ Sound \& music computing. Computing methodologies $\to$ Modeling methodologies. General \& reference $\to$ Surveys \& overviews, Reference works.
	\item Additional Key Words \& Phrases: music, acoustics, psychophysics, digital audio, signal processing.
	\item {\sf1. Introduction.} Music is usually defined as art whose medium is sound. Definition might also state: medium includes silences \& temporal organization of structures, or music is also a cultural activity or product. In physics \& in this document, sounds are longitudinal waves of mechanical pressure. Human auditory system perceives sounds in frequency bandwidth between 20Hz \& 20kHz, with actual boundaries depending on person, climate conditions \& sonic characteristics themselves. Since speed of sound $\approx343.2$ m{\tt/}s, such frequency limits corresponds to wavelengths of $\frac{343.2}{20}\approx17.6$ m \& $\frac{343.2}{20000}\approx17.16$ mm. Hearing involves stimuli in bones, stomach, ears, transfer functions of head \& torso (thân mình), \& processing by nervous system. Ear is a dedicated organ or appreciation of these waves, which decomposes them into their sinusoidal spectra \& delivers to nervous system. Sinusoidal components are crucial to musical phenomena, as one can recognize in constitution of sounds of musical interest (e.g. harmonic sounds \& noises, discussed in Sects. 2--3), \& higher level musical structures (e.g. tunings, scales, \& chords, Sect. 4) [55]
	
	Representation of sound can take many forms, from musical scores \& texts in a phonetic language to electric analog signals \& binary data. It includes sets of features e.g. wavelet or sinusoidal components. Although terms `audio' \& `sound' are often used without distinction \& `audio' has many definitions which depend on context \& author, audio most often means a representation of amplitude through time. In this sense, audio expresses sonic waves yield by synthesis or input by microphones, although these sources are not always neatly distinguishable e.g. as captured sounds are processed to generate new sonorities (âm thanh). Digital audio protocols often imply in quality loss (to achieve smaller files, ease storage \& transfer) \& are called {\it lossy} [47]. This is case e.g. of MP3 \& Ogg Vorbis. Non-lossy representations of digital audio, called {\it lossless} protocols or formats, on other hand, assures perfect reconstruction of analog wave within any convenient precision. Standard paradigm of lossless audio consists of representing sound with samples equally spaced by a duration $\delta_s$, \& specifying amplitude of each sample by a fixed number of bits. This is linear Pulse Code Modulation (LPCM) representation of sound, herein referred to as PCM. A PCM audio format has 2 essential attributes: a sampling frequency $f_s = \frac{1}{\delta_s}$ (also called e.g. sampling rate or sample rate), which is number of samples used for representing a second of sound; \& a bit depth, which is number of bits used for specifying amplitude of each sample. {\sf Fig. 1. Example of PCM audio: a sound wave is represented by 25 samples equally spaced in time where each sample has an amplitude specified with 4 bits.} shows 25 samples of a PCM audio with a bit depth of 4, which yields $2^4 = 16$ possible values for amplitude of each sample \& a total of $4\cdot25 = 100$ bits for representing whole sound.
	
	Fixed sampling frequency \& bit depth yield quantization error or quantization noise. This noise diminishes as bit depth increases while greater sampling frequency allows higher frequencies to be represented. Nyquist theorem asserts: sampling frequency is twice maximum frequency: represented signal can contain [49]. Thus, for general musical purposes, suitable to use a sample rate of at least twice highest frequency heard by humans, i.e., $f_s\ge2\cdot20$ kHz $= 40$ kHz. This is basic reason for adoption of sampling frequencies e.g. 44.1 kHz \& 48 kHz, which are standards in Compact Disks (CD) \& broadcast systems (radio \& television), resp.
	
	Within this framework for representing sounds, musical notes can be characterized. Note often stands as `fundamental unit' of musical structures (e.g. atoms in matter or cells in macroscopic organisms) \&, in practice, it can unfold into sounds that uphold other approaches to music. This is of capital importance because science \& scholastic artists widened traditional comprehension of music in 20th century to encompass discourse without explicit rhythm, melody or harmony. This is evident, e.g., in concrete, electronic, electroacoustic, \& spectral musical styles. In 1990s, it became evident: popular (commercial) music had also incorporated sound amalgrams \& abstract discursive arcs. [There are well known incidences of such characteristics in ethnic music, e.g. in Pygmy music, but western theory assimilated them only in last century [74].] Notes are also convenient for another reason: average listener -- \& a considerable part of specialists -- presupposes rhythmic \& pitch organization (made explicit in Sect. 4) as fundamental musical properties, \& these are developed in traditional musical theory in terms of notes. Thereafter, in this article describe musical notes in PCM audio through equations \& then indicate mechanisms for deriving higher level musical structures. Understand: this is not unique approach to mathematically express music in digital audio, but musical theory \& practice suggest: this is a proper framework for understanding \& making computer music, as should become patent in reminder of this text \& is verifiable by usage of MASS toolbox. Hopefully, interested reader or programmer will be able to use this framework to synthesize music beyond traditional conceptualizations when intended.
	
	This document provides a fundamental description of musical structures in discrete-time audio. Results include mathematical relations, usually in terms of musical characteristics \& PCM samples, concise musical theory considerations, \& their implementation as software routines both as very raw \& straightforward algorithms \& in context of rendering musical pieces. Despite general interests involved, there are only a few books \& computer implementations that tackle subject directly. These mainly focus on computer implementations \& way to mimic traditional instruments, with scattered mathematical formalisms for basic notations. Articles on topic appear to be lacking, to best of our knowledge, although advanced \& specialized developments are often reported. A compilation of such works \& their contributions is in Appendix G of [21]. Although current music software uses analytical descriptions presented here, there is no concise mathematical description of them, \& far from trivial to achieve equations by analyzing available software implementations.
	
	Accordingly, objectives of this paper:
	\begin{enumerate}
		\item Present a concise set of mathematical \& algorithmic relations between basic musical elements \& sequences of PCM audio samples.
		\item Introduce a framework for sound \& musical synthesis with control at sample level which entails potential uses in psychoacoustic experiments, data sonification \& synthesis with extreme precision (recap in Sect. 5).
		\item Provide a powerful theoretical framework which can be used to synthesize musical pieces \& albums.
		\item Provide approachability to developed framework [All analytic relations presented in this article are implemented as small scripts in public domain. They constitute MASS toolbox, available in an open source Git repository [9]. These routines are written in Python \& make use of Numpy, which performs numerical routines efficiently (e.g. through LAPACK), but language \& packages are by no means mandatory. Part of scripts has been ported to JavaScript (which favors their use in Web browsers e.g. Firefox \& Chromium) \& native Python [48, 56, 70]. These are all open technologies, published using licenses that grant permission for copying, distributing, modifying \& usage in research, development, art \& education. Hence, work presented here aims at being compliant with recommended practices for availability \& validation \& should ease co-authorship processes [43, 52].]
		\item Provide a didactic (mang tính giáo huấn) presentation of content, which is highly multidisciplinary, involving signal processing, music, psychoacoustics \& programming.
	\end{enumerate}
	Reminder of this article is organized as follows: Sect. 2 characterizes basic musical note; Sect. 3 develops internal dynamics of musical notes; Sect. 4 tackles organization of musical notes into higher level musical structures [14, 41, 42, 54, 62, 72, 74, 76]. As these descriptions require knowledge on topics e.g. psychoacoustics, cultural traditions, \& mathematical formalisms, text points to external complements as needed \& presents methods, results, \& discussions altogether. Sect. 5 is dedicated to final considerations \& further work.	
	\begin{itemize}
		\item {\sf1.1. Additional material.} 1 Supporting Information document [27] holds commented listings of all equations, figures, tables, \& sects in this document \& scripts in MASS toolbox. Another Supporting Information document [28] is a PDF version of code that implements equations \& concepts in each sect [Toolbox contains a collection of Python scripts which
		\begin{itemize}
			\item implements each of equations
			\item render music \& illustrate concepts
			\item render each of figures used in this article.
		\end{itemize}
		Documentation of toolbox consists of this article, Supporting Information documents \& scripts themselves.]. Git repository [26] holds all PDF documents \& Python scripts. Rendered musical pieces are referenced when convenient \& linked directly through URLs, \& constitute another component of framework. They are not very traditional, which facilitates understanding of specific techniques \& extrapolation of note concept. There are MASS-based software packages [23, 25] \& further musical pieces that are linked in Git repository.
		\item {\sf1.2. Synonymy, polysemy \& theoretical frames (disclaimer).} Given: main topic of this article (expression of musical elements in PCM audio) is multidisciplinary \& involves art, reader should be aware: much of vocabulary admits different choices of terms \& defs. More specifically, often case where many words can express same concept \& where 1 word can carry different meanings. This is a very deep issue which might receive a dedicated manuscript. Reader might need to read rest of this document to understand this small selection of synonymy \& polysemy (đa nghĩa) in literature, but important to illustrate point before more dense sects:
		\begin{itemize}
			\item a ``note'' can mean a pitch or an abstract construct with pitch \& duration or a sound emitted from a musical instrument or a specific note in a score or a music.
			\item Sampling rate is also called {\it sampling frequency} or {\it sample rate}.
			\item A harmonic in a sound is most often a sinusoidal component which is in harmonic series of fundamental frequency. Many times, however, terms harmonic \& component are not distinguished. A harmonic can also be a note performed in an instrument by preventing certain overtones (components).
			\item Harmony can refer to chords or to note sets related to chords or even to ``harmony'' in a more general sense, as a kind of balance \& consistency.
			\item A ``tremolo'' can mean different things: e.g. in a piano score, a tremolo is a fast alternation of 2 notes (pitches) while in computer music theory it is (most often) an oscillation of loudness.
		\end{itemize}
		Strived to avoid nomenclature clashes \& use of more terms than needed. Also, there are many theoretical standpoints for understanding musical phenomena, which is an evidence: most often there is not a single way to express or characterize musical structures. Therefore, in this article, adjectives e.g. ``often'', ``commonly'', \& ``frequently'' are abundant \& they would probably be even more numerous if wanted to be pedantically precise. Some of these issues are exposed when content is convenient, e.g. in 1st considerations of timbre.
		
		-- Cố gắng tránh xung đột danh pháp \& sử dụng nhiều thuật ngữ hơn mức cần thiết. Ngoài ra, có nhiều quan điểm lý thuyết để hiểu các hiện tượng âm nhạc, đây là bằng chứng: thường không có một cách duy nhất để diễn đạt hoặc mô tả các cấu trúc âm nhạc. Do đó, trong bài viết này, các tính từ như ``thường xuyên'', ``thường xuyên'', \& ``thường xuyên'' rất nhiều \& chúng có thể còn nhiều hơn nữa nếu muốn chính xác về mặt học thuật. Một số vấn đề này được nêu ra khi nội dung thuận tiện, ví dụ như trong những cân nhắc đầu tiên về âm sắc.
	\end{itemize}	
	\item {\sf2. Characterization of musical note in discrete-time audio.} In diverse artistic \& theoretical contexts, music is conceived as constituted by fundamental units referred to as notes, ``atoms'' that constitute music itself [44, 72, 74]. In a cognitive perspective, notes are understood as discernible elements that facilitate \& enrich transmission of information through music [41, 55]. Canonically, basic characteristics of a musical note are duration, loudness, pitch, \& timbre (âm sắc) [41]. All relations described in this sect are implemented in file {\tt src/sections/eqs2.1.py}. Musical pieces {\it5 sonic portraits \& reduced-fi} are also available online to corroborate \& illustrate concepts.
	\begin{itemize}
		\item {\sf2.1. Duration.} Sample frequency $f_s$ is defined as number of samples in each sec of discrete-time signal. Let $T = \{t_i\}$ be an ordered set of real samples separated by $\delta_s = \frac{1}{f_s}$ secs ($f_s = 44.1$ kHz $\Rightarrow\delta_s = \frac{1}{44100}\approx0.023$ ms). A musical note of duration $\Delta$ secs can be expressed as a sequence $T^\Delta$ with $\Lambda = \lfloor\Delta\cdot f_s\rfloor$ samples. I.e., integer part of multiplication is considered, \& an error of $\le\delta_s$ missing secs is admitted, which is usually fine for musical purposes. Thus
		\begin{equation*}
			T^\Delta = \{t_i\}_{i=0}^{\lfloor\Delta f_s\rfloor - 1} = \{t_i\}_0^{\Lambda - 1}.
		\end{equation*}
		\item {\sf2.2. Loudness.} Loudness [Loudness \& ``volume'' are often used indistinctly. In technical contexts, loudness is used for subjective perception of sound intensity while volume might be used for some measurement of loudness or to a change in intensity of signal by equipment. Accordingly, one can perceive a sound as loud or soft \& change volume by turning a knob. Will use term loudness \& avoid more ambiguous term volume.] is a perception of sonic intensity that depends on reverberation, spectrum, \& other characteristics described in Sect. 3 [11]. One can achieve loudness variations through power of wave [11]:
		\begin{equation*}
			{\rm pow}(T) = \frac{\sum_{i=0}^{\Lambda - 1} t_i^2}{\Lambda}.
		\end{equation*}
		Final loudness is dependent on amplification of signal by speakers. Thus, what matters: relative power of a note in relation to the others around it, or power of a musical sect in relation to the rest. Differences in loudness are result of complex psychophysical phenomena but can often be reasoned about in terms of decibels, calculated directly from amplitudes through energy or power:
		\begin{equation*}
			V_{\rm dB} = 10\log_{10} \frac{{\rm pow}(T')}{{\rm pow}(T)}.
		\end{equation*}
		\item {\sf2.3. Pitch.}
		\item {\sf2.4. Timbre.}
		\item {\sf2.5. Spectra of sampled sounds.}
		\item {\sf2.6. Basic note.}
		\item {\sf2.7. Spatialization: localization \& reverberation.}
		\item {\sf2.8. Musical usages.}
	\end{itemize}
	\item {\sf3. Variation in Basic Note.}
	\begin{itemize}
		\item {\sf3.1. Lookup table.}
		\item {\sf3.2. Incremental variations of frequency \& intensity.}
		\item {\sf3.3. Application of digital filters.}
		\item {\sf3.4. Noise.}
		\item {\sf3.5. Tremolo \& vibrato, AM \& FM.}
		\item {\sf3.6. Musical usages.}
	\end{itemize}
	\item {\sf4. Organization of notes in music.}
	\begin{itemize}
		\item {\sf4.1. Tuning, intervals, scales, \& chords.}
		\item {\sf4.2. Atonal \& tonal harmonies, harmonic expansion \& modulation.}
		\item {\sf4.3. Counterpoint.}
		\item {\sf4.4. Rhythm.}
		\item {\sf4.5. Repetition \& variation: motifs \& larger units.}
		\item {\sf4.6. Directional structures.}
		\item {\sf4.7. Cyclic structures.}
		\item {\sf4.8. Serialism \& post-serial techniques.}
		\item {\sf4.9. Musical idiom?}
		\item {\sf4.10. Musical usages.}
	\end{itemize}
	\item {\sf5. Conclusions \& Further Developments.}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{\cite{Horn_West_Roberts2022}. {\sc Michael S. Horn, Melanie West, Cameron Roberts}. Introduction to Digital Music with Python Programming: Learning Music with Code}
{\sf[4 Amazon ratings]}
\begin{itemize}
	\item {\sf Amazon reviews.} {\it Introduction to Digital Music with Python Programming} provides a foundation in music \& code for beginner. It shows how coding empowers new forms of creative expression while simplifying \& automating many of tedious aspects of production \& composition.
	
	With help of online, interactive examples, this book covers fundamentals of rhythm, chord structure, \& melodic composition alongside basics of digital production. Each new concept is anchored in a real-world musical example that will have you making beats in a matter of minutes.
	
	Music is also a great way to learn core programming concepts e.g. loops, variables, lists, \& functions, {\it Introduction to Digital Music with Python Programming} is designed for beginners of all backgrounds, including high school students, undergraduates, \& aspiring professionals, \& requires no prev experience with music or code.
	
	A beginner's approach to digital music production focuses on key concepts, ensuring ease \& progress in learning.
	
	Streamline your programming education by incorporating music, making complex core concepts easier to grasp \& apply.
	
	Amplify your music creativity by generating unique beats with code in minutes, without needing advanced technical skills.
	
	A great book for learning Python programming \& exploring digital music.
	
	This broad manual combines music theory \& programming basics, providing interactive examples \& real-world applications to help you compose \& produce music from scratch.
	
	Perfect for aspiring musicians \& programmers exploring music-code fusion.
	\item {\sf About Author.} {\sc Michael S. Horn} is Associate Prof of CS \& Learning Sciences at Northwestern University in Evanston, Illinois, where he directs Tangible Interaction Design \& Learning (TIDAL) Lab.
	
	{\sc Melanie West} is a PhD student in Learning Sciences at Northwestern University \& co-founder of Tiz Media Foundation, a nonprofit dedicated to empowering underrepresented youth through science, technology, engineering, \& mathematics (STEM) programs.
	
	{\sc Cameron Roberts} is a software developer \& musician living in Chicago. He holds degrees from Northwestern University in Music Performance \& CS.
	\item {\sf Foreword.} When I was a kid growing up in Texas, I ``learned'' how to play viola. I put {\it learned} in quotes because it was really just a process of rote memorization -- hours \& hours of playing same songs over \& over again. I learned how to read sheet music, bu only to extent that I knew note names \& could translate them into grossest of physical movements. I never learned to read music as literature, to understand its deeper meaning, structure, or historical context. I never understood anything about music theory beyond being annoyed that I had to pay attention to accidentals in different keys. I never composed {\it anything}, not even informally scratching out a tune. I never developed habits of deep listening, of taking songs apart in my head \& puzzling over how they were put together in 1st place. I never played just for fun. \&, despite best intentions of parents \& teachers, I never fell in love with music.
	
	Learning how to code was complete opposite experience for me. I was largely self-taught. Courses I took in school were electives (môn tự chọn) that I chose for myself. Teachers gave me important scaffolding at just right times, but it never felt forced. I spent hours working on games or other projects (probably when I should have been practicing viola). I drew artwork, planned out algorithms, \& even synthesized my own rudimentary sound effects (hiệu ứng âm thanh thô sơ). I had no idea what I was doing, but that was liberating. No one was around to point out my mistakes or to show me ow to do things ``right'' way (at least, not until college). I learned how to figure things out for myself, \& skills I picked up from those experiences are still relevant today. I fell in love with coding. [I was also fortunate to have grown up in a time \& place where these activities were seen as socially acceptable for a person of my background \& identity.]
	
	But I know many people whose stories are flipped 180 degrees. For them, music was so personally, socially, \& culturally motivating that they couldn't get enough. They'd practice for hours \& hours, not just for fun but for something \fbox{much deeper}. For some it was an instrument like guitar that got them started. For others it was an app like GarageBand that gave them a playful entry point into musical ideas. To extent that they had coding experiences, those experiences ranged from uninspiring to off-putting (từ không hấp dẫn đến khó chịu). It's not that they necessarily hated coding, but it was something they saw as not being for them.
	
	In foreword of his book, {\it Mindstorms: Children, Computers, \& Powerful Ideas}, {\sc Seymour Papert} wrote: ``fell in love with gears'' as a way of helping us imagine a future in which children (like me) would fall in love with computer programming, not for its own sake, but for creative worlds \& powerful ideas that programming could open up. Part of what he was saying was: love \& learning go hand in hand, \& that computers could be an entry point into many creative \& artistic domains e.g. mathematics \& music. Coding can revitalize subjects that have become painfully rote in schools.
	
	Process of developing TunePad over past several years has been a fascinating rediscovery of musical ideas for me. Code has given me a different kind of language for thinking about things like rhythm, chords, \& harmony. I can experiment with composition unencumbered by my maladroit hands. Music has become something creative \& alive in a way that it never was for me before. Music theory is no longer a thicket of confusing terminology \& instead has become a fascinating world of mathematical beauty that structures creative process.
	
	-- Quá trình phát triển TunePad trong nhiều năm qua là một sự khám phá lại đầy hấp dẫn đối với tôi về các ý tưởng âm nhạc. Mã đã cho tôi một loại ngôn ngữ khác để suy nghĩ về những thứ như nhịp điệu, hợp âm, \& sự hòa âm. Tôi có thể thử nghiệm sáng tác mà không bị cản trở bởi đôi tay vụng về của mình. Âm nhạc đã trở thành một thứ gì đó sáng tạo \& sống động theo cách mà trước đây tôi chưa từng có. Lý thuyết âm nhạc không còn là một mớ thuật ngữ khó hiểu \& thay vào đó đã trở thành một thế giới hấp dẫn của vẻ đẹp toán học cấu trúc nên quá trình sáng tạo.
	
	{\sc Melanie, Cameron}, \& I hope: this book gives a similarly joyful learning experience with music \& code. Hope: feel empowered to explore algorithmic \& mathematical beauty of music. Hope: discover, as we have: music \& code reinforce one another in surprising \& powerful ways that open new creative opportunities for you. Hope, regardless of your starting point -- as a coder, as a musician, as neither, as both -- will discover something new about yourself \& what you can become.
	\item {\sf1. Why music \& coding?} This book is designed for people who {\it love} music \& are interested in intersection of music \& coding. Maybe you're an aspiring musician or music producer who wants to know more about coding \& what it can do. Or maybe already know a little about coding, \& want to expand your creative musical horizon. Or maybe a total beginner in both. Regardless of your starting point, this book is designed for you to learn about music \& coding as mutually reinforcing skills. Code gives us an elegant language to think about musical ideas, \& music gives us a context within which code makes sense \& is immediately useful. Together they form a powerful new way to create music that will be interconnected with digital production tools of future.
	
	More \& more code will be used to produce music, to compose music, \& even to perform music for live audiences. Digital production tools e.g. Logic, Reason, Pro Tools, FL Studio, \& Ableton Live are complex software applications created with {\it millions} of lines of code written by huge teams of software engineers. With all of these tools can write code to create custom plugins \& effects. Beyond production tools, live coding is an emerging from of musical performance art in which Information Age DJs write computer code to generate music in real time for live audiences.
	
	In other ways, still on cusp of a radical transformation in way use code to create music. History of innovation in music has always been entwined with innovations in technology. Whether talking about {\sc Franz Liszt} in 19th century, who pioneered persona of modern music virtuoso based on technological breakthroughs of piano [Fans were so infatuated with {\sc Liszt}'s piano ``rockstar'' status that they fought over his silk handkerchiefs \& velvet gloves at his performances.], or DJ {\sc Kool Herc} in 20th century, who pioneered hip-hop with 2 turntables \& a crate full of funk records in Bronx, technologies have created new opportunities for musical expression that have challenged status quo \& given birth to new genres. Don't have {\sc Franz Liszt} or DJ {\sc Kool Herc} of coding yet, but it's only a matter of time before coding virtuosos of tomorrow expand boundaries of what's possible in musical composition, production, \& performance.
	\begin{itemize}
		\item {\sf1.1. What is Python?} In this book learn how to create your own digital music using a computer programming language called {\it Python}. If not familiar with programming languages, Python is a general-purpose language 1st released in 1990s that is now 1 of most widely used languages in world. Python is designed to be easy to read \& write, which makes it a popular choice for beginners. Also fully featured \& powerful, making it a good choice for professionals working in fields as diverse as DS, web development, arts, \& video game development. Because Python has been around for decades, it runs on every major computer OS. Examples in this book even use a version of Python that runs directly inside of your web browser without need for any special software installation.
		
		Unlike many other common beginner programming languages, Python is ``text-based'', i.e., type code into an editor instead of dragging code blocks on computer screen. This makes Python a little harder to learn than other beginner languages, but it also greatly expands what you can do. By time yet through this book should feel comfortable writing short Python programs \& have conceptual tools need to explore more on your own.
		\item {\sf1.2. What this book is {\it not}.} Before get into a concrete example of what you can do with a little bit of code, just a quick note about what this book is {\it not}. This book is not a comprehensive guide to Python programming. There are many excellent books \& tutorials designed for beginners, several of which are free. [Recommend \url{https://www.w3schools.com/python/}.]
		
		This book is also not a comprehensive guide to music theory or Western music notation. Get into core ideas behind rhythm, harmony, melody, \& composition, but there are, again, many other resources available for beginners who want to go deeper. What offering is a different approach that combines learning music with learning code in equal measure.
		\item {\sf1.3. What this book {\it is}.} What will do is give an intuitive understanding of fundamental concepts behind both music \& coding. Code \& music are highly technical skills, full of arcane symbols \& terminology, seem almost designed to intimidate beginners. In this book put core concepts to use immediately to start making music. Get to play with ideas at your own pace \& get instant feedback as bring ideas to life. Skip most of technical jargon \& minutiae for now -- can come later. Instead, focus on developing your confidence \& understanding. Importantly, skills, tools, \& ways of thinking introduce in this book will be broadly applicable in many other areas as well. Working in Python code, but core structures of variables, functions, loops, conditional logical, \& classes are same across many programming languages including JavaScript, Java, C, C++, \& C\#. After learn 1 programming language, each additional language is that much easier to pick up.
		\item {\sf1.4. TunePad \& EarSketch.} This book uses 2 free online platforms that combine music \& Python coding. 1st, called TunePad \url{https://tunepad.com}, was developed by a team of researchers at Northwestern University in Chicago. TunePad lets create short musical loops that you can layer together using a simple digital audio workstation (DAW) interface. 2nd platform, called EarSketch \url{https://earsketch.gatech.edu}, was created by researchers at Georgia Tech in Atlanta. EarSketch uses Python code to arrange samples \& loops into full-length compositions. Both platforms are browser-based apps, so all need to get started is a computer (tablets or Chromebooks are fine), an internet connection, \& a web browser like Chrome or Firefox. External speakers or headphones are also nice but not required. Both platforms have been around for years \& have been used by many thousands of students from middle school all way up to college \& beyond. TunePad \& EarSketch are designed primarily as learning platforms, but there are easy ways to export your work to professional production software if want to go further.
		\item {\sf1.5. A quick example.} A quick example of what coding in Python looks like. This program runs in TunePad to create a simple beat pattern, variants of which have been used in literally thousands of songs e.g. {\it Blinding Lights} by The Weeknd \& {\it Roses} by SAINt JHN.
		\begin{verbatim}
			playNote(1) # play a kick drum sound
			playNote(2) # play a snare drum sound
			playNote(1)
			playNote(2)
			rewind(4)   # rewind 4 beats
			for i in range(4):
			    rest(0.5)
			    playNote(4, beats = 0.5) # play hat for a half beat
		\end{verbatim}
		These 8 lines of Python code tell TunePad to play a pattern of kick drums, snare drums, \& high-hats. Most of lines are {\it playNote} instructions, \& those instructions tell TunePad to play musical sounds indicated by numbers inside of parentheses. This example also includes sth called a {\it loop}. Loop is an easy way to repeat a set of actions over \& over again. In this case, loop tells Python to repeat lines 7 \& 8 4 times in a row. Screenshot {\sf Fig. 1.1: A TunePad program to play a simple rock beat.} shows what this looks like in TunePad. Can try out example for yourself with this link: \url{https://tunepad.com/examples/roses}.
		\item {\sf1.6. 5 reasons to learn code.} Now seen a brief example of what can do with a few lines of Python code, here are top 5 reasons to get started with programming \& music if still in doubts.
		\begin{itemize}
			\item {\sf1.6.1. Reason 1: Like it or note, music is already defined by code.} Looking across modern musical landscape, clear: music is already defined by code. 1 of biggest common factors of almost all modern music from any popular genre: {\it everything} is edited, if not created entirely, with sophisticated computer software. Hard to overstate how profoundly such software has shaped sound of music in 21st century. Relatively inexpensive DAW applications \& myriad ubiquitous plugins that work across platforms have had a disruptive \& democratizing effect across music industry. Think about effects plugins like autotune, reverb, or ability to change pitch of a sample without changing tempo. These effects are all generated with sophisticated software. Production studios size of small offices containing hundreds of thousands of dollars' worth of equipment now fit on screen of a laptop computer available to any aspiring producer with passion, a WiFi connection, \& a small budget. Reasons behind shift to digital production tools are obvious. Computers have gotten to a point where they are cheap enough, fast enough, \& capacious enough to do real-time audio editing. Can convert sound waves into editable digital information with microsecond precision \& then hear effects of our changes in real time. These DAWs didn't just appear out of nowhere. They were constructed by huge teams of software engineers writing code -- millions of lines of it. E.g., TunePad was created with $> 1.5$ million lines of code written in over a dozen computer languages e.g. Python, HTML, JavaScript, CSS, \& Dart. Regardless of how feel about digital nature of modern music, not going away. Learning to code will help understand a little more about how all of this works under hood. More to point, it's increasingly common for producers to write their own code to manipulate sound. E.g., in Logic, can write JavaScript code to process incoming MIDI (Musical Instrument Digital Interface) data to do things like create custom arpeggiators. Learning to code can give you more control \& help expand your creative potential {\sf Fig. 1.2: Typical DAW software}.
			\item {\sf1.6.2. Reason 2: Code is a powerful way to make music.} Don't always think about it this way, but music is {\it algorithmic} in nature -- it's full of mathematical relationships, logical structure, \& recursive patterns. Beauty of Baroque fugue is in part a reflection of beauty of mathematical \& computational ideas behind music. Call Bach a genius not just because his music is so compelling, but also because he was able to hold complex algorithms in his mind \& then transcribe them to paper using representation system called Western music notation. I.e., music notation is a language for recording output of composition process, but not a language for capturing algorithmic nature of composition process itself.
			
			Code, on other hand, is a language specifically designed to capture mathematical relationships, logical structure, \& recursive patterns. E.g., take stuttered hi-hat patterns that are 1 of defining characteristics of trap music. Here are a few lines of Python code that generate randomized hi-hat stutters that can bring an otherwise conventional beat to life with sparking energy.
			\begin{verbatim}
				for _ in range(16):
				    if randint(6) > 1: # roll die for a random number
				        playNote(4, beats=0.5) # play an 8th note
				    else:
				        playNote(4, beats=0.25) # or play 16th notes
				        playNote(4, beats=0.25)
			\end{verbatim}
			Or, as another example, here's a 2-line Python program that plays a snare drum riser effect common in house, EDM, or pop music. Often hear this technique right before beat drops. This code uses a decay function so that each successive note is a little shorter resulting in a gradual acceleration effect.
			\begin{verbatim}
				for i in range(50): # play 50 snares
				    playNote(2, beats = pow(2, -0.09 * i))
			\end{verbatim}
			What's cool about these effects: they're {\it parametrized}. Because code describes algorithms to generate music, \& not music itself, i.e., can create infinite variation by adjusting numbers involved. E.g., in trap hi-hat code, can easily play around with how frequently stuttered hats are inserted into pattern by increasing or decreasing 1 number. Can think of code as sth like a power drill; can swap out different bits to make holes of different sizes. Drill bits are like parameters that change what tool does in each specific instance. In same way, algorithms are vastly more general-purpose tools that can accomplish myriad tasks by changing input parameters.
			
			Creating a snare drum riser with code is obviously a very different kind of thing than picking up 2 drumsticks \& banging out a pattern on a real drum. \&, to be clear, not advocating for code to replace learning how to perform with live musical instruments. But, code can be another tool in your musical repertoire for generating repetitive patterns, exploring mathematical ideas, or playing sequences that are too fast or intricate to play by hand.
			
			-- Tạo 1 bộ phận nâng cao trống snare bằng mã rõ ràng là một việc rất khác so với việc nhặt 2 dùi trống \& đánh một mẫu trên một chiếc trống thật. \&, nói rõ hơn, không ủng hộ việc sử dụng mã để thay thế việc học cách biểu diễn với các nhạc cụ sống. Nhưng mã có thể là một công cụ khác trong tiết mục âm nhạc của bạn để tạo ra các mẫu lặp lại, khám phá các ý tưởng toán học hoặc chơi các chuỗi quá nhanh hoặc phức tạp để chơi bằng tay.
			\item {\sf1.6.3. Reason 3: Code lets you build your own musical toolkit.} Becoming a professional in any field is about developing expertise with tools -- acquiring equipment \& knowing how to use it. Clearly, this is true in music industry, but also true in software. Professional software engineers acquire specialized equipment \& software packages. Develop expertise in a range of programming languages \& technical frameworks. But, they also build their own specialized tools that they use across projects. In this book, show how to build up your own library of Python functions. Can think of functions as specialized tools that you create to perform different musical tasks. In addition to examples described above, might write a function to generate a chord progression or play an arpeggio, \& can use functions again \& again across many musical projects.
			\item {\sf1.6.4. Reason 4: Code is useful for a thousand \& 1 other things.} Python is 1 of most powerful, multi-purpose languages in world. Used to create web servers \& social media platforms as much as video games, animation, \& music. Used for research \& DS, politics \& journalism. Knowing a little Python gives access to powerful ML \& AI (AI{\tt/}ML) techniques that are poised to transform most aspects of human work, including in creative domains e.g. music. Python is both a scripting language \& a software engineering platform -- equal parts duct tape \& table saw -- \& it's capable of everything from quick fixes to durable software applications. Learning a little Python won't make you a software engineer, just like learning a few guitar chords won't make you a performance musician. But it's a start down a path. An open door that was previously closed, \& a new way of using your mind \& a new way of thinking about music.
			\item {\sf1.6.5. Reason 5: Coding makes us more human.} When think about learning to code, tend to think about economic payoff. Hear arguments that learning to code is a resume builder \& a path to a high-paying job. Not that this perspective is wrong, but it might be wrong reason for you to learn how to code.
			
			Just like people who are good at music {\it love} music, people who are good at coding tend to {\it love} coding. Craft of building software can be tedious \& frustrating, but it can also be rewarding. A way to express oneself creatively \& to engage in craftwork. People don't learn to knit, cook, or play an instrument for lucrative (có lợi nhuận) career paths that these pursuits open up -- although by all means those pursuits can lead to remarkable careers. People learn these things because they have a {\it passion} for them. Because they are personally fulfilling. These passions connect us to centuries of tradition; they connect us to communities of teachers, learners, \& practitioners; \&, in end, they make us more {\it human}. So when things get a little frustrating -- \& things always get a little frustrating when learning any worthwhile skill -- remember: just like poetry, literature, or music, code is an arts as much as it is a science. \&, just like woodworking, knitting, or cooking, code is a craft as much as it is an engineering discipline. Be patient \& give yourself a chance to fall in love with coding.
		\end{itemize}
		\item {\sf1.7. Future of music \& code.} Before get on with book, wanted to leave you with a brief thought about future of technology, music, \& code. For as long as there have been people on this planet there has been music. \&, as long as there has been music, people have created technology to expand \& enhance their creative potential. A drum is a kind of technology -- a piece of animal hide stretched across a hollow log \& tied in place. It's a polylithic accomplishment, an assembly of parts that requires skill \& craft to make. One must know how to prepare animal hide, to make rope from plant fiber, \& to craft \& sharpen tools. More than that, one must know how to perform with drum, to connect with an audience, to enchant them to move their bodies through an emotional \& rhythmic connection to beat. Technology brings together materials \& tools with knowledge. People must have knowledge both to craft an artifact \& to wield it. \&, over time -- over generations -- that knowledge is refined as it gets passed down from teacher to student. It becomes stylized \& diversified. Tools, artifacts, knowledge, \& practice all become sth greater. Sth we call culture.
		
		Again \& again world of music has been disrupted, democratized, \& redefined by new technologies. Hip-hop was a rebellion against musical status quo fueled by low-cost technologies like recordable cassette tapes, turntables, \& 808 drum machines. Early innovators shattered norms of artistic expression, redefining music, poetry, visual art, \& dance in process. Inexpensive access to technology coupled with a need for new forms of authentic self-expression was a match to dry tinder of racial \& economic oppression.
		
		Hard to overstate how quickly world is still changing as a result of technological advancements. Digital artifacts \& infrastructures are so ubiquitous that they have reconfigured social, economic, legal, \& political systems; revolutionized scientific research; upended arts \& culture; \& even wormed their way into most intimate aspects of our personal \& romantic lives. Already talked about transformative impact that digital tools have had on world of music in 21st century, but exhilarating (\& scary) part: we're on precipice of another wave of transformation in which human creativity will be redefined by AI \& ML. Imagine AI accompanists that can improvise harmonies or melodies in real time with human musicians. Or DL algorithms that can listen to millions of songs \& innovate music in same genre. Or silicon poets that grasp human language well enough to compose intricate rap lyrics. Or machines with trillions of transistor synapses so complex that they begin to ``dream'' -- inverted ML algorithms that ooze imagery unhinged enough to disturb absinthe slumber of surrealist painters. Now, imagine: this is not speculative science fiction, but reality of our world today. These things are here now \& already challenging what we mean by human creativity. What are implications of a society of digital creative cyborgs?
		
		But here's trick: we've always been cyborgs. Western music notation is as much a technology as Python code. Becoming literate in any sufficiently advanced representation system profoundly shapes how think about \& perceive world around us. Classical music notation, theory, \& practice shaped mind of Beethoven as much as he shaped music with it -- so much so that he was still able to compose many of his most famous works while almost totally deaf. {\sc Beethoven} was a creative cyborg enhanced by technology of Western music notation \& theory. Difference: now we've externalized many of cognitive processes into machines that think alongside us. \&, increasingly, these tools are available to everyone. How that changes what it means to be a creative human being is anyone's guess.
		\item {\sf1.8. Book overview.} Excited to have you with us on this journey through music \& code. A short guide for where go from here. Chaps. 2--3 cover foundations of rhythm, pitch, \& harmony. These chaps are designed to move quickly \& get you coding in Python early on. Cover Python variables, loops, which both connect directly to musical concepts. Chaps. 4--6 cover foundations of chords, scales, \& keys using Python lists, functions, \& data structures. Chaps. 7, 8, 10 shift from music composition to music production covering topics e.g. frequency domain, modular synthesis, \& other production effects. In Chap. 9, switch to EarSketch platform to talk about how various musical elements are combined to compose full-length songs. Finally, Chap. 11 provides a short overview of history of music \& code along with a glimpse of what future might hold. Between each chap, provide a series of short {\it interludes} that are like step-by-step tutorials to introduce new music \& coding concepts.
		
		A few notes about how to read this book. Any time include Python code, it will be shown in a programming font. Sometimes write code in a table with line numbers so that can refer to specific lines. When introduce new terms, bold word. If get confused by any of programming or music terminology, check out appendices, which contain quick overviews of all of important concepts. Often invite to follow along with online examples. Best way to learn is by doing it yourself, so strongly encourage to try coding in Python online as go through chaps.
	\end{itemize}
	\item {\sf Interlude 1: Basic Pop Beat.} In this interlude, get familiar with TunePad interface by creating a basic rock beat in style of songs like {\it Roses} by SAINt JHN. Can follow along online by visiting \url{https://tunepad.com/interlude/pop-beat}
	\begin{enumerate}
		\item {\bf Step 1: Deep listening.} Good to get in habit of deep listening. Deep listening is practice of trying every possible way of listening to sounds. Start by loading a favorite song in a streaming service \& listening -- really listening -- to it. Take song apart element by element. What sounds do you hear? How are they layered together? When do different parts come into track \& how do they change over time? Think about how producer balances sounds across frequency spectrum or opens up space for transitions in lyrics. Try focusing on just drums. Can start to recognize individual percussion sounds \& their rhythmic patterns?
		\item {\bf Step 2: Create a new TunePad project.} Visit \url{https://tunepad.com} on a laptop or Chromebook \& set up an account. [Recommend using free Google Chrome browser for best overall experience.] After signing in, click on {\tt New Project} button to create an empty project workspace. Your project will look sth like this {\sf Fig. 1.3: TunePad project workspace}.
		\item {\bf Step 3: Kick drums.} In your project window, click on {\tt ADD CELL} button \& then select {\tt Drums} {\sf Fig. 1.4: Selecting instruments in TunePad.} In TunePad can think of a ``cell'' as an instrument that you can program to play music. Name new instrument ``Kicks'' \& then add this Python code.
		\begin{verbatim}
			# play four kick drums
			playNote(1)
			playNote(1)
			playNote(1)
			playNote(1)
		\end{verbatim}
		When done, your project should look sth like {\sf Fig. 1.5: Parts of a TunePad cell}.
		
		Go ahead \& press Play button at top left to hear how this sounds.
		
		{\it Syntax errors.} Occasionally your code won't work right \& get a red error message box that looks sth like {\sf Fig. 1.6: Python syntax error in TunePad}. This kind of error message is called a ``syntax'' error. In this case, code was written as {\tt playnote} as a lowecase ``n'' instead of an uppercase ``N''. Can fix this error by changing code to read {\tt playNote} on line 2.
		\item {\bf Step 4: Snare drums.} In your project window, click on {\tt ADD CELL} button again \& select {\tt Drums}. Now should have 2 drum cells one appearing above the other in your project. Name 2nd instrument ``Snare Drums'' \& then add this Python code.
		\begin{verbatim}
			# play 2 snare drums on the up beats only
			rest(1) # skip a beat
			playNote(2) # play a snare drum sound
			rest(1)
			playNote(2)
		\end{verbatim}
		Might start to notice text that comes after hashtag symbol \# is a special part of your program. This text is called a {\it comment}, \& it's for human coders to help organize \& document their code. Anything that comes after hashtag on a line is ignored by Python. Try playing this snare drum cell to hear how it sounds. Can also play kick drum cell at same time to see how they sound together.
		\item {\bf Step 5: Hi-hats.} Click on {\tt ADD CELL} button again to add a 3rd drum cell. Change title of this cell to be ``Hats'' \& add code:
		\begin{verbatim}
			# play four hats between the kicks and snares
			rest(0.5) # rest for half a beat
			playNote(4, beats=0.5) # play a hat for half a beat
			rest(0.5)
			playNote(4, beats=0.5)
			rest(0.5)
			playNote(4, beats=0.5)
			rest(0.5)
			playNote(4, beats=0.5)
		\end{verbatim}
		When play all 3 of drum cells together, should hear a basic rock beat pattern: {\tt kick - hat - snare - hat - kick - hat - snare - hat}.
		\item {\bf Step 6: Fix your kicks.} Might notice kick drums feel a little heavy in this mix. Can make some space in pattern by resting on up beats (beats 2 \& 4) when snare drums are playing. Scroll back up to your {\tt Kick drum cell} \& change code to look like this:
		\begin{verbatim}
			# play kicks on the down beats only
			playNote(1)
			rest(1)
			playNote(1)
			rest(1)
			playNote(1)
			rest(1)
			playNote(1)
			rest(0.5) # rest a half beat
			playNote(1, beats = 0.5) # half beat pickup kick
		\end{verbatim}
		\item {\bf Step 7: Adding a bass line.} Add a new cell to your project, but this time select {\tt Bass} instead of {\tt Drums}. Once cell is loaded up, change voice to {\tt Plucked Bass} {\sf Fig. 1.7: Selecting an instrument's voice in TunePad.}
		
		Entering this code to create a simplified bass line in style of {\it Roses} by SAINt JHN. When done, try playing everything together to get full sound.
		\begin{verbatim}
			playNote(5, beats=0.5) # start on low F
			playNote(17, beats=0.5) # up an octave
			rest(1)
			
			playNote(10, beats=0.5) # A sharp
			playNote(22, beats=0.5) # up an octave
			rest(1)
			
			playNote(8, beats=0.5) # G sharp
			playNote(20, beats=0.5) # up an octave
			rest(0.5)
			
			playNote(8, beats=0.5) # G sharp - G - G
			playNote(12, beats=0.5)
			playNote(24, beats=0.5)
			
			playNote(10, beats=0.75) # C sharp
			playNote(22, beats=0.25) # D sharp
		\end{verbatim}
	\end{enumerate}	
	\item {\sf2. Rhythm \& tempo.} This chap dives into fundamentals of {\it rhythm} in music. Start with beat -- what it is, how it's measured, \& how can visualize beat to compose, edit, \& play music. From there provide examples of some common rhythmic motifs from different genres of music \& how to code them with Python. Main programming concepts for this chap include loops, variables, calling function, \& passing parameter values. This chap covers a lot of ground, but it will give you a solid start on making music with code.
	\begin{itemize}
		\item {\sf2.1. Beat \& tempo.} {\it Beat} is foundation of rhythm in music. Term {\it beat} has a number of different meanings in music, [Term beat can also refer to main groove in a dance track (``drop the beat'') or instrumental music that accompanies vocals in a hip-hop track (``she produced a beat for a new artist'') in addition to other meanings.] but this chap uses it to mean a unit of time, or how long an individual note is played -- e.g., ``rest for 2 beats'' or ``play a note for half a beat''. Based on beat, musical notes are combined in repeated patterns that move through time to make rhythmic sense to our ears.
		
		{\it Tempo} refers to speed at which rhythm moves, or how quickly 1 beat follows another in a piece of music. As a listener, can feel tempo by tapping your foot to rhythmic pulse. Standard way to measure tempo is in beats per minute (BPM or bpm), meaning total number of beats played in 1 minute's time. This is almost always a whole number like 60, 120, or 78. At a tempo of 60 bpm, your foot taps 60 times each minute (or 1 beat per sec). At 120 bpm, get 2 beats every sec; \&, at 90 bpm, get 1.5 beats every sec. Later in this chap when start working with TunePad, can set tempo by clicking on bpm indicator in top bar of a project, see {\sf Fig. 2.1: TunePad project information bar. Can click on tempo, time signature, or key to change settings for your project.}
		
		Different genres of music have their own typical tempo ranges (although every song \& every artist is different). E.g., hip-hop usually falls in 60--110 bpm range, while rock is faster in 100--140 bpm range. House{\tt/}techno{\tt/}trance is faster still, with tempos between 120--140 bpm. {\sf[Table: Genre: Tempo Range (BPM)]}.
		
		It takes practice for musicians to perform at a steady tempo, \& they sometimes use a device called a {\it metronome} to help keep their playing constant with pulse of music. Can create a simple metronome in TunePad using 4 lines of code in a drum cell. This works best if switch instrument to {\tt Drums $\to$ Percussion Sounds}.
		\begin{verbatim}
			playNote(3, velocity = 100) # louder 1st note
			playNote(3, velocity = 60)
			playNote(3, velocity = 60)
			playNote(3, velocity = 60)
		\end{verbatim}
		Can adjust tempo of your metronome with bpm indicator {\sf Fig. 2.1: TunePad project information bar. Can click on tempo, time signature, or key to change settings for your project}. As this example illustrates, computers excel at keeping a perfectly steady tempo. This is great if want precision, but there's also a risk that resulting music will sound too rigid \& machine-like. When real people play music they \fbox{often speed up or slow down, either for dramatic effect or just as a result of being a human}. Depending on genre, performers might add slight variations in rhythm called swing or shuffle, that's a kind of back \& forth rocking of beat that you can feel almost more than you can hear. Show how to add a more human feel to computer generated music later in book.
		\item {\sf2.2. Rhythmic notations.} Over centuries, musicians \& composers have developed many different written systems to record \& share music. With invention of digital production software, a number of other interactive representations for mixing \& editing have become common as well. Here are 4 common visual representations of same rhythmic pattern. Pattern has a total duration of 4 beats \& can be counted as ``1 \& 2, 3 \& 4''. 1st 2 notes are $\frac{1}{2}$ beats long followed by a note that is 1 beat long. Then pattern repeats.
		\begin{itemize}
			\item {\sf2.2.1. Representation 1: Standard Western music notation.} 1st representation shows standard music notation (or Western notation), a system of recording notes that has been developed over many hundreds of years. 2 thick vertical lines on left side of illustration indicate: this is rhythmic notation, i.e., there is no information about musical pitch, only rhythmic timing. Dots on long horizontal lines are notes whose shapes indicate duration of each to be played. Sometimes different percussion instruments will have their notes drawn on different lines. Describe what various note symbols mean in more detail in {\sf Fig. 2.2: Standard notation example}.
			\item {\sf2.2.2. Representation 2: Audio waveforms.} 2nd representation shows a visualization of actual audio waveform that gets sent to speakers when play music. Waveform shows amplitude (or volume) of audio signal over time. Next chap talks more about audio waveforms, but for now can think of a waveform as a graph that shows literal intensity of vibration of your speakers over time. When compose a beat in TunePad, can switch to waveform view by clicking on small dropdown arrow at top-left side of timeline {\sf Fig. 2.3: Waveform representation of Fig. 2.2}.
			\item {\sf2.2.3. Representation 3: Piano (MIDI) roll.} 3rd representation shows a piano roll (or MIDI (Musical Instrument Digital Interface) roll). This uses solid lines to show individual notes. Length of lines represents length of individual notes, \& vertical position of lines represents percussion sound being played (kick drums \& snare drums in this case). This representation is increasingly common in music production software. Many tools even allow for drag \& drop interaction with individual notes to compose \& edit music {\sf Fig. 2.4: Piano or MIDI roll representation of Fig. 2.2}.
			\item {\sf2.2.4. Representation 4: Python code.} A final representation for now shows Python code in TunePad. In this representation, duration of each note is set using {\tt beats} parameter of {\tt playNote} function calls.
			\begin{verbatim}
				playNote(2, beats = 0.5)
				playNote(2, beats = 0.5)
				playNote(6, beats = 1)
				
				playNote(2, beats = 0.5)
				playNote(2, beats = 0.5)
				playNote(6, beats = 1)
			\end{verbatim}
		\end{itemize}
		Each of these representation has advantages \& disadvantages; they are good for conveying some kinds of information \& less good at conveying others. E.g., standard rhythm notation has been refined over centuries \& is accessible to an enormous, worldwide community of musicians. On other hand, it can be confusing for people who haven't learn how to read sheet music. Timing of individual notes is communicated using tails \& flags attached to notes, but there's no consistent mapping between horizontal space \& timing.
		
		Audio waveform is good at showing what sound {\it actually} looks like -- how long each note rings out (``release'') \& how sharp its onset is (``attack''). Helpful for music production, mixing, \& mastering. On other hand, waveforms don't really tell you much about pitch of a note or its intended timing as recorded by composer.
		
		Python code is easier for computers to read than humans -- it's definitely not sth you would hand to a musician to sight read. On other hand, it has advantage that it can be incorporated into computer {\it algorithms} \& manipulated \& transformed in endless ways.
		
		There are many, many other notation systems designed to transcribe a musical performance -- what hear at a live performance -- onto a sheet of paper or a computer screen. Each of these representations was invented for a specific purpose \&{\tt/}or genre of music. Might pick a representation based on context \& whether you're in role of a musician (\& what kind of instrument you play), a singer, a composer, a sound engineer, or a producer. Music notation systems are as rich \& varied as cultures \& musical traditions that invented them. 1 nice thing about working with software: easy to switch between multiple representations of music depending on task trying to accomplish.
		\item {\sf2.3. Standard rhythmic notation.} This sect will review a standard musical notation system that has roots in European musical traditions. This system is versatile \& has been refined \& adapted over a long period of time across many countries \& continents to work with an increasingly diverse range of instruments \& musical genres. Start with percussive rhythmic note values in this chap, \& move on to working with pitched instruments in Chap. 3.
		
		{\sf Fig. 2.5: Common note symbols starting with a whole note (4 beats) on top down to 16th notes on bottom. Notes on each new row are half length of row above.} shows most common symbols used in rhythmic music notation. Notes are represented with oval-shaped dots that are either open or closed. All notes except for whole note have tails attached to them that can point either up or down. It doesn't matter which direction (up or down) tail points. Notes that are faster than a quarter note also have horizontal flags or beams connected to tails. Each additional flag or beam indicates note is twice as fast.
		
		Symbol: Name: Beats: TunePad code:
		\begin{enumerate}
			\item Whole Note: Larger open circle with no tail \& no flag: 4: {\tt playNote(1, beats = 4)}
			\item Half Note: Open circle with a tail \& no flag: 2: {\tt playNote(1, beats = 2)}
			\item Quarter Note: Solid circle with a tail \& no flag: 1: {\tt playNote(1, beats = 1)}
			\item 8th Note: Solid circle with a tail \& 1 flag or bar: 0.5 or $\frac{1}{2}$: {\tt playNote(1, beats = 0.5)}
			\item 16th Note: Solid circle with a tail \& 2 flags or bars: 0.25 or $\frac{1}{4}$: {\tt playNote(1, beats = 0.25)}
			\item Dotted Half Note: Open circle with a tail. Dot adds an extra beat to half note: 3: {\tt playNote(1, beats = 3)}
			\item Dotted Quarter Note: Solid circle with a tail. Dot adds an extra half-beat: 1.5: {\tt playNote(1, beats = 1.5)}
			\item Dotted 8th Note: Solid circle with tail \& 1 flag. Dot adds an extra quarter beat: 0.75. {\tt playNote(1, beats = 0.75)}
		\end{enumerate}
		Standard notation also includes {\it dotted notes}, where a small dot follows note symbol. With a dotted note, take original note's duration \& add half of its value to it. So, a dotted quarter note is 1.5 beats long, a dotted half note is 3 beats long, etc.
		
		There are also symbols representing different durations of silence or ``rests''.
		
		Symbol: Name: Beats: TunePad code
		\begin{enumerate}
			\item Whole Rest: 4: {\tt rest(beats = 4)}
			\item Half Rest: 2: {\tt rest(beats = 2)}
			\item Quarter Rest: 1: {\tt rest(beats = 1)}
			\item 8th Rest: 0.5 or $\frac{1}{2}$: {\tt rest(beats = 0.5)}
			\item 16th Rest: 0.25 or $\frac{1}{4}$: {\tt rest(beats = 0.25)}
		\end{enumerate}
		\item {\sf2.4. Time signatures.} In standard notation, notes are grouped into segments called {\it measures} (or bars). Each measure contains a fixed number of beats, \& duration of all notes in a measure should add up to that amount. Relationship between measures \& beats is represented by a fraction called a {\it time signature}. Numerator (or top number) indicates number of beats in measure, \& denominator (bottom number) indicates beat duration.
		
		-- Trong ký hiệu chuẩn, các nốt nhạc được nhóm thành các đoạn gọi là {\it nhịp} (hoặc ô nhịp). Mỗi ô nhịp chứa một số phách cố định, \& thời lượng của tất cả các nốt nhạc trong một ô nhịp phải cộng lại bằng số lượng đó. Mối quan hệ giữa các ô nhịp \& nhịp được biểu diễn bằng một phân số gọi là {\it nhịp điệu}. Tử số (hoặc số trên cùng) biểu thị số phách trong ô nhịp, \& mẫu số (số dưới cùng) biểu thị thời lượng của phách.
		\begin{enumerate}
			\item $\frac{4}{4}$: 4-4 Time or ``Common tTime'': There are 4 beats in each measure, \& each beat is a quarter note. This time signature is sometimes indicated using a special symbol
			\item $\frac{2}{2}$: 2-2 Time or ``Cut Time'': There are 2 beats in each measure, \& beat value is a half note. Cut time is sometimes indicated with a `C' with a line through it.
			\item $\frac{2}{4}$: 2-4 Time: There are 2 beats in each measure, \& quarter note gets beat.
			\item $\frac{3}{4}$: 3-4 Time: There are 3 beats in each measure, \& quarter note gets beat.
			\item $\frac{3}{8}$: 3-8 Time: There are 3 beats in each measure, \& 8th note gets beat.
		\end{enumerate}
		Most common time signature is $\frac{4}{4}$. So common, in fact, referred to as {\it common time}. Often denoted by a C symbol shown in table. In common time, there are 4 beats to each measure, \& quarter note ``gets beat'' meaning: 1 beat is same as 1 quarter note.
		
		Vertical lines separate measures in standard notation. In example, there are 2 measures in 4/4 time (4 beats in each measure, \& each beat is a quarter note).
		
		\item {\sf2.5. Percussion sounds \& instruments.}
	\end{itemize}
	\item {\sf3. Pitch, harmony, \& dissonance.}
	\item {\sf4. Chords.}
	\item {\sf5. Scales, keys, \& melody.}
	\item {\sf6. Diatonic chords \& chord progressions.}
	\item {\sf7. Frequency, fourier, \& filters.}
	\item {\sf8. Note-based production effects.}
	\item {\sf9. Song composition \& EarSketch.}
	\item {\sf10. Modular synthesis.}
	\item {\sf11. History of music \& computing.}
	\item {\sf Appendix A: Python ref.}
	\item {\sf Appendix B: TunePad programming ref.}
	\item {\sf Appendix C: Music ref.}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{\cite{Valimaki_Pakarinen_Erkut_Karjalainen2006}. {\sc Vesa Välimäki, Jyri Pakarinen, Cumhur Erkut, Matti Karjalainen}. Discrete-Time Modeling of Musical Instruments}
{\sf[283 citations]}
\begin{question}[Cf. Continuous-time modeling vs. Discrete-time modeling]
	How about continuous-time modeling of musical instruments?
\end{question}

\begin{question}[Cf. Mathematical modeling technique vs. Physical modeling technique]
	Compare Mathematical modeling technique vs. Physical modeling technique.
\end{question}

\begin{itemize}
	\item {\bf Abstract.} This article describes physical modeling techniques that can be used for simulating musical instruments. Methods are closely related to digital signal processing. They discretize system w.r.t. time, because aim: run simulation using a computer. Physics-based modeling methods can be classified as mass-spring, modal, wave digital, finite difference, digital waveguide \& source-filter models. Present basic theory \& a discussion on possible extensions for each modeling technique. For some methods, a simple model example is chosen from existing literature demonstrating a typical use of method. E.g., in case of digital waveguide modeling technique a vibrating string model is discussed, \& in case of wave digital filter technique, present a classical piano hammer model. Tackle some nonlinear \& time-varying models \& include new results on digital waveguide modeling of a nonlinear string. Discuss current trends \& future directions in physical modeling of musical instruments.
	\item {\sf1. Introduction.} Musical instruments have historically been among most complicated mechanical systems made by humans. They have been a topic of interest for physicists \& acousticians for over a century. Modeling of musical instruments using computers is newest approach to understanding how these instruments work.
	
	This paper presents an overview of physics-based modeling of musical instruments. Specifically, this paper focuses on sound synthesis methods derived using physical modeling approach. Several previously published tutorial \& review papers discussed physical modeling synthesis techniques for musical instruments sounds [73, 129, 251, 255, 256, 274, 284, 294]. Purpose of this paper: give a unified introduction to 6 main classes of discrete-time physical modeling methods, namely mass--spring, modal, wave digital, finite difference, digital waveguide \& source--filter models. This review also tackles mixed \& hybrid models in which usually 2 different modeling techniques are combined.
	
	Physical models of musical instruments have been developed for 2 main purposes: research of acoustical properties \& sound synthesis. Methods discussed in this paper can be applied to both purpose, but here main focus is sound synthesis. Basic idea of physics-based sound synthesis: build a simulation model of sound production mechanism of a musical instrument \& to generate sound with a computer program or signal processing hardware that implements that model. Motto of physical modeling synthesis: when a model has been designed properly, so that it behaves much like actual acoustic instrument, synthetic sound will automatically be natural in response to performance. In practice, various simplifications of model cause sound output to be similar to, but still clearly different from, original sound. Simplifications may be caused by intentional approximations that reduce computational cost or by inadequate knowledge of what is actually happening in acoustic instrument. A typical \& desirable simplification: linearization of slightly nonlinear phenomena, which may avert unnecessary complexities, \& hence may improve computational efficiency.
	
	In speech technology, idea of accounting for physics of sound source, human voice production organs, is an old tradition, which has led to useful results in speech coding \& synthesis. While 1st experiments on physics-based musical sound synthesis were documented several decades ago, 1st commercial products based on physical modeling synthesis were introduced in 1990s. Thus, topic is still relatively young. Research in field has been very active in recent years.
	
	1 of motivations for developing a physically based sound synthesis: musicians, composers, \& other users of electronic musical instruments have a constant hunger for better digital instruments \& for new tools for organizing sonic events. A major problem in digital musical instruments has always been how to control them. For some time, researchers of physical models have hoped: these models would offer more intuitive, \& in some ways better, controllability than previous sound synthesis methods. In addition to its practical applications, physical modeling of musical instruments is an interesting research topic for other reasons. It helps to solve old open questions, e.g. which specific features in a musical instrument's sound make it recognizable to human listeners or why some musical instruments sound sophisticated while others sound cheap. Yet another fascinating aspect of this field: when physical principles are converted into computational methods, possible to discover new algorithms. This way, possible to learn new signal processing methods from nature.
	\item {\sf2. Brief history.} Modeling of musical instruments is fundamentally based on understanding of their sound production principles. 1st person attempting to understand how musical instruments work might have been Pythagoras, who lived in ancient Greece around 500 BC. At that time, understanding of musical acoustics was very limited \& investigations focused on tuning of string instruments. Only after late 18th century, when rigorous mathematical methods e.g. PDEs were developed, was it possible to build formal models of vibrating strings \& plates.
	
	Earliest work on physics-based discrete-time sound synthesis was probably conducted by {\sc Kelly \& Lochbaum} in context of vocal-tract modeling [145]. A famous early musical example is `Bicycle Built for 2' (1961), where singing voice was produced using a discrete-time model of human vocal tract. This was result of collaboration between {\sc Mathews, Kelly \& Lochbaum} [43]. 1st vibrating string simulations were conducted in early 1970s by {\sc Hiller \& Ruiz} [113, 114], who discretized wave equation to calculate waveform of a single point of a vibrating string. Computing 1 s of sampled waveform took minutes. A few years later, {\sc Cadoz} \& his colleagues developed discrete-time mass-spring models \& built dedicated computing hardware to run real-time simulations [38].
	
	In late 1970s \& early 1980s, {\sc McIntyre, Woodhouse, \& Schumacher} made important contributions by introducing simplified discrete-time models of bowed strings, clarinet \& flute [173, 174, 235], \& {\sc Karplus \& Strong} [144] invented a simple algorithm that produces string-instrument-like sounds with few arithmetic operations. Based on these ideas \& their generalizations, {\sc Smith \& Jaffe} introduced a signal-processing oriented simulation technique for vibrating strings [120, 244]. Soon thereafter, {\sc Smith} proposed term `digital waveguide' \& developed general theory [247, 249, 253].
	
	1st commercial product based on physical modeling synthesis, an electronic keyboard instrument by Yamaha, was introduced in 1994 [168]; it used digital waveguide techniques. More recently, digital waveguide techniques have been also employed in MIDI synthesizers on personal computer soundcards. Currently, much of practical sound synthesis is based on software, \& there are many commercial \& freely available pieces of synthesis software that apply 1 or more physical modeling methods.
	\item {\sf3. General concepts of physics-based modeling.} In this sect, discuss a number of physical \& signal processing concepts \& terminology that are important in understanding modeling paradigms discussed in subsequent sects. Each paradigm is also characterized briefly in end of this sect. A  reader familiar with basic concepts in context of physical modeling \& sound synthesis may go directly to Sect. 4.
	\begin{itemize}
		\item {\sf3.1. Physical domains, variables, \& parameters.} Physical phenomena can be categorized as belonging to different `physical domains'. Most important ones for sound sources e.g. musical instruments are acoustical \& mechanical domains. In addition, electrical domain is needed for electroacoustic instruments \& as a domain to which phenomena from other domains are often mapped. Domains may interact with one another, or they can be used as analogies (equivalent models) of each other. Electrical circuits \& networks are often applied as analogies to describe phenomena of other physical domains.
		
		Quantitative description of a physical system is obtained through measurable quantities that typically come in pairs of variables, e.g. force \& velocity in mechanical domain, pressure \& volume velocity in acoustical domain or voltage \& current in electrical domain. Members of such dual variable pairs are categorized generically as `across variable' or `potential variable', e.g. voltage, force or pressure, \& `through variable' or `kinetic variable', e.g. current, velocity or volume velocity. If there is a linear relationship between dual variables, this relation can be expressed as a parameter, e.g. impedance $Z = \frac{U}{I}$ being ratio of voltage $U$ \& current $I$, or by its inverse, admittance $Y = \frac{I}{U}$. An example from mechanical domain is mobility (mechanical admittance) defined as ratio of velocity \& force. When using such parameters, only 1 of dual variables is needed explicitly, because the other one is achieved through constraint rule.
		
		Modeling methods discussed in this paper use 2 types of variables for computation, `K-variables' \& `wave variables' (also denoted as `W-variables'). `K' comes from Kirchhoff \& refers to Kirchhoff continuity rules of quantities in electric circuits \& networks [185]. `W' is shortform for wave, referring to wave components of physical variables. Instead of pairs of across \& through as with K-variables, wave variables come in pairs of incident \& reflected wave components. Details of wave modeling are discussed in Sects. 7--8, while K-modeling is discussed particularly in Sects. 4 \& 10. It will become obvious: these are different formulations of same phenomenon, \& possibility to combine both approaches in hybrid modeling will be discussed in Sect. 10.
		
		Decomposition into wave components is prominent in such wave propagation phenomena where opposite-traveling waves add up to actual observable K-quantities. A wave quantity is directly observable only when there is no other counterpart. It is, however, a highly useful abstraction to apply wave components to any physical case, since this helps in solving computability (causality) problems in discrete-time modeling.
		\item {\sf3.2. Modeling of physical structure \& interaction.} Physical phenomena are observed as structures \& processes in space \& time. In sound source modeling, interested in dynamic behavior that is modeled by variables, while slowly varying or constant properties are parameters. Physical interaction between entities in space always propagates with a finite velocity, which may differ by orders of magnitude in different physical domains, speed of light being upper limit.
		
		`Causality' is a fundamental physical property that follows from finite velocity of interaction from a cause to corresponding effect. In many mathematical relations used in physical models causality is not directly observable. E.g., relation of voltage across \& current through an impedance is only a constraint, \& variables can be solved only within context of whole circuit. Requirement of causality (more precisely temporal order of cause preceding effect) introduces special computability problems in discrete-time simulation, because 2-way interaction with a delay shorter than a unit delay (sampling period) leads to `delay-free loop problem'. Use of wave variables is advantageous, since incident \& reflected waves have a causal relationship. In particular, wave digital filter (WDF) theory, discussed in Sect. 8, carefully treats this problem through use of wave variables \& specific scheduling of computation operations.
		
		Taking finite propagation speed into account requires using a spatially distributed model. Depending on case at hand, this can be a full 3D model e.g. used for room acoustics, a 2D model e.g. for a drum membrane (discarding air loading) or a 1D model e.g. for a vibrating sting. If object to be modeled behaves homogeneously enough as a whole, e.g. due to its small size compared with wavelength of wave propagation, it can be considered a lumped entity that does not need a description of spatial dimensions.
		
		-- Việc tính đến tốc độ lan truyền hữu hạn đòi hỏi phải sử dụng một mô hình phân bố không gian. Tùy thuộc vào trường hợp cụ thể, đây có thể là mô hình 3D đầy đủ, ví dụ như được sử dụng cho âm học phòng, mô hình 2D, ví dụ như cho màng trống (loại bỏ tải trọng không khí) hoặc mô hình 1D, ví dụ như cho một cú chích rung. Nếu vật thể được mô hình hóa hoạt động đủ đồng nhất như một tổng thể, ví dụ như do kích thước nhỏ so với bước sóng truyền sóng, thì nó có thể được coi là một thực thể tập trung không cần mô tả về kích thước không gian.
		\item {\sf3.3. Signals, signal processing, \& discrete-time modeling.} In signal processing, signal relationships are typically represented as 1-directional cause-effect chains. Contrary to this, bi-directional interaction is common in (passive) physical systems, e.g. in systems where reciprocity principle is valid. In true physics-based modeling, 2-way interaction must be taken into account. I.e., from signal processing viewpoint, such models are full of feedback loops, which further implicates: concepts of computability (causality) \& stability become crucial.
		
		In this paper, apply digital signal processing (DSP) approach to physics-based modeling whenever possible. Motivation for this: DSP is an advanced theory \& tool that emphasizes computational issues, particularly maximal efficiency. This efficiency is crucial for real-time simulation \& sound synthesis. Signal flow diagrams are also a good graphical means to illustrate algorithms underlying simulations. Assume: reader is fmailiar with fundamentals of DSP, e.g. sampling theorem [242] to avoid aliasing (also spatial aliasing) due to sampling in time \& space as well as quantization effects due to finite numerical precision.
		
		An important class of systems is those that are linear \& time invariant (LTI). They can be modeled \& simulated efficiently by digital filters. They can be analyzed \& processed in frequency domain through linear transforms, particularly by Z-transform \& discrete Fourier transform (DFT) in discrete-time case. While DFT processing through fast Fourier transform (FFT) is a powerful tool, it introduces a block delay \& does not easily fit to sample-by-sample simulation, particularly when bi-directional physical interaction is modeled.
		
		Nonlinear \& time-varying systems bring several complications to modeling. Nonlinearities create new signal frequencies that easily spread beyond Nyquist limit, thus causing aliasing, which is perceived as very disturbing distortion. In addition to aliasing, delay-free loop problem \& stability problems can become worse than they are in linear systems. If nonlinearities in a system to be modeled are spatially distributed, modeling task is even more difficult than with a localized nonlinearity. Nonlinearities will be discussed in several sects of this paper, most completely in Sect. 11.
		\item {\sf3.4. Energetic behavior \& stability.} Product of dual variables e.g. voltage \& current gives power, which, when integrated in time, yields energy. Conservation of energy in a closed system is a fundamental law of physics that should also be obeyed in true physics-based  modeling. In musical instruments, resonators are typically passive, i.e. they do not produce energy, while excitation (plucking, bowing, blowing, etc.) is an active process that injects energy to passive resonators.
		
		Stability of a physical system is closely related to its energetic behavior. Stability can be defined so that energy of system remains finite for finite energy excitations. From a signal processing viewpoint, stability may also be defined so that variables, e.g. voltages, remain within a linear operating range for possible inputs in order to avoid signal clipping \& distortion.
		
		In signal processing systems with 1-directional input--output connections between stable subblocks, an instability can appear only if there are feedback loops. In general, impossible to analyze such a system's stability without knowing its whole feedback structure. Contrary to this, in models with physical 2-way interaction, if each element is passive, then any arbitrary network of such elements remains stable.
		\item {\sf3.5. Modularity \& locality of computation.} For a computational realization, desirable to decompose a model systematically into blocks \& their interconnections. Such an object-based approach helps manage complex models through use of modularity principle. Abstractions to macro blocks on basis of more elementary ones helps hiding details when building excessively complex models.
		
		For 1-directional interactions used in signal processing, enough to provide input \& output terminals for connecting blocks. For physical interaction, connections need to be done through ports, with each port having a pair of K- or wave variables depending on modeling method used. This allows mathematical principles used for electrical networks [185]. Details on block-wise construction of models will be discussed in following sects for each modeling paradigm.
		
		Locality of interaction is a desirable modeling feature, which is also related to concept of causality. For a physical system with a finite propagation speed of waves, enough: a block interacts only with its nearest neighbors; it does not need global connections to compute its task \& effect automatically propagates throughout system.
		
		In a discrete-time simulation with bi-directional interactions, delays shorter than a unit delay (including 0 delay) introduce delay-free loop problem that we face several times in this paper. While possible to realize fractional delays [154], delayers shorter than unit delay contain a delay-free component. There are ways to make such `implicit' system computable, but cost in time (or accuracy) may become prohibitive for real-time processing.
		\item {\sf3.6. Physics-based discrete-time modeling paradigms.} This paper presents an overview of physics-based methods \& techniques for modeling \& synthesizing musical instruments. Have excluded some methods often used in acoustics, because they do not easily solve task of efficient discrete-time modeling \& synthesis. E.g., finite element \& boundary element methods (FEM \& BEM) are generic \& powerful for solving system behavior numerically, particularly for linear systems, but focus on inherently time-domain methods for sample-by-sample computation.
		
		Main paradigms in discrete-time modeling of musical instruments can be briefly characterized as follows.
		\begin{itemize}
			\item {\sf3.6.1. Finite difference models.} In Sect. 4, finite difference models are numerical replacement for solving PDEs. Differentials are approximated by finite differences so that time \& position will be discretized. Through proper selection of discretization to regular meshes, computational algorithms become simple \& relatively efficient. Finite difference time domain (FDTD) schemes are K-modeling methods, since wave components are not explicitly utilized in computation. FDTD schemes have been applied successfully to 1D, 2D, \& 3D systems, although in linear 1D cases digital waveguides are typically superior in computational efficiency \& robustness. In multidimensional mesh structures, FDTD approach is more efficient. It also shows potential to deal systematically with nonlinearities (Sect. 11). FDTD algorithms can be problematic due to lack of numerical robustness \& stability, unless carefully designed.
			\item {\sf3.6.2. Mass--spring networks.} In Sect. 5, mass--spring networks are a modeling approach, where intuitive basic elements in mechanics -- masses, springs, \& damping elements -- are used to construct vibrating structures. It is inherently a K-modeling methodology, which has been used to construct small- \& large-scale mesh-like \& other structures. It has resemblance to FDTD schemes in mesh structures \& to WDFs for lumped element modeling. Mass--spring networks can be realized systematically also by WDFs using wave variables (Sect. 8).
			\item {\sf3.6.3. Modal decomposition methods.} In Sect. 6 modal decomposition methods represent another approach to look at vibrating systems, conceptually from a frequency-domain viewpoint. Eigenmodes of a linear system are exponentially decaying sinusoids at eigenfrequencies in response of a system to impulse excitation. Although thinking by modes is normally related to frequency domain, time-domain simulation by modal methods can be relatively efficient, \& therefore suitable to discrete-time computation. Modal decomposition methods are inherently based on use of K-variables. Modal synthesis has been applied to make convincing sound synthesis of different musical instruments. Functional transform method (FTM) is a recent development of systematically exploiting idea of spatially distributed modal behavior, \& it has also been extended to nonlinear system modeling.
			\item {\sf3.6.4. Digital waveguides.} Digital waveguides (DWGs) in Sect. 7 are most popular physics-based method of modeling \& synthesizing musical instruments that are based on 1D resonators, e.g. strings \& wind instruments. Reason for this is their extreme computational efficiency in their basic formulations. DWGs have been used also in 2D \& 3D mesh structures, but in such cases wave-based DWGs are not superior in efficiency. Digital waveguides are based on use of traveling wave components; thus, they form a wave modeling (W-modeling) paradigm [Term digital waveguide is used also to denote K-modeling, e.g. FDTD mesh-structures, \& source-filter models derived from traveling wave solutions, which may cause methodological confusion.]. Therefore, they are also compatible with WDFs (Sect. 8), but in order to be compatible with K-modeling techniques, special conversion algorithms must be applied to construct hybrid models, as discussed in Sect. 10.
			\item {\sf3.6.5. Wave digital filters.} WDFs in Sect. 8 are another wave-based modeling technique, originally developed for discrete-time simulation of analog electric circuits \& networks. In their original form, WDFs are best suited for lumped element modeling; thus, they can be easily applied to wave-based mass--spring modeling. Due to their compatibility with digital waveguides, these methods complement each other. WDFs have also been extended to multidimensional networks \& to systematic \& energetically consistent modeling of nonlinearities. They have been applied particularly to deal with lumped \& nonlinear elements in models, where wave propagation parts are typically realized by digital waveguides.
			\item {\sf3.6.6. Source--filter models.} In Sect. 9 source--filter models form a paradigm between physics-based modeling \& signal processing models. True spatial structure \& bi-directional interactions are not visible, but are transformed into a transfer function that can be realized as a digital filter. Approach is attractive in sound synthesis because digital filters are optimized to implement transfer functions efficiently. Source part of a source--filter model is often a wavetable, consolidating different physical or synthetic signal components needed to feed filter part. Source--filter paradigm is frequently used in combination with other modeling paradigms in more or less ad hoc ways.
		\end{itemize}
	\end{itemize}
	\item {\sf4. Finite difference models.} Finite difference schemes can be used for solving PDEs, e.g. those describing vibration of a string, a membrane or an air column inside a tube [264]. Key idea in finite difference scheme: replace derivatives with finite difference approximations. An early example of this approach in physical modeling of musical instruments is work done by {\sc Hiller \& Ruiz} in early 1970s [113, 114]. This line of research has been continued \& extended by {\sc Chaigne} \& colleagues [45, 46, 48] \& recently by others [25, 26, 29, 30, 81, 103, 131].
	
	Finite difference approach leads to a simulation algorithm that is based on a difference equation, which can be easily programmed with a computer. E.g., how basic wave equation, which describes small-amplitude vibration of a lossless, ideally flexible string, is discretized using this principle. Here present a formulation after Smith [253] using an ideal string as a starting point for discrete-time modeling. A more thorough continuous-time analysis of physics of strings can be found in [96].
	\begin{itemize}
		\item {\sf4.1. Finite difference models for an ideal vibrating string.} {\sf Fig. 1: Part of an ideal vibrating string.} depicts a snapshot of an ideal (lossless, linear, flexible) vibrating string by showing displacement as a function of position. Wave equation for string is given by \fbox{$Ky'' = \epsilon\ddot{y}$}
		\item {\sf4.2. Boundary conditions \& string excitation.}
		\item {\sf4.3. Finite difference approximation of a lossy string.}
		\item {\sf4.4. Stiffness in finite difference strings.}
	\end{itemize}
	\item {\sf5. Mass-spring networks.}
	\begin{itemize}
		\item {\sf5.1. Basic theory.}
		\item {\sf5.2. CORDIS-ANIMA.}
		\item {\sf5.3. Other mass-spring systems.}
	\end{itemize}
	\item {\sf6. Modal decomposition methods.}
	\begin{itemize}
		\item {\sf6.1. Modal synthesis.}
		\item {\sf6.2. Filter-based modal methods.}
		\item {\sf6.3. Functional transform method.}
	\end{itemize}
	\item {\sf7. Digital waveguides.}
	\begin{itemize}
		\item {\sf7.1. From wave propagation to digital waveguides.}
		\item {\sf7.2. Modeling of losses \& dispersion.}
		\item {\sf7.3. Modeling of waveguide termination \& scattering.}
		\item {\sf7.4. Digital waveguide meshes \& networks.}
		\item {\sf7.5. Reduction of a DWG model to a single delay loop structure.}
		\item {\sf7.6. Commuted DWG synthesis.}
		\item {\sf7.7. Case study: modeling \& synthesis of acoustic guitar.} Acoustic guitar is an example of a musical instruments for which DWG modeling is found to be an efficient method, especially for real-time sound synthesis [134, 137, 142, 160, 286, 295]. DWG principle in {\sf Fig. 14: A DWG block diagram of 2 strings coupled through a common bridge impedance $Z_{\rm b}$ \& terminated at other end by nut impedances $Z_{\rm t1},Z_{\rm t2}$. Plucking points are for force insertion from wavetables ${\rm WT}_i$ into junctions in delay-lines ${\rm DL}_{ij}$. Output is taken as bridge velocity.} allows for true physically distributed modeling of strings \& their interaction, while SDL commuted synthesis ({\sf Fig. 17: Reduction of bi-directional delay-line waveguide model (top) to a single delay line loop structure (bottom).} \& {\sf Fig. 18: Principles of commuted DWG synthesis: (a) cascaded excitation, string \& body, (b) body \& string blocks commuted \& (c) excitation \& body blocks consolidated into a wavetable for feeding string model.}) allows for more efficient computation. In this subsect discuss principles of commuted waveguide synthesis as applied to high-quality synthesis of acoustic guitar.
		
		There are several features that must be added to simple commuted SDL structure in order to achieve natural sound \& control of playing features. {\sf Fig. 19: Degrees of freedom for string vibration in guitar: Torsional, Longitudinal, Vertical, Horizontal.} depicts degrees of freedom for vibration of strings in guitar. Transversal directions, i.e. vertical \& horizontal polarizations of vibration, are most prominent ones. Vertical vibration connects strongly to bridge, resulting in stronger initial sound \& faster decay than horizontal vibrations that start more weakly \& decay more slowly. Effect of longitudinal vibration is weak but can be observed in generation of some partials of sound [320]. Longitudinal effects are more prominent in piano [16, 58], but are particularly important in such instruments as kantele [82] through nonlinear effect of tension modulation (Sect. 11). Torsional vibration of strings in guitar is not shown to have a remarkable effect on sound. In violin it has a more prominent physical role, although it makes virtually no contribution to sound.
		
		In commuted waveguide synthesis, 2 transversal polarizations can be realized by 2 separate SDL string models, $S_{\rm v}(z)$ for vertical \& $S_{\rm h}(z)$ for horizontal polarization in {\sf Fig. 20: Dual-polarization string model with sympathetic vibration coupling between strings. Multiple wavetables are used for varying plucking styles. Filter $E(z)$ can control detailed timbre of plucking \& $P(z)$ is a plucking point comb filter.}, each one with slightly different delay \& decay parameters. Coefficient $m_{\rm p}$ is used to control relative excitation amplitudes of each polarization, depending on initial direction of string movement after plucking. Coefficients $m_{\rm o}$ can be used to mix vibration signal components at bridge.
		
		{\sf Fig. 20} also shows another inherent feature of guitar, sympathetic coupling between strings at bridge, which causes an undamped string to gain energy from another string set in vibration. While principle shown in {\sf Fig. 14} implements this automatically if string \& bridge admittances are correctly set, model in {\sf Fig. 20} requires special signal connections from point C to vertical polarization model of other strings. This is just a rough approximation of physical phenomenon that guarantees stability of model. There is also a connection through $g_{\rm c}$ that allows for simple coupling from horizontal polarization to excite vertical vibration, with a final result of a coupling between polarizations.
		
		Dual-polarization model in {\sf Fig. 20} is excited by wavetables containing commuted waveguide excitations for different plucking styles. Filter $E(z)$ can be used to control timbre details of selected excitation, \& filter $E(z)$ can be used to control timbre details of selected excitation, \& filter $P(z)$ is a plucking point comb filter, as prev discussed.
		
		-- Mô hình phân cực kép trong {\sf Hình 20} được kích thích bằng các bảng sóng chứa các kích thích ống dẫn sóng chuyển mạch cho các kiểu gảy khác nhau. Bộ lọc $E(z)$ có thể được sử dụng để kiểm soát các chi tiết âm sắc của kích thích đã chọn, \& bộ lọc $E(z)$ có thể được sử dụng để kiểm soát các chi tiết âm sắc của kích thích đã chọn, \& bộ lọc $P(z)$ là bộ lọc lược điểm gảy, như đã thảo luận trước đó.
		
		For solid body electric guitars, a magnetic pickup model is needed, but body effect can be neglected. Magnetic pickup can be modeled as a lowpass filter [124,137] in series with a comb filter similar to plucking point filter, but in this case corresponding to pickup position.
		
		Calibration of model parameters is an important task when simulating a particular instrument. Methods for calibrating a string instrument model are presented, e.g., in [8, 14, 24, 27, 137, 142, 211, 244, 286, 295, 320].
		
		-- Hiệu chuẩn các tham số mô hình là 1 nhiệm vụ quan trọng khi mô phỏng một nhạc cụ cụ thể. Các phương pháp hiệu chuẩn mô hình nhạc cụ dây được trình bày $\ldots$
		
		A typical procedure: apply time-frequency analysis to recorded sound of plucked or struck string, in order to estimate decay rate of each harmonic. Parametric models e.g. FZ-ARMA analysis [133, 138] may yield more complete information of modal components in string behavior. This information is used to design a low-order loop filter which approximates frequency-dependent losses in SDL loop structure [14,17,79,244,286]. A recent novel idea has been to design a sparse FIR loop filter, which is of high order but has few nonzero coefficients [163, 209, 293]. This approach offers a computationally efficient way to imitate large deviations in decay rates of harmonic components. Through implementing a slight difference in delays \& decay rates of 2 polarizations, beating or 2-stage decay of signal envelope can be approximated. for plucking point comb filter: required to estimate plucking point from a recorded tone [199, 276, 277, 286].
		
		{\sf Fig. 21: Detailed SDL loop structure for string instrument sound synthesis.} depicts a detailed structure used in practice to realize SDL loop. Fundamental frequency of string sound is inversely proportional to total delay of loop blocks. Accurate tuning requires application of a fractional delay, because an integral number of unit delays is not accurate enough when a fixed sampling rate is used. Fractional delays are typically approximated by 1st-order allpass filters or 1st- to 5th-order Lagrange interpolators as discussed in [154].
		
		When loop filter properties are estimated properly, excitation wavetable signal is obtained by inverse filtering (deconvolution) of recorded sound by SDL response. For practical synthesis, only initial transient part of inverse-filtered excitation is used, typically covering several 10s of milliseconds.
		
		After careful calibration of model, a highly realistic sounding synthesis can be obtained by parametric control \& modification of sound features. Synthesis is possible even in cases which are not achievable in practice in real acoustic instruments.		
		\item {\sf7.8. DWG modeling of various musical instruments.} Digital waveguide modeling has been applied to a variety of musical instruments other than acoustic guitar. In this subsect, present a brief overview of such models \& features that need special attention to each case. For an in-depth presentation on DWG modeling techniques applied to different instrument families, see [254].
		\begin{itemize}
			\item {\sf7.8.1. Other plucked string instruments.}
			\item {\sf7.8.2. Struck string instruments.}
			\item {\sf7.8.3. Bowed string instruments.}
			\item {\sf7.8.4. Wind instruments.}
			\item {\sf7.8.5. Percussion instruments.}
			\item {\sf7.8.6. Speech \& singing voice.}
			\item {\sf7.8.7. Inharmonic SDL type of DWG models.}
		\end{itemize}
	\end{itemize}
	\item {\sf8. Wave digital filters.} Purpose of this sect: provide a general overview of physical modeling using WDFs in context of musical instruments. Only essential basics of topic will be discussed in detail; the rest will be glossed over. For more information about project, reader is encouraged to refer to [254]. Also, another definitive work can be found in [94].
	\begin{itemize}
		\item {\sf8.1. What are wave digital filters?} WDFs were developed in late 1960s by {\sc Alfred Fettweis} [93] for digitizing lumped analog electrical circuits. Traveling-wave formulation of lumped electrical elements, where WDF approach is based, was introduced earlier by {\sc Belevitch} [21, 254].
		
		WDFs are certain types of digital filters with valid interpretations in physical world. I.e., can simulate behavior of a lumped physical system (hệ thống vật lý tập trung) using a digital filter whose coefficients depend on parameters of this physical system. Alternatively, WDFs can be seen as a particular type of finite difference schemes with excellent numerical properties [254]. As discussed in Sect. 4, task of finite difference schemes in general: provide discrete versions of PDEs for simulation \& analysis purposes.
		
		WDFs are useful for physical modeling in many respects. 1stly, they are modular: same building blocks can be used for modeling very different systems; all that needs to be changed: \fbox{topology of wave digital network}. 2ndly, preservation of energy \& hence also stability is usually addressed, since elementary blocks can be made passive, \& energy preservation between blocks are evaluated using Kirchhoff's laws. Finally, WDFs have good numerical properties, i.e., they do not experience artificial damping at high frequencies.
		
		Physical systems were originally considered to be lumped in basic wave digital formalism. I.e., system to be modeled, say a drum, will become a point-like black box, which has functionality of drum. However, its inner representation, as well as its spatial dimensions, is lost. Must bear in mind, however: question of whether a physical system can be considered lumped depends naturally not only on which of its aspects wish to model but also on frequency scale want to use in modeling (Sect. 3).
		\item {\sf8.2. Analog circuit theory.}
		\item {\sf8.3. Wave digital building blocks.}
		\item {\sf8.4. Interconnection \& adaptors.}
		\item {\sf8.5. Physical modeling using WDFs.}
		\item {\sf8.6. Current research.}
	\end{itemize}
	\item {\sf9. Source-filter models.}
	\begin{itemize}
		\item {\sf9.1. Subtractive synthesis in computer music.}
		\item {\sf9.2. Source-filter models in speech synthesis.}
		\item {\sf9.3. Instrument body modeling by digital filters.}
		\item {\sf9.4. Karplus--Strong algorithm.}
		\item {\sf9.5. Virtual analog synthesis.}
	\end{itemize}
	\item {\sf10. Hybrid models.}
	\begin{itemize}
		\item {\sf10.1. KW-hybrids.}
		\item {\sf10.2. KW-hybrids modeling examples.}
	\end{itemize}
	\item {\sf11. Modeling of nonlinear \& time-varying phenomena.}
	\begin{itemize}
		\item {\sf11.1. Modeling of nonlinearities in musical instruments.}
		\item {\sf11.2. Case study: nonlinear string model using generalized time-varying allpass filters.}
		\item {\sf11.3. Modeling of time-varying phenomena.}
	\end{itemize}
	\item {\sf12. Current trends \& further research.}
	\item {\sf13. Conclusions.}
\end{itemize}

%------------------------------------------------------------------------------%

\section{Wikipedia}

\subsection{Wikipedia{\tt/}computer music}
``{\it Computer music} is application of \href{https://en.wikipedia.org/wiki/Computing_technology}{computing technology} in \href{https://en.wikipedia.org/wiki/Musical_composition}{music composition}, to help human composers create new music or to have computers independently create music, e.g. with \href{https://en.wikipedia.org/wiki/Algorithmic_composition}{algorithmic composition} programs. it includes theory \& application of new \& existing computer software technologies \& basic aspects of music, e.g. \href{https://en.wikipedia.org/wiki/Sound_synthesis}{sound synthesis}, \href{https://en.wikipedia.org/wiki/Digital_signal_processing}{digital signal processing}, \href{https://en.wikipedia.org/wiki/Sound_design}{sound design}, sonic diffusion, \href{https://en.wikipedia.org/wiki/Acoustics}{acoustics}, \href{https://en.wikipedia.org/wiki/Electrical_engineering}{electrical engineering}, \& \href{https://en.wikipedia.org/wiki/Psychoacoustics}{psychoacoustics}. Field of computer music can trace its roots back to origins of \href{https://en.wikipedia.org/wiki/Electronic_music}{electric music}, \& 1st experiments \& innovations with electronic instruments at turn of 20th century.

\subsubsection{History}

\subsubsection{Advances}

\subsubsection{Research}

\subsubsection{Machine improvisation}

\subsubsection{Live coding}

'' -- \href{https://en.wikipedia.org/wiki/Computer_music}{Wikipedia{\tt/}computer music}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}transcription (music)}
``{\sf A {\sc J. S. Bach} keyboard piece transcribed for guitar.} In music, {\it transcription} is practice of \href{https://en.wikipedia.org/wiki/Musical_notation}{notating} a piece or a sound which was previously unnotated \&{\tt/}or unpopular as a written music, e.g., a \href{https://en.wikipedia.org/wiki/Jazz_improvisation}{jazz improvisation} or a \href{https://en.wikipedia.org/wiki/Video_game_soundtrack}{video game soundtrack}. When a musician is tasked with creating \href{https://en.wikipedia.org/wiki/Sheet_music}{sheet music} from a recording \& they write down notes that make up piece in \href{https://en.wikipedia.org/wiki/Music_notation}{music notation}, it is said: they created a {\it musical transcription} of that recording. Transcription may also mean rewriting a piece of music, either solo or \href{https://en.wikipedia.org/wiki/Musical_ensemble}{ensemble}, for another instrument or other instruments than which it was originally intended. \href{https://en.wikipedia.org/wiki/Beethoven_Symphonies_(Liszt)}{Beethoven Symphonies} transcribed for solo piano by \href{https://en.wikipedia.org/wiki/Franz_Liszt}{Franz Liszt} are an example. Transcription in this sense is sometimes called \href{https://en.wikipedia.org/wiki/Arrangement}{\it arrangement}, although strictly speaking transcriptions are faithful adaptations, whereas arrangements change significant aspects of original piece.

Further examples of music transcription include \href{https://en.wikipedia.org/wiki/Ethnomusicology}{ethnomusicological} notation of \href{https://en.wikipedia.org/wiki/Oral_tradition}{oral traditions} of folk music, e.g. Béla Bartók's \& Ralph Vaughan Williams' collections of national folk music of Hungary \& England resp. French composer Olivier Messiaen transcribed \href{https://en.wikipedia.org/wiki/Bird_song}{birdsong} in wild, \& incorporated it into many of his compositions, e.g. his \href{https://en.wikipedia.org/wiki/Catalogue_d%27oiseaux}{Catalogue d'oiseaux} for solo piano. Transcription of this nature involves scale degree recognition \& harmonic analysis, both of which transcriber will need \href{https://en.wikipedia.org/wiki/Relative_pitch}{relative} or \href{https://en.wikipedia.org/wiki/Perfect_pitch}{perfect pitch} to perform. 

In popular music \& rock, there are 2 forms of transcription. Individual performers copy a note-for-note guitar solo or other melodic line. As well, music publishers transcribe entire recordings of guitar solos \& bass lines \& sell sheet music in bound books. Music publishers also publish PVG (piano{\tt/}vocal{\tt/}guitar) transcriptions of popular music, where melody line is transcribed, \& then accompaniment on recording is arranged as a piano part. Guitar aspect of PVG label is achieved through guitar chords written above melody. Lyrics are also included below melody.

\subsubsection{Adaptation}
Some composers have rendered homage to other composers by creating ``identical'' versions of earlier composers' pieces while adding their own creativity through use of completely new sounds arising from difference in instrumentation. Most widely known example of this is {\sc Ravel}'s arrangement for orchestra of {\sc Mussorgsky}'s piano piece \href{https://en.wikipedia.org/wiki/Pictures_at_an_Exhibition}{\it Pictures at an Exhibition}. {\sc Webern} used his transcription for orchestra of 6-part \href{https://en.wikipedia.org/wiki/Ricercar}{ricercar} from {\sc Bach}'s \href{https://en.wikipedia.org/wiki/The_Musical_Offering}{\it The Musical Offering} to analyze structure of Bach piece, by using different instruments to play different subordinate \href{https://en.wikipedia.org/wiki/Motif_(music)}{motifs} of Bach's themes \& melodies.

In transcription of this form, new piece can simultaneously imitate original sounds while recomposing them with all technical skills of an expert composer in such a way that it seems: piece was originally written for new medium. But some transcriptions \& arrangements have been done for purely pragmatic or contextual reasons. E.g., in Mozart's time, overtures \& songs from this popular operas were transcribed for small \href{https://en.wikipedia.org/wiki/Wind_ensemble}{wind ensemble} simply because such ensembles were common ways of providing popular entertainment in public places. {\sc Mozart} himself did this in his own opera \href{https://en.wikipedia.org/wiki/The_Marriage_of_Figaro}{\it The Marriage of Figaro}. A more contemporary example is {\sc Stravinsky}'s transcription for 4 hands piano of \href{https://en.wikipedia.org/wiki/The_Rite_of_Spring}{\it The Rite of Spring}, to be used on ballet's rehearsals. Today musicians who play in cafes or restaurants will sometimes play transcriptions or arrangements of pieces written for a larger group of instruments.

Other examples of this type of transcription include {\sc Bach}'s arrangement of {\sc Vivaldi}'s 4-violin concerti for 4 keyboard instruments \& orchestra; {\sc Mozart}'s arrangement of some Bach \href{https://en.wikipedia.org/wiki/Fugue}{fugues} from \href{https://en.wikipedia.org/wiki/The_Well-Tempered_Clavier}{\it The Well-Tempered Clavier} for string \href{https://en.wikipedia.org/wiki/Trio_(music)}{trio}; {\sc Beethoven}'s arrangement of his \href{https://en.wikipedia.org/wiki/Gro%C3%9Fe_Fuge}{\it Gro$\beta$e Fuge}, originally written for \href{https://en.wikipedia.org/wiki/String_quartet}{string quartet}, for \href{https://en.wikipedia.org/wiki/Piano}{piano} duet, \& his arrangement of his \href{https://en.wikipedia.org/wiki/Violin_Concerto_(Beethoven)}{Violin Concerto} as a \href{https://en.wikipedia.org/wiki/Piano_concerto}{piano concerto}; Franz Liszt's piano arrangements of works of many composers, including \href{https://en.wikipedia.org/wiki/Beethoven_Symphonies_(Liszt)}{symphonies of Beethoven}; {\sc Tchaikovsky}'s arrangement of 4 Mozart piano pieces into an \href{https://en.wikipedia.org/wiki/Orchestral_suite}{orchestral suite} called ``\href{https://en.wikipedia.org/wiki/Orchestral_Suite_No._4_Mozartiana_(Tchaikovsky)}{Mozartiana}''; {\sc Mahler}'s re-orchestration of {\sc Schumann} symphonies; \& {\sc Schoenberg}'s arrangement for orchestra of {\sc Brahms}'s piano quintet \& {\sc Bach}'s ``St. Anne'' Prelude \& Fugue for organ.

Since piano became a popular instrument, a large literature has sprung up of transcriptions \& arrangements for piano of works for orchestra or chamber music ensemble. These are sometimes called ``\href{https://en.wikipedia.org/wiki/Reduction_(music)}{piano reductions}'', because multiplicity of orchestral parts -- in an orchestral piece there may be as many as 2 dozen separate instrumental parts being played simultaneously -- has to be reduced to what a single pianist (or occasionally 2 pianists, or 1 or 2 pianos, e.g. different arrangements for {\sc George Gershwin}'s \href{https://en.wikipedia.org/wiki/Rhapsody_in_Blue}{\it Rhapsody in Blue}) can manage to play.

Piano reductions are frequently made of orchestral accompaniments to choral works, for purposes of rehearsal or of performance with keyboard alone.

Many orchestral pieces have been transcribed for \href{https://en.wikipedia.org/wiki/Concert_band}{concert band}.

\subsubsection{Transcription aids}

\begin{itemize}
	\item {\bf Notation software.} Since advent of desktop publishing, musicians can acquire \href{https://en.wikipedia.org/wiki/Music_notation_software}{music notation software}, which can receive user's mental analysis of notes \& then store \& format those notes into standard music notation for personal printing or professional publishing of sheet music. Some notation software can accept a Standard \href{https://en.wikipedia.org/wiki/MIDI}{MIDI} File (SMF) or MIDI performance as input instead of manual note entry. These notation applications can export their scores in a variety of formats like \href{https://en.wikipedia.org/wiki/Encapsulated_PostScript}{EPS}, \href{https://en.wikipedia.org/wiki/Portable_Network_Graphics}{PNG}, \& \href{https://en.wikipedia.org/wiki/Scalable_Vector_Graphics}{SVG}. Often software contains a sound library that allows user's score to be played aloud by application for verification.
	\item {\bf Slow-down software.} Prior to invention of digital transcription aids, musicians would slow down a record or a tape recording to be able to hear melodic lines \& chords at a slower, more digestible pace. Problem with this approach was: it also changed pitches, so once a piece was transcribed, it would then have to be transposed into correct key. Software designed to slow down tempo of music without changing pitch of music can be very helpful for recognizing pitches, melodies, chords, rhythms, \& lyrics when transcribing music. However, unlike slow-down effect of a record player, pitch \& original octave of notes will stay same, \& not descend in pitch. This technology is simple enough that it is available in many free software applications.
	
	Software generally goes through a 2-step process to accomplish this. 1st, audio file is played back at a lower sample rate than that of original file. This has same effect as playing a tape or vinyl record at slower speed -- pitch is lowered meaning music can sound like it is in a different key. 2nd step: use \href{https://en.wikipedia.org/wiki/Digital_signal_processing}{Digital Signal Processing} (or DSP) to shift pitch back up to original pitch level or musical key.
	\item {\bf Pitch tracking software.} Main article: \href{https://en.wikipedia.org/wiki/Pitch_tracker}{Wikipedia{\tt/}pitch tracker}. As mentioned in the Automatic music transcription sect, some commercial software can roughly track pitch of dominant melodies in polyphonic musical recordings. Note scans are not exact, \& often need to be manually edited by user before saving to file in either a proprietary file format or in Standard MIDI File Format. Some pitch tracking software also allows scanned note lists to be animated during audio playback.
\end{itemize}

\subsubsection{Automatic music transcription (AMT)}
Term ``automatic music transcription'' was 1st used by audio researchers {\sc James A. Moorer,  Martin Piszczalski, \& Bernard Galler} in 1977. With their knowledge of digital audio engineering, these researchers believed: a computer could be programmed to analyze a \href{https://en.wikipedia.org/wiki/Digital_recording}{digital recording} of music s.t. pitches of melody lines \& chord patterns could be detected, along with rhythmic accents of percussion instruments. Task of AMT concerns 2 separate activities: making an analysis of a musical piece, \& printing out a score from that analysis.

This was not a simple goal, but one that would encourage academic research for at least another 3 decades. Because of close scientific relationship of speech to music, much academic \& commercial research that was directed toward more financially resourced \href{https://en.wikipedia.org/wiki/Speech_recognition}{speech recognitions} technology would be recycled into research about music recognition technology. While many musicians \& educators insist that manually doing transcriptions is a valuable exercise for developing musicians, motivation for AMT remains same as motivation for sheet music: musicians who do not have intuitive transcription skills will search for sheet music or a chord chart, so that they may quickly learn how to play a song. A collection of tools created by this ongoing research could be of great aid to musicians. Since much recorded music does not have available sheet music, an automatic transcription device could also offer transcriptions that are otherwise unavailable in sheet music. To date, no software application can yet completely fulfill {\sc James Moorer};s definition of AMT. However, pursuit of AMT has spawned creation of many software applications that can aid in manual transcription. Some can slow down music while maintaining original pitch \& octave, some can track pitch of melodies, some can track chord changes, \& others can track beat of music.

Automatic transcription most fundamentally involves identifying pitch \& duration of performed notes. This entails tracking pitch \& identifying note onsets. After capturing those physical measurements, this information is mapped into traditional music notation, i.e., sheet music.

\href{https://en.wikipedia.org/wiki/Digital_signal_processing}{Digital Signal Processing} is branch of engineering that provides software engineers with tools \& algorithms needed to analyze a digital recording in terms of pitch (note detection of melodic instruments), \& energy content of un-pitched sounds (detection of percussion instruments). Musical recordings are sampled at a given recording rate \& its frequency data is stored in any digital wave format in computer. Such format represents sound by \href{https://en.wikipedia.org/wiki/Sampling_(signal_processing)}{digital sampling}.
\begin{itemize}
	\item {\bf Pitch detection.} \href{https://en.wikipedia.org/wiki/Pitch_detection}{Pitch detection} is often detection of individual \href{https://en.wikipedia.org/wiki/Musical_note}{notes} that might make up a \href{https://en.wikipedia.org/wiki/Melody}{melody} in music, or notes in a \href{https://en.wikipedia.org/wiki/Chord_(music)}{chord}. When a single key is pressed upon a piano, what we hear is not just {\it1} \href{https://en.wikipedia.org/wiki/Frequency}{frequency} of sound vibration, but a {\it composite} of multiple sound vibrations occurring at different mathematically related frequencies. Elements of this composite of vibrations at differing frequencies are referred to as \href{https://en.wikipedia.org/wiki/Harmonic}{harmonics} or partials.
	
	E.g., if note $A_3$ (220 Hz) is played, individual \href{https://en.wikipedia.org/wiki/Frequency}{frequencies} of composite's \href{https://en.wikipedia.org/wiki/Harmonic_series_(music)}{harmonic series} will start at 220 Hz as \href{https://en.wikipedia.org/wiki/Fundamental_frequency}{fundamental frequency}: 440 Hz would be 2nd harmonic, 660 Hz would be 3rd harmonic, 880 Hz would be 4th harmonic, etc. These are integer multiples of fundamental frequency (e.g., $2\cdot220 = 440$, 2nd harmonic). While only about 8 harmonics are really needed to audibly recreate note, total number of harmonics in this mathematical series can be large, although higher harmonic's numerical weaker magnitude \& contribution of that harmonic. Contrary to intuition, a musical recording at its lowest physical level is not a collection of individual \href{https://en.wikipedia.org/wiki/Musical_note}{notes}, but is really a collection of individual harmonics. That is why very similar-sounding recordings can be created with differing collections of instruments \& their assigned notes. As long as total harmonics of recording are recreated to some degree, it does not really matter which instruments or which notes were used.
	\item {\bf Beat detection.}
	\item {\bf How ATM works.}
	\item {\bf Detailed computer steps behind AMT.}
\end{itemize}

'' -- \href{https://en.wikipedia.org/wiki/Transcription_(music)}{Wikipedia{\tt/}transcription (music)}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}