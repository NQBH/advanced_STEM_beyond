\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,fancyvrb,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{Bài toán}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
    \mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Reinforcement Learning -- Học Tăng Cường}
\author{Nguyễn Quản Bá Hồng\footnote{A scientist- {\it\&} creative artist wannabe, a mathematics {\it\&} computer science lecturer of Department of Artificial Intelligence {\it\&} Data Science (AIDS), School of Technology (SOT), UMT Trường Đại học Quản lý {\it\&} Công nghệ TP.HCM, Hồ Chí Minh City, Việt Nam.\\E-mail: {\sf nguyenquanbahong@gmail.com} {\it\&} {\sf hong.nguyenquanba@umt.edu.vn}. Website: \url{https://nqbh.github.io/}. GitHub: \url{https://github.com/NQBH}.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
    This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:

    {\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.

    Latest version:
    \begin{itemize}
        \item {\it Reinforcement Learning -- Học Tăng Cường}.

        PDF: {\sc url}: \url{.pdf}.

        \TeX: {\sc url}: \url{.tex}.
        \item {\it }.

        PDF: {\sc url}: \url{.pdf}.

        \TeX: {\sc url}: \url{.tex}.
    \end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basic Reinforcement Learning}

%------------------------------------------------------------------------------%

\section{Reinforcement Learning for Optimal Job Scheduling}

%------------------------------------------------------------------------------%

\subsection{{\sc Xinquan Wu, Xuefeng Yan, Mingqiang Wei, Donghai Guan}. An Efficient Deep Reinforcement Learning Environment for Flexible Job-Shop Scheduling. Sep 10, 2025}

\begin{itemize}
    \item {\sf Abstract.} Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial optimization problem that has a wide-range of applications in real world. In order to generate fast \& accurate scheduling solutions for FJSP, various deep reinforcement learning (DRL) scheduling methods have been developed. However, these methods are mainly focused on design of DRL scheduling Agent, overlooking modeling of DRL environment. This paper presents a simple chronological DRL environment for FJSP based on discrete event simulation \& an end-to-end DRL scheduling model is proposed based on proximal policy optimization (PPO). Furthermore, a short novel state representation of FJSP is proposed based on 2 state variables in scheduling environment \& a novel comprehensive reward function is designed based on scheduling area of machines. Experimental results on public benchmark instances show: performance of simple priority dispatching rules (PDR) is improved in our scheduling environment \& our DRL scheduling model obtains competing performance compared with OR-Tools, meta-heuristic, DRL \& PDR scheduling methods.

    -- Bài toán Lập lịch Xưởng Linh hoạt (FJSP) là 1 bài toán tối ưu hóa tổ hợp cổ điển có phạm vi ứng dụng rộng rãi trong thực tế. Để tạo ra các giải pháp lập lịch nhanh \& chính xác cho FJSP, nhiều phương pháp lập lịch học tăng cường sâu (DRL) đã được phát triển. Tuy nhiên, các phương pháp này chủ yếu tập trung vào thiết kế Tác nhân lập lịch DRL, bỏ qua việc mô hình hóa môi trường DRL. Bài báo này trình bày 1 môi trường DRL theo trình tự thời gian đơn giản cho FJSP dựa trên mô phỏng sự kiện rời rạc \& 1 mô hình lập lịch DRL đầu cuối được đề xuất dựa trên tối ưu hóa chính sách gần đúng (PPO). Hơn nữa, 1 biểu diễn trạng thái mới ngắn gọn của FJSP được đề xuất dựa trên 2 biến trạng thái trong môi trường lập lịch \& 1 hàm thưởng toàn diện mới được thiết kế dựa trên vùng lập lịch của máy. Kết quả thử nghiệm trên các trường hợp chuẩn công khai cho thấy: hiệu suất của các quy tắc phân bổ ưu tiên đơn giản (PDR) được cải thiện trong môi trường lập lịch của chúng tôi \& mô hình lập lịch DRL của chúng tôi đạt được hiệu suất cạnh tranh so với các phương pháp lập lịch OR-Tools, meta-heuristic, DRL \& PDR.
    \item {\sf1. Introduction.} Job shop scheduling problem (JSSP) is a classical NP-hard combinatorial optimization problem. It has been studied for decades \& has been applied in a wide-range of areas including semiconductor, machine manufacturing, metallurgy, automobile manufacturing, supply chain \& other fields [25]. Flexible Job-shop Scheduling Problem (FJSP) is a generalization of JSSP, which allows each operation to be processed on multiple candidate machines [6]. This makes it a more challenging problem with more complex topology \& larger solution space.

    -- Bài toán lập lịch xưởng gia công (JSSP) là 1 bài toán tối ưu tổ hợp NP-khó kinh điển. Nó đã được nghiên cứu trong nhiều thập kỷ \& đã được ứng dụng trong nhiều lĩnh vực bao gồm bán dẫn, chế tạo máy, luyện kim, sản xuất ô tô, chuỗi cung ứng \& các lĩnh vực khác [25]. Bài toán lập lịch xưởng gia công linh hoạt (FJSP) là 1 dạng tổng quát của JSSP, cho phép mỗi thao tác được xử lý trên nhiều máy ứng viên [6]. Điều này làm cho nó trở thành 1 bài toán khó hơn với cấu trúc phức tạp hơn \& không gian nghiệm lớn hơn.

    Extensive researches have been widely explored to solve FJSP in recent years, including exact, heuristic, meta-heuristic, hyper-heuristic \& RL methods. Exact approaches e.g. mixed integer linear programming (MILP) [20] may suffer from curse of dimension. So it is intractable to find exact scheduling solutions in a given time limit. Heuristic dispatching rules are widely used in real world manufacturing due to their simple \& fast nature. Simple priority dispatching rules (PDR)[21], e.g. most work remaining (MWKR) possess advantages of high flexibility \& easy implementation, but performance of PDR methods is not stable for different optimization objectives. Hyper-heuristic methods [29] e.g. genetic programming-based hyper-heuristic (GPHH), provide a way of automatically designing dispatching rules [28]. However, GP-evolved rules often has a large number of features in terminal set, making it difficult to identify promising search areas \& determine optimal features.

    -- Các nghiên cứu sâu rộng đã được khai thác rộng rãi để giải quyết FJSP trong những năm gần đây, bao gồm các phương pháp chính xác, heuristic, meta-heuristic, hyper-heuristic \& RL. Các phương pháp chính xác, ví dụ như lập trình tuyến tính số nguyên hỗn hợp (MILP) [20] có thể bị lời nguyền về chiều. Do đó, việc tìm ra các giải pháp lập lịch chính xác trong 1 giới hạn thời gian nhất định là khó khăn. Các quy tắc điều phối heuristic được sử dụng rộng rãi trong sản xuất thực tế do bản chất đơn giản \& nhanh chóng của chúng. Các quy tắc điều phối ưu tiên đơn giản (PDR) [21], ví dụ như hầu hết công việc còn lại (MWKR) có ưu điểm là tính linh hoạt cao \& dễ triển khai, nhưng hiệu suất của các phương pháp PDR không ổn định đối với các mục tiêu tối ưu khác nhau. Các phương pháp hyper-heuristic [29], ví dụ như hyper-heuristic dựa trên lập trình di truyền (GPHH), cung cấp 1 cách để tự động thiết kế các quy tắc điều phối [28]. Tuy nhiên, các quy tắc phát triển GP thường có 1 số lượng lớn các tính năng trong tập đầu cuối, khiến việc xác định các khu vực tìm kiếm đầy hứa hẹn \& xác định các tính năng tối ưu trở nên khó khăn.

    Different from PDR, meta-heuristic methods e.g. Tabu Search [12], Genetic Algorithm [19], Differential Evolution [7] \& Particle Swarm Optimization [1], can produce higher solution quality than PDR due to introduction of iterative, randomized search strategies. Nevertheless, these methods also have shortcomings e.g. slow convergence speed, difficulty in obtaining global optimal solutions for large-scale problems \& are difficult to apply to dynamic scenarios that need real-time decisions.

    -- Khác với PDR, các phương pháp siêu heuristic như Tìm kiếm Tabu [12], Thuật toán Di truyền [19], Tiến hóa Vi phân [7] \& Tối ưu hóa Bầy đàn Hạt [1] có thể tạo ra chất lượng giải pháp cao hơn PDR nhờ áp dụng các chiến lược tìm kiếm lặp lại, ngẫu nhiên. Tuy nhiên, các phương pháp này cũng có những hạn chế, ví dụ như tốc độ hội tụ chậm, khó đạt được giải pháp tối ưu toàn cục cho các bài toán quy mô lớn \& khó áp dụng cho các tình huống động cần quyết định theo thời gian thực.

    RL is 1 of most important branches of ML \& attracts interests of researchers from numerous fields especially for scheduling. Early RL scheduling methods represent scheduling policies using arrays or tables [2], which are only suitable for small-scale scheduling problems because of curse of dimension. However, scheduling tasks in real world usually have a higher dimensional state space \& large action space, which limits applications of RL. Since deep neural networks have a strong fitting capability to process high-dimensional data, deep reinforcement learning (DRL) has been used to solve scheduling tasks with large or continuous state space \& shows great potential to solve various scheduling problems. In DRL scheduling methods, scheduling policy is usually designed based on convolutional neural network (CNN) in [9], multi-layer perception (MLP) in [9, 10], RNN in [14], GNN in [15, 22, 26], attention networks in [23] \& other deep neural networks in [16]. Scheduling agent is often trained by RL algorithms e.g. deep Q-learning (DQN), proximal policy optimization (PPO), Deep Deterministic Policy Gradient (DDPG). However, above DRL scheduling methods obtained low accurate solutions \& some are even inferior to simple PDR. Even though recent GMAS model [13] whose policy is designed based on Graph Convolutional Network (GCN), obtained SOTA results on standard benchmark instances, it is a decentralized Multi-agent rRL (MARL) model. Besides, current DRL scheduling methods focus mainly on design of scheduling agents, overlooking modeling of scheduling environment which is of vital important for improving performance of DRL scheduling methods.

    -- RL là 1 trong những nhánh quan trọng nhất của ML \& thu hút sự quan tâm của các nhà nghiên cứu từ nhiều lĩnh vực, đặc biệt là đối với việc lập lịch. Các phương pháp lập lịch RL ban đầu biểu diễn các chính sách lập lịch sử dụng mảng hoặc bảng [2], chỉ phù hợp với các vấn đề lập lịch quy mô nhỏ do giới hạn chiều. Tuy nhiên, các tác vụ lập lịch trong thế giới thực thường có không gian trạng thái nhiều chiều \& không gian hành động lớn, điều này hạn chế các ứng dụng của RL. Vì mạng nơ-ron sâu có khả năng khớp mạnh để xử lý dữ liệu nhiều chiều, nên học tăng cường sâu (DRL) đã được sử dụng để giải quyết các tác vụ lập lịch với không gian trạng thái lớn hoặc liên tục \& cho thấy tiềm năng to lớn trong việc giải quyết nhiều vấn đề lập lịch khác nhau. Trong các phương pháp lập lịch DRL, chính sách lập lịch thường được thiết kế dựa trên mạng nơ-ron tích chập (CNN) trong [9], nhận thức đa lớp (MLP) trong [9, 10], RNN trong [14], GNN trong [15, 22, 26], mạng chú ý trong [23] \& các mạng nơ-ron sâu khác trong [16]. Tác nhân lập lịch thường được đào tạo bằng các thuật toán RL, ví dụ: Học sâu Q (DQN), tối ưu hóa chính sách gần đúng (PPO), Gradient Chính sách Xác định Sâu (DDPG). Tuy nhiên, các phương pháp lập lịch DRL trên cho kết quả độ chính xác thấp \& 1 số thậm chí còn kém hơn PDR đơn giản. Mặc dù mô hình GMAS gần đây [13], với chính sách được thiết kế dựa trên Mạng Tích chập Đồ thị (GCN), đã đạt được kết quả SOTA trên các trường hợp chuẩn mực, nhưng nó là 1 mô hình rRL Đa tác tử phi tập trung (MARL). Bên cạnh đó, các phương pháp lập lịch DRL hiện tại chủ yếu tập trung vào thiết kế các tác nhân lập lịch, bỏ qua việc mô hình hóa môi trường lập lịch, vốn rất quan trọng để cải thiện hiệu suất của các phương pháp lập lịch DRL.

    There are mainly 4 modeling methods for FJSP: mathematical [20], Petri Nets (PN)-based [17], Disjunctive Graph (DG)-based [3] \& simulation-based modeling [24]. Mathematical modeling methods usually formulates FJSP as a MILP \& formulated problem is then be solved by optimization methods e.g. Genetic Algorithm. PN is a versatile modeling tool that can be used to model scheduling problems as they can model parallel activities, resource sharing \& synchronization. Both modeling methods are not suitable for DRL scheduling methods. DG is another alternative to model FJSP which integrates machine status \& operation dependencies, \& provides critical structural information for scheduling decisions. However, DG fails to incorporate dynamically changing state information of FJSP, requiring extra handcrafted node features. Besides, allocation order provided by DG model is not strictly chronological. Simulation-based modeling method: develop algorithms to simulate laws of activities in real world. In current simulation environment of DRL scheduling methods, scheduling process is promoted by events in candidate queue, ignoring occurrence temporal order of these events.

    -- Có chủ yếu 4 phương pháp mô hình hóa cho FJSP: toán học [20], dựa trên Petri Nets (PN) [17], dựa trên Disjunctive Graph (DG) [3] \& mô hình hóa dựa trên mô phỏng [24]. Các phương pháp mô hình hóa toán học thường xây dựng FJSP như 1 bài toán MILP \& sau đó được giải quyết bằng các phương pháp tối ưu hóa, ví dụ như thuật toán di truyền. PN là 1 công cụ mô hình hóa đa năng có thể được sử dụng để mô hình hóa các vấn đề lập lịch vì chúng có thể mô hình hóa các hoạt động song song, chia sẻ tài nguyên \& đồng bộ hóa. Cả hai phương pháp mô hình hóa đều không phù hợp với các phương pháp lập lịch DRL. DG là 1 giải pháp thay thế khác cho mô hình FJSP tích hợp trạng thái máy \& phụ thuộc hoạt động, \& cung cấp thông tin cấu trúc quan trọng cho các quyết định lập lịch. Tuy nhiên, DG không kết hợp thông tin trạng thái thay đổi động của FJSP, yêu cầu các tính năng nút thủ công bổ sung. Bên cạnh đó, thứ tự phân bổ do mô hình DG cung cấp không hoàn toàn theo thứ tự thời gian. Phương pháp mô hình hóa dựa trên mô phỏng: phát triển các thuật toán để mô phỏng các quy luật hoạt động trong thế giới thực. Trong môi trường mô phỏng hiện tại của các phương pháp lập lịch DRL, quá trình lập lịch được thúc đẩy bởi các sự kiện trong hàng đợi ứng viên, bỏ qua thứ tự thời gian xảy ra của các sự kiện này.

    In this paper, a chronological DRL scheduling environment for FJSP is presented based on discrete event simulation algorithm where at each decision step, accurate state changes of scheduling process are recorded by state variables \& a novel comprehensive reward function is designed based on scheduling area. Furthermore, an end to end DRL model for FJSP is proposed based on actor-critic PPO. Specially, a very short state representation is expressed by 2 state variables which are derived directly from necessary variables in environment simulation algorithm \& action space is constructed using 6 PDRs from literature. Besides, in order to reduce computation time for RL agent, a simple scheduling policy is designed based on MLP with only 1 hidden layer \& Softmax function.

    -- Bài báo này trình bày 1 môi trường lập lịch DRL theo thời gian cho FJSP dựa trên thuật toán mô phỏng sự kiện rời rạc, trong đó tại mỗi bước quyết định, các thay đổi trạng thái chính xác của quy trình lập lịch được ghi lại bởi các biến trạng thái \& 1 hàm thưởng toàn diện mới được thiết kế dựa trên vùng lập lịch. Hơn nữa, 1 mô hình DRL đầu cuối cho FJSP được đề xuất dựa trên PPO tác nhân-phê bình. Cụ thể, 1 biểu diễn trạng thái rất ngắn được thể hiện bằng 2 biến trạng thái được suy ra trực tiếp từ các biến cần thiết trong thuật toán mô phỏng môi trường \& không gian hành động được xây dựng bằng cách sử dụng 6 PDR từ tài liệu. Bên cạnh đó, để giảm thời gian tính toán cho tác nhân RL, 1 chính sách lập lịch đơn giản được thiết kế dựa trên MLP với chỉ 1 lớp ẩn \& hàm Softmax.

    Contributions of this work are listed as follows.
    \begin{enumerate}
        \item A basic simulation algorithm based on chronological discrete event is designed for FJSP, providing a general environment for DRL scheduling methods.
        \item A simple DRL model for FJSP is proposed based on PPO where a very short state representation for FJSP is presented, avoiding handcrafted state features \& massive experiments for feature selection \& a novel comprehensive reward function is proposed based on scheduling area.
        \item Experimental results show: performance of single PDR is improved in our environment, even better than some DRL scheduling methods \& proposed DRL scheduling model obtains competing performance compared with OR-Tools, meta-heuristic, DRL \& PDR scheduling methods.
    \end{enumerate}
    -- Những đóng góp của công trình này được liệt kê như sau.
    \begin{enumerate}
        \item 1 thuật toán mô phỏng cơ bản dựa trên sự kiện rời rạc theo trình tự thời gian được thiết kế cho FJSP, cung cấp 1 môi trường chung cho các phương pháp lập lịch DRL.
        \item 1 mô hình DRL đơn giản cho FJSP được đề xuất dựa trên PPO, trong đó 1 biểu diễn trạng thái rất ngắn cho FJSP được trình bày, tránh các đặc trưng trạng thái thủ công \& các thử nghiệm hàng loạt để lựa chọn đặc trưng \& 1 hàm thưởng toàn diện mới được đề xuất dựa trên khu vực lập lịch.
        \item Kết quả thử nghiệm cho thấy: hiệu suất của PDR đơn lẻ được cải thiện trong môi trường của chúng tôi, thậm chí còn tốt hơn 1 số phương pháp lập lịch DRL \& mô hình lập lịch DRL được đề xuất đạt được hiệu suất cạnh tranh so với các phương pháp lập lịch OR-Tools, meta-heuristic, DRL \& PDR.
    \end{enumerate}
    Rest of paper is organized as follows. A chronological discrete event simulation-based scheduling environment for FJSP is introduced in Sect. II. In Sect. III, proposed DRL scheduling model for FJSP are constructed. Sect. IV demonstrates detailed experiments on standard benchmarks with different sizes \& results are compared \& discussed, while conclusions \& future work reside in Sect. V.

    -- Phần còn lại của bài báo được tổ chức như sau. 1 môi trường lập lịch dựa trên mô phỏng sự kiện rời rạc theo thời gian cho FJSP được giới thiệu trong Phần II. Trong Phần III, mô hình lập lịch DRL đề xuất cho FJSP đã được xây dựng. Phần IV trình bày các thí nghiệm chi tiết trên các chuẩn mực chuẩn với các kích thước khác nhau \& kết quả được so sánh \& thảo luận, trong khi kết luận \& các nghiên cứu trong tương lai nằm trong Phần V.
    \item {\sf2. Simulation environment for FJSP.} In this sect, 1st introduced basics of FJSPs. Then defined storage structure of FJSP \& rules of state changing when scheduling. Finally, detailed simulation algorithm for FJSP is presented.

    -- {\sf Môi trường mô phỏng cho FJSP.} Trong phần này, trước tiên chúng tôi giới thiệu những kiến thức cơ bản về FJSP. Sau đó, chúng tôi định nghĩa cấu trúc lưu trữ của FJSP \& các quy tắc thay đổi trạng thái khi lập lịch. Cuối cùng, thuật toán mô phỏng chi tiết cho FJSP được trình bày.
    \begin{itemize}
        \item {\sf2.1. Flexible Job-shop Scheduling Problems.} FJSP differs from JSSP in that it allows an operation to be processed by any machine from a given set \& different machines may have different processing times. Problem: assign each operation to eligible machine s.t. maximal completion time (makespan) of all jobs is minimized. It can be presented as $FJ||Cmax$ by 3-field notations [6].

        -- FJSP khác với JSSP ở chỗ nó cho phép 1 thao tác được xử lý bởi bất kỳ máy nào trong 1 tập hợp nhất định \& các máy khác nhau có thể có thời gian xử lý khác nhau. Vấn đề: gán mỗi thao tác cho máy đủ điều kiện, thời gian hoàn thành tối đa (makespan) của tất cả các công việc được giảm thiểu. Nó có thể được biểu diễn dưới dạng $FJ||Cmax$ theo ký hiệu 3 trường [6].

        In public benchmark of FJSP, environment information is passed through an instance file. As is depicted in {\sf Fig. 1: An example of flexible job-shop scheduling problem (MK1 [4])}, 1st line in instance file consists of 3 integers representing number of jobs, number of machines \& average number of machines per operation (optional), resp. Each of following lines describes information of a job where 1st integer is number of operations \& followed integers depicts operation information. Operation information includes 2 integers: 1st represents index of a machine \& 2nd is processing time of operation on this machine. Processing operation order in a job is advanced from left to right.

        -- Trong chuẩn mực công khai của FJSP, thông tin môi trường được truyền qua 1 tệp thể hiện. Như được mô tả trong {\sf Hình 1: Ví dụ về bài toán lập lịch xưởng công việc linh hoạt (MK1 [4])}, dòng đầu tiên trong tệp thể hiện bao gồm 3 số nguyên biểu diễn số lượng công việc, số lượng máy \& số lượng máy trung bình trên mỗi thao tác (tùy chọn), tương ứng. Mỗi dòng sau mô tả thông tin của 1 công việc, trong đó số nguyên đầu tiên là số lượng thao tác \& các số nguyên tiếp theo mô tả thông tin thao tác. Thông tin thao tác bao gồm 2 số nguyên: số 1 biểu diễn chỉ số của 1 máy \& số 2 là thời gian xử lý thao tác trên máy này. Thứ tự thao tác trong 1 công việc được sắp xếp theo thứ tự từ trái sang phải.
        \item {\sf2.2. Data structure for FJSP.} In this paper, based on benchmark instance file, propose a new \& efficient storage structure for FJSP instances. Scheduling information is represented by a 2D table where job information is described in rows \& column records processing stage information. Each element in this table includes 2 sets: machine set \& remaining processing time set. {\sf Fig. 2: Storage structure of FJSP instance.} demonstrates an example of a FJSP instance where storage structure is marked with a red box. It is efficient to retrieve any operation information using job index \& processing stage index.

        -- {\sf Cấu trúc dữ liệu cho FJSP.} Trong bài báo này, dựa trên tệp phiên bản chuẩn, chúng tôi đề xuất 1 cấu trúc lưu trữ mới \& hiệu quả cho các phiên bản FJSP. Thông tin lập lịch được biểu diễn bằng 1 bảng 2D, trong đó thông tin công việc được mô tả theo hàng \& cột ghi lại thông tin giai đoạn xử lý. Mỗi phần tử trong bảng này bao gồm 2 tập hợp: tập máy \& tập thời gian xử lý còn lại. {\sf Hình 2: Cấu trúc lưu trữ của phiên bản FJSP.} minh họa 1 ví dụ về phiên bản FJSP, trong đó cấu trúc lưu trữ được đánh dấu bằng hộp đỏ. Việc truy xuất bất kỳ thông tin thao tác nào bằng chỉ mục công việc \& chỉ mục giai đoạn xử lý đều hiệu quả.
        \item {\sf2.3. Rules of state updating.} In order to accurately record state changes of each scheduling step, 4 rules of state updating are defined based on type of operations. Job operations are divided into 4 categories: operations on processing, waiting operations without predecessors, completed operations \& operations whose predecessors have not been completed. 4 rules are listed as follows \& {\sf Fig. 3: Use of state update rules on a FJSP instance at time 4} provides an example to demonstrate use of these rules.

        -- {\sf Quy tắc cập nhật trạng thái.} Để ghi lại chính xác các thay đổi trạng thái của mỗi bước lập lịch, 4 quy tắc cập nhật trạng thái được xác định dựa trên loại thao tác. Các thao tác công việc được chia thành 4 loại: thao tác đang xử lý, thao tác chờ không có thao tác tiền nhiệm, thao tác đã hoàn thành \& thao tác có thao tác tiền nhiệm chưa hoàn thành. 4 quy tắc được liệt kê như sau \& {\sf Hình 3: Sử dụng quy tắc cập nhật trạng thái trên 1 thể hiện FJSP tại thời điểm 4} cung cấp 1 ví dụ để minh họa việc sử dụng các quy tắc này.
        \begin{itemize}
            \item Rule 1: For an operation on processing, machine set has only 1 element which is negative index of selected machine while remaining time is recorded in remaining processing time set \& its value decreases as time advances until completion of this operation.

            -- Quy tắc 1: Đối với 1 thao tác xử lý, tập hợp máy chỉ có 1 phần tử là chỉ số âm của máy được chọn trong khi thời gian còn lại được ghi vào tập hợp thời gian xử lý còn lại \& giá trị của nó giảm dần theo thời gian cho đến khi hoàn thành thao tác này.
            \item Rule 2: For a waiting operation without predecessors whose needed machine is occupied, elements in machine set are all negative index of machines \& elements in remaining processing time set stay unchanged. When needed machine is released, value of machine index is restored to be positive.

            -- Quy tắc 2: Đối với 1 thao tác chờ không có tiền nhiệm mà máy cần thiết đang bị chiếm dụng, các phần tử trong tập máy đều có chỉ số máy âm \& các phần tử trong tập thời gian xử lý còn lại không đổi. Khi máy cần thiết được giải phóng, giá trị chỉ số máy sẽ được khôi phục thành dương.
            \item Rule 3: For a completed operation, values in machine set \& remaining processing time set equal to negative value of machine index \& 0, resp.

            -- Quy tắc 3: Đối với 1 hoạt động đã hoàn thành, các giá trị trong bộ máy \& thời gian xử lý còn lại được đặt bằng giá trị âm của chỉ số máy \& 0, tương ứng.
            \item Rule 4: For operations whose predecesors have not been proposed, values in machine set \& remaining processing time set remain unchanged.

            -- Quy tắc 4: Đối với các hoạt động mà hoạt động trước đó chưa được đề xuất, các giá trị trong tập hợp máy \& thời gian xử lý còn lại vẫn không thay đổi.
        \end{itemize}
        This representation for FJSP instance can be recorded in a instance file \& reloaded to new environment, i.e., it can be applied to nonzero or re-entrant scenarios.

        -- Biểu diễn này cho phiên bản FJSP có thể được ghi lại trong tệp phiên bản \& tải lại vào môi trường mới, tức là có thể áp dụng cho các trường hợp khác không hoặc có thể nhập lại.
        \item {\sf2.4. Simulation algorithm for FJSP.} In this paper, proposed scheduling environment model is a simulation model based on chronological discrete events. There is a timer used to record current time \& triggering time of events are recorded in a state variable. Once current time reaches triggering time of any events, this event will occur \& corresponding event response program will be executed. Detailed process is illustrated in {\sf Algorithm 1: Simulation algorithm for FJSP}.

        -- {\sf Thuật toán mô phỏng cho FJSP.} Trong bài báo này, mô hình môi trường lập lịch được đề xuất là 1 mô hình mô phỏng dựa trên các sự kiện rời rạc theo trình tự thời gian. Có 1 bộ đếm thời gian được sử dụng để ghi lại thời gian hiện tại \& thời gian kích hoạt của các sự kiện được ghi lại trong 1 biến trạng thái. Khi thời gian hiện tại đạt đến thời gian kích hoạt của bất kỳ sự kiện nào, sự kiện này sẽ xảy ra \& chương trình phản hồi sự kiện tương ứng sẽ được thực thi. Quy trình chi tiết được minh họa trong {\sf Thuật toán 1: Thuật toán mô phỏng cho FJSP}.

        Proposed algorithm is mainly composed of 4 parts: selection of jobs \& machines, state update of jobs \& machines, time advance (3rd while statement) \& machine release (4th while statement). In 1st part, decision action is divided into PDRs for job \& machine selection \& then specific job number \& machine number are derived from PDRs. After allocating jobs \& machines, their states are updated using state variables \& table dictionaries. E.g., completion time of this job operation (or release time of machine) is recorded in \verb|next_time_on_machine| \& job-machine allocation information is recorded in \verb|job_on_machine|. Once a job is allocated on a machine, state of this job is \verb|assignable_job| changes to 0 \& states of jobs which need that machine, are updated. If candidate machines of a job are all occupied by other jobs this job will become not assignable.

        -- Thuật toán đề xuất chủ yếu bao gồm 4 phần: lựa chọn công việc \& máy, cập nhật trạng thái của công việc \& máy, tiến độ thời gian (câu lệnh while thứ 3) \& giải phóng máy (câu lệnh while thứ 4). Trong phần 1, hành động quyết định được chia thành các PDR để lựa chọn công việc \& máy \& sau đó số công việc cụ thể \& số máy được lấy từ PDR. Sau khi phân bổ công việc \& máy, trạng thái của chúng được cập nhật bằng các biến trạng thái \& từ điển bảng. Ví dụ: thời gian hoàn thành của hoạt động công việc này (hoặc thời gian giải phóng máy) được ghi lại trong \verb|next_time_on_machine| \& thông tin phân bổ công việc-máy được ghi lại trong \verb|job_on_machine|. Sau khi 1 công việc được phân bổ trên 1 máy, trạng thái của công việc này là \verb|assignable_job| thay đổi thành 0 \& trạng thái của các công việc cần máy đó được cập nhật. Nếu tất cả các máy ứng viên của 1 công việc đều bị các công việc khác chiếm giữ thì công việc này sẽ không thể gán được.

        When there are no assignable jobs, time advance stage is coming. In this part, current time is 1st updated based on values in \verb|next_time_on_machine| (if current time is smaller than any value in \verb|next_time_on_machine|, take smallest value, otherwise take next smallest of them). Length of time step that needs to advance is then calculated based on \verb|next_time_on_machine| \& current time. Finally, next time of machines whose next time is slower than current time is advanced to current time. Machine release part releases machines whose release time reaches current time \& updates status of jobs \& machines. When current job operation is completed, occupied machine becomes idle \& current operation of this job move to next operation. States of jobs \& machines are updated in \verb|assignable_job, job_on_machine|. If all operation of current job is completed, this job become not assignable \& if machine needed for next operation is occupied, this job is still not assignable.

        -- Khi không có công việc nào có thể gán được, giai đoạn tiến độ thời gian sẽ diễn ra. Trong phần này, thời gian hiện tại được cập nhật lần đầu tiên dựa trên các giá trị trong \verb|next_time_on_machine| (nếu thời gian hiện tại nhỏ hơn bất kỳ giá trị nào trong \verb|next_time_on_machine|, hãy lấy giá trị nhỏ nhất, nếu không thì lấy giá trị nhỏ tiếp theo trong số chúng). Sau đó, độ dài bước thời gian cần tiến lên được tính toán dựa trên \verb|next_time_on_machine| \& thời gian hiện tại. Cuối cùng, thời gian tiếp theo của các máy có thời gian tiếp theo chậm hơn thời gian hiện tại sẽ được tiến lên thời gian hiện tại. Bộ phận nhả máy sẽ nhả các máy có thời gian nhả đạt đến thời gian hiện tại \& cập nhật trạng thái của công việc \& máy. Khi thao tác công việc hiện tại hoàn thành, máy đang được chiếm dụng sẽ trở nên nhàn rỗi \& thao tác hiện tại của công việc này chuyển sang thao tác tiếp theo. Trạng thái của công việc \& máy được cập nhật trong \verb|assignable_job, job_on_machine|. Nếu tất cả thao tác của công việc hiện tại đã hoàn thành, công việc này sẽ không thể gán được \& nếu máy cần cho thao tác tiếp theo bị chiếm dụng, công việc này vẫn không thể gán được.
    \end{itemize}
    \item {\sf3. A DRL scheduling framework for FJSP.} In this sect, introduce overall DRL framework for FJSP, illustrated in {\sf Fig. 4: DRl framework for flexible job-shop scheduling problems}. It is composed of:
    \begin{enumerate}
        \item state representation based on 2 state variables
        \item PDR action space
        \item reward function based on scheduling area
        \item scheduling policy based on deep neural networks \& a Softmax function
        \item PPO agent with an actor-critic learning architecture.
    \end{enumerate}
    -- {\sf Khung lập lịch DRL cho FJSP.} Trong phần này, giới thiệu khung DRL tổng thể cho FJSP, được minh họa trong {\sf Hình 4: Khung DRL cho các bài toán lập lịch xưởng linh hoạt}. Khung này bao gồm:
    \begin{enumerate}
        \item biểu diễn trạng thái của mục dựa trên 2 biến trạng thái
        \item Không gian hành động PDR
        \item Hàm thưởng dựa trên vùng lập lịch
        \item Chính sách lập lịch dựa trên mạng nơ-ron sâu \& 1 hàm Softmax
        \item Tác tử PPO với kiến trúc học tập tác nhân-phê bình.
    \end{enumerate}
    \begin{itemize}
        \item {\sf3.1. State representation based on state variables.} State representation depicts features of scheduling environment \& determines size of state space, which is very crucial in RL scheduling methods. Various state representations for FJSP are presented in literature \& are mainly focused on disjunctive graph [15, 22], feature matrix [24] \& handcrafted state variables [8, 10, 14]. However, whether it is node features of disjunctive graph or matrix, or variable features, state features of job shop are mainly manually designed, which requires a large amount of processional domain knowledge, computing resources \& time.

        -- {\sf Biểu diễn trạng thái dựa trên biến trạng thái.} Biểu diễn trạng thái mô tả các đặc điểm của môi trường lập lịch \& xác định kích thước của không gian trạng thái, điều này rất quan trọng trong các phương pháp lập lịch RL. Nhiều biểu diễn trạng thái cho FJSP được trình bày trong tài liệu \& chủ yếu tập trung vào đồ thị rời rạc [15, 22], ma trận đặc trưng [24] \& các biến trạng thái thủ công [8, 10, 14]. Tuy nhiên, cho dù là đặc trưng nút của đồ thị rời rạc hay ma trận, hay đặc trưng biến, đặc trưng trạng thái của job shop chủ yếu được thiết kế thủ công, đòi hỏi lượng lớn kiến thức về miền xử lý, tài nguyên tính toán \& thời gian.

        In this paper, a novel short state representation is proposed to reduce feature redundancy \& to avoid handcrafted feature design \& feature selection, which requires less computation time of environment state variables \& scheduling policy networks. 2 state variables: \verb|assignable_job, completed_op_of_job| are selected from Algorithm 1 as state features of job shop scheduling environment. Variable \verb|assignable_job| is a Boolean vector to represent whether a job can be allocated or not. \verb|completed_op_of_job| represents number of completed operation of a job \& this value is then scaled by maximum number of operations in jobs to be in range $[0,1]$. Length of both variables equals number of jobs. Finally, in order to represent environment state, scaled variables are simply concatenated to a vector whose length equals 2 times of number of jobs.

        -- Trong bài báo này, 1 biểu diễn trạng thái ngắn mới được đề xuất để giảm sự dư thừa tính năng \& để tránh thiết kế tính năng thủ công \& lựa chọn tính năng, đòi hỏi ít thời gian tính toán hơn của các biến trạng thái môi trường \& mạng chính sách lập lịch. 2 biến trạng thái: \verb|assignable_job, completed_op_of_job| được chọn từ Thuật toán 1 làm các tính năng trạng thái của môi trường lập lịch job shop. Biến \verb|assignable_job| là 1 vectơ Boolean để biểu diễn liệu 1 công việc có thể được phân bổ hay không. \verb|completed_op_of_job| biểu diễn số lượng hoạt động đã hoàn thành của 1 công việc \& giá trị này sau đó được chia tỷ lệ theo số lượng hoạt động tối đa trong các công việc nằm trong phạm vi $[0,1]$. Độ dài của cả hai biến bằng số lượng công việc. Cuối cùng, để biểu diễn trạng thái môi trường, các biến được chia tỷ lệ chỉ cần được nối thành 1 vectơ có độ dài bằng 2 lần số lượng công việc.

        There are many advantages of our state representation:
        \begin{enumerate}
            \item length of state features is much less than that in previous research which means less computation time of environment state variables \& scheduling policy networks
            \item state features are unique in a scheduling solution, which suggests that state features can be easily distinguished by scheduling agent
            \item state features are directly derived from 2 state variables in Algorithm 1, avoiding massive experiments on feature design \& selection.
        \end{enumerate}
        -- Biểu diễn trạng thái của chúng tôi có nhiều ưu điểm:
        \begin{enumerate}
            \item Độ dài của các đặc trưng trạng thái ngắn hơn nhiều so với nghiên cứu trước đây, đồng nghĩa với việc giảm thời gian tính toán các biến trạng thái môi trường \& mạng lưới chính sách lập lịch.
            \item Các đặc trưng trạng thái của mục là duy nhất trong 1 giải pháp lập lịch, điều này cho thấy các đặc trưng trạng thái có thể dễ dàng được phân biệt bởi tác nhân lập lịch.
            \item Các đặc trưng trạng thái của mục được suy ra trực tiếp từ 2 biến trạng thái trong Thuật toán 1, tránh được các thử nghiệm lớn về thiết kế \& lựa chọn đặc trưng.
        \end{enumerate}
        \item {\sf3.2. Action space based on PDR.} In DRl scheduling methods based on single agent, action is output of scheduling policy networks, which is usually an integer. Since FJSP needs to select a job \& a machine at each decision, decompose this integer into 2 parts by dividing it by number of PDRs for machine selection where quotient is index of PDR for jobs assignment \& remainder represents index of PDR for machine selection.

        -- {\sf Không gian hành động dựa trên PDR.} Trong các phương pháp lập lịch DRl dựa trên tác nhân đơn lẻ, hành động là đầu ra của mạng chính sách lập lịch, thường là 1 số nguyên. Vì FJSP cần chọn 1 công việc \& 1 máy tại mỗi quyết định, hãy phân tích số nguyên này thành 2 phần bằng cách chia nó cho số PDR để chọn máy, trong đó thương là chỉ số của PDR để gán công việc \& phần dư biểu thị chỉ số của PDR để chọn máy.

        In this paper, 6 PDRs are selected to construct action space for simplicity of implementation \& ease of generalization. For selecting a job, 4 PDRs are selected directly from literature [27] including Shortest Processing (SPT), Most Work Remaining (MWRK), Most Operations Remaining (MOR) \& Minimum ratio of Flow Due Date to Most Work Remaining (FDD/MWRK). Longest Remaining Machine time not including current operation processing time (LRM) is selected from [11] due to its excellent performance. First In First Out (FIFO) is selected because it is widely used in various scheduling problems. In addition, action space for selecting machine given a job is composed of SPT \& Longest Processing Time (LPT). So that total size of action space equals number of PDRs for selecting jobs multiplied by number of PDRs for selecting machines. Definitions of these PDR are listed as follows:
        \begin{itemize}
            \item SPT: $\min Z_{ij} = p_{ij}$
            \item LPT: $\max Z_{ij} = p_{ij}$
            \item FDD/MWKR: $\min Z_{ij} = \frac{\sum_1^j p_{ij}}{\sum_j^{n_i} p_{ij}}$
            \item MOR: $\max Z_{ij} = n_i - j + 1$
            \item LRM: $\max Z_{ij} = \sum_{j+1}^{n_i} p_{ij}$
            \item FIFO: $\max Z_{ij} = t - Rt_i$
        \end{itemize}
        where $Z_{ij}$ is priority index of operation $O_{ij}$. $p_{ij}$ is processing time of operation $O_{ij}$. $n_i$: number of operations for job $J_i$. $j$: number of completed operations. $t$: current time \& $Rt_i$: release time of $J_i$.

        -- Trong bài báo này, 6 PDR được chọn để xây dựng không gian hành động nhằm đơn giản hóa việc triển khai \& dễ dàng khái quát hóa. Để lựa chọn 1 công việc, 4 PDR được chọn trực tiếp từ tài liệu [27] bao gồm Xử lý ngắn nhất (SPT), Công việc còn lại nhiều nhất (MWRK), Hoạt động còn lại nhiều nhất (MOR) \& Tỷ lệ tối thiểu giữa Ngày đến hạn luồng \& Công việc còn lại nhiều nhất (FDD/MWRK). Thời gian máy còn lại dài nhất không bao gồm thời gian xử lý hoạt động hiện tại (LRM) được chọn từ [11] do hiệu suất tuyệt vời của nó. Vào trước ra trước (FIFO) được chọn vì nó được sử dụng rộng rãi trong nhiều vấn đề lập lịch khác nhau. Ngoài ra, không gian hành động để lựa chọn máy cho 1 công việc bao gồm SPT \& Thời gian xử lý dài nhất (LPT). Do đó, tổng kích thước của không gian hành động bằng số PDR để lựa chọn công việc nhân với số PDR để lựa chọn máy. Định nghĩa của các PDR này được liệt kê như sau:
        \begin{itemize}
            \item SPT: $\min Z_{ij} = p_{ij}$
            \item LPT: $\max Z_{ij} = p_{ij}$
            \item FDD/MWKR: $\min Z_{ij} = \frac{\sum_1^j p_{ij}}{\sum_j^{n_i} p_{ij}}$
            \item MOR: $\max Z_{ij} = n_i - j + 1$
            \item LRM: $\max Z_{ij} = \sum_{j+1}^{n_i} p_{ij}$
            \item FIFO: $\max Z_{ij} = t - Rt_i$
        \end{itemize}
        trong đó $Z_{ij}$ là chỉ số ưu tiên của phép toán $O_{ij}$. $p_{ij}$ là thời gian xử lý của thao tác $O_{ij}$. $n_i$: số thao tác cho công việc $J_i$. $j$: số thao tác đã hoàn thành. $t$: thời gian hiện tại \& $Rt_i$: thời gian giải phóng $J_i$.
        \item {\sf3.3. Reward function based on scheduling area.} In this paper, propose a comprehensible reward function based on scheduling area (1)
        \begin{equation*}
            {\rm reward}(s_t,a_t) = -p_{a,j} - \sum_M {\rm vacancy}_m(s_t,s_{t+1})
        \end{equation*}
        where processing time of allocated operations \& time vacancy of all machines is computed after each dispatching action, where $s_t$: current state \& $s_{t+1}$ is next state after applying action $a_t$; $a_t$ is $j$th operation of a job with processing time $p_{a,j}$; ${\rm vacancy}(s_t,s_{t+1})$ is a function returning total time vacancy on machine set $M$ while transitioning from state $s_t$ to $s_{t+1}$.

        -- {\sf Hàm thưởng dựa trên vùng lập lịch.} Trong bài báo này, chúng tôi đề xuất 1 hàm thưởng dễ hiểu dựa trên vùng lập lịch (1)
        \begin{equation*}
            {\rm reward}(s_t,a_t) = -p_{a,j} - \sum_M {\rm vacancy}_m(s_t,s_{t+1})
        \end{equation*}
        trong đó thời gian xử lý của các tác vụ được phân bổ \& thời gian trống của tất cả các máy được tính sau mỗi hành động điều phối, trong đó $s_t$: trạng thái hiện tại \& $s_{t+1}$ là trạng thái tiếp theo sau khi áp dụng tác vụ $a_t$; $a_t$ là tác vụ thứ $j$ của 1 tác vụ có thời gian xử lý $p_{a,j}$; ${\rm vacancy}(s_t,s_{t+1})$ là 1 hàm trả về tổng thời gian trống trên tập máy $M$ khi chuyển từ trạng thái $s_t$ sang $s_{t+1}$.

        Proposed reward function is motivated by fact: total processing time of all jobs \& time vacancy on all machines constructs scheduling area of all machines which equals maximum make-span multiplied by number of machines. As is demonstrated in {\sf Fig. 5: An example to show the scheduling area.}, scheduling area is composed of total processing time of all jobs (shaded area) \& time vacancy on all machines (white color area). Relationship between accumulated reward \& maximum scheduling makespan is derived from (2)
        \begin{equation*}
            R = -a - b = -(a + b) = -S = -|M|*{\rm makespan}
        \end{equation*}
        where $R$ is accumulated reward, $a$: total processing time of all jobs, $b$: total time vacancy on all machines, $S$: scheduling time area \& $|M|$: number of machines.

        -- Hàm phần thưởng được đề xuất dựa trên thực tế: tổng thời gian xử lý của tất cả các công việc \& thời gian trống trên tất cả các máy tạo nên vùng lập lịch của tất cả các máy, bằng khoảng thời gian hoàn thành tối đa nhân với số máy. Như được minh họa trong {\sf Hình 5: Ví dụ minh họa vùng lập lịch.}, vùng lập lịch bao gồm tổng thời gian xử lý của tất cả các công việc (vùng tô bóng) \& thời gian trống trên tất cả các máy (vùng màu trắng). Mối quan hệ giữa phần thưởng tích lũy \& khoảng thời gian hoàn thành tối đa được suy ra từ (2)
        \begin{equation*}
            R = -a - b = -(a + b) = -S = -|M|*{\rm khoảng thời gian hoàn thành}
        \end{equation*}
        trong đó $R$ là phần thưởng tích lũy, $a$: tổng thời gian xử lý của tất cả các công việc, $b$: tổng thời gian trống trên tất cả các máy, $S$: vùng thời gian lập lịch \& $|M|$: số máy.

        Obviously shown from (2): total reward \& maximum makespan are negatively linearly dependent \& coefficient is number of machines. I.e., minimizing maximum scheduling makespan is equivalent to maximizing total reward.

        -- Hiển nhiên từ (2): tổng phần thưởng \& thời gian hoàn thành tối đa phụ thuộc tuyến tính âm \& hệ số là số máy. Nghĩa là, việc giảm thiểu thời gian hoàn thành tối đa tương đương với việc tối đa hóa tổng phần thưởng.
        \item {\sf3.4. Model training method based on PPO.} In order to strengthen representation ability of RL scheduling method, scheduling policy is usually represented by DNNs e.g. CNN, RNN, \& MLP. In our method, state feature vector is 1st fed to MLP to obtain a scalar score for each action \& a Softmax function is then applied to output a distribution over computed score, which is shown in (3)
        \begin{equation*}
            p(a_t|s_t) = {\rm Softmax}({\rm MLP}_{\pi_\theta}(s_t)),
        \end{equation*}
        where $p(a_t|s_t)$ is selection probability of action $a_t$ at time $t$ in state $s_t$ \& $\theta$ is parameter of scheduling policy $\pi$. Structure of our scheduling policy is demonstrate in {\sf Fig. 4}, which constructs actor network of PPO agent. Similar to actor network, critic network is implemented by MLP with 1 hidden layer. Parameters of our scheduling policy are learned by a clipped PPO whose loss function is expressed in (4)
        \begin{equation*}
            L(\theta) = E_t[\min(r_t(\theta)A_t,{\rm clip}(r_t(\theta),1\pm\epsilon)A_t)]
        \end{equation*}
        where $r_t(\theta) = \dfrac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\rm old}}(a_t|s_t)}$, $A_t$ is an estimator of advantage function at time step $t$, ${\rm clip}$ is a clipping function \& $\epsilon$ is a hyper parameter which is used to limit boundary of objective function.

        -- {\sf Phương pháp huấn luyện mô hình dựa trên PPO.} Để tăng cường khả năng biểu diễn của phương pháp lập lịch RL, chính sách lập lịch thường được biểu diễn bằng các DNN, ví dụ: CNN, RNN, \& MLP. Trong phương pháp của chúng tôi, vectơ đặc trưng trạng thái đầu tiên được đưa vào MLP để thu được điểm số vô hướng cho mỗi hành động \& sau đó áp dụng hàm Softmax để đưa ra phân phối trên điểm số đã tính toán, được thể hiện trong (3)
        \begin{equation*}
            p(a_t|s_t) = {\rm Softmax}({\rm MLP}_{\pi_\theta}(s_t)),
        \end{equation*}
        trong đó $p(a_t|s_t)$ là xác suất lựa chọn hành động $a_t$ tại thời điểm $t$ trong trạng thái $s_t$ \& $\theta$ là tham số của chính sách lập lịch $\pi$. Cấu trúc của chính sách lập lịch của chúng tôi được minh họa trong {\sf Hình 4}, trong đó xây dựng mạng lưới tác nhân của tác tử PPO. Tương tự như mạng diễn viên, mạng phê bình được triển khai bằng MLP với 1 lớp ẩn. Các tham số của chính sách lập lịch của chúng tôi được học bởi 1 PPO bị cắt xén có hàm mất mát được biểu thị trong (4)
        \begin{equation*}
            L(\theta) = E_t[\min(r_t(\theta)A_t,{\rm clip}(r_t(\theta),1\pm\epsilon)A_t)]
        \end{equation*}
        trong đó $r_t(\theta) = \dfrac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\rm old}}(a_t|s_t)}$, $A_t$ là 1 ước lượng của hàm lợi thế tại bước thời gian $t$, ${\rm clip}$ là 1 hàm cắt xén \& $\epsilon$ là 1 siêu tham số được sử dụng để giới hạn biên của hàm mục tiêu.

        Detailed training process is provided by {\sf Algorithm 2: Model training method based on PPO}. Training process includes 2 aspects: data collection \& policy learning. When collecting training data, $T$ independent complete trajectories of scheduling are generated \& they are collected in memory buffer $M$. On policy learning stage, training data are used for $K$ times. At each time, these data are randomly divided into batches whose length is related to scale of instances \& scheduling agent learns from each batch data. After replaying these experience samples evenly, prioritized experience replay is performed for $C$ times. Training process will stop until episode reaches maximum iterations or result is convergent or scheduling is time out. Define: result is thought as convergent if makespan values are same in 30 decision steps \& training time is limited in an hour.

        -- Quá trình đào tạo chi tiết được cung cấp bởi {\sf Thuật toán 2: Phương pháp đào tạo mô hình dựa trên PPO}. Quá trình đào tạo bao gồm 2 khía cạnh: thu thập dữ liệu \& học chính sách. Khi thu thập dữ liệu đào tạo, $T$ quỹ đạo hoàn chỉnh độc lập của lập lịch được tạo \& chúng được thu thập trong bộ đệm $M$. Ở giai đoạn học chính sách, dữ liệu đào tạo được sử dụng trong $K$ lần. Tại mỗi thời điểm, những dữ liệu này được chia ngẫu nhiên thành các lô có độ dài liên quan đến quy mô của các trường hợp \& tác nhân lập lịch học từ mỗi dữ liệu lô. Sau khi phát lại các mẫu trải nghiệm này 1 cách đồng đều, việc phát lại trải nghiệm được ưu tiên sẽ được thực hiện trong $C$ lần. Quá trình đào tạo sẽ dừng lại cho đến khi tập đạt số lần lặp tối đa hoặc kết quả hội tụ hoặc lập lịch hết thời gian. Định nghĩa: kết quả được coi là hội tụ nếu các giá trị makespan giống nhau trong 30 bước quyết định \& thời gian đào tạo bị giới hạn trong 1 giờ.
    \end{itemize}
    \item {\sf4. Experiments.} In this sect, in order to show effectiveness of our DRL scheduling environment for FJSP (Algorithm 1) \& to evaluate performance of proposed DRL scheduling methods, a group of experiments are performed on public FJSP benchmarks with various sizes \& results are compared with different types of scheduling methods from recent literature. Finally, training details e.g. training time are demonstrated.

    -- Trong phần này, để chứng minh tính hiệu quả của môi trường lập lịch DRL cho FJSP (Thuật toán 1) \& đánh giá hiệu suất của các phương pháp lập lịch DRL được đề xuất, 1 nhóm các thử nghiệm được thực hiện trên các chuẩn FJSP công khai với nhiều kích thước khác nhau \& kết quả được so sánh với các loại phương pháp lập lịch khác nhau từ các tài liệu gần đây. Cuối cùng, các chi tiết đào tạo, ví dụ như thời gian đào tạo, sẽ được trình bày.
    \begin{itemize}
        \item {\sf4.1. Benchmark instances \& baseline models.} In this paper, 2 well-known benchmarks of FJSP are used to evaluate our proposed methods including MK instances (MK01-MK10) in [4] \& 3 group of LA instances (edata, rdata, \& vdata each with 40 instances) in [12].

        -- Trong bài báo này, 2 chuẩn mực nổi tiếng của FJSP được sử dụng để đánh giá các phương pháp đề xuất của chúng tôi bao gồm các thể hiện MK (MK01-MK10) trong [4] \& 3 nhóm thể hiện LA (edata, rdata, \& vdata mỗi nhóm có 40 thể hiện) trong [12].

        This paper compared with PDR, exact solver, meta-heuristic \& DRL models. 6 PDRs are used to compare scheduling results where 4 out of 6 PDRs are compared in old scheduling environment \& our proposed environment. Also compare with well-known Google OR-Tools which is a powerful constraint programming solver showing strong performance in solving industrial scheduling problems. For meta-heuristic scheduling methods, 2 recent improved Genetic Algorithms are selected from [18] \& [5]. For DRL scheduling methods, compared with competing models in recent 3 years, including 4 DRl methods proposed by Feng et al.[9], Zeng et al.[26], Song et al.[22] \& Lei et al.[15] resp., GSMA model [13] which is a MARL scheduling method \& obtains state-of-art results, DANILE model [23] which is based on attention mechanism.

        -- Bài báo này so sánh với PDR, bộ giải chính xác, mô hình meta-heuristic \& DRL. 6 PDR được sử dụng để so sánh kết quả lập lịch trong đó 4 trong số 6 PDR được so sánh trong môi trường lập lịch cũ \& môi trường đề xuất của chúng tôi. Cũng so sánh với Google OR-Tools nổi tiếng, 1 bộ giải lập trình ràng buộc mạnh mẽ cho thấy hiệu suất cao trong việc giải quyết các vấn đề lập lịch công nghiệp. Đối với các phương pháp lập lịch meta-heuristic, 2 Thuật toán di truyền được cải tiến gần đây được chọn từ [18] \& [5]. Đối với các phương pháp lập lịch DRL, so sánh với các mô hình cạnh tranh trong 3 năm gần đây, bao gồm 4 phương pháp DRl do Feng et al.[9], Zeng et al.[26], Song et al.[22] \& Lei et al.[15] đề xuất, tương ứng, mô hình GSMA [13] là 1 phương pháp lập lịch MARL \& thu được kết quả tiên tiến, mô hình DANILE [23] dựa trên cơ chế chú ý.
        \item {\sf4.2. Model configurations.} PPO agent adopts actor-critic architecture where actor networks represent scheduling policy \& critic networks calculate state-value of policy networks. Actor network is implemented by MLP \& Softmax function while critic network is only represented by MLP. Both networks are optimized by Adam optimizer, use ReLU as activation function \& have only 1 hidden layer with dynamic hidden dimension which equals length of state features. For each problem size, train policy network for $\le8000$ iterations, each of which contains 9 independent trajectories (i.e., complete scheduling process of instances) \& use a dynamic batch size which equals 2 times of scale (total number of operations of all jobs) of an instance. For PPO, set epochs of updating network to 10 \& clipping parameter epsilon to 0.2. Set discount factor $\gamma$ to $0.999$ \& learning rate are $1e-3,3e-3$ for actor \& critic network resp. For prioritized experience replay, parameter $\alpha$ is set to 0.6, value of $\beta$ anneals from 0.4 to 1, number of prioritized experience replay $C$ is set to 1, \& number of convergence training steps is 2000.

        -- Tác nhân PPO áp dụng kiến trúc actor-critic trong đó các mạng actor biểu diễn chính sách lập lịch \& các mạng critic tính toán giá trị trạng thái của các mạng policy. Mạng actor được triển khai bởi hàm MLP \& Softmax trong khi mạng critic chỉ được biểu diễn bởi MLP. Cả hai mạng đều được tối ưu hóa bởi trình tối ưu hóa Adam, sử dụng ReLU làm hàm kích hoạt \& chỉ có 1 lớp ẩn với chiều ẩn động bằng với độ dài của các đặc trưng trạng thái. Đối với mỗi kích thước vấn đề, hãy huấn luyện mạng policy cho $\le8000$ lần lặp, mỗi lần lặp chứa 9 quỹ đạo độc lập (tức là quy trình lập lịch hoàn chỉnh của các trường hợp) \& sử dụng kích thước lô động bằng 2 lần quy mô (tổng số thao tác của tất cả các công việc) của 1 trường hợp. Đối với PPO, đặt các kỷ nguyên cập nhật mạng thành 10 \& tham số cắt epsilon thành 0,2. Đặt hệ số chiết khấu $\gamma$ thành $0,999$ \& tốc độ học là $1e-3,3e-3$ tương ứng với mạng actor \& critic. Đối với phát lại trải nghiệm được ưu tiên, tham số $\alpha$ được đặt thành 0,6, giá trị của $\beta$ được ủ từ 0,4 đến 1, số lần phát lại trải nghiệm được ưu tiên $C$ được đặt thành 1, \& số bước đào tạo hội tụ là 2000.
        \item {\sf4.3. Results analysis.} In order to evaluate our proposed scheduling environment, 1st test performance of 6 PDRs in our proposed scheduling environment on MK benchmark instances where 6 PDRs are used to select a job while SPT is used for selection of machines. Results are shown in {\sf Table 1: Scheduling results of PDRs on MK benchmark instances}. Widely-used 4 PDRs (SPT, MWKR, FIFO, MOR) are performed in our environment \& results are compared with that in old scheduling environment where their best results are selected from literature [22]. Symbol ``-'' means: specific results are not recorded in literature. Best scheduling result of all 6 PDRs on each instance are recorded in minPDR row. Besides, results of 2 DRL scheduling methods proposed resp. by Feng et al.[9] \& Zeng et al.[26] are also compared in Table 1 \& their results are directly from their literature instead of our implementation environment.

        -- Để đánh giá môi trường lập lịch đề xuất của chúng tôi, hiệu suất thử nghiệm đầu tiên của 6 PDR trong môi trường lập lịch đề xuất của chúng tôi trên các phiên bản chuẩn MK trong đó 6 PDR được sử dụng để chọn 1 công việc trong khi SPT được sử dụng để chọn máy. Kết quả được hiển thị trong {\sf Bảng 1: Kết quả lập lịch của PDR trên các phiên bản chuẩn MK}. 4 PDR được sử dụng rộng rãi (SPT, MWKR, FIFO, MOR) được thực hiện trong môi trường của chúng tôi \& kết quả được so sánh với kết quả trong môi trường lập lịch cũ, trong đó kết quả tốt nhất của chúng được chọn từ tài liệu [22]. Ký hiệu ``-'' có nghĩa là: kết quả cụ thể không được ghi lại trong tài liệu. Kết quả lập lịch tốt nhất của tất cả 6 PDR trên mỗi phiên bản được ghi lại trong hàng minPDR. Bên cạnh đó, kết quả của 2 phương pháp lập lịch DRL do Feng et al.[9] \& Zeng et al.[26] đề xuất cũng được so sánh trong Bảng 1 \& kết quả của chúng được lấy trực tiếp từ tài liệu của họ thay vì môi trường triển khai của chúng tôi.

        As shown in table 1, results indicate: performance of SPT, FIFO, MOR in our environment is improved while performance of MWKR is suddenly worse than before. LRM obtained best average results among all 6 PDRs \& even better than some DRL scheduling methods e.g. models proposed by Feng et al. \& Zeng et al., which is surprisingly interesting. Best result on different instances is distributed in different PDRs, e.g. MWKR obtained best results on MK1 \& FIFO performs best on MK3. So that, minPDR can get better results than LRM.

        -- Như thể hiện trong bảng 1, kết quả cho thấy: hiệu suất của SPT, FIFO, MOR trong môi trường của chúng tôi được cải thiện trong khi hiệu suất của MWKR đột nhiên kém hơn trước. LRM đạt được kết quả trung bình tốt nhất trong số tất cả 6 PDR \& thậm chí còn tốt hơn 1 số phương pháp lập lịch DRL, ví dụ như các mô hình do Feng \& cộng sự đề xuất \& Zeng \& cộng sự, điều này thật đáng ngạc nhiên. Kết quả tốt nhất trên các trường hợp khác nhau được phân phối trong các PDR khác nhau, ví dụ: MWKR đạt được kết quả tốt nhất trên MK1 \& FIFO hoạt động tốt nhất trên MK3. Do đó, minPDR có thể đạt được kết quả tốt hơn LRM.

        2nd group of experiments are also performed on MK benchmark instances to evaluate generality of our proposed DRL scheduling agent in our environment. Train these instances independently for 5 times \& average results are recorded in ``Ours'' column. As shown in Table 2, compare performance of our proposed DRL scheduling model with exact solver (OR-Tools), meta-heuristic scheduling methods (2SGA[18] \& SLGA [5]), DRL scheduling methods (GMAS[13], DANILE[23], \& models proposed by Song et al.[22]) \& minPDR method. LB \& UB columns resp. record lower bound \& upper bound scheduling results in literature. Results in these compared methods are directly from literature except results of PDRs which are performed in our environment.

        -- Nhóm thí nghiệm thứ 2 cũng được thực hiện trên các trường hợp chuẩn MK để đánh giá tính tổng quát của tác nhân lập lịch DRL được đề xuất của chúng tôi trong môi trường của chúng tôi. Đào tạo các trường hợp này độc lập trong 5 lần \& kết quả trung bình được ghi lại trong cột ``Của chúng tôi''. Như thể hiện trong Bảng 2, hãy so sánh hiệu suất của mô hình lập lịch DRL được đề xuất của chúng tôi với bộ giải chính xác (OR-Tools), các phương pháp lập lịch meta-heuristic (2SGA[18] \& SLGA [5]), các phương pháp lập lịch DRL (GMAS[13], DANILE[23], \& các mô hình do Song et al.[22] đề xuất) \& phương pháp minPDR. Các cột LB \& UB tương ứng ghi lại kết quả lập lịch giới hạn dưới \& giới hạn trên trong tài liệu. Kết quả trong các phương pháp được so sánh này được lấy trực tiếp từ tài liệu, ngoại trừ kết quả của PDR được thực hiện trong môi trường của chúng tôi.

        Average makespan is usually used to evaluate accuracy of scheduling solutions. As demonstrated in {\sf Table 2: Makespan performance of our DRL model on MK benchmark instances}, GMAS which is a DRL scheduling method based on MARL, obtained best average result. Nevertheless, they used optimal results of each instance rather than average results. Average makespan of our proposed DRL model is smaller than SLGA, DANILE, Song, \& minPDR methods \& is larger than OR-Tools, 2SGA \& GMAS methods, which shows: performance of DRL scheduling methods are getting close to meta-heuristic \& exact solver. However, different from complex scheduling networks of GMAS, our model is simple \& easy to train to converge, which is stable.

        -- Makespan trung bình thường được sử dụng để đánh giá độ chính xác của các giải pháp lập lịch. Như được minh họa trong {\sf Bảng 2: Hiệu suất Makespan của mô hình DRL của chúng tôi trên các trường hợp chuẩn MK}, GMAS, 1 phương pháp lập lịch DRL dựa trên MARL, đã thu được kết quả trung bình tốt nhất. Tuy nhiên, họ đã sử dụng kết quả tối ưu của từng trường hợp thay vì kết quả trung bình. Makespan trung bình của mô hình DRL đề xuất của chúng tôi nhỏ hơn các phương pháp SLGA, DANILE, Song, \& minPDR \& lớn hơn các phương pháp OR-Tools, 2SGA \& GMAS, điều này cho thấy: hiệu suất của các phương pháp lập lịch DRL đang tiến gần đến trình giải meta-heuristic \& chính xác. Tuy nhiên, khác với các mạng lập lịch phức tạp của GMAS, mô hình của chúng tôi đơn giản \& dễ huấn luyện để hội tụ, điều này rất ổn định.

        Beside, evaluate convergence performance of our DRL scheduling method. As is demonstrated in {\sf Fig. 6: Taining trajectories \& training time on MK benchmarks.}, our DRL model on all MK instances is convergent in half an hour on average \& number of needed trajectories is $< 900$. Convergence time of 8 out of 10 instances is $< 600$ seconds, which is close to industrial time limitation.

        -- Ngoài ra, hãy đánh giá hiệu suất hội tụ của phương pháp lập lịch DRL của chúng tôi. Như được minh họa trong {\sf Hình 6: Định vị quỹ đạo \& thời gian huấn luyện trên chuẩn MK.}, mô hình DRL của chúng tôi trên tất cả các trường hợp MK đều hội tụ trung bình trong nửa giờ \& số quỹ đạo cần thiết là $< 900$. Thời gian hội tụ của 8 trên 10 trường hợp là $< 600$ giây, gần với giới hạn thời gian công nghiệp.

        In order to show stability \& generality of our proposed DRL scheduling agent in our environment, more experiments are performed on LA benchmark instances, including edta, rdata, \& vdata, whose flexibility is getting increased. As shown in {\sf Table 3: Average makespan comparison LA instances}, compare with various scheduling methods in literature e.g. exact solver OR-Tools, meta-heuristic 2SGA, DRL scheduling methods: DANILE, GMAS, Lei [15] \& Song as well as PDR methods. Results of these methods are directly from literature. Results of PDR methods are from running in our proposed environment \& only minimum result of 6 PDRs on each instance is recorded in minPDR column.

        -- Để chứng minh tính ổn định \& tính tổng quát của tác nhân lập lịch DRL được đề xuất trong môi trường của chúng tôi, nhiều thử nghiệm hơn đã được thực hiện trên các phiên bản chuẩn LA, bao gồm edta, rdata, \& vdata, với tính linh hoạt ngày càng tăng. Như được thể hiện trong {\sf Bảng 3: So sánh makespan trung bình của các phiên bản LA}, hãy so sánh với các phương pháp lập lịch khác nhau trong tài liệu, ví dụ: bộ giải chính xác OR-Tools, meta-heuristic 2SGA, các phương pháp lập lịch DRL: DANILE, GMAS, Lei [15] \& Song cũng như các phương pháp PDR. Kết quả của các phương pháp này được trích dẫn trực tiếp từ tài liệu. Kết quả của các phương pháp PDR được thực hiện trong môi trường được đề xuất của chúng tôi \& chỉ có kết quả tối thiểu là 6 PDR trên mỗi phiên bản được ghi lại trong cột minPDR.

        As is demonstrated in Table 3, our proposed DRL scheduling agent got smaller average makespan than DANILE, Lei, Song, \& PDR models \& obtained larger average makespan than OR-Tools, GMAS, \& 2SGA, which shows competing performance of our proposed DRL scheduling agent in our environment. More interestingly, minPDR obtained smaller makespan than Lei model which is used to solve large-scale dynamic FJSP. That shows efficiency of our proposed environment again.

        -- Như được minh họa trong Bảng 3, tác nhân lập lịch DRL đề xuất của chúng tôi có makespan trung bình nhỏ hơn các mô hình DANILE, Lei, Song, \& PDR \& đạt makespan trung bình lớn hơn OR-Tools, GMAS, \& 2SGA, điều này cho thấy hiệu suất cạnh tranh của tác nhân lập lịch DRL đề xuất trong môi trường của chúng tôi. Thú vị hơn, minPDR đạt makespan nhỏ hơn mô hình Lei, được sử dụng để giải các bài toán FJSP động quy mô lớn. Điều này 1 lần nữa cho thấy hiệu quả của môi trường chúng tôi đề xuất.

        Finally, training time of our proposed DRL scheduling agent on edata, rdata, \& vdata are depicted in {\sf Fig. 7: Training time on edata, rdata, \& vdata instances.} Our proposed DRL model can be trained to converge in an hour on all benchmark instances \& total training time on edta, rdata, \& vdata are 22064, 31650 \& 40137 seconds, resp., which shows training time rises as increase of complicity of scheduling instances.

        -- Cuối cùng, thời gian đào tạo của tác nhân lập lịch DRL được đề xuất của chúng tôi trên edata, rdata, \& vdata được mô tả trong {\sf Hình 7: Thời gian đào tạo trên các phiên bản edata, rdata, \& vdata.} Mô hình DRL được đề xuất của chúng tôi có thể được đào tạo để hội tụ trong 1 giờ trên tất cả các phiên bản chuẩn \& tổng thời gian đào tạo trên edta, rdata, \& vdata tương ứng là 22064, 31650 \& 40137 giây, cho thấy thời gian đào tạo tăng lên khi mức độ phức tạp của các phiên bản lập lịch tăng lên.
    \end{itemize}
    \item {\sf5. Conclusion.} In this paper, proposed a chronological discrete event simulation based DRL environment for FJSP. Scheduling process is described by a simulation algorithm where state changes are captured by state variables \& reward function is calculated based on scheduling area of machines at each decision step. In this simulaiton environment, an end-to-end DRL framework is proposed based on actor-critic PPO, providing a flexible infrastructure for design of state representation, action space \& scheduling policy networks. Besides, a simple DRL model for FJSP is presented by defining state representation of very short state features based on 2 state variables in simulation environment, action space composed of widely-used priority dispatching rules in literature \& scheduling policy baesd on MLP with only 1 hidden layer. Various experiments are performed on classic benchmark instances with different sizes \& results show: performance of PDR is improved in our environment, even better than some DRL methods. Besides, our DRL scheduling method provides competing scheduling performance compared with DRL, meta-heuristic \& PDR methods.

    -- Bài báo này đề xuất 1 môi trường DRL dựa trên mô phỏng sự kiện rời rạc theo trình tự thời gian cho FJSP. Quá trình lập lịch được mô tả bằng 1 thuật toán mô phỏng, trong đó các thay đổi trạng thái được ghi lại bởi các biến trạng thái \& hàm thưởng được tính toán dựa trên diện tích lập lịch của máy tại mỗi bước quyết định. Trong môi trường mô phỏng này, 1 khuôn khổ DRL đầu cuối được đề xuất dựa trên PPO tác nhân-phê bình, cung cấp 1 cơ sở hạ tầng linh hoạt cho việc thiết kế biểu diễn trạng thái, không gian hành động \& mạng lưới chính sách lập lịch. Bên cạnh đó, 1 mô hình DRL đơn giản cho FJSP được trình bày bằng cách định nghĩa biểu diễn trạng thái của các đặc trưng trạng thái rất ngắn dựa trên 2 biến trạng thái trong môi trường mô phỏng, không gian hành động bao gồm các quy tắc phân bổ ưu tiên được sử dụng rộng rãi trong tài liệu \& chính sách lập lịch dựa trên MLP với chỉ 1 lớp ẩn. Nhiều thử nghiệm khác nhau được thực hiện trên các phiên bản chuẩn cổ điển với các kích thước khác nhau \& kết quả cho thấy: hiệu suất của PDR được cải thiện trong môi trường của chúng tôi, thậm chí còn tốt hơn 1 số phương pháp DRL. Hơn nữa, phương pháp lập lịch DRL của chúng tôi cung cấp hiệu suất lập lịch cạnh tranh so với các phương pháp DRL, meta-heuristic \& PDR.

    Future research will mainly focus on design of scheduling policy networks as well as state representation. State features can be thought as texts or images so that various networks in NLP \& CV fields can be used in scheduling policy networks, e.g. SPP networks, TextCNN, \& Transformer.

    -- Nghiên cứu trong tương lai sẽ chủ yếu tập trung vào thiết kế mạng lưới chính sách lập lịch cũng như biểu diễn trạng thái. Các đặc điểm trạng thái có thể được hiểu dưới dạng văn bản hoặc hình ảnh, do đó, các mạng lưới khác nhau trong trường NLP \& CV có thể được sử dụng trong mạng lưới chính sách lập lịch, ví dụ: mạng SPP, TextCNN, \& Transformer.
\end{itemize}


%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]

\end{document}