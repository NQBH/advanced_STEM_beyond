\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\def\labelitemii{$\circ$}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Linear Algebra -- Đại Số Tuyến Tính}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Linear Algebra -- Đại Số Tuyến Tính}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/linear_algebra/NQBH_linear_algebra.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/linear_algebra/NQBH_linear_algebra.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basic}
Tôi được giải Nhì Đại số Olympic Toán Sinh viên 2014 (VMC2014) khi còn học năm nhất Đại học \& được giải Nhất Đại số Olympic Toán Sinh viên 2015 (VMC2015) khi học năm 2 Đại học. Nhưng điều đó không có nghĩa là tôi giỏi Đại số. Bằng chứng là 10 năm sau khi nhận các giải đó, tôi đang tự học lại Đại số tuyến tính với hy vọng có 1 hay nhiều cách nhìn mới mẻ hơn \& mang tính ứng dụng hơn cho các đề tài cá nhân của tôi.

\noindent\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{itemize}
	\item \cite{Hung_linear_algebra}. {\sc Nguyễn Hữu Việt Hưng}. {\it Đại Số Tuyến Tính}.
	\item \cite{Tiep_ML_co_ban}. {\sc Vũ Hữu Tiệp}. {\it Machine Learning Cơ Bản}.
	
	Mã nguồn cuốn ebook ``Machine Learning Cơ Bản'': \url{https://github.com/tiepvupsu/ebookMLCB}.
	
	Phép nhân từng phần{\tt/}tích Hadamard (Hadamard product) thường xuyên được sử dụng trong ML. Tích Hadamard của 2 ma trận cùng kích thước $A,B\in\mathbb{R}^{m\times n}$, được ký hiệu là $A\odot B = (a_{ij}b_{ij})_{i,j=1}^{m,n}\in\mathbb{R}^{m\times n}$.
	
	Việc chuyển đổi hệ cơ sở sử dụng ma trận trực giao có thể được coi như 1 phép xoay trục tọa độ. Nhìn theo 1 cách khác, đây cũng chính là 1 phép xoay vector dữ liệu theo chiều ngược lại, nếu ta coi các trục tọa độ là cố định.
	
	Việc phân tích 1 đại lượng toán học ra thành các đại lượng nhỏ hơn mang lại nhiều hiệu quả. Phân tích 1 số thành tích các thừa số nguyên tố giúp kiểm tra 1 số có bao nhiêu ước số. Phân tích đa thức thành nhân tử giúp tìm nghiệm của đa thức. Việc phân tích 1 ma trận thành tích của các ma trận đặc biệt cũng mang lại nhiều lợi ích trong việc giải hệ phương trình tuyến tính, tính lũy thừa của ma trận, xấp xỉ ma trận, $\ldots$
	
	{\bf Phép phân tích trị riêng.} Cách biểu diễn 1 ma trận vuông $A$ với ${\bf x}_i\ne{\bf 0}$ là các vector riêng của 1 ma trận vuông $A$ ứng với các giá trị riêng lặp hoặc phức $\lambda_i$: $A{\bf x}_i = \lambda_i{\bf x}_i$, $\forall i = 1,\ldots,n$: $A = X\Lambda X^{-1}$ với $\Lambda = {\rm diag}(\lambda_1,\ldots,\lambda_n)$, $X = [{\bf x}_1,\ldots,{\bf x}_n]$.
	
	{\bf Norm -- Chuẩn.} Khoảng cách Euclid chính là độ dài đoạn thẳng nối 2 điểm trong mặt phẳng. Đôi khi, để đi từ 1 điểm này tới 1 điểm kia, không thể đi bằng đường thẳng vì còn phụ thuộc vào hình dạng đường đi nối giữa 2 điểm. Cf. đường trắc địa trong Hình học Vi phân -- geodesics in Differential Geometry. Việc đo khoảng cách giữa 2 điểm dữ liệu nhiều chiều rất cần thiết trong ML -- chính là lý do khái niệm {\it chuẩn} (norm) ra đời.
	
	{\bf Trace -- Vết.} {\it Vết} (trace) của 1 ma trận vuông $A$ được ký hiệu là $\operatorname{trace}A$ là tổng tất cả các phần tử trên đường chéo chính của nó. Hàm vết xác định trên tập các ma trận vuông được sử dụng nhiều trong tối ưu vì nó có các tính chất đẹp.
	
	{\bf Kiểm tra gradient.} Việc tính gradient của hàm nhiều biến thông thường khá phức tạp \& rất dễ mắc lỗi. Trong thực nghiệm, có 1 cách để kiểm tra liệu gradient tính được có chính xác không. Cách này dựa trên định nghĩa của đạo hàm cho hàm 1 biến.
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{\cite{Trefethen_Bau1997,Trefethen_Bau2022}. {\sc Lloyd N. Trefethen, David Bau III}. Numerical Linear Algebra}
[NQBH]: There is a new version 2e 2022 but I cannot download it from Libgen or anywhere.
\begin{itemize}
	\item {\sf Preface.} Since early 1980s, 1st author has taught a graduate course in numerical linear algebra at MIT \& Cornell. Alumni of this course, now numbering in hundreds, have been graduate students in all fields of engineering \& physical sciences. This book is an attempt to put this course on paper.
	
	In field of numerical linear algebra, there is already an encyclopedic treatment on market: {\it Matrix Computations}, by {\sc Golub \& Van Loan}, now in its 3e. This book is in no way an attempt to duplicate that one. It is small, scaled to size of 1 university semester. Its aim: present fundamental ideas in as elegant a fashion as possible. Hope: every reader of this book will have access also to {\sc Golub \& Van Loan} for pursuit of further details \& additional topics, \& for its extensive references to research literature. 2 other important recent books are those of {\sc Higham \& Demmel}.
		
	Field of numerical linear algebra is more beautiful, \& more fundamental, than its rather dull name may suggest. More beautiful, because it is full of powerful ideas that are quite unlike those normally emphasized in a linear algebra course in a mathematics department. (At end of semester, students invariably comment: there is more to this subject than they ever imagined.) More fundamental, because, thanks to a trick of history, ``numerical'' linear algebra is really {\it applied} linear algebra. Here one finds essential ideas that every mathematical scientist needs to work effectively with vectors \& matrices. In fact, our subject is more than just vectors \& matrices, for virtually everything we do carries over to functions \& operators. Numerical linear algebra is really functional analysis, but with emphasis always on practical algorithmic ideas rather than mathematical technicalities.
	
	Book is divided into 40 lectures. Have tried to build each lecture around 1 or 2 central ideas, emphasizing unity between topics \& never getting lost in details. In many places our treatment is nonstandard. This is not place to list all of these points (see Notes), but will mention 1 unusual aspect of this book. Have departed from customary practice by not starting with Gaussian elimination. That algorithm is atypical (khác biệt) of numerical linear algebra, exceptionally difficult to analyze, yet at same time tediously familiar to every student entering a course like this. Instead, begin with QR factorization, which is more important, less complicated, \& a fresher idea to most students. QR factorization is thread that connects most of algorithms of numerical linear algebra, including methods for least squares, eigenvalue, \& singular value problems, as well as iterative methods $\forall$ of these \& also for systems of equations. Since 1970s, iterative methods have moved to center stage in scientific computing, \& to them, devote last part of book.
	
	Hope reader will come to share our view: if any other mathematical topic is as fundamental to mathematical sciences as calculus \& differential equations, it is numerical linear algebra.
	\item {\sf I. Fundamentals.}
	\begin{itemize}
		\item {\sf1. Matrix-Vector Multiplication.}
		\item {\sf2. Orthogonal Vectors \& Matrices.}
		\item {\sf3. Norms.}
		\item {\sf4. Singular Value Decomposition SVD.}
		\item {\sf5. More on SVD.}
	\end{itemize}
	\item {\sf II. QR Factorization \& Least Squares.}
	\begin{itemize}
		\item {\sf6. Projectors.}
		\item {\sf7. QR Factorization.}
		\item {\sf8. Gram--Schmidt Orthogonalization.}
		\item {\sf9. MATLAB.}
		\item {\sf10. Householder Triangularization.}
		\item {\sf11. Least Squares Problems.} Least squares data-fitting has been an indispensable tool since its invention by {\sc Gauss \& Legendre} around 1800, with ramifications (hậu quả) extending throughout mathematical science. In language of linear algebra, problem here is solution of an overdetermined system of equaitons $A{\bf x} = {\bf b}$ -- rectangular with more rows than columns. Least squares idea: ``solve'' such a system by minimizing 2-norm of residual ${\bf b} - A{\bf x}$.
		\begin{itemize}
			\item {\sf Problem.} Consider a linear system of equations having $n$ unknowns but $m > n$ equations. Symbolically, wish to find a vector ${\bf x}\in\mathbb{C}^n$ that satisfies $A{\bf x} = {\bf b}$, where $A\in\mathbb{C}^{m\times n},b\in\mathbb{C}^m$. In general, such a problem has no solution. A suitable vector ${\bf x}$ exists only if ${\bf b}$ lies in ${\rm range}(A)$, \& since ${\bf b}$ is an $m$-vector, whereas ${\rm range}(A)$ is of dimension $\le n$, this is true only for exceptional choices of ${\bf b}$. Say: a rectangular system of equations with $m > n$ is {\it overdetermined}. Vector known as {\it residual}: ${\bf r} = {\bf b} - A{\bf x}\in\mathbb{C}^m$, can perhaps be made quite small by a suitable choice of ${\bf x}$, but in general it cannot be made $= 0$.
			
			What can it mean to solve a problem that has no solution? In case of an overdetermined system of equations, there is a natural answer to this question. Since residual $r$ cannot be made to be 0, instead made it as small as possible. Measuring smallness of $r$ entails choosing a norm. If choose 2-norm, problem takes following form: (11.2)
			\begin{quote}
				Given $A\in\mathbb{C}^{m\times n},m\ge n,b\in\mathbb{C}^m$, find ${\bf x}\in\mathbb{C}^n$ s.t. $\|{\bf b} - A{\bf x}\|_2$ is minimized.
			\end{quote}
			This is our formulation of general (linear) {\it least squares problem}. Choice of 2-norm can be defended by various geometric \& statistical arguments, \& it certainly leads to a simple algorithms -- ultimately because derivative of a quadratic function, which must be set to 0 for minimization, is linear.
			
			2-norm corresponds to Euclidean distance, so there is a simple geometric interpretation of (11.2). Seek a vector ${\bf x}\in\mathbb{C}^n$ s.t. vector $A{\bf x}\in\mathbb{C}^m$ is closest point in ${\rm range}(A)$ to ${\bf b}$
			\item {\sf Example: Polynomial Data-Fitting.} Compare polynomial interpolation, which leads to a square system of equations, \& least squares polynomial data-fitting, where system is rectangular.
			
			\begin{example}[Polynomial Interpolation]
				Suppose given $m$ distinct points $x_1,\ldots,x_m\in\mathbb{C}$ \& data $y_1,\ldots,y_m\in\mathbb{C}$ at these points. Then there exists a unique \emph{polynomial interpolant} to these data in these points, i.e., a polynomial of degree at most $m - 1$, $p(x) = \sum_{i=0}^{m-1} c_ix^i$, with property: at each $x_i$, $p(x_i) = y_i$. Relationship of data $\{x_i\},\{y_i\}$ to coefficients $\{c_i\}$ can be expressed by square Vandermonde system. To determine coefficients $\{c_i\}$ for a given set of data, can solve this system of equations, which is guaranteed to be nonsingular as long as points $\{x_i\}$ are distinct because $D_n = \prod_{1\le j < i\le n} (x_i - x_j)$.
				
				{\sf Fig. 11.1: Degree 10 polynomial interpolant to 11 data points. Axis scales are not given, as these have no effect on picture.} presents an example of this process of polynomial interpolation. Have 11 data points in form of a discrete square wave, represented by crosses, \& curve $p(x)$ passes through them, as it must. However, fit is not at all pleasing. near ends of interval, $p(x)$ exhibits large oscillations that are clearly an artifact (hiện vật) of interpolation process, not a reasonable reflection of data.
				
				This unsatisfactory behavior is typical of polynomial interpolation. Fits it produces are often bad, \& they tend to get worse rather than better if more data are utilized. Even if fit is good, interpolation process may be ill-conditioned, i.e., sensitive to perturbations of data. To avoid these problems, one can utilize a nonuniform set of interpolation points e.g. Chebyshev points in interval $[-1,1]$. In applications, however, it will not always be possible to choose interpolation points at will.
			\end{example}
			
			\begin{example}[Polynomial Least Squares Fitting]
				Without changing data points, can do better by reducing degree of polynomial. Given $x_1,\ldots,x_m,y_1,\ldots,y_m$ again, consider now a degree $n - 1$ polynomial $p(x) = \sum_{i=0}^{n-1} c_ix^i$ for some $n < m$. Such a polynomial is a least squares fit to data if it minimizes sum of squares of deviation from data,
				\begin{equation*}
					\sum_{i=1}^m |p(x_i) - y_i|^2.
				\end{equation*}
				This sum of squares $=$ square of norm of residual $\|{\bf r}\|_2^2$ for rectangular Vandermonde system (11.7). {\sf Fig. 11.2: Degree 7 polynomial least squares fit to same 11 data points.} illustrates what we get if fit same 11 data points from last example with a polynomial of degree 7. New polynomial does not interpolate data, but it \fbox{captures their overall behavior much better} than polynomial of Example 11.1. Though on cannot see this in figure, it is also less sensitive to perturbations.
			\end{example}
			\item {\sf Orthogonal Projection \& Normal Equations.} How was Fig. 11.2 computed? How are least squares problems solved in general? Key to deriving algorithms is orthogonal projection.
			
			Idea is illustrated in {\sf Fig. 11.3: Formulation of least squares problem (11.2) in terms of orthogonal projection.} Goal: find closest point $A{\bf x}$ in ${\rm range}(A)$ to ${\bf b}$, so that norm of residual ${\bf r} = {\bf b} - A{\bf x}$ is minimized. Clear geometrically: this will occur provided $A{\bf x} = P{\bf b}$ where $P\in\mathbb{C}^{m\times m}$: orthogonal projector that maps $\mathbb{C}^m$ onto ${\rm range}(A)$. I.e., residual ${\bf r} = {\bf b} - A{\bf x}$ must be orthogonal to ${\rm range}(A)$. Formulate this condition as following theorem.
			\begin{theorem}
				Let $A\in\mathbb{C}^{m\times n}$, $m\ge n$, \& ${\bf b}\in\mathbb{C}^m$ be given. A vector ${\bf x}\in\mathbb{C}^n$ minimizes residual norm $\|{\bf r}\|_2 = \|{\bf b} - A{\bf x}\|_2$, thereby solving least squares problem (11.2), iff ${\bf-- r}\bot{\rm range}(A)$, i.e., (11.8)--(11.9)
				\begin{equation*}
					A^*{\bf r} = {\bf0}\Leftrightarrow A^*A{\bf x} = A^*{\bf b}\Leftrightarrow Pb = A{\bf x},
				\end{equation*}
				where $P\in\mathbb{C}^{m\times m}$: orthogonal projector onto ${\rm range}(A)$. $n\times n$ system of equations (11.9), known as {\rm normal equations}, is nonsingular iff $A$ has full rank. Consequently solution ${\bf x}$ is unique iff $A$ has full rank.
			\end{theorem}
			\item {\sf Pseudoinverse.}
			\item {\sf Normal Equations.}
			\item {\sf QR Factorization.}
			\item {\sf SVD.}
			\item {\sf Comparison of Algorithms.}
		\end{itemize}
	\end{itemize}
	\item {\sf III. Conditioning \& Stability.}
	\begin{itemize}
		\item {\sf12. Conditioning \& Condition Numbers.}
		\item {\sf13. Floating Point Arithmetic.}
		\item {\sf14. Stability.}
		\item {\sf15. More on Stability.}
		\item {\sf16. Stability of Householder Triangularization.}
		\item {\sf17. Stability of Back Substitution.}
		\item {\sf18. Conditioning of Least Squares Problems.}
		\item {\sf19. Stability of Least Squares Algorithms.}
	\end{itemize}
	\item {\sf IV. Systems of Equations.}
	\begin{itemize}
		\item {\sf20. Gaussian Elimination.}
		\item {\sf21. Pivoting.}
		\item {\sf22. Stability of Gaussian Elimination.}
		\item {\sf23. Cholesky Factorization.}
	\end{itemize}
	\item {\sf V. Eigenvalues.}
	\begin{itemize}
		\item {\sf24. Eigenvalue Problems.}
		\item {\sf25. Overview of Eigenvalue Algorithms.}
		\item {\sf26. Reduction to Hessenberg or Tridiagonal Form.}
		\item {\sf27. Rayleigh Quotient, Inverse Iteration.}
		\item {\sf28. QR Algorithm without Shifts.}
		\item {\sf29. QR Algorithm with Shifts.}
		\item {\sf30. Other Eigenvalue Algorithms.}
		\item {\sf31. Computing SVD.}
	\end{itemize}
	\item {\sf VI. Iterative Methods.}
	\begin{itemize}
		\item {\sf32. Overview of Iterative Methods.}
		\item {\sf33. Arnoldi Iteration.}
		\item {\sf34. How Arnoldi Locates Eigenvalues.}
		\item {\sf35. GMRES.}
		\item {\sf36. Lanczos Iteration.}
		\item {\sf37. From Lanczos to Gauss Quadrature.}
		\item {\sf38. Conjugate Gradients.}
		\item {\sf39. Biorthogonalization Methods.}
		\item {\sf40. Preconditioning.}
	\end{itemize}
	\item {\sf Appendix. Def of Numerical Analysis.}
\end{itemize}

%------------------------------------------------------------------------------%

\section{Wikipedia}

\subsection{Wikipedia{\tt/}abstract structure}
``An {\it abstract structure} is an \href{https://en.wikipedia.org/wiki/Abstraction}{abstraction} that might be of the \href{https://en.wikipedia.org/wiki/Euclidean_space}{geometric spaces} or a set structure, or a \href{https://en.wikipedia.org/wiki/Hypostatic_abstraction}{hypostatic abstraction} that is defined by a set of mathematical theorems \& laws, properties, \& relationships in a way that is logically if not always historically independent\footnote{However historical dependencies are partially considered in event theory as part of the \href{https://en.wikipedia.org/wiki/Combinatorics}{combinatorics} theory in \href{https://en.wikipedia.org/wiki/Kolmogorov_complexity}{Kolmogorov complexity} \& \href{https://en.wikipedia.org/wiki/Wiener-Khinchin_theorem}{Kolmogorov-Khinchin} equations.} of the structure of contingent experiences, e.g., those involving physical objects. Abstract structures are studied not only in \href{https://en.wikipedia.org/wiki/Logic}{local} \& \href{https://en.wikipedia.org/wiki/Mathematics}{mathematics} but in the fields that apply them, as \href{https://en.wikipedia.org/wiki/Computer_science}{computer science} \& \href{https://en.wikipedia.org/wiki/Computer_graphics}{computer graphics}, \& in the studies that reflect on them, such as \href{https://en.wikipedia.org/wiki/Philosophy}{philosophy} (especially the \href{https://en.wikipedia.org/wiki/Philosophy_of_mathematics}{philosophy of mathematics}). Indeed, modern mathematics has been defined in a very general sense as the study of abstract structures (by the \href{https://en.wikipedia.org/wiki/Nicolas_Bourbaki}{Bourbaki} group: see discussion there, at \href{https://en.wikipedia.org/wiki/Algebraic_structure}{algebraic structure} \& also structure).

An abstract structure may be represented (perhaps with some degree of approximation) by 1 or more physical objects -- this is called an implementation or \href{https://en.wikipedia.org/wiki/Instantiation_principle}{instantiation} of the abstract structure. But the abstract structure itself is defined in a way that is not dependent on the properties of any particular implementation.

An abstract structure has a richer structure than a \href{https://en.wikipedia.org/wiki/Concept}{concept} or an \href{https://en.wikipedia.org/wiki/Idea}{idea}. An abstract structure must include precise rules of behavior which can be used to determine whether a candidate implementation actually matches the abstract structure in question, \& it must be free from \href{https://en.wikipedia.org/wiki/Contradiction}{contradictions}. Thus we may debate how well a particular government fits the concept of \href{https://en.wikipedia.org/wiki/Democracy}{democracy}, but there is no room for debate over whether a given sequence of moves is or is not a valid game of chess (e.g. \href{https://en.wikipedia.org/wiki/Kasparov}{Kasparovian} approaches).

\subsubsection{Examples}

\begin{itemize}
	\item A \href{https://en.wikipedia.org/wiki/Sorting_algorithm}{sorting algorithm} is an abstract structure, but a \href{https://en.wikipedia.org/wiki/Recipe}{recipe} is not, because it depends on the properties \& quantities of its ingredients.
	\item A simple \href{https://en.wikipedia.org/wiki/Melody}{melody} is an abstract structure, but an \href{https://en.wikipedia.org/wiki/Orchestration}{orchestration} is not, because it depends on the properties of particular instruments.
	\item \href{https://en.wikipedia.org/wiki/Euclidean_geometry}{Euclidean geometry} is an abstract structure, but the theory of \href{https://en.wikipedia.org/wiki/Continental_drift}{continent drift} is not, because it depends on the geology of the \href{https://en.wikipedia.org/wiki/Earth}{Earth}.
	\item A \href{https://en.wikipedia.org/wiki/Formal_language}{formal language} is an abstract structure, but a \href{https://en.wikipedia.org/wiki/Natural_language}{natural language} is not, because its rules of grammar \& syntax are open to debate \& interpretation.
\end{itemize}

\subsection{Wikipedia{\tt/}direct sum}
``The {\it direct sum} is an \href{https://en.wikipedia.org/wiki/Operation_(mathematics)}{operation} between \href{https://en.wikipedia.org/wiki/Mathematical_structure}{structures} in \href{https://en.wikipedia.org/wiki/Abstract_algebra}{abstract algebra}, a branch of mathematics. It is defined differently, but analogously, for different kinds of structures. E.g., the direct sum of 2 abelian groups $A,B$ is another abelian group $A\oplus B$ consisting of the ordered pairs $(a,b)$ where $a\in A,b\in B$. To add ordered pairs, we define the sum $(a,b) + (c,d)\coloneqq(a + c,b + d)$, i.e., addition is defined coordinate-wise. E.g., the direct sum $\mathbb{R}\oplus\mathbb{R}$ where $\mathbb{R}$ is \href{https://en.wikipedia.org/wiki/Real_coordinate_space}{real coordinate space}, is the \href{https://en.wikipedia.org/wiki/Cartesian_plane}{Cartesian plane} $\mathbb{R}^2$. A similar process can be used to form the direct sum of 2 \href{https://en.wikipedia.org/wiki/Vector_space}{vector spaces} or 2 \href{https://en.wikipedia.org/wiki/Module_(mathematics)}{modules}.

We can also form direct sums with any finite number of summands, e.g., $A\oplus B\oplus C$, provided $A,B,C$ are the same kinds of algebraic structures (e.g., all abelian groups, or all vector spaces). This relies on the fact that the direct sum is \href{https://en.wikipedia.org/wiki/Associative}{associative} \href{https://en.wikipedia.org/wiki/Up_to}{up to} \href{https://en.wikipedia.org/wiki/Isomorphism}{isomorphism}. I.e., $(A\oplus B)\oplus C\cong A\oplus(B\oplus C)$ for any algebraic structures $A,B,C$ of the same kind. The direct sum is also \href{https://en.wikipedia.org/wiki/Commutative}{commutative} up to isomorphism, i.e., $A\oplus B\cong B\oplus A$ for any algebraic structures $A,B$ of the same kind.

The direct sum of finitely many abelian groups, vector spaces, or modules is \href{https://en.wikipedia.org/wiki/Isomorphism}{canonically isomorphic} to the corresponding \href{https://en.wikipedia.org/wiki/Direct_product}{direct product}. This is false, however, for some algebraic object, like nonabelian groups.

In the case where infinitely many objects are combined, the direct sum \& direct product are not isomorphic, even for abelian groups, vector spaces, or modules. E.g., consider the direct sum \& direct product of (countably) infinitely many copies of the integers. All element in the direct product is an infinite sequence, e.g., $(1,2,3,\ldots)$ but in the direct sum, there is a requirement that all but finitely many coordinates be zero, so the sequence $(1,2,3,\ldots)$ would be an element of the direct product but not of the direct sum, while $(1,2,0,0,\ldots)$ would be an element of both. Often, if a $+$ sign is used, all but finitely many coordinates must be zero, while if some form of multiplication is used, all but finitely many coordinates must be 1. In more technical language, if the summands are $(A_i) _{i\in I}$, the direct sum $\bigoplus_{i\in I} A_i$ is defined to be the set of tuples $(a_i)_{i\in I}$ with $a_i\in A_i$ s.t. $a_i = 0$ for all but finitely many $i$. The direct sum $\bigoplus_{i\in I} A_i$ is contained in the \href{https://en.wikipedia.org/wiki/Direct_product}{direct product} $\prod_{i\in I} A_i$, but is strictly smaller when the \href{https://en.wikipedia.org/wiki/Index_set}{index set} $I$ is infinite, because an element of the direct product can have infinitely many nonzero coordinates.

\subsubsection{Examples}
The $xy$-plane, a 2D \href{https://en.wikipedia.org/wiki/Vector_space}{vector space}, can be thought of as the direct sum of 2 1D vector spaces, namely the $x$ \& $y$ axes. In this direct sum, the $x,y$ axes intersect only at the origin (the zero vector). Addition is defined coordinate-wise, i.e., $(x_1,y_1) + (x_2,y_2)\coloneqq(x_1 + x_2,y_1 + y_2)$, which is the same as vector addition.

Given 2 structures $A,B$, their direct sum is written as $A\oplus B$. Given an \href{https://en.wikipedia.org/wiki/Indexed_family}{indexed family} of structures $A_i$, indexed with $i\in I$, the direct sum may be written $A = \bigoplus_{i\in I} A_i$. Each $A_i$ is called a {\it direct summand} of $A$. If the index set is finite, the direct sum is the same as the direct product. In the case of groups, if the group operation is written as $+$ the phrase ``direct sum'' is used, while if the group operation is written $*$ the phrase ``direct product'' is used. When the index set is infinite, the direct sum is not the same as the direct product since the direct sum has the extra requirement that all but finitely many coordinates must be 0.

\paragraph{Internal \& external direct sums.} A distinction is made between internal \& external direct sums, though the 2 are isomorphic. If the summands are defined 1st, \& then the direct sum is defined in terms of the summands, we have an external direct sum. E.g., if we define the real numbers $\mathbb{R}$ \& then define $\mathbb{R}\oplus\mathbb{R}$ the direct sum is said to be {\it external}.

If, on the other hand, 1st define some algebraic structure $S$ \& then write $S$ as a direct sum of 2 substructures $V,W$, then the direct sum is said to be internal. In this case, each element of $S$ is expressible uniquely as an algebraic combination of an element of $V$ \& an element of $W$. For an example of an internal direct sum, consider $\mathbb{Z}_6$ (the integers modulo 6), whose elements are $\{0,1,2,3,4,5\}$. This is expressible as an internal direct sum $\mathbb{Z}_6 = \{0,2,4\}\oplus\{0,3\}$.

\subsubsection{Types of direct sum}
[$\ldots$]

\subsubsection{Homomorphisms}
The direct sum $\bigoplus_{i\in I} A_i$ comes equipped with a \href{https://en.wikipedia.org/wiki/Projection_(mathematics)}{\it projection} \href{https://en.wikipedia.org/wiki/Homomorphism}{homomorphism} $\pi_j:\bigoplus_{i\in I} A_i\to A_j$ for each $j\in I$ \& a {\it coprojection} $\alpha_j:A_j\to\bigoplus_{i\in I} A_i$ for each $j\in I$. Given another algebraic structure $B$ (with the same additional structure) \& homomorphisms $g_j:A_j\to B$, $\forall j\in I$, there is a unique homomorphism $g:\bigoplus_{i\in I} A_i\to B$, called the sum of the $g_j$, s.t. $g\alpha_j = g_j$, $\forall j$. Thus the direct sum is the \href{https://en.wikipedia.org/wiki/Coproduct}{coproduct} in the appropriate \href{https://en.wikipedia.org/wiki/Category_(mathematics)}{category}.'' -- \href{https://en.wikipedia.org/wiki/Direct_sum}{Wikipedia{\tt/}direct sum}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}mathematical structure}
``In mathematics, a structure on a set (or on some sets) refers to providing it (or them) with certain additional features (e.g. an \href{https://en.wikipedia.org/wiki/Operation_(mathematics)}{operation}, \href{https://en.wikipedia.org/wiki/Relation_(mathematics)}{relation}, \href{https://en.wikipedia.org/wiki/Metric_(mathematics)}{metric}, or \href{https://en.wikipedia.org/wiki/Topological_space}{topology}). The additional features are attached or related to the set (or to the sets), so as to provide it (or them) with some additional meaning or significance.

A partial list of possible structures are \href{https://en.wikipedia.org/wiki/Measure_theory}{measures}, \href{https://en.wikipedia.org/wiki/Algebraic_structure}{algebraic structures} (\href{https://en.wikipedia.org/wiki/Group_(mathematics)}{groups}, \href{https://en.wikipedia.org/wiki/Field_(mathematics)}{fields}, etc.), \href{https://en.wikipedia.org/wiki/Topology}{topologies}, \href{https://en.wikipedia.org/wiki/Metric_space}{metric structures} (\href{https://en.wikipedia.org/wiki/Geometry}{geometries}), \href{https://en.wikipedia.org/wiki/Order_theory}{orders}, \href{https://en.wikipedia.org/wiki/Graph_theory}{graphs}, \href{https://en.wikipedia.org/wiki/Event_structure}{events}, \href{https://en.wikipedia.org/wiki/Equivalence_relation}{equivalence relations}, \href{https://en.wikipedia.org/wiki/Differential_structure}{differential structures}, \& \href{https://en.wikipedia.org/wiki/Category_(mathematics)}{categories}.

Sometimes, a set is endowed with $> 1$ feature simultaneously, which allows mathematicians to study the interaction between the different structures more richly. E.g., an ordering imposes a rigid form, shape, or topology on the set, \& if a set has both a topology feature \& a group feature, s.t. these 2 features are related in a certain way, then the structure becomes a \href{https://en.wikipedia.org/wiki/Topological_group}{topological group}.

\href{https://en.wikipedia.org/wiki/Map_(mathematics)}{Map} between 2 sets with the same type of structure, which preserve this structure [\href{https://en.wikipedia.org/wiki/Morphism}{morphism}: structure in the \href{https://en.wikipedia.org/wiki/Domain_of_a_function}{domain} is mapped properly to the (same type) structure in the \href{https://en.wikipedia.org/wiki/Codomain}{codomain}] is of special interest in many fields of mathematics. E.g.: \href{https://en.wikipedia.org/wiki/Homomorphisms}{homomorphisms}, which preserve algebraic structures; \href{https://en.wikipedia.org/wiki/Continuous_functions}{continuous functions}, which preserve topological structures; \& \href{https://en.wikipedia.org/wiki/Differentiable_functions}{differential functions}, which preserve differential structures.

\subsubsection{History}
In 1939, the French group with the pseudonym \href{https://en.wikipedia.org/wiki/Nicolas_Bourbaki}{\sc Nicolas Bourbaki} saw structures as the root of mathematics. They 1st mentioned them in their ``Fascicule'' of {\it Theory of Sets} \& expanded it into Chap. IV of the 1957 edition. They identified 3 {\it mother structures}: algebraic, topological, \& order.

\subsubsection{Example: the real numbers}
``The set of \href{https://en.wikipedia.org/wiki/Real_number}{real numbers} has several standard structures:
\begin{itemize}
	\item An order: each number is either less than or greater than any other number.
	\item Algebraic structure: there are operations of addition \& multiplication, the 1st of which makes it into a \href{https://en.wikipedia.org/wiki/Group_theory}{group} \& the pair of which together make it into a \href{https://en.wikipedia.org/wiki/Field_(mathematics)}{field}.
	\item A measure: \href{https://en.wikipedia.org/wiki/Interval_(mathematics)}{intervals} of the real line have a specific \href{https://en.wikipedia.org/wiki/Length}{length}, which can be extended to the \href{https://en.wikipedia.org/wiki/Lebesgue_measure}{Lebesgue measure} on many of its \href{https://en.wikipedia.org/wiki/Subset}{subsets}.
	\item A metric: there is a notion of \href{https://en.wikipedia.org/wiki/Metric_(mathematics)}{distance} between points.
	\item A geometry: it is equipped with a \href{https://en.wikipedia.org/wiki/Metric_(mathematics)}{metric} \& is \href{https://en.wikipedia.org/wiki/Flat_space}{flat}.
	\item A topology: there is a notion of \href{https://en.wikipedia.org/wiki/Open_set}{open sets}.
\end{itemize}
There are interfaces among these:
\begin{itemize}
	\item Its order \&, independently, its metric structure induce its topology.
	\item Its order \& algebraic structure make it into an \href{https://en.wikipedia.org/wiki/Ordered_field}{ordered field}.
	\item Its algebraic structure \& topology make it into a \href{https://en.wikipedia.org/wiki/Lie_group}{Lie group}, a type of \href{https://en.wikipedia.org/wiki/Topological_group}{topological group}.'' -- \href{https://en.wikipedia.org/wiki/Mathematical_structure}{Wikipedia{\tt/}mathematical structure}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}numerical linear algebra}
``{\it Numerical linear algebra}, sometimes called {\it applied linear algebra}, is the study of how \href{https://en.wikipedia.org/wiki/Matrix_operation}{matrix operations} can be used to create \href{https://en.wikipedia.org/wiki/Algorithms}{compute algorithms} which \href{https://en.wikipedia.org/wiki/Algorithmic_efficiency}{efficiently} \& accurately provide approximate answers to questions in continuous mathematics. It is a subfield of numerical analysis, \& a type of linear algebra. Computers use \href{https://en.wikipedia.org/wiki/Floating-point_arithmetic}{floating-point arithmetic} \& cannot exactly represent \href{https://en.wikipedia.org/wiki/Irrational_number}{irrational} data, so when a computer is applied to a matrix of data, it can sometimes \href{https://en.wikipedia.org/wiki/Propagation_of_uncertainty}{increase the difference} between a number stored in the computer \& the true number that it is an approximation of. Numerical linear algebra uses properties of vectors \& matrices to develop computer algorithms that minimize the error introduced by the computer, \& is also concerned with ensuring that the algorithm is as efficient as possible.

Numerical linear algebra aims to solve problems of continuous mathematics using finite precise computers, so its applications to the \href{https://en.wikipedia.org/wiki/Natural_science}{natural science} \& \href{https://en.wikipedia.org/wiki/Social_science}{social sciences} are as vast as the applications of continuous mathematics. It is often a fundamental part of \href{https://en.wikipedia.org/wiki/Engineering}{engineering} \& \href{https://en.wikipedia.org/wiki/Computational_science}{computational science} problems, e.g., \href{https://en.wikipedia.org/wiki/Image_processing}{image processing} \& \href{https://en.wikipedia.org/wiki/Signal_processing}{signal processing}, \href{https://en.wikipedia.org/wiki/Telecommunication}{telecommunication}, \href{https://en.wikipedia.org/wiki/Computational_finance}{computational finance}, \href{https://en.wikipedia.org/wiki/Materials_science}{materials science} simulations, \href{https://en.wikipedia.org/wiki/Structural_biology}{structural biology}, \href{https://en.wikipedia.org/wiki/Data_mining}{data mining}, \href{https://en.wikipedia.org/wiki/Bioinformatics}{bioinformatics}, \& \href{https://en.wikipedia.org/wiki/Fluid_dynamics}{fluid dynamics}. Matrix methods are particularly used in FDMs, FEMs, \& the modeling of differential equations. Noting the broad applications of numerical linear algebra, \href{https://en.wikipedia.org/wiki/Lloyd_N._Trefethen}{\sc Lloyd N. Trefethen} \& {\sc David Bau III} argue that it is ``as fundamental to the mathematical sciences as calculus \& differential equations'', even though it is a comparatively small field. Because many properties of matrices \& vectors also apply to functions \& operators, numerical linear algebra can also be viewed as a type of functional analysis which has a particular emphasis on practical algorithms.

Common problems in numerical linear algebra include obtaining matrix decompositions like \href{https://en.wikipedia.org/wiki/Singular_value_decomposition}{singular value decomposition}, \href{https://en.wikipedia.org/wiki/QR_factorization}{QR factorization}, \href{https://en.wikipedia.org/wiki/LU_factorization}{LU factorization}, or \href{https://en.wikipedia.org/wiki/Eigendecomposition}{eigendecomposition}, which can then be used to answer common linear algebraic problems like solving linear systems of equations, locating eigenvalues, or least squares optimization. Numerical linear algebra's central concern with developing algorithms that do not introduce errors when applied to real data on a finite precision computer is often achieved by \href{https://en.wikipedia.org/wiki/Iterative}{iterative} methods rather than direct ones.

\subsubsection{History}
Numerical linear algebra was developed by computer pioneers like \href{https://en.wikipedia.org/wiki/John_von_Neumann}{\sc John von Neumann}, \href{https://en.wikipedia.org/wiki/Alan_Turing}{\sc Alan Turing}, \href{https://en.wikipedia.org/wiki/James_H._Wilkinson}{\sc James H. Wilkinson}, \href{https://en.wikipedia.org/wiki/Alston_Scott_Householder}{\sc Alston Scott Householder}, \href{https://en.wikipedia.org/wiki/George_Forsythe}{\sc George Forsythe}, \& \href{https://en.wikipedia.org/wiki/Heinz_Rutishauser}{Heinz Rutishauser}, in order to apply the earliest computers to problems in continuous mathematics, e.g. ballistics problems \& the solutions to systems of PDEs. The 1st serious attempt to minimize computer error in the application of algorithms to real data is {\sc John von Neumann} \& \href{https://en.wikipedia.org/wiki/Herman_Goldstine}{\sc Herman Goldstine}'s work in 1947. The field has grown as technology has increasingly enabled researchers to solve complex problems on extremely large high-precision matrices, \& some numerical algorithms have grown in prominence as technologies like \href{https://en.wikipedia.org/wiki/Parallel_computing}{parallel computing} have made them practical approaches to scientific problems.

\subsubsection{Matrix decompositions}
Main article: \href{https://en.wikipedia.org/wiki/Matrix_decomposition}{Wikipedia{\tt/}matrix decomposition}.
\begin{itemize}
	\item {\bf Partitioned matrices.} Main article: \href{https://en.wikipedia.org/wiki/Block_matrix}{Wikipedia{\tt/}block matrix}. For many problems in applied linear algebra, it is useful to adopt the perspectives of a matrix as being a concatenation of column vectors. E.g., when solving the linear system ${\bf x} = A^{-1}{\bf b}$, rather than understanding ${\bf x}$ as the product $A^{-1}$ with ${\bf b}$, it is helpful to think of ${\bf x}$ as the vector of \href{https://en.wikipedia.org/wiki/Coefficient}{coefficients} in the linear expansion of ${\bf b}$ in the \href{https://en.wikipedia.org/wiki/Basis_(linear_algebra)}{basis} formed by the columns of $A$. Thinking of matrices as a concatenation of columns is also a practical approach for the purposes of matrix algorithms. This is because matrix algorithms frequently contain 2 nested loops: one over the columns of a matrix $A$, \& another over the rows of $A$. E.g., for matrices $A^{m\times n}$ \& vectors $x^{n\times1},y^{m\times1}$, we could use the column partitioning perspective to compute $y\coloneqq Ax + y$ as
	\begin{verbatim}
for q = 1:n
    for p = 1:m
        y(p) = A(p,q)*x(q) + y(p)
    end
end
	\end{verbatim}
	\item {\bf Singular value decomposition.} Main article: \href{https://en.wikipedia.org/wiki/Singular_value_decomposition}{Wikipedia{\tt/}singular value decompostion}. The singular value decomposition of a matrix $A^{m\times n}$ is $A = U\Sigma V^\star$ where $U,V$ are \href{https://en.wikipedia.org/wiki/Unitary_matrix}{unitary}, \& $\Sigma$ is \href{https://en.wikipedia.org/wiki/Diagonal_matrix}{diagonal}. The diagonal entries of $\Sigma$ are called the \href{https://en.wikipedia.org/wiki/Singular_values}{singular values} of $A$. Because singular values are the square roots of the \href{https://en.wikipedia.org/wiki/Eigenvalues}{eigenvalues} of $AA^\star$, there is a tight connection between the singular value decomposition \& eigenvalue decompositions, i.e., most methods for computing the singular value decomposition are similar to eigenvalue methods; perhaps the most common method involves \href{https://en.wikipedia.org/wiki/Householder_transformation}{Householder procedures}.
	\item {\bf QR factorization.} Main article: \href{https://en.wikipedia.org/wiki/QR_decomposition}{Wikipedia{\tt/}QR decompostion}. The QR factorization of a matrix $A^{m\times n}$ is a matrix $Q^{m\times m}$ \& a matrix $R^{m\times n}$ so that $A = QR$, where $Q$ is \href{https://en.wikipedia.org/wiki/Orthogonal_matrix}{orthogonal} \& $R$ is \href{https://en.wikipedia.org/wiki/Triangular_matrix}{upper triangular}. The 2 main algorithms for computing QR factorizations are the \href{https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process}{Gram--Schmidt process} \& the \href{https://en.wikipedia.org/wiki/Householder_transformation}{Householder transformation}. The QR factorization is often used to solve \href{https://en.wikipedia.org/wiki/Linear_least-squares}{linear least-squares} problems, \& eigenvalue problems (by way of the iterative \href{https://en.wikipedia.org/wiki/QR_algorithm}{QR algorithm}).
	\item {\bf LU factorization.} Main article: \href{https://en.wikipedia.org/wiki/LU_decomposition}{Wikipedia{\tt/}LU decomposition}. An LU factorization of a matrix $A$ consists of a lower triangular matrix $L$ \& an upper triangular matrix $U$ so that $A = LU$. The matrix $U$ is found by an upper triangularization procedure which involves left-multiplying $A$ by a series of matrices $M_1,\ldots,M_{n-1}$ to form the product $M_{n-1}\cdots M_1A = U$, so that equivalently $L = M_1^{-1}\cdots M_{n-1}^{-1}$.
	\item {\bf Eigenvalue decomposition.} Main article: \href{https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix}{Wikipedia{\tt/}eigendecomposition of a matrix}. The eigenvalue decomposition of a matrix $A^{m\times m}$ is $A = X\Lambda X^{-1}$, where the columns of $X$ are the eigenvectors of $A$, \& $\Lambda$ is a diagonal matrix the diagonal entries of which are the corresponding eigenvalues of $A$. There is no direct method for finding the eigenvalue decomposition of an arbitrary matrix. Because it is not possible to write a program that finds the exact roots of an arbitrary polynomial in finite time, any general eigenvalue solver must necessarily be iterative.
\end{itemize}

\subsubsection{Algorithms}

\begin{enumerate}
	\item {\bf Gaussian elimination.}
	\item {\bf Solutions of linear systems.}
	\item {\bf Least squares optimization.}
\end{enumerate}

\subsubsection{Conditioning \& stability}

\subsubsection{Iterative methods}

\subsubsection{Software}
Main article: \href{https://en.wikipedia.org/wiki/List_of_numerical_analysis_software}{Wikipedia{\tt/}list of numerical analysis software}. Several programming languages use numerical linear algebra optimization techniques \& are designed to implement numerical linear algebra algorithms. These languages include \href{https://en.wikipedia.org/wiki/MATLAB}{MATLAB}, \href{https://en.wikipedia.org/wiki/Analytica_(software)}{Analytica}, \href{https://en.wikipedia.org/wiki/Maple_(software)}{Maple}, \& \href{https://en.wikipedia.org/wiki/Mathematica}{Mathematica}. Other programming languages which are not explicitly designed for numerical linear algebra have libraries that provide numerical linear algebra routines \& optimization; C \& Fortran have packages like \href{https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms}{Basic Linear Algebra Subprograms} \& \href{https://en.wikipedia.org/wiki/LAPACK}{LAPACK}, Python has the library \href{https://en.wikipedia.org/wiki/NumPy}{NumPy}, \& \href{https://en.wikipedia.org/wiki/Perl}{Perl} has the \href{https://en.wikipedia.org/wiki/Perl_Data_Language}{Perl Data Language}. Many numerical linear algebra commands in R rely on these more fundamental libraries like LAPACK. More libraries can be found on the \href{https://en.wikipedia.org/wiki/List_of_numerical_libraries}{list of numerical libraries}.'' -- \href{https://en.wikipedia.org/wiki/Numerical_linear_algebra}{Wikipedia{\tt/}numerical linear algebra}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}