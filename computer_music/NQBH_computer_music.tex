\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
	\mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Computer Music -- Âm Nhạc Máy Tính}
\author{Nguyễn Quản Bá Hồng\footnote{A Scientist {\it\&} Creative Artist Wannabe. E-mail: {\tt nguyenquanbahong@gmail.com}. Bến Tre City, Việt Nam.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:
	
	{\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.
	
	Latest version:
	\begin{itemize}
		\item {\it Computer Music -- Âm Nhạc Máy Tính}.
		
		PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/computer_music/NQBH_computer_music.pdf}.
		
		\TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/computer_music/NQBH_computer_music.tex}.
		\item {\it }.
		
		PDF: {\sc url}: \url{.pdf}.
		
		\TeX: {\sc url}: \url{.tex}.
	\end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Basic Computer Music}

\subsection{{\sc Renato Fabbri, Vilson Vieira de Silva Junior, Ant\^onio Carlos Silvano Pessotti, D\'ebora Cristina Corr\^ea, Osvaldo N. Oliveira Jr.} Musical Elements in Discrete-Time Representation of Sound}
{\sf[2 citations]}
\begin{itemize}
	\item {\sf Abstract.} Representation of basic elements of music in terms of discrete audio signals is often used in software for musical creation \& design. Nevertheless, there is no unified approach that relates these elements to discrete samples of digitized sound. In this article, each musical element is related by equations \& algorithms to discrete-time samples of sounds, \& each of these relations are implemented in scripts within a software toolbox, referred to as MASS (Music \& Audio in Sample Sequences). Fundamental element, musical note with duration, volume, pitch, \& timbre, is related quantitatively to characteristics of digital signal. Internal variations of a note, e.g. tremolos, vibratos, \& spectral fluctuations, are also considered, which enables synthesis of notes inspired by real instruments \& new sonorities. With this representation of notes, resources are provided for generation of higher scale musical structures, e.g. rhythmic meter, pitch intervals \& cycles. This framework enables precise \& trustful scientific experiments, data sonification \& is useful for education \& art. Efficacy of MASS is confirmed by synthesis of small musical pieces using basic notes, elaborated notes \& notes in music, which reflects organization of toolbox \& thus of this article. Possible to synthesize whole albums through collage of scripts \& settings specified by user. With open source paradigm, toolbox can promptly scrutinized, expanded in co-authorship processes \& used with freedom by musicians, engineers, \& other interested parties. In fact, MASS has already been employed for diverse purposes which include music production, artistic presentations, psychoacoustic experiments \& computer language diffusion where appeal of audiovisual artifacts is exploited for education.
	\item CCS Concepts: Applied computing $\to$ Sound \& music computing. Computing methodologies $\to$ Modeling methodologies. General \& reference $\to$ Surveys \& overviews, Reference works.
	\item Additional Key Words \& Phrases: music, acoustics, psychophysics, digital audio, signal processing.
	\item {\sf1. Introduction.} Music is usually defined as art whose medium is sound. Definition might also state: medium includes silences \& temporal organization of structures, or music is also a cultural activity or product. In physics \& in this document, sounds are longitudinal waves of mechanical pressure. Human auditory system perceives sounds in frequency bandwidth between 20Hz \& 20kHz, with actual boundaries depending on person, climate conditions \& sonic characteristics themselves. Since speed of sound $\approx343.2$ m{\tt/}s, such frequency limits corresponds to wavelengths of $\frac{343.2}{20}\approx17.6$ m \& $\frac{343.2}{20000}\approx17.16$ mm. Hearing involves stimuli in bones, stomach, ears, transfer functions of head \& torso (thân mình), \& processing by nervous system. Ear is a dedicated organ or appreciation of these waves, which decomposes them into their sinusoidal spectra \& delivers to nervous system. Sinusoidal components are crucial to musical phenomena, as one can recognize in constitution of sounds of musical interest (e.g. harmonic sounds \& noises, discussed in Sects. 2--3), \& higher level musical structures (e.g. tunings, scales, \& chords, Sect. 4) [55]
	
	Representation of sound can take many forms, from musical scores \& texts in a phonetic language to electric analog signals \& binary data. It includes sets of features e.g. wavelet or sinusoidal components. Although terms `audio' \& `sound' are often used without distinction \& `audio' has many definitions which depend on context \& author, audio most often means a representation of amplitude through time. In this sense, audio expresses sonic waves yield by synthesis or input by microphones, although these sources are not always neatly distinguishable e.g. as captured sounds are processed to generate new sonorities (âm thanh). Digital audio protocols often imply in quality loss (to achieve smaller files, ease storage \& transfer) \& are called {\it lossy} [47]. This is case e.g. of MP3 \& Ogg Vorbis. Non-lossy representations of digital audio, called {\it lossless} protocols or formats, on other hand, assures perfect reconstruction of analog wave within any convenient precision. Standard paradigm of lossless audio consists of representing sound with samples equally spaced by a duration $\delta_s$, \& specifying amplitude of each sample by a fixed number of bits. This is linear Pulse Code Modulation (LPCM) representation of sound, herein referred to as PCM. A PCM audio format has 2 essential attributes: a sampling frequency $f_s = \frac{1}{\delta_s}$ (also called e.g. sampling rate or sample rate), which is number of samples used for representing a second of sound; \& a bit depth, which is number of bits used for specifying amplitude of each sample. {\sf Fig. 1. Example of PCM audio: a sound wave is represented by 25 samples equally spaced in time where each sample has an amplitude specified with 4 bits.} shows 25 samples of a PCM audio with a bit depth of 4, which yields $2^4 = 16$ possible values for amplitude of each sample \& a total of $4\cdot25 = 100$ bits for representing whole sound.
	
	Fixed sampling frequency \& bit depth yield quantization error or quantization noise. This noise diminishes as bit depth increases while greater sampling frequency allows higher frequencies to be represented. Nyquist theorem asserts: sampling frequency is twice maximum frequency: represented signal can contain [49]. Thus, for general musical purposes, suitable to use a sample rate of at least twice highest frequency heard by humans, i.e., $f_s\ge2\cdot20$ kHz $= 40$ kHz. This is basic reason for adoption of sampling frequencies e.g. 44.1 kHz \& 48 kHz, which are standards in Compact Disks (CD) \& broadcast systems (radio \& television), resp.
	
	Within this framework for representing sounds, musical notes can be characterized. Note often stands as `fundamental unit' of musical structures (e.g. atoms in matter or cells in macroscopic organisms) \&, in practice, it can unfold into sounds that uphold other approaches to music. This is of capital importance because science \& scholastic artists widened traditional comprehension of music in 20th century to encompass discourse without explicit rhythm, melody or harmony. This is evident, e.g., in concrete, electronic, electroacoustic, \& spectral musical styles. In 1990s, it became evident: popular (commercial) music had also incorporated sound amalgrams \& abstract discursive arcs. [There are well known incidences of such characteristics in ethnic music, e.g. in Pygmy music, but western theory assimilated them only in last century [74].] Notes are also convenient for another reason: average listener -- \& a considerable part of specialists -- presupposes rhythmic \& pitch organization (made explicit in Sect. 4) as fundamental musical properties, \& these are developed in traditional musical theory in terms of notes. Thereafter, in this article describe musical notes in PCM audio through equations \& then indicate mechanisms for deriving higher level musical structures. Understand: this is not unique approach to mathematically express music in digital audio, but musical theory \& practice suggest: this is a proper framework for understanding \& making computer music, as should become patent in reminder of this text \& is verifiable by usage of MASS toolbox. Hopefully, interested reader or programmer will be able to use this framework to synthesize music beyond traditional conceptualizations when intended.
	
	This document provides a fundamental description of musical structures in discrete-time audio. Results include mathematical relations, usually in terms of musical characteristics \& PCM samples, concise musical theory considerations, \& their implementation as software routines both as very raw \& straightforward algorithms \& in context of rendering musical pieces. Despite general interests involved, there are only a few books \& computer implementations that tackle subject directly. These mainly focus on computer implementations \& way to mimic traditional instruments, with scattered mathematical formalisms for basic notations. Articles on topic appear to be lacking, to best of our knowledge, although advanced \& specialized developments are often reported. A compilation of such works \& their contributions is in Appendix G of [21]. Although current music software uses analytical descriptions presented here, there is no concise mathematical description of them, \& far from trivial to achieve equations by analyzing available software implementations.
	
	Accordingly, objectives of this paper:
	\begin{enumerate}
		\item Present a concise set of mathematical \& algorithmic relations between basic musical elements \& sequences of PCM audio samples.
		\item Introduce a framework for sound \& musical synthesis with control at sample level which entails potential uses in psychoacoustic experiments, data sonification \& synthesis with extreme precision (recap in Sect. 5).
		\item Provide a powerful theoretical framework which can be used to synthesize musical pieces \& albums.
		\item Provide approachability to developed framework [All analytic relations presented in this article are implemented as small scripts in public domain. They constitute MASS toolbox, available in an open source Git repository [9]. These routines are written in Python \& make use of Numpy, which performs numerical routines efficiently (e.g. through LAPACK), but language \& packages are by no means mandatory. Part of scripts has been ported to JavaScript (which favors their use in Web browsers e.g. Firefox \& Chromium) \& native Python [48, 56, 70]. These are all open technologies, published using licenses that grant permission for copying, distributing, modifying \& usage in research, development, art \& education. Hence, work presented here aims at being compliant with recommended practices for availability \& validation \& should ease co-authorship processes [43, 52].]
		\item Provide a didactic (mang tính giáo huấn) presentation of content, which is highly multidisciplinary, involving signal processing, music, psychoacoustics \& programming.
	\end{enumerate}
	Reminder of this article is organized as follows: Sect. 2 characterizes basic musical note; Sect. 3 develops internal dynamics of musical notes; Sect. 4 tackles organization of musical notes into higher level musical structures [14, 41, 42, 54, 62, 72, 74, 76]. As these descriptions require knowledge on topics e.g. psychoacoustics, cultural traditions, \& mathematical formalisms, text points to external complements as needed \& presents methods, results, \& discussions altogether. Sect. 5 is dedicated to final considerations \& further work.	
	\begin{itemize}
		\item {\sf1.1. Additional material.} 1 Supporting Information document [27] holds commented listings of all equations, figures, tables, \& sects in this document \& scripts in MASS toolbox. Another Supporting Information document [28] is a PDF version of code that implements equations \& concepts in each sect [Toolbox contains a collection of Python scripts which
		\begin{itemize}
			\item implements each of equations
			\item render music \& illustrate concepts
			\item render each of figures used in this article.
		\end{itemize}
		Documentation of toolbox consists of this article, Supporting Information documents \& scripts themselves.]. Git repository [26] holds all PDF documents \& Python scripts. Rendered musical pieces are referenced when convenient \& linked directly through URLs, \& constitute another component of framework. They are not very traditional, which facilitates understanding of specific techniques \& extrapolation of note concept. There are MASS-based software packages [23, 25] \& further musical pieces that are linked in Git repository.
		\item {\sf1.2. Synonymy, polysemy \& theoretical frames (disclaimer).} Given: main topic of this article (expression of musical elements in PCM audio) is multidisciplinary \& involves art, reader should be aware: much of vocabulary admits different choices of terms \& defs. More specifically, often case where many words can express same concept \& where 1 word can carry different meanings. This is a very deep issue which might receive a dedicated manuscript. Reader might need to read rest of this document to understand this small selection of synonymy \& polysemy (đa nghĩa) in literature, but important to illustrate point before more dense sects:
		\begin{itemize}
			\item a ``note'' can mean a pitch or an abstract construct with pitch \& duration or a sound emitted from a musical instrument or a specific note in a score or a music.
			\item Sampling rate is also called {\it sampling frequency} or {\it sample rate}.
			\item A harmonic in a sound is most often a sinusoidal component which is in harmonic series of fundamental frequency. Many times, however, terms harmonic \& component are not distinguished. A harmonic can also be a note performed in an instrument by preventing certain overtones (components).
			\item Harmony can refer to chords or to note sets related to chords or even to ``harmony'' in a more general sense, as a kind of balance \& consistency.
			\item A ``tremolo'' can mean different things: e.g. in a piano score, a tremolo is a fast alternation of 2 notes (pitches) while in computer music theory it is (most often) an oscillation of loudness.
		\end{itemize}
		Strived to avoid nomenclature clashes \& use of more terms than needed. Also, there are many theoretical standpoints for understanding musical phenomena, which is an evidence: most often there is not a single way to express or characterize musical structures. Therefore, in this article, adjectives e.g. ``often'', ``commonly'', \& ``frequently'' are abundant \& they would probably be even more numerous if wanted to be pedantically precise. Some of these issues are exposed when content is convenient, e.g. in 1st considerations of timbre.
		
		-- Cố gắng tránh xung đột danh pháp \& sử dụng nhiều thuật ngữ hơn mức cần thiết. Ngoài ra, có nhiều quan điểm lý thuyết để hiểu các hiện tượng âm nhạc, đây là bằng chứng: thường không có 1 cách duy nhất để diễn đạt hoặc mô tả các cấu trúc âm nhạc. Do đó, trong bài viết này, các tính từ như ``thường xuyên'', ``thường xuyên'', \& ``thường xuyên'' rất nhiều \& chúng có thể còn nhiều hơn nữa nếu muốn chính xác về mặt học thuật. Một số vấn đề này được nêu ra khi nội dung thuận tiện, ví dụ như trong những cân nhắc đầu tiên về âm sắc.
	\end{itemize}	
	\item {\sf2. Characterization of musical note in discrete-time audio.} In diverse artistic \& theoretical contexts, music is conceived as constituted by fundamental units referred to as notes, ``atoms'' that constitute music itself [44, 72, 74]. In a cognitive perspective, notes are understood as discernible elements that facilitate \& enrich transmission of information through music [41, 55]. Canonically, basic characteristics of a musical note are duration, loudness, pitch, \& timbre (âm sắc) [41]. All relations described in this sect are implemented in file {\tt src/sections/eqs2.1.py}. Musical pieces {\it5 sonic portraits \& reduced-fi} are also available online to corroborate \& illustrate concepts.
	\begin{itemize}
		\item {\sf2.1. Duration.} Sample frequency $f_s$ is defined as number of samples in each sec of discrete-time signal. Let $T = \{t_i\}$ be an ordered set of real samples separated by $\delta_s = \frac{1}{f_s}$ secs ($f_s = 44.1$ kHz $\Rightarrow\delta_s = \frac{1}{44100}\approx0.023$ ms). A musical note of duration $\Delta$ secs can be expressed as a sequence $T^\Delta$ with $\Lambda = \lfloor\Delta\cdot f_s\rfloor$ samples. I.e., integer part of multiplication is considered, \& an error of $\le\delta_s$ missing secs is admitted, which is usually fine for musical purposes. Thus
		\begin{equation*}
			T^\Delta = \{t_i\}_{i=0}^{\lfloor\Delta f_s\rfloor - 1} = \{t_i\}_0^{\Lambda - 1}.
		\end{equation*}
		\item {\sf2.2. Loudness.} Loudness [Loudness \& ``volume'' are often used indistinctly. In technical contexts, loudness is used for subjective perception of sound intensity while volume might be used for some measurement of loudness or to a change in intensity of signal by equipment. Accordingly, one can perceive a sound as loud or soft \& change volume by turning a knob. Will use term loudness \& avoid more ambiguous term volume.] is a perception of sonic intensity that depends on reverberation, spectrum, \& other characteristics described in Sect. 3 [11]. One can achieve loudness variations through power of wave [11]:
		\begin{equation*}
			{\rm pow}(T) = \frac{\sum_{i=0}^{\Lambda - 1} t_i^2}{\Lambda}.
		\end{equation*}
		Final loudness is dependent on amplification of signal by speakers. Thus, what matters: relative power of a note in relation to the others around it, or power of a musical sect in relation to the rest. Differences in loudness are result of complex psychophysical phenomena but can often be reasoned about in terms of decibels, calculated directly from amplitudes through energy or power:
		\begin{equation*}
			V_{\rm dB} = 10\log_{10} \frac{{\rm pow}(T')}{{\rm pow}(T)}.
		\end{equation*}
		\item {\sf2.3. Pitch.}
		\item {\sf2.4. Timbre.}
		\item {\sf2.5. Spectra of sampled sounds.}
		\item {\sf2.6. Basic note.}
		\item {\sf2.7. Spatialization: localization \& reverberation.}
		\item {\sf2.8. Musical usages.}
	\end{itemize}
	\item {\sf3. Variation in Basic Note.}
	\begin{itemize}
		\item {\sf3.1. Lookup table.}
		\item {\sf3.2. Incremental variations of frequency \& intensity.}
		\item {\sf3.3. Application of digital filters.}
		\item {\sf3.4. Noise.}
		\item {\sf3.5. Tremolo \& vibrato, AM \& FM.}
		\item {\sf3.6. Musical usages.}
	\end{itemize}
	\item {\sf4. Organization of notes in music.}
	\begin{itemize}
		\item {\sf4.1. Tuning, intervals, scales, \& chords.}
		\item {\sf4.2. Atonal \& tonal harmonies, harmonic expansion \& modulation.}
		\item {\sf4.3. Counterpoint.}
		\item {\sf4.4. Rhythm.}
		\item {\sf4.5. Repetition \& variation: motifs \& larger units.}
		\item {\sf4.6. Directional structures.}
		\item {\sf4.7. Cyclic structures.}
		\item {\sf4.8. Serialism \& post-serial techniques.}
		\item {\sf4.9. Musical idiom?}
		\item {\sf4.10. Musical usages.}
	\end{itemize}
	\item {\sf5. Conclusions \& Further Developments.}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{\cite{Horn_West_Roberts2022}. {\sc Michael S. Horn, Melanie West, Cameron Roberts}. Introduction to Digital Music with Python Programming: Learning Music with Code}
{\sf[4 Amazon ratings]}
\begin{itemize}
	\item {\sf Amazon reviews.} {\it Introduction to Digital Music with Python Programming} provides a foundation in music \& code for beginner. It shows how coding empowers new forms of creative expression while simplifying \& automating many of tedious aspects of production \& composition.
	
	With help of online, interactive examples, this book covers fundamentals of rhythm, chord structure, \& melodic composition alongside basics of digital production. Each new concept is anchored in a real-world musical example that will have you making beats in a matter of minutes.
	
	Music is also a great way to learn core programming concepts e.g. loops, variables, lists, \& functions, {\it Introduction to Digital Music with Python Programming} is designed for beginners of all backgrounds, including high school students, undergraduates, \& aspiring professionals, \& requires no prev experience with music or code.
	
	A beginner's approach to digital music production focuses on key concepts, ensuring ease \& progress in learning.
	
	Streamline your programming education by incorporating music, making complex core concepts easier to grasp \& apply.
	
	Amplify your music creativity by generating unique beats with code in minutes, without needing advanced technical skills.
	
	A great book for learning Python programming \& exploring digital music.
	
	This broad manual combines music theory \& programming basics, providing interactive examples \& real-world applications to help you compose \& produce music from scratch.
	
	Perfect for aspiring musicians \& programmers exploring music-code fusion.
	\item {\sf About Author.} {\sc Michael S. Horn} is Associate Prof of CS \& Learning Sciences at Northwestern University in Evanston, Illinois, where he directs Tangible Interaction Design \& Learning (TIDAL) Lab.
	
	{\sc Melanie West} is a PhD student in Learning Sciences at Northwestern University \& co-founder of Tiz Media Foundation, a nonprofit dedicated to empowering underrepresented youth through science, technology, engineering, \& mathematics (STEM) programs.
	
	{\sc Cameron Roberts} is a software developer \& musician living in Chicago. He holds degrees from Northwestern University in Music Performance \& CS.
	\item {\sf Foreword.} When I was a kid growing up in Texas, I ``learned'' how to play viola. I put {\it learned} in quotes because it was really just a process of rote memorization -- hours \& hours of playing same songs over \& over again. I learned how to read sheet music, bu only to extent that I knew note names \& could translate them into grossest of physical movements. I never learned to read music as literature, to understand its deeper meaning, structure, or historical context. I never understood anything about music theory beyond being annoyed that I had to pay attention to accidentals in different keys. I never composed {\it anything}, not even informally scratching out a tune. I never developed habits of deep listening, of taking songs apart in my head \& puzzling over how they were put together in 1st place. I never played just for fun. \&, despite best intentions of parents \& teachers, I never fell in love with music.
	
	Learning how to code was complete opposite experience for me. I was largely self-taught. Courses I took in school were electives (môn tự chọn) that I chose for myself. Teachers gave me important scaffolding at just right times, but it never felt forced. I spent hours working on games or other projects (probably when I should have been practicing viola). I drew artwork, planned out algorithms, \& even synthesized my own rudimentary sound effects (hiệu ứng âm thanh thô sơ). I had no idea what I was doing, but that was liberating. No one was around to point out my mistakes or to show me ow to do things ``right'' way (at least, not until college). I learned how to figure things out for myself, \& skills I picked up from those experiences are still relevant today. I fell in love with coding. [I was also fortunate to have grown up in a time \& place where these activities were seen as socially acceptable for a person of my background \& identity.]
	
	But I know many people whose stories are flipped 180 degrees. For them, music was so personally, socially, \& culturally motivating that they couldn't get enough. They'd practice for hours \& hours, not just for fun but for something \fbox{much deeper}. For some it was an instrument like guitar that got them started. For others it was an app like GarageBand that gave them a playful entry point into musical ideas. To extent that they had coding experiences, those experiences ranged from uninspiring to off-putting (từ không hấp dẫn đến khó chịu). It's not that they necessarily hated coding, but it was something they saw as not being for them.
	
	In foreword of his book, {\it Mindstorms: Children, Computers, \& Powerful Ideas}, {\sc Seymour Papert} wrote: ``fell in love with gears'' as a way of helping us imagine a future in which children (like me) would fall in love with computer programming, not for its own sake, but for creative worlds \& powerful ideas that programming could open up. Part of what he was saying was: love \& learning go hand in hand, \& that computers could be an entry point into many creative \& artistic domains e.g. mathematics \& music. Coding can revitalize subjects that have become painfully rote in schools.
	
	Process of developing TunePad over past several years has been a fascinating rediscovery of musical ideas for me. Code has given me a different kind of language for thinking about things like rhythm, chords, \& harmony. I can experiment with composition unencumbered by my maladroit hands. Music has become something creative \& alive in a way that it never was for me before. Music theory is no longer a thicket of confusing terminology \& instead has become a fascinating world of mathematical beauty that structures creative process.
	
	-- Quá trình phát triển TunePad trong nhiều năm qua là 1 sự khám phá lại đầy hấp dẫn đối với tôi về các ý tưởng âm nhạc. Mã đã cho tôi 1 loại ngôn ngữ khác để suy nghĩ về những thứ như nhịp điệu, hợp âm, \& sự hòa âm. Tôi có thể thử nghiệm sáng tác mà không bị cản trở bởi đôi tay vụng về của mình. Âm nhạc đã trở thành 1 thứ gì đó sáng tạo \& sống động theo cách mà trước đây tôi chưa từng có. Lý thuyết âm nhạc không còn là 1 mớ thuật ngữ khó hiểu \& thay vào đó đã trở thành 1 thế giới hấp dẫn của vẻ đẹp toán học cấu trúc nên quá trình sáng tạo.
	
	{\sc Melanie, Cameron}, \& I hope: this book gives a similarly joyful learning experience with music \& code. Hope: feel empowered to explore algorithmic \& mathematical beauty of music. Hope: discover, as we have: music \& code reinforce one another in surprising \& powerful ways that open new creative opportunities for you. Hope, regardless of your starting point -- as a coder, as a musician, as neither, as both -- will discover something new about yourself \& what you can become.
	\item {\sf1. Why music \& coding?} This book is designed for people who {\it love} music \& are interested in intersection of music \& coding. Maybe you're an aspiring musician or music producer who wants to know more about coding \& what it can do. Or maybe already know a little about coding, \& want to expand your creative musical horizon. Or maybe a total beginner in both. Regardless of your starting point, this book is designed for you to learn about music \& coding as mutually reinforcing skills. Code gives us an elegant language to think about musical ideas, \& music gives us a context within which code makes sense \& is immediately useful. Together they form a powerful new way to create music that will be interconnected with digital production tools of future.
	
	More \& more code will be used to produce music, to compose music, \& even to perform music for live audiences. Digital production tools e.g. Logic, Reason, Pro Tools, FL Studio, \& Ableton Live are complex software applications created with {\it millions} of lines of code written by huge teams of software engineers. With all of these tools can write code to create custom plugins \& effects. Beyond production tools, live coding is an emerging from of musical performance art in which Information Age DJs write computer code to generate music in real time for live audiences.
	
	In other ways, still on cusp of a radical transformation in way use code to create music. History of innovation in music has always been entwined with innovations in technology. Whether talking about {\sc Franz Liszt} in 19th century, who pioneered persona of modern music virtuoso based on technological breakthroughs of piano [Fans were so infatuated with {\sc Liszt}'s piano ``rockstar'' status that they fought over his silk handkerchiefs \& velvet gloves at his performances.], or DJ {\sc Kool Herc} in 20th century, who pioneered hip-hop with 2 turntables \& a crate full of funk records in Bronx, technologies have created new opportunities for musical expression that have challenged status quo \& given birth to new genres. Don't have {\sc Franz Liszt} or DJ {\sc Kool Herc} of coding yet, but it's only a matter of time before coding virtuosos of tomorrow expand boundaries of what's possible in musical composition, production, \& performance.
	\begin{itemize}
		\item {\sf1.1. What is Python?} In this book learn how to create your own digital music using a computer programming language called {\it Python}. If not familiar with programming languages, Python is a general-purpose language 1st released in 1990s that is now 1 of most widely used languages in world. Python is designed to be easy to read \& write, which makes it a popular choice for beginners. Also fully featured \& powerful, making it a good choice for professionals working in fields as diverse as DS, web development, arts, \& video game development. Because Python has been around for decades, it runs on every major computer OS. Examples in this book even use a version of Python that runs directly inside of your web browser without need for any special software installation.
		
		Unlike many other common beginner programming languages, Python is ``text-based'', i.e., type code into an editor instead of dragging code blocks on computer screen. This makes Python a little harder to learn than other beginner languages, but it also greatly expands what you can do. By time yet through this book should feel comfortable writing short Python programs \& have conceptual tools need to explore more on your own.
		\item {\sf1.2. What this book is {\it not}.} Before get into a concrete example of what you can do with a little bit of code, just a quick note about what this book is {\it not}. This book is not a comprehensive guide to Python programming. There are many excellent books \& tutorials designed for beginners, several of which are free. [Recommend \url{https://www.w3schools.com/python/}.]
		
		This book is also not a comprehensive guide to music theory or Western music notation. Get into core ideas behind rhythm, harmony, melody, \& composition, but there are, again, many other resources available for beginners who want to go deeper. What offering is a different approach that combines learning music with learning code in equal measure.
		\item {\sf1.3. What this book {\it is}.} What will do is give an intuitive understanding of fundamental concepts behind both music \& coding. Code \& music are highly technical skills, full of arcane symbols \& terminology, seem almost designed to intimidate beginners. In this book put core concepts to use immediately to start making music. Get to play with ideas at your own pace \& get instant feedback as bring ideas to life. Skip most of technical jargon \& minutiae for now -- can come later. Instead, focus on developing your confidence \& understanding. Importantly, skills, tools, \& ways of thinking introduce in this book will be broadly applicable in many other areas as well. Working in Python code, but core structures of variables, functions, loops, conditional logical, \& classes are same across many programming languages including JavaScript, Java, C, C++, \& C\#. After learn 1 programming language, each additional language is that much easier to pick up.
		\item {\sf1.4. TunePad \& EarSketch.} This book uses 2 free online platforms that combine music \& Python coding. 1st, called TunePad \url{https://tunepad.com}, was developed by a team of researchers at Northwestern University in Chicago. TunePad lets create short musical loops that you can layer together using a simple digital audio workstation (DAW) interface. 2nd platform, called EarSketch \url{https://earsketch.gatech.edu}, was created by researchers at Georgia Tech in Atlanta. EarSketch uses Python code to arrange samples \& loops into full-length compositions. Both platforms are browser-based apps, so all need to get started is a computer (tablets or Chromebooks are fine), an internet connection, \& a web browser like Chrome or Firefox. External speakers or headphones are also nice but not required. Both platforms have been around for years \& have been used by many thousands of students from middle school all way up to college \& beyond. TunePad \& EarSketch are designed primarily as learning platforms, but there are easy ways to export your work to professional production software if want to go further.
		\item {\sf1.5. A quick example.} A quick example of what coding in Python looks like. This program runs in TunePad to create a simple beat pattern, variants of which have been used in literally thousands of songs e.g. {\it Blinding Lights} by The Weeknd \& {\it Roses} by SAINt JHN.
		\begin{verbatim}
			playNote(1) # play a kick drum sound
			playNote(2) # play a snare drum sound
			playNote(1)
			playNote(2)
			rewind(4)   # rewind 4 beats
			for i in range(4):
			    rest(0.5)
			    playNote(4, beats = 0.5) # play hat for a half beat
		\end{verbatim}
		These 8 lines of Python code tell TunePad to play a pattern of kick drums, snare drums, \& high-hats. Most of lines are {\it playNote} instructions, \& those instructions tell TunePad to play musical sounds indicated by numbers inside of parentheses. This example also includes sth called a {\it loop}. Loop is an easy way to repeat a set of actions over \& over again. In this case, loop tells Python to repeat lines 7 \& 8 4 times in a row. Screenshot {\sf Fig. 1.1: A TunePad program to play a simple rock beat.} shows what this looks like in TunePad. Can try out example for yourself with this link: \url{https://tunepad.com/examples/roses}.
		\item {\sf1.6. 5 reasons to learn code.} Now seen a brief example of what can do with a few lines of Python code, here are top 5 reasons to get started with programming \& music if still in doubts.
		\begin{itemize}
			\item {\sf1.6.1. Reason 1: Like it or note, music is already defined by code.} Looking across modern musical landscape, clear: music is already defined by code. 1 of biggest common factors of almost all modern music from any popular genre: {\it everything} is edited, if not created entirely, with sophisticated computer software. Hard to overstate how profoundly such software has shaped sound of music in 21st century. Relatively inexpensive DAW applications \& myriad ubiquitous plugins that work across platforms have had a disruptive \& democratizing effect across music industry. Think about effects plugins like autotune, reverb, or ability to change pitch of a sample without changing tempo. These effects are all generated with sophisticated software. Production studios size of small offices containing hundreds of thousands of dollars' worth of equipment now fit on screen of a laptop computer available to any aspiring producer with passion, a WiFi connection, \& a small budget. Reasons behind shift to digital production tools are obvious. Computers have gotten to a point where they are cheap enough, fast enough, \& capacious enough to do real-time audio editing. Can convert sound waves into editable digital information with microsecond precision \& then hear effects of our changes in real time. These DAWs didn't just appear out of nowhere. They were constructed by huge teams of software engineers writing code -- millions of lines of it. E.g., TunePad was created with $> 1.5$ million lines of code written in over a dozen computer languages e.g. Python, HTML, JavaScript, CSS, \& Dart. Regardless of how feel about digital nature of modern music, not going away. Learning to code will help understand a little more about how all of this works under hood. More to point, it's increasingly common for producers to write their own code to manipulate sound. E.g., in Logic, can write JavaScript code to process incoming MIDI (Musical Instrument Digital Interface) data to do things like create custom arpeggiators. Learning to code can give you more control \& help expand your creative potential {\sf Fig. 1.2: Typical DAW software}.
			\item {\sf1.6.2. Reason 2: Code is a powerful way to make music.} Don't always think about it this way, but music is {\it algorithmic} in nature -- it's full of mathematical relationships, logical structure, \& recursive patterns. Beauty of Baroque fugue is in part a reflection of beauty of mathematical \& computational ideas behind music. Call Bach a genius not just because his music is so compelling, but also because he was able to hold complex algorithms in his mind \& then transcribe them to paper using representation system called Western music notation. I.e., music notation is a language for recording output of composition process, but not a language for capturing algorithmic nature of composition process itself.
			
			Code, on other hand, is a language specifically designed to capture mathematical relationships, logical structure, \& recursive patterns. E.g., take stuttered hi-hat patterns that are 1 of defining characteristics of trap music. Here are a few lines of Python code that generate randomized hi-hat stutters that can bring an otherwise conventional beat to life with sparking energy.
			\begin{verbatim}
				for _ in range(16):
				    if randint(6) > 1: # roll die for a random number
				        playNote(4, beats=0.5) # play an 8th note
				    else:
				        playNote(4, beats=0.25) # or play 16th notes
				        playNote(4, beats=0.25)
			\end{verbatim}
			Or, as another example, here's a 2-line Python program that plays a snare drum riser effect common in house, EDM, or pop music. Often hear this technique right before beat drops. This code uses a decay function so that each successive note is a little shorter resulting in a gradual acceleration effect.
			\begin{verbatim}
				for i in range(50): # play 50 snares
				    playNote(2, beats = pow(2, -0.09 * i))
			\end{verbatim}
			What's cool about these effects: they're {\it parametrized}. Because code describes algorithms to generate music, \& not music itself, i.e., can create infinite variation by adjusting numbers involved. E.g., in trap hi-hat code, can easily play around with how frequently stuttered hats are inserted into pattern by increasing or decreasing 1 number. Can think of code as sth like a power drill; can swap out different bits to make holes of different sizes. Drill bits are like parameters that change what tool does in each specific instance. In same way, algorithms are vastly more general-purpose tools that can accomplish myriad tasks by changing input parameters.
			
			Creating a snare drum riser with code is obviously a very different kind of thing than picking up 2 drumsticks \& banging out a pattern on a real drum. \&, to be clear, not advocating for code to replace learning how to perform with live musical instruments. But, code can be another tool in your musical repertoire for generating repetitive patterns, exploring mathematical ideas, or playing sequences that are too fast or intricate to play by hand.
			
			-- Tạo 1 bộ phận nâng cao trống snare bằng mã rõ ràng là 1 việc rất khác so với việc nhặt 2 dùi trống \& đánh 1 mẫu trên 1 chiếc trống thật. \&, nói rõ hơn, không ủng hộ việc sử dụng mã để thay thế việc học cách biểu diễn với các nhạc cụ sống. Nhưng mã có thể là 1 công cụ khác trong tiết mục âm nhạc của bạn để tạo ra các mẫu lặp lại, khám phá các ý tưởng toán học hoặc chơi các chuỗi quá nhanh hoặc phức tạp để chơi bằng tay.
			\item {\sf1.6.3. Reason 3: Code lets you build your own musical toolkit.} Becoming a professional in any field is about developing expertise with tools -- acquiring equipment \& knowing how to use it. Clearly, this is true in music industry, but also true in software. Professional software engineers acquire specialized equipment \& software packages. Develop expertise in a range of programming languages \& technical frameworks. But, they also build their own specialized tools that they use across projects. In this book, show how to build up your own library of Python functions. Can think of functions as specialized tools that you create to perform different musical tasks. In addition to examples described above, might write a function to generate a chord progression or play an arpeggio, \& can use functions again \& again across many musical projects.
			\item {\sf1.6.4. Reason 4: Code is useful for a thousand \& 1 other things.} Python is 1 of most powerful, multi-purpose languages in world. Used to create web servers \& social media platforms as much as video games, animation, \& music. Used for research \& DS, politics \& journalism. Knowing a little Python gives access to powerful ML \& AI (AI{\tt/}ML) techniques that are poised to transform most aspects of human work, including in creative domains e.g. music. Python is both a scripting language \& a software engineering platform -- equal parts duct tape \& table saw -- \& it's capable of everything from quick fixes to durable software applications. Learning a little Python won't make you a software engineer, just like learning a few guitar chords won't make you a performance musician. But it's a start down a path. An open door that was previously closed, \& a new way of using your mind \& a new way of thinking about music.
			\item {\sf1.6.5. Reason 5: Coding makes us more human.} When think about learning to code, tend to think about economic payoff. Hear arguments that learning to code is a resume builder \& a path to a high-paying job. Not that this perspective is wrong, but it might be wrong reason for you to learn how to code.
			
			Just like people who are good at music {\it love} music, people who are good at coding tend to {\it love} coding. Craft of building software can be tedious \& frustrating, but it can also be rewarding. A way to express oneself creatively \& to engage in craftwork. People don't learn to knit, cook, or play an instrument for lucrative (có lợi nhuận) career paths that these pursuits open up -- although by all means those pursuits can lead to remarkable careers. People learn these things because they have a {\it passion} for them. Because they are personally fulfilling. These passions connect us to centuries of tradition; they connect us to communities of teachers, learners, \& practitioners; \&, in end, they make us more {\it human}. So when things get a little frustrating -- \& things always get a little frustrating when learning any worthwhile skill -- remember: just like poetry, literature, or music, code is an arts as much as it is a science. \&, just like woodworking, knitting, or cooking, code is a craft as much as it is an engineering discipline. Be patient \& give yourself a chance to fall in love with coding.
		\end{itemize}
		\item {\sf1.7. Future of music \& code.} Before get on with book, wanted to leave you with a brief thought about future of technology, music, \& code. For as long as there have been people on this planet there has been music. \&, as long as there has been music, people have created technology to expand \& enhance their creative potential. A drum is a kind of technology -- a piece of animal hide stretched across a hollow log \& tied in place. It's a polylithic accomplishment, an assembly of parts that requires skill \& craft to make. One must know how to prepare animal hide, to make rope from plant fiber, \& to craft \& sharpen tools. More than that, one must know how to perform with drum, to connect with an audience, to enchant them to move their bodies through an emotional \& rhythmic connection to beat. Technology brings together materials \& tools with knowledge. People must have knowledge both to craft an artifact \& to wield it. \&, over time -- over generations -- that knowledge is refined as it gets passed down from teacher to student. It becomes stylized \& diversified. Tools, artifacts, knowledge, \& practice all become sth greater. Sth we call culture.
		
		Again \& again world of music has been disrupted, democratized, \& redefined by new technologies. Hip-hop was a rebellion against musical status quo fueled by low-cost technologies like recordable cassette tapes, turntables, \& 808 drum machines. Early innovators shattered norms of artistic expression, redefining music, poetry, visual art, \& dance in process. Inexpensive access to technology coupled with a need for new forms of authentic self-expression was a match to dry tinder of racial \& economic oppression.
		
		Hard to overstate how quickly world is still changing as a result of technological advancements. Digital artifacts \& infrastructures are so ubiquitous that they have reconfigured social, economic, legal, \& political systems; revolutionized scientific research; upended arts \& culture; \& even wormed their way into most intimate aspects of our personal \& romantic lives. Already talked about transformative impact that digital tools have had on world of music in 21st century, but exhilarating (\& scary) part: we're on precipice of another wave of transformation in which human creativity will be redefined by AI \& ML. Imagine AI accompanists that can improvise harmonies or melodies in real time with human musicians. Or DL algorithms that can listen to millions of songs \& innovate music in same genre. Or silicon poets that grasp human language well enough to compose intricate rap lyrics. Or machines with trillions of transistor synapses so complex that they begin to ``dream'' -- inverted ML algorithms that ooze imagery unhinged enough to disturb absinthe slumber of surrealist painters. Now, imagine: this is not speculative science fiction, but reality of our world today. These things are here now \& already challenging what we mean by human creativity. What are implications of a society of digital creative cyborgs?
		
		But here's trick: we've always been cyborgs. Western music notation is as much a technology as Python code. Becoming literate in any sufficiently advanced representation system profoundly shapes how think about \& perceive world around us. Classical music notation, theory, \& practice shaped mind of Beethoven as much as he shaped music with it -- so much so that he was still able to compose many of his most famous works while almost totally deaf. {\sc Beethoven} was a creative cyborg enhanced by technology of Western music notation \& theory. Difference: now we've externalized many of cognitive processes into machines that think alongside us. \&, increasingly, these tools are available to everyone. How that changes what it means to be a creative human being is anyone's guess.
		\item {\sf1.8. Book overview.} Excited to have you with us on this journey through music \& code. A short guide for where go from here. Chaps. 2--3 cover foundations of rhythm, pitch, \& harmony. These chaps are designed to move quickly \& get you coding in Python early on. Cover Python variables, loops, which both connect directly to musical concepts. Chaps. 4--6 cover foundations of chords, scales, \& keys using Python lists, functions, \& data structures. Chaps. 7, 8, 10 shift from music composition to music production covering topics e.g. frequency domain, modular synthesis, \& other production effects. In Chap. 9, switch to EarSketch platform to talk about how various musical elements are combined to compose full-length songs. Finally, Chap. 11 provides a short overview of history of music \& code along with a glimpse of what future might hold. Between each chap, provide a series of short {\it interludes} that are like step-by-step tutorials to introduce new music \& coding concepts.
		
		A few notes about how to read this book. Any time include Python code, it will be shown in a programming font. Sometimes write code in a table with line numbers so that can refer to specific lines. When introduce new terms, bold word. If get confused by any of programming or music terminology, check out appendices, which contain quick overviews of all of important concepts. Often invite to follow along with online examples. Best way to learn is by doing it yourself, so strongly encourage to try coding in Python online as go through chaps.
	\end{itemize}
	\item {\sf Interlude 1: Basic Pop Beat.} In this interlude, get familiar with TunePad interface by creating a basic rock beat in style of songs like {\it Roses} by SAINt JHN. Can follow along online by visiting \url{https://tunepad.com/interlude/pop-beat}
	\begin{enumerate}
		\item {\bf Step 1: Deep listening.} Good to get in habit of deep listening. Deep listening is practice of trying every possible way of listening to sounds. Start by loading a favorite song in a streaming service \& listening -- really listening -- to it. Take song apart element by element. What sounds do you hear? How are they layered together? When do different parts come into track \& how do they change over time? Think about how producer balances sounds across frequency spectrum or opens up space for transitions in lyrics. Try focusing on just drums. Can start to recognize individual percussion sounds \& their rhythmic patterns?
		\item {\bf Step 2: Create a new TunePad project.} Visit \url{https://tunepad.com} on a laptop or Chromebook \& set up an account. [Recommend using free Google Chrome browser for best overall experience.] After signing in, click on {\tt New Project} button to create an empty project workspace. Your project will look sth like this {\sf Fig. 1.3: TunePad project workspace}.
		\item {\bf Step 3: Kick drums.} In your project window, click on {\tt ADD CELL} button \& then select {\tt Drums} {\sf Fig. 1.4: Selecting instruments in TunePad.} In TunePad can think of a ``cell'' as an instrument that you can program to play music. Name new instrument ``Kicks'' \& then add this Python code.
		\begin{verbatim}
			# play four kick drums
			playNote(1)
			playNote(1)
			playNote(1)
			playNote(1)
		\end{verbatim}
		When done, your project should look sth like {\sf Fig. 1.5: Parts of a TunePad cell}.
		
		Go ahead \& press Play button at top left to hear how this sounds.
		
		{\it Syntax errors.} Occasionally your code won't work right \& get a red error message box that looks sth like {\sf Fig. 1.6: Python syntax error in TunePad}. This kind of error message is called a ``syntax'' error. In this case, code was written as {\tt playnote} as a lowecase ``n'' instead of an uppercase ``N''. Can fix this error by changing code to read {\tt playNote} on line 2.
		\item {\bf Step 4: Snare drums.} In your project window, click on {\tt ADD CELL} button again \& select {\tt Drums}. Now should have 2 drum cells one appearing above the other in your project. Name 2nd instrument ``Snare Drums'' \& then add this Python code.
		\begin{verbatim}
			# play 2 snare drums on the up beats only
			rest(1) # skip a beat
			playNote(2) # play a snare drum sound
			rest(1)
			playNote(2)
		\end{verbatim}
		Might start to notice text that comes after hashtag symbol \# is a special part of your program. This text is called a {\it comment}, \& it's for human coders to help organize \& document their code. Anything that comes after hashtag on a line is ignored by Python. Try playing this snare drum cell to hear how it sounds. Can also play kick drum cell at same time to see how they sound together.
		\item {\bf Step 5: Hi-hats.} Click on {\tt ADD CELL} button again to add a 3rd drum cell. Change title of this cell to be ``Hats'' \& add code:
		\begin{verbatim}
			# play four hats between the kicks and snares
			rest(0.5) # rest for half a beat
			playNote(4, beats=0.5) # play a hat for half a beat
			rest(0.5)
			playNote(4, beats=0.5)
			rest(0.5)
			playNote(4, beats=0.5)
			rest(0.5)
			playNote(4, beats=0.5)
		\end{verbatim}
		When play all 3 of drum cells together, should hear a basic rock beat pattern: {\tt kick - hat - snare - hat - kick - hat - snare - hat}.
		\item {\bf Step 6: Fix your kicks.} Might notice kick drums feel a little heavy in this mix. Can make some space in pattern by resting on up beats (beats 2 \& 4) when snare drums are playing. Scroll back up to your {\tt Kick drum cell} \& change code to look like this:
		\begin{verbatim}
			# play kicks on the down beats only
			playNote(1)
			rest(1)
			playNote(1)
			rest(1)
			playNote(1)
			rest(1)
			playNote(1)
			rest(0.5) # rest a half beat
			playNote(1, beats = 0.5) # half beat pickup kick
		\end{verbatim}
		\item {\bf Step 7: Adding a bass line.} Add a new cell to your project, but this time select {\tt Bass} instead of {\tt Drums}. Once cell is loaded up, change voice to {\tt Plucked Bass} {\sf Fig. 1.7: Selecting an instrument's voice in TunePad.}
		
		Entering this code to create a simplified bass line in style of {\it Roses} by SAINt JHN. When done, try playing everything together to get full sound.
		\begin{verbatim}
			playNote(5, beats=0.5) # start on low F
			playNote(17, beats=0.5) # up an octave
			rest(1)
			
			playNote(10, beats=0.5) # A sharp
			playNote(22, beats=0.5) # up an octave
			rest(1)
			
			playNote(8, beats=0.5) # G sharp
			playNote(20, beats=0.5) # up an octave
			rest(0.5)
			
			playNote(8, beats=0.5) # G sharp - G - G
			playNote(12, beats=0.5)
			playNote(24, beats=0.5)
			
			playNote(10, beats=0.75) # C sharp
			playNote(22, beats=0.25) # D sharp
		\end{verbatim}
	\end{enumerate}	
	\item {\sf2. Rhythm \& tempo.} This chap dives into fundamentals of {\it rhythm} in music. Start with beat -- what it is, how it's measured, \& how can visualize beat to compose, edit, \& play music. From there provide examples of some common rhythmic motifs from different genres of music \& how to code them with Python. Main programming concepts for this chap include loops, variables, calling function, \& passing parameter values. This chap covers a lot of ground, but it will give you a solid start on making music with code.
	\begin{itemize}
		\item {\sf2.1. Beat \& tempo.} {\it Beat} is foundation of rhythm in music. Term {\it beat} has a number of different meanings in music, [Term beat can also refer to main groove in a dance track (``drop the beat'') or instrumental music that accompanies vocals in a hip-hop track (``she produced a beat for a new artist'') in addition to other meanings.] but this chap uses it to mean a unit of time, or how long an individual note is played -- e.g., ``rest for 2 beats'' or ``play a note for half a beat''. Based on beat, musical notes are combined in repeated patterns that move through time to make rhythmic sense to our ears.
		
		{\it Tempo} refers to speed at which rhythm moves, or how quickly 1 beat follows another in a piece of music. As a listener, can feel tempo by tapping your foot to rhythmic pulse. Standard way to measure tempo is in beats per minute (BPM or bpm), meaning total number of beats played in 1 minute's time. This is almost always a whole number like 60, 120, or 78. At a tempo of 60 bpm, your foot taps 60 times each minute (or 1 beat per sec). At 120 bpm, get 2 beats every sec; \&, at 90 bpm, get 1.5 beats every sec. Later in this chap when start working with TunePad, can set tempo by clicking on bpm indicator in top bar of a project, see {\sf Fig. 2.1: TunePad project information bar. Can click on tempo, time signature, or key to change settings for your project.}
		
		Different genres of music have their own typical tempo ranges (although every song \& every artist is different). E.g., hip-hop usually falls in 60--110 bpm range, while rock is faster in 100--140 bpm range. House{\tt/}techno{\tt/}trance is faster still, with tempos between 120--140 bpm. {\sf[Table: Genre: Tempo Range (BPM)]}.
		
		It takes practice for musicians to perform at a steady tempo, \& they sometimes use a device called a {\it metronome} to help keep their playing constant with pulse of music. Can create a simple metronome in TunePad using 4 lines of code in a drum cell. This works best if switch instrument to {\tt Drums $\to$ Percussion Sounds}.
		\begin{verbatim}
			playNote(3, velocity = 100) # louder 1st note
			playNote(3, velocity = 60)
			playNote(3, velocity = 60)
			playNote(3, velocity = 60)
		\end{verbatim}
		Can adjust tempo of your metronome with bpm indicator {\sf Fig. 2.1: TunePad project information bar. Can click on tempo, time signature, or key to change settings for your project}. As this example illustrates, computers excel at keeping a perfectly steady tempo. This is great if want precision, but there's also a risk that resulting music will sound too rigid \& machine-like. When real people play music they \fbox{often speed up or slow down, either for dramatic effect or just as a result of being a human}. Depending on genre, performers might add slight variations in rhythm called swing or shuffle, that's a kind of back \& forth rocking of beat that you can feel almost more than you can hear. Show how to add a more human feel to computer generated music later in book.
		\item {\sf2.2. Rhythmic notations.} Over centuries, musicians \& composers have developed many different written systems to record \& share music. With invention of digital production software, a number of other interactive representations for mixing \& editing have become common as well. Here are 4 common visual representations of same rhythmic pattern. Pattern has a total duration of 4 beats \& can be counted as ``1 \& 2, 3 \& 4''. 1st 2 notes are $\frac{1}{2}$ beats long followed by a note that is 1 beat long. Then pattern repeats.
		\begin{itemize}
			\item {\sf2.2.1. Representation 1: Standard Western music notation.} 1st representation shows standard music notation (or Western notation), a system of recording notes that has been developed over many hundreds of years. 2 thick vertical lines on left side of illustration indicate: this is rhythmic notation, i.e., there is no information about musical pitch, only rhythmic timing. Dots on long horizontal lines are notes whose shapes indicate duration of each to be played. Sometimes different percussion instruments will have their notes drawn on different lines. Describe what various note symbols mean in more detail in {\sf Fig. 2.2: Standard notation example}.
			\item {\sf2.2.2. Representation 2: Audio waveforms.} 2nd representation shows a visualization of actual audio waveform that gets sent to speakers when play music. Waveform shows amplitude (or volume) of audio signal over time. Next chap talks more about audio waveforms, but for now can think of a waveform as a graph that shows literal intensity of vibration of your speakers over time. When compose a beat in TunePad, can switch to waveform view by clicking on small dropdown arrow at top-left side of timeline {\sf Fig. 2.3: Waveform representation of Fig. 2.2}.
			\item {\sf2.2.3. Representation 3: Piano (MIDI) roll.} 3rd representation shows a piano roll (or MIDI (Musical Instrument Digital Interface) roll). This uses solid lines to show individual notes. Length of lines represents length of individual notes, \& vertical position of lines represents percussion sound being played (kick drums \& snare drums in this case). This representation is increasingly common in music production software. Many tools even allow for drag \& drop interaction with individual notes to compose \& edit music {\sf Fig. 2.4: Piano or MIDI roll representation of Fig. 2.2}.
			\item {\sf2.2.4. Representation 4: Python code.} A final representation for now shows Python code in TunePad. In this representation, duration of each note is set using {\tt beats} parameter of {\tt playNote} function calls.
			\begin{verbatim}
				playNote(2, beats = 0.5)
				playNote(2, beats = 0.5)
				playNote(6, beats = 1)
				
				playNote(2, beats = 0.5)
				playNote(2, beats = 0.5)
				playNote(6, beats = 1)
			\end{verbatim}
		\end{itemize}
		Each of these representation has advantages \& disadvantages; they are good for conveying some kinds of information \& less good at conveying others. E.g., standard rhythm notation has been refined over centuries \& is accessible to an enormous, worldwide community of musicians. On other hand, it can be confusing for people who haven't learn how to read sheet music. Timing of individual notes is communicated using tails \& flags attached to notes, but there's no consistent mapping between horizontal space \& timing.
		
		Audio waveform is good at showing what sound {\it actually} looks like -- how long each note rings out (``release'') \& how sharp its onset is (``attack''). Helpful for music production, mixing, \& mastering. On other hand, waveforms don't really tell you much about pitch of a note or its intended timing as recorded by composer.
		
		Python code is easier for computers to read than humans -- it's definitely not sth you would hand to a musician to sight read. On other hand, it has advantage that it can be incorporated into computer {\it algorithms} \& manipulated \& transformed in endless ways.
		
		There are many, many other notation systems designed to transcribe a musical performance -- what hear at a live performance -- onto a sheet of paper or a computer screen. Each of these representations was invented for a specific purpose \&{\tt/}or genre of music. Might pick a representation based on context \& whether you're in role of a musician (\& what kind of instrument you play), a singer, a composer, a sound engineer, or a producer. Music notation systems are as rich \& varied as cultures \& musical traditions that invented them. 1 nice thing about working with software: easy to switch between multiple representations of music depending on task trying to accomplish.
		\item {\sf2.3. Standard rhythmic notation.} This sect will review a standard musical notation system that has roots in European musical traditions. This system is versatile \& has been refined \& adapted over a long period of time across many countries \& continents to work with an increasingly diverse range of instruments \& musical genres. Start with percussive rhythmic note values in this chap, \& move on to working with pitched instruments in Chap. 3.
		
		{\sf Fig. 2.5: Common note symbols starting with a whole note (4 beats) on top down to 16th notes on bottom. Notes on each new row are half length of row above.} shows most common symbols used in rhythmic music notation. Notes are represented with oval-shaped dots that are either open or closed. All notes except for whole note have tails attached to them that can point either up or down. It doesn't matter which direction (up or down) tail points. Notes that are faster than a quarter note also have horizontal flags or beams connected to tails. Each additional flag or beam indicates note is twice as fast.
		
		Symbol: Name: Beats: TunePad code:
		\begin{enumerate}
			\item Whole Note: Larger open circle with no tail \& no flag: 4: {\tt playNote(1, beats = 4)}
			\item Half Note: Open circle with a tail \& no flag: 2: {\tt playNote(1, beats = 2)}
			\item Quarter Note: Solid circle with a tail \& no flag: 1: {\tt playNote(1, beats = 1)}
			\item 8th Note: Solid circle with a tail \& 1 flag or bar: 0.5 or $\frac{1}{2}$: {\tt playNote(1, beats = 0.5)}
			\item 16th Note: Solid circle with a tail \& 2 flags or bars: 0.25 or $\frac{1}{4}$: {\tt playNote(1, beats = 0.25)}
			\item Dotted Half Note: Open circle with a tail. Dot adds an extra beat to half note: 3: {\tt playNote(1, beats = 3)}
			\item Dotted Quarter Note: Solid circle with a tail. Dot adds an extra half-beat: 1.5: {\tt playNote(1, beats = 1.5)}
			\item Dotted 8th Note: Solid circle with tail \& 1 flag. Dot adds an extra quarter beat: 0.75. {\tt playNote(1, beats = 0.75)}
		\end{enumerate}
		Standard notation also includes {\it dotted notes}, where a small dot follows note symbol. With a dotted note, take original note's duration \& add half of its value to it. So, a dotted quarter note is 1.5 beats long, a dotted half note is 3 beats long, etc.
		
		There are also symbols representing different durations of silence or ``rests''.
		
		Symbol: Name: Beats: TunePad code
		\begin{enumerate}
			\item Whole Rest: 4: {\tt rest(beats = 4)}
			\item Half Rest: 2: {\tt rest(beats = 2)}
			\item Quarter Rest: 1: {\tt rest(beats = 1)}
			\item 8th Rest: 0.5 or $\frac{1}{2}$: {\tt rest(beats = 0.5)}
			\item 16th Rest: 0.25 or $\frac{1}{4}$: {\tt rest(beats = 0.25)}
		\end{enumerate}
		\item {\sf2.4. Time signatures.} In standard notation, notes are grouped into segments called {\it measures} (or bars). Each measure contains a fixed number of beats, \& duration of all notes in a measure should add up to that amount. Relationship between measures \& beats is represented by a fraction called a {\it time signature}. Numerator (or top number) indicates number of beats in measure, \& denominator (bottom number) indicates beat duration.
		
		-- Trong ký hiệu chuẩn, các nốt nhạc được nhóm thành các đoạn gọi là {\it nhịp} (hoặc ô nhịp). Mỗi ô nhịp chứa 1 số phách cố định, \& thời lượng của tất cả các nốt nhạc trong 1 ô nhịp phải cộng lại bằng số lượng đó. Mối quan hệ giữa các ô nhịp \& nhịp được biểu diễn bằng 1 phân số gọi là {\it nhịp điệu}. Tử số (hoặc số trên cùng) biểu thị số phách trong ô nhịp, \& mẫu số (số dưới cùng) biểu thị thời lượng của phách.
		\begin{enumerate}
			\item $\frac{4}{4}$: 4-4 Time or ``Common tTime'': There are 4 beats in each measure, \& each beat is a quarter note. This time signature is sometimes indicated using a special symbol
			\item $\frac{2}{2}$: 2-2 Time or ``Cut Time'': There are 2 beats in each measure, \& beat value is a half note. Cut time is sometimes indicated with a `C' with a line through it.
			\item $\frac{2}{4}$: 2-4 Time: There are 2 beats in each measure, \& quarter note gets beat.
			\item $\frac{3}{4}$: 3-4 Time: There are 3 beats in each measure, \& quarter note gets beat.
			\item $\frac{3}{8}$: 3-8 Time: There are 3 beats in each measure, \& 8th note gets beat.
		\end{enumerate}
		Most common time signature is $\frac{4}{4}$. So common, in fact, referred to as {\it common time}. Often denoted by a C symbol shown in table. In common time, there are 4 beats to each measure, \& quarter note ``gets beat'' meaning: 1 beat is same as 1 quarter note.
		
		Vertical lines separate measures in standard notation. In example, there are 2 measures in 4/4 time (4 beats in each measure, \& each beat is a quarter note).
		
		If have a time signature of 3/4, then there are 3 beats per measure, \& each beat's duration is a quarter note. Some examples of songs is 3/4 time are {\it My Favorite Things} from {\it The Sound of Music, My 1st Song} by Jay Z, {\it Manic Depression} by {\sc Jimi Hendrix}, \& {\it Kiss from a Rose} by {\sc Seal}.
		
		If those notes were 8th notes, it would look like {\sf Fig.}
		
		Other common time signatures include 2/4 time (with 2 quarter note beats per measure) \& 2/2 time (with 2 {\it half note} beats in each measure). With 2/2 there are actually 4 quarter notes in each measure because 1 half note has same duration as 2 quarter notes. For this reason, 2/2 time is performed similarly to common time, but is generally faster. It is referred to as {\it cut time} \& is denoted by a C symbol with a line through it.
		
		Can adjust time signature of your TunePad project by clicking on time indicator in top bar (see {\sf Fig. 2.1}).		
		\item {\sf2.5. Percussion sounds \& instruments.} Working with rhythm, come across lots of terminology for different percussion instruments \& sounds. A quick rundown on some of most common drum sounds that you'll work with in digital music ({\sf Fig. 2.6: Drums in a typical drum kit.})
		
		{\sf Drum names: Description: TunePad note number}
		\begin{enumerate}
			\item Kick or bass drum: Kick drum (or bass drum) makes a loud, low thumping sound. Kicks are commonly placed on beats 1 \& 3 in rock, pop, house, \& electronic dance music. In other genres like hip-hop \& funk, kick drums are very prominent, but their placement is more varied: 0 \& 1
			\item Snare: Snare drums make a recognizable sharp staccato sound that cuts across frequency spectrum. They are built with special wires called snares that give drums its unique snapping sound. Snare drums are commonly used on beats 2 \& 4: 2 \& 3
			\item Hi-hat: Hi-hat is a combination of 2 cymbals sandwiched together on a metal rod. A foot pedal opens or closes cymbals together. In closed position hi-hat makes a bright tapping sound. In open position cymbal is allowed to ring out. Hi-hats have become an integral part of rhythm across almost all genres of popular music.: 4 (closed), 5 (open)
			\item Low, mid, high tom: Tom drums (tom-toms) are cylindrical drums that have a less snappy sound than snare drum. Drum kits typically have multiple tom drums with slightly different pitches (e.g. low, mid, \& high).: 6, 7, 8
			\item Crash cymbal: A large cymbal that makes a loud crash sound, often used as a percussion accent: 9
			\item Claps \& shakers: Different TunePad drum kits include a range of other percussion sounds common in popular music including various claps, shakers, \& other sounds.: 10 \& 11
		\end{enumerate}
		\begin{itemize}
			\item {\sf2.5.1. 808 Drum kit.} Released in early 1980s, Roland 808 drum machine was a hugely influential sound in early hip-hop music (\& other genres as well). 8.8 used electronic synthesis techniques to create synthesis replicas of drum sounds like kicks, snares, hats, toms, cowbells, \& rim shots. Tinkerers would also open up 808s \& hack circuits to create entirely new sounds. Today 808s usually refers to low, booming bass lines that were 1st generated using tweaked versions of 808s' kick drums. TunePad's default drum kit uses samples that sound like original electronically synthesized 808s ({\sf Fig. 2.7: Roland 808 drum sequencer.}).
			\item {\sf2.5.2. Selecting TunePad instruments.} When coding in Tunepad, sound that your code makes will depend on instrument you have selected. If coding a rhythm, can choose from several different drum kits including an 808 \& rock kits. Can change instrument by clicking on selector shown below {\sf Fig. 2.8: Changing an instrument's voice in TunePad.}
		\end{itemize}
		\item {\sf2.6. Coding rhythm in Python.}
		\begin{itemize}
			\item {\sf2.6.1. Syntax errors.} Python is a text-based language, i.e., you're going to be typing code that has to follow strict grammatical rules. When speak a natural language like English, grammar is important, but can usually bend or break rules \& still get your message across. When say something ambiguous it can be ironic, humorous, or poetic. This isn't case in Python. Python has no sense of humor \& no appreciation for poetry. If make a grammatical mistake in coding, Python gives a message called a {\it syntax error}. These messages can be confusing, but they're there to help you fix your code in same way that a spell checker helps you fix typos. Here's what a syntax error looks like in TunePad ({\sf Fig. 2.9: Example of a Python syntax error in TunePad. This line of code was missing a parenthesis symbol.})
			
			This line of code was missing a parenthesis symbol, which generated error message ``bad input on line 5''. Notice Python is giving hints about where problems are \& how to fix them, but those hints aren't always that helpful \& can be frustrating for beginners.
			\item {\sf2.6.2. Flow of control.} A Python program is made up of a list of statements. For most part, each statement goes on its own line in your program. Python will read \& perform each line of code from top to bottom in order that you write them. In programming this is called {\it flow of control}. This is similar to way you would read words in a book or notes on a line of sheet music. Difference: programming languages also have special rules that let you change flow of control. Those rules include {\it loops} (which repeat some part of your code multiple times), {\it conditional logic} (which runs some part of your code only if some condition is met), \& {\it user-defined functions} (which lets you create your own functions that can be called). Talk about these special ``control structures'' later in book.
		\end{itemize}
		\item {\sf2.7. Calling functions.} Almost everything you do in Python involves {\it calling} functions. A function (sometimes called a command or an instruction) tells Python to do something or to compute a value. E.g., {\tt playNote} function tells TunePad to make a sound. There are 3 things you have to do to call a function:
		
		1st, have to write name of function. Functions have 1-word names (with no spaces) that can consist of letters, numbers, \& underscore \verb|_| character. Multi-word functions will either use underscore character between words as in \verb|my_multi_word_function()| or each new word will be capitalized as in {\tt playNote()}.
		
		2nd, after type name of function, have to include parentheses. Parentheses tell Python that you're calling a function.
		
		3rd, include any {\it parameters} that you want to {\it pass} to function in between left \& right parentheses. A parameter provides extra information or tells function how to behave. E.g., {\tt playNote} statement needs at least 1 parameter to tell it which note or sound to play. Sometimes functions accept multiple parameters (some of which can be optional). {\tt playNote} function accepts several optional parameters described in next sect. Each additional parameter is separated with a comma ({\sf Fig. 2.10: Calling {\tt playNote} function in TunePad with 2 parameters inside parentheses.})
		\item {\sf2.8. playNote functions.} {\tt playNote} function tells TunePad to play a percussion sound or a musical note. {\tt playNote} function accepts up to 4 parameters contained within parentheses.
		\begin{verbatim}
			playNote(1, beats = 1, velocity = 100, sustain = 0)
		\end{verbatim}
		{\sf Name: Description}
		\begin{itemize}
			\item {\tt note}: This is a {\it required} parameter that says which note or percussion sound to play. Kind of sound depends on which instrument you have selected in TunePad for this code. Can play more than 1 note at same time by enclosing notes in square brackets.
			\item {\tt beats}: An {\it optional} parameter that says how long to play note. TunePad {\it playhead} will be moved forward by duration given. This parameter can be a whole number (like 1 or 2), a decimal number (like 1.5 or 0.25), or a fraction (like 1/2).
			\item {\tt velocity}: An {\it optional} parameter that says how loud to play note or sound. A value of 100 is full volume, \& a value of 0 is no volume (muted). Velocity is a technical term in digital music that means how fast or how hard you hit instrument. You might imagine it as how loud a drum sounds based on how hard it gets hit.
			\item {\tt sustain}: An {\it optional} parameter that allows a note to ring out for an additional number of beats without advancing playhead.
		\end{itemize}
		\begin{itemize}
			\item {\sf2.8.1. Optional parameters.} Sometimes parameters are {\it optional}, i.e., they have a value that gets provided by default if you don't specify one. For {\tt playNote}, only note parameter is required. If don't pass other parameters, it provides values for you by default. Can also include {\it names} of parameters in a function call. E.g., all 4 of lines below do exactly same thing; they play a note for 1 beat. 1st 2 use parameters without their names. 2nd 2 include names of parameter, followed by equals sign $=$, followed by parameter value.
			\begin{verbatim}
				playNote(60) # the beats parameter is optional
				playNote(60, 1) # with the beats parameter set to 1
				playNote(60, beats = 1) # with a parameter name for beats
				playNote(note = 60, beats = 1) # with a parameter name for note and beats
			\end{verbatim}
			\item {\sf2.8.2. Comments.} In code above, some of text appears after hashtag \# symbols on each line. This text is called a {\it comment}. A comment is a freedom note that programmers add to make their code easier to understand. Comment text is ignored by Python, so you can write anything you want after hashtag symbol on a line. Can also use a hashtag at beginning of a line to temporarily disable code. This is called ``commenting out'' code.
		\end{itemize}
		\item {\sf2.9. {\tt rest} function.} Silence is an important element of music. {\tt rest} function generates silence, or a break in sound. It only takes 1 parameter, which is length of time the rest is held. So {\tt rest(beats = 2)} will trigger a rest for a length of 2 beats. If don't provide a parameter, {\tt rest} uses a value of 1.0 by default.
		\begin{verbatim}
			rest() # rest for one beat
			rest(1.0) # rest for one beat
			rest(0.25) # rest for one quarter beat
			rest(beats = 0.25) # rest for one quarter beat
		\end{verbatim}
		\item {\sf2.10. Examples of {\tt playNote, rest}.} Try a few examples of {\tt playNote, rest} to get warmed up. This rhythm plays 2 8 notes (beats = 0.5) followed by a quarter note (beats = 1). Pattern then repeats a 2nd time. Here's how would code this in TunePad with a kick drum \& snare:
		\begin{verbatim}
			playNote(1, beats = 0.5) # play a kick drum (1) for half a beat
			playNote(1, beats = 0.5)
			playNote(2, beats = 1) # play snare (2) for one beat
			playNote(1, beats = 0.5) # play kick (1) for half a beat
			playNote(1, beats = 0.5)
			playNote(2, beats = 1) # play snare (2) for one beat
		\end{verbatim}
		Here's another example that plays a quarter note followed by a rest of 0.5 beats followed by an 8 note (beats = 0.5). Pattern is repeated 2 times in a row:
		\begin{verbatim}
			playNote(2, beats = 1) # play a snare drum (2) for one beat
			rest(beats = 0.5) # rest for half a beat
			playNote(1, beats = 0.5) # play a kick drum (1) for half a beat
			playNote(2, beats = 1) # play a snare drum (2) for one beat
			rest(beats = 0.5) # rest for half a beat
			playNote(1, beats = 0.5)
		\end{verbatim}
		A 3rd example that plays 8 notes in a row, each an 8 note (beats = 0.5).
		\item {\sf2.11. Loops.} All of examples in prev sect included repeated elements. \&, if listen closely, can hear repeated elements at all levels of music. There are repeated rhythmic patterns, recurring melodic motifs, \& storylines defined by song sects that get repeated \& elaborated. It turns out: there are many circumstances in both music \& computer programming where we want to repeat something over \& over again.
		
		To show how can take advantage of some of capabilities of Python, start with last example from prev sec where we wanted to tap out a run of 8th notes (0.5 beats) on hi-hat. 1 way to program that rhythm would be to just type in 8 {\tt playNotes} in a row.
		\begin{verbatim}
			for i in range(8):
			    playNote(4, beats = 0.5)
			    print(i)
		\end{verbatim}
		This will get job done, but there are a few problems with this style of code. 1 problem: it violates 1 of most important character traits of a computer programmer -- laziness! A lazy programmer is someone who works smart, not hard. A lazy programmer avoids doing repetitive, error-prone work. A lazy programmer knows that there are some things that computer can do better than a human can.
		
		In Python (\& just about any other programming language), when want to do something multiple times, can use a loop. Python has a number of different kinds of loops, but, in this case, our best option is sth called a {\tt for} loop. Version of code on right repeats 8 times in a row. For each iteration of loop, TunePad {\tt playNote} function gets called.
		
		With original code on left, had to do a lot of tying (or, more likely, copying \& pasting) to enter our program -- a warning sign that we're not being lazy enough. We generated a lot of repetitive code, which makes program harder to read (not as legible), error prone, \& not as elegant as it could be. Right-side code accomplishes same thing with just 3 lines of code instead of 8.
		
		Finally, code on left is harder to change \& reuse. What if wanted to use a different drum sound (like a snare instead of a hat)? Or, what if wanted to tap out a run of 16 16th notes instead of 8 8th notes? Would have to go through code line by line making same change over \& over again. This is a slow, error-prone process that is definitely not lazy or elegant.
		
		To see why this is better, try changing code on right so that it plays 16 16th notes instead of 8 8th notes. Or try changing drum sound from a hat to sth else. {\tt print} statement on line 3 is just there to help you see what's going on with your code. If click on {\tt Show Python Output} option, can see how variable called {\tt i} (that gets created on line 1) counts up from 0 to 7 {\sf Fig. 2.11: How to show print output of your code in a TunePad cell.} More detail about anatomy of a {\tt for} loop ({\sf Fig. 2.12: Anatomy of a {\tt for} loop in Python.}) A {\tt for} loop with range function:
		\begin{itemize}
			\item begins with {\tt for} keyword
			\item includes a loop {\it variable} name; this can be anything you want (above it is {\tt i}). Each time loop goes around, loop variable is incremented by 1.
			\item includes {\tt in} keyword
			\item includes {\tt range} function that says how many times to repeat
			\item includes a colon :
			\item includes a block of code indented by 4 spaces
		\end{itemize}
		Python uses {\it indentation} to determine what's {\it inside} loop, meaning it's sect of code that gets repeated multiple times. Intended block of code is repeated total number of times specified by {\tt range}. Try adding a few extras to prev example. In version below, add a run of 16th notes for last beat.
		\begin{verbatim}
			for i in range(6):
			    playNote(4, beats = 0.5)
			for i in range(4):
			    playNote(4, beats = 0.25)
		\end{verbatim}
		But there are lots of other things we could do as well. If wanted to play an even faster run, could use code like:
		\begin{verbatim}
			for i in range(8):
			    playNote(4, beats = 0.125)
		\end{verbatim}
		Or, if wanted to play a triplet that divides a half-beat into 3 equal parts, could do sth like this:
		\begin{verbatim}
			for i in range(3):
			playNote(4, beats = 0.25 / 3) # divide into 3 parts
		\end{verbatim}
		If open this example in TunePad, can experiment with different combinations of numbers to get different effects: \url{https://tunepad.com/examples/loops-and-hats}
		\item {\sf2.12. Variables.} A {\it variable} is a name you give to some piece of information in a Python program. Can think of a variable as a kind of nickname or alias. Similar to loops, variables help make your code more elegant, easier to read, \& easier to change in future. E.g., code on left plays a drum pattern without variables, \& code on right plays same thing with variables. Notice how variables help make code easier to understand because they give us descriptive names for various drum sounds instead of just numbers.
		\begin{verbatim}
			playNote(0)
			playNote(4)
			playNote(2)
			playNote(4)
			
			kick = 0
			hat = 4
			snare = 2
			
			playNote(kick)
			playNote(hat)
			playNote(snare)
			playNote(hat)
		\end{verbatim}
		In version on right defined a variable called {\tt kick} on line 1, a variable called {\tt hat} on line 2, \& a variable called {\tt snare} on line 3. Each variable is {\it initialized} to a different number for corresponding drum sound. Also possible to change value of a variable later in program by assigning it a different number.
		\begin{verbatim}
			kick = 0
			playNote(kick) # plays sound 0
			kick = 1       # set kick to a different value
			playNote(kick) # plays sound 1
		\end{verbatim}
		Variable names can be anything you want as long as they're 1 word long (no spaces) \& consist only of letters, numbers, \& underscore character \verb|_|. Variable names cannot start with a number, \& they can't be same as any existing Python keyword.
		
		As begin to get comfortable with code \& to exercise your creativity, find yourself wanting to experiment with sounds. Might want to try different sounds for same rhythmic pattern, maybe change a high-hat sound to a shaker to get a more organic feel. Using variables makes it easy to experiment by changing values around.
		
		Another example with a hi-hat pattern. Imagine really like this pattern, but wondering how it would sound with a different percussion instrument. Maybe you want to change 4 sound to a shaker sound (like 11). Nice thing about variables: can give them just about any name you want as long as Python is not already using that name for something else. This way you can make name meaningful to you. So, for our shaker example could create a variable with a meaningful name like {\tt shake} \& set it equal to 11. When use variable {\tt shake} you are inserting whatever number is currently assigned to it.
		\begin{verbatim}
			for i in range(8):
			    playNote(4,
			    beats = 0.5)
			for i in range(8):
			    playNote(4,
			    beats = 0.25)
			for i in range(4):
			    playNote(4,
			    beats = 0.5)
			    
			shake = 11
			for i in range(8):
			    playNote(shake,
			    beats = 0.5)
			for i in range(8):
			    playNote(shake,
			    beats = 0.25)
			for i in range(4):
			    playNote(shake,
			    beats = 0.5)
		\end{verbatim}
		As progress with coding, find that loops \& variables help create a smoother workflow that gives you more flexibility, freedom, \& creative power. Try out using variables with exercise \url{https://tunepad.com/examples/variables}.		
		\item {\sf2.13. More on syntax errors.} Python code is like a language with strict grammatical rules called syntax. When make a mistake in coding -- \& everyone makes coding mistakes all time -- Python will give feedback about what error is \& approximately what line it's on. E.g., if been trying to code exercises in this chap, may have seen a message like {\sf Fig. 2.13: Example of a Python syntax error. Command `ployNote' should instead say `playNote'.}
		
		This is telling us: there is an error on line 6 that can be fixed by changing text, ``ployNote''. When using a variable or function in your code, Python is expecting you to type it {\it exactly} the same as it was defined. A simple typo can stop your program from running, but it's also easily fixed. Here just need to update line to say {\tt playNote} instead of {\tt ployNote}.
		
		Other syntax errors are trickier. Message in {\sf Fig. 2.14: Example of a Python syntax error. Here problem is actually on line 1, not line 2.} Message in Fig. 2.14 is confusing because problem is actually on line 1 even though syntax error says line 2. Problem is a missing right parenthesis on line 1.
		
		1 technique coders use to find source of errors like this: comment out lines of code before \& after an error. E.g., to comment out1st line of code above, could change it to look like this:
		\begin{verbatim}
			# playNote(60
			rest(1)
		\end{verbatim}
		Adding hashtag at beginning of 1st line means Python ignores it, in this case fixing syntax error \& giving us another clue about source of problem.
		
		Another surprisingly helpful trick: just paste your error message verbatim into your favorite search engine. There are huge communities of Python coders out there who have figured out how to solve almost every problem with code imaginable. You can often find a quick fix to your problem just by browsing through a few of top search results.
		
		If want practice fixing syntax errors in your code, can try 1 of mystery-melody challenges on TunePad: \url{https://tunepad.com/examples/mystery-melody}.
		\item {\sf2.14. {\tt playhead}.} Timing of notes in TunePad is determined by position of an object called {\tt playhead}. In early days of music production, recordings were made using analog tape. Sound wave signals coming from a microphone or some other source were physically stored on magnetic tape using a mechanism called a {\it record head}. As tape moved by, record head would inscribe patterns of magnetic material inside of tape, thus creating a recording of music. To play recording back, a {\tt playhead} would pick up fluctuations in tape's magnetic material \& convert it back into sound waves for listeners to hear. Fast forward to digital realm. No longer have playheads or record heads, but maintain that metaphor when referring to notion of sound moving in time. Concept of a playhead is common across audio production software as point in time where audio is playing.
		
		-- Thời gian của các nốt nhạc trong TunePad được xác định bởi vị trí của 1 đối tượng được gọi là {\tt playhead}. Vào những ngày đầu của quá trình sản xuất âm nhạc, các bản ghi âm được thực hiện bằng băng analog. Tín hiệu sóng âm phát ra từ micrô hoặc 1 số nguồn khác được lưu trữ vật lý trên băng từ bằng 1 cơ chế được gọi là {\it record head}. Khi băng di chuyển qua, đầu ghi sẽ khắc các mẫu vật liệu từ tính bên trong băng, do đó tạo ra bản ghi âm nhạc. Để phát lại bản ghi âm, {\tt playhead} sẽ thu các dao động trong vật liệu từ tính của băng \& chuyển đổi nó trở lại thành sóng âm để người nghe có thể nghe. Chuyển nhanh đến thế giới kỹ thuật số. Không còn đầu phát hoặc đầu ghi nữa, nhưng vẫn duy trì phép ẩn dụ đó khi đề cập đến khái niệm âm thanh chuyển động theo thời gian. Khái niệm về đầu phát phổ biến trong các phần mềm sản xuất âm thanh như 1 điểm thời gian mà âm thanh đang phát.
		
		In TunePad, when place a note with {\tt playNote} function, it advances playhead forward in time by duration of note specified by {\tt beats} parameter. There are several functions available to get information about position of playhead \& move it forward or backward in time.
		
		{\sf Function: Description}
		\begin{itemize}
			\item {\tt getPlayhead()}: Returns current position of playhead in beats. Note: {\tt getPlayhead} returns elapsed number of beats, i.e. if playhead is at beginning of track function will return 0. If 1.5 beats have elapsed, {\tt getPlayhead} will return 1.5. If 40 beats have elapsed, it will return 40, \& so on.
			\item {\tt getMeasure()}: Returns current measure as an integer value. Note: {\tt getMeasure} returns an elapsed number of measures. So, if playhead is at beginning of track or anywhere before end of 1st measure, function will return 0. If playhead $\ge$ start of 2nd measure, {\tt getMeasure} will return 1, \& so on.
			\item {\tt getBeat()}: Returns an elapsed number of beats {\it within current measure} as a decimal number. E.g., if playhead has advanced by a quarter beat within a measure, {\tt getBeat} will return 0.25. Value returned by {\tt getBeat} will always $<$ total number of beats in a measure.
			\item {\tt fastForward(beats)}: Advance playhead forward by given number of beats relative to current position. Note: this is identical to {\tt rest} function. Negative beat values move playhead backward.
			\item {\tt rewind(beats)}: Move playhead back in time by given number of beats. This can be a useful way to play multiple notes at same time. Beats parameter specifies number of {\it beats} to move playhead. Negative values of beats move playhead forward.
			\item {\tt rest(beats)}: Advance playhead forward by given number of beats without playing a sound. This is identical to {\tt fastForward} function.
			\item {\tt moveTo(beats)}: Move playhead to an arbitrary position. {\tt beats} parameter specifies point that playhead will be placed as an elapsed number of beats. E.g., {\tt moveTo(0)} will move playhead to beginning of a track (zero elapsed beats). {\tt moveTo(1)} will place playhead at end of 1st beat \& right before start of 2nd beat.
		\end{itemize}
		Can control where playhead is relative to music we make by using {\tt moveTo, fastForward, rewind} commands. {\tt rewind} \& {\tt fastForward} functions move playhead backward or forward relative to current point in time. {\tt moveTo} function takes playhead \& moves it to an arbitrary point in time. In TunePad, playhead represents an {\it elapsed} number of beats. So, to move to beginning of a track, would use {\tt moveTo(0)}, i.e. 0 elapsed beats. To move to beginning of 2nd beat, would use {\tt moveTo(1)}, i.e. 1 elapsed beat. These commands are useful for adding multiple overlapping rhythms to a single TunePad cell. See more on how these commands can be used in Chap. 8.
		\item {\sf2.15. Basic drum patterns.} Code some foundational drum patterns. There is also a link to code in TunePad so that you can play around with beat \& make it your own.
		\begin{itemize}
			\item {\sf2.15.1. 4-on-the-floor.} 4-on-the-floor pattern is a staple of House, EDM, disco, \& pop music. It has a driving dance beat defined by 4 kick drum hits on each beat (thus 4 beats on floor). This beat is simple but versatile. Can spice it up by moving hi-hats around \& adding kicks, snares, \& other drums in unexpected places. Can make basic pattern with just 3 instruments: kick, snare, \& hats.
			
			Kick drums are lowest drum sound in a drum kit. Start by laying down kick drums on each beat of measure. These low sounds give this pattern a driving rhythmic structure that sounds great at higher tempos. Then add snare hits on even beats (2 \& 4). Snare adds energy \& texture to beat. Finally, add hi-hats. These are highest pitch instruments in most drum beats, \& they help outline groove to emphasize beat. Can find this example at \url{https://tunepad.com/examples/four-on-the-foor}.
			\begin{verbatim}
				# define instrument variables
				kick = 0
				snare = 2
				hat = 4
				
				# lay down four kicks (on the floor)
				playNote(kick)
				playNote(kick)
				playNote(kick)
				playNote(kick)
				
				moveTo(0) # reset playhead to the beginning
				
				# add snares on the even beats
				rest(1)
				playNote(snare)
				rest(1)
				playNote(snare)
				
				moveTo(0) # reset playhead to the beginning
				
				# hi-hat pattern with a loop!
				for i in range(8):
				    playNote(hat, 0.5)
			\end{verbatim}
			\item {\sf2.15.2. Blues.} Blues is a genre of music that evolved from African American experience, starting as field songs, evolving into spirituals, \& eventually became Blues. This beat is in 3/4 time, i.e. there are 3 beats in each measure. Can find this example at \url{https://tunepad.com/examples/blues-beat}.
			\begin{verbatim}
				# define instrument variables
				kick = 0
				snare = 2
				hat = 4
				
				# lay down kick and snare pattern
				playNote(kick, beats = 0.5)
				playNote(kick, beats = 0.25)
				playNote(snare, beats = 0.25)
				rest(.25)
				playNote(kick, beats = 0.25)
				playNote(kick, beats = 0.25)
				rest(.25)
				playNote(kick, beats = 0.25)
				playNote(snare, beats = 0.25)
				playNote(kick, beats = 0.25)
				playNote(kick, beats = 0.25)
				
				moveTo(0) # reset playhead to beginning
				
				# add hi-hats
				playNote (4, beats = 0.25)
				for i in range(3):
				    rest(0.25)
				    playNote (hat, beats = 0.25)
				    playNote (hat, beats = 0.25)
				rest(0.25)
				playNote(hat, beats = 0.25)
			\end{verbatim}
			\item {\sf2.15.3. Latin.} Latin beats are known for their syncopated rhythms that emphasize so-called ``weak beats'' in a measure. This drum pattern is 2 measures long. Our kick pattern sounds much like a heartbeat \& solidly grounds our entire beat. Our snare is playing a {\it clave} pattern, which is common in many forms of Afro-Cuban music e.g. salsa, mambo, reggae, reggaeton, \& dancehall. In 2nd measure, have hits on 1st beat \& 2nd half of 2nd beat (counts 1 \& 2.5). Finally, add a hi-hat on every 8th note. Can find this example at \url{https://tunepad.com/examples/latin-beat}.
			\begin{verbatim}
				# define instrument variables
				kick = 0
				snare = 3
				hat = 4
				
				# lay down kicks for the heartbeat
				for i in range(2):
				    playNote(kick, beats = 1.5)
				    playNote(kick, beats = 0.5)
				    playNote(kick, beats = 1.5)
				    playNote(kick, beats = 0.5)
				    
				moveTo(0) # reset playhead
				
				# add snare
				rest(1.0)
				playNote(snare)
				playNote(snare)
				rest(1.0)
				playNote(snare, beats = 1.5)
				playNote(snare, beats = 1.5)
				playNote(snare)
				
				moveTo(0) # reset playhead
				
				# lay down hi-hats
				for i in range(16):
				    playNote(hat, beats = 0.5)
			\end{verbatim}
			\item {\sf2.15.4. Reggae.} A common reggae beat is {\it1-drop beat}, which gets its name due to fact there's no hit on 1st beat. Rather, accent is on 3rd beat which contributes to strong backbeat \& laid back feel in reggae. Using swung 8th notes for our hi-hats, \& adding an open hi-hat hit on very last note to add texture. 1st hit is held for $\frac{2}{3}$ of beat \& 2nd for $\frac{1}{3}$. Both our kick \& snare hit on 3rd beat. Can find this example at \url{https://tunepad.com/examples/reggae-beat}.
			\begin{verbatim}
				# define instrument variables
				kick = 0
				snare = 2
				hat = 4
				open_hat = 5
				
				# lay down kick and snare together
				rest(2)
				playNote([kick, snare])
				
				moveTo(0) # reset playhead
				
				# lay down swung hi-hat pattern
				for i in range(3):
				    playNote(hat, 2.0 / 3) # two-thirds
				    playNote(hat, 1.0 / 3) # one-third
				
				playNote(hat, 2.0 / 3)
				playNote(open_hat, 1.0 / 3)
			\end{verbatim}
			\item {\sf2.15.5. Other common patterns.} Here are a few other drum patterns in different genres that you can try coding for yourself. Hip-hop (late-1990s) 90 bpm, Hip-hop (mid-2000s) 85 bpm, basic pop{\tt/}rock 130 bpm, Trap (mid-2010s) 130 bpm (double dots mean stuttered hi-hats), pop{\tt/}hip-hop 70 bpm, West coast beat (late 2010s) 100 bpm, Dance{\tt/}EDM{\tt/}Hip-hop (circa 1982) 130 bpm, Hip-hop (mid-1990s) 85 bpm.
		\end{itemize}
		\item {\sf2.16. Drum sequencers.} A drum sequencer is a tool for creating drum patterns. Early sequencers like Roland 808 were physical pieces of hardward. Now most people use software-based sequencers, although basic principles are the same: Sequencers look like a grid with rows for different drum sounds \& columns for short time slices (usually 16th notes or 32nd notes).
		
		TunePad includes a drum sequencer ({\sf Fig. 2.15: TunePad composer interface provides drum \& bass sequencers.}) that can be helpful for playing around with different rhythmic ideas \url{tunepad.com/composer}. Can add drum sounds at different time slices by clicking on gray squares of grid, \& once have a pattern you like you can convert it into Python code. When converting a drum sequencer pattern to code, it can be helpful to code column by column instead row by row. What that means: we work left to right across drum pattern. For each column, look at all sounds that hit at that time slice. Can then cue up each of those sounds using a simple {\tt playNote} statement. {\sf A quick example pattern.}
		
		Look at 1st column \& see there's a single kick drum.
		\begin{verbatim}
			playNote(0, beats = 0.25)
		\end{verbatim}
		Look at 2nd column \& see that it's empty, so rest:
		\begin{verbatim}
			rest(0.25)
		\end{verbatim}
		3rd column includes both a hat (note 4) \& a kick (note 0). To play these together, can use a single {\tt playNote} command with both sounds enclosed in square brackets like this:
		\begin{verbatim}
			playNote([ 0, 4 ], beats = 0.25)
		\end{verbatim}
		A special Python structure: list -- a convenient way to play more than 1 sound at same time. If keep going with this column-by-column strategy, complete code:
		\begin{verbatim}
			playNote(0, beats = 0.25)
			rest(0.25)
			playNote([ 0, 4 ], beats = 0.25)
			rest(0.25) # kick + hat
			playNote(2, beats = 0.25)
			rest(0.25)
			playNote(4, beats = 0.25)
			playNote(10, beats = 0.25)
			rest(0.25)
			playNote(10, beats = 0.25)
			playNote([ 0, 4 ], beats = 0.25) # kick + hat
			playNote(0, beats = 0.25)
			playNote(2, beats = 0.25)
			rest(0.25)
			playNote(4, beats = 0.25)
			rest(0.25)
		\end{verbatim}
		Coding column by column can be a little quicker \& produces more compact code.
		\begin{note}
			Term beat can also refer to main groove in a dance track (``drop beat'') or instrumental music that accompanies vocals in a hip-hop track (``she produced a beat for a new artist'') in addition to other meanings.
		\end{note}
	\end{itemize}
	\item {\sf Interlude 2: Custom Trap Beat.} In this interlude, run with skills you picked up in preceding chap to create a custom Trap beat. This beat will use kick drum, snare, \& hi-hats. Can follow along online by visiting \url{https://tunepad.com/interlude/trap-beat}.
	\begin{enumerate}
		\item {\bf Step 1: Defining variables.} Start by logging into TunePad \& creating a new project called ``Custom Trap Beat''. Add a new {\it Drums} instrument to your project. In this call, declare {\it variables} for your drum sounds.
		\begin{verbatim}
			# variables for drums
			kick = 1
			snare = 2
			hat = 4
		\end{verbatim}
		\item {\bf Step 2: Basic drum pattern.} Code for a basic drum pattern. Add this code to your Drum call after variables:
		\begin{verbatim}
			# kick and snare
			playNote(kick, beats = 0.75)
			playNote(kick, beats = 0.25)
			playNote(snare, beats = 1.5)
			playNote(kick, beats = 0.75)
			playNote(kick, beats = 0.75)
			playNote(kick, beats = 2)
			playNote(snare, beats = 1)
		\end{verbatim}
		Break down each of these lines 1 by 1 {\sf[Table]} When done, pattern should look sth like {\sf Fig. 2.16: Basic drum pattern}.
		\item {\bf Step 3: Add hi-hat rolls \& stutters.} Now add a new {\tt Drum Cell} to your project for hi-hat rolls \& stutters. To add our hi-hat runs, 1st review {\tt for loops} in Python {\sf Fig. 2.17: Declaring a for loop for hi-hat runs in Python.}
		
		Indented block of code is run total number of times specified by {\it range} of loop. Try this example pattern in your project:
		\begin{verbatim}
			for i in range(4):
			    playNote(hat, beats = 0.25)
			    
			for i in range(4):
			    playNote(hat, beats = 0.25 / 2)
			    
			for i in range(8):
			    playNote(hat, beats = 0.25)
			    
			for i in range(5):
			    playNote(hat, beats = 0.25 / 5)
			
			playNote(hat, beats = 0.25)
		\end{verbatim}
		Your cell should now have a pattern like {\sf Fig. 2.18: Hi-hat stutter patterns.}
		\item {\bf Step 4: Customize.} After trying example in Step 3, make up your own stutter pattern to go with your kick \& snare drums. Can use any combination of beats, but make sure it adds up to a multiple of 4 beats so that your beat loops correctly! Here are a few for loops that play stutters at different speeds:
		\begin{verbatim}
			# couplet
			for i in range(2):
			    playNote(hat, beats = 0.25 / 2) # divide in half
			    
			# triplet
			for i in range(3):
			    playNote(hat, beats = 0.25 / 3) # divide into 3 parts
			    
			# quad
			for i in range(4):
			    playNote(hat, beats = 0.25 / 4) # divide into 4 parts
			    
			# fifthlet?
			for i in range(5):
			    playNote(hat, beats = 0.25 / 5) # divide into 5 parts
		\end{verbatim}
		Try out different instrument sounds by changing values of variables \& switching to a different drum kit. Can also experiment with changing tempo. For more inspiration, this TunePad project has several popular hip-hop beat patterns that you can experiment with \url{https://tunepad.com/interlude/drum-examples}.
	\end{enumerate}
	\item {\sf3. Pitch, harmony, \& dissonance.} Chap. 2 introduced basics of rhythm \& how to use Python programming language to code beats with percussion sounds. In this chap, explore topics of pitch, harmony, \& dissonance -- or what happens when you bring tonal instruments \& human voice into music. Start with physical properties of sound (including frequency, amplitude, \& wavelength) \& why different musical notes sound harmonious or dissonant when played together. Also talk about different ways to represent pitch, including frequency value, musical note names, \& MIDI (Musical Instrument Digital Interface) note numbers that we can use with Python code \& TunePad.
	
	-- Chương 2 giới thiệu những điều cơ bản về nhịp điệu \& cách sử dụng ngôn ngữ lập trình Python để mã hóa nhịp điệu với âm thanh bộ gõ. Trong chương này, hãy khám phá các chủ đề về cao độ, sự hòa hợp, \& sự bất hòa -- hoặc điều gì xảy ra khi bạn đưa nhạc cụ có âm \& giọng nói của con người vào âm nhạc. Bắt đầu với các đặc tính vật lý của âm thanh (bao gồm tần số, biên độ, \& bước sóng) \& lý do tại sao các nốt nhạc khác nhau nghe có vẻ hòa hợp hay bất hòa khi chơi cùng nhau. Ngoài ra, hãy nói về các cách khác nhau để biểu diễn cao độ, bao gồm giá trị tần số, tên nốt nhạc, \& số nốt MIDI (Giao diện kỹ thuật số nhạc cụ) mà chúng ta có thể sử dụng với mã Python \& TunePad.
	\begin{itemize}
		\item {\sf3.1. Sound Waves.} All sound, no matter how simple or complex, is made up of waves of energy that travel through air, water, or some other physical medium. If could see a sound wave, it might look sth like ripples of water from a pebble dropped in a still pound. Pebble is like source of sound, \& ripples are sound waves that expand outward in all directions. Any source of sound (car horns, cell phone rings, chirping birds, or a plucked guitar string) sends vibrating waves of air pressure out at around 343 meters per sec (speed of sound) from source. It's not that air molecules themselves travel from source of sound to our ears; it's that small localized movements in molecules create fluctuations in air pressure that propagate outward over long distances.
		
		-- Mọi âm thanh, dù đơn giản hay phức tạp, đều được tạo thành từ các sóng năng lượng truyền qua không khí, nước hoặc 1 số môi trường vật lý khác. Nếu có thể nhìn thấy sóng âm, nó có thể trông giống như gợn sóng nước từ 1 viên sỏi thả vào 1 pound đứng yên. Viên sỏi giống như nguồn âm thanh, \& gợn sóng là sóng âm lan ra ngoài theo mọi hướng. Bất kỳ nguồn âm thanh nào (tiếng còi xe, chuông điện thoại di động, tiếng chim hót hoặc dây đàn guitar gảy) đều phát ra sóng rung động của áp suất không khí với tốc độ khoảng 343 mét 1 giây (tốc độ âm thanh) từ nguồn. Không phải bản thân các phân tử không khí di chuyển từ nguồn âm thanh đến tai chúng ta; mà là các chuyển động cục bộ nhỏ trong các phân tử tạo ra sự dao động trong áp suất không khí lan truyền ra ngoài trên những khoảng cách xa.
		
		Once those waves reach human ear, they are captured by outer ear \& funneled to a seashell-shaped muscle in inner ear called {\it cochlea}. This muscles has tiny hairs that resonate at different frequencies causing messages to get sent to brain that we interpret as sound.
		
		\begin{remark}[Protect your hearing]
			As musicians or music producers, your sense of hearing is 1 of your most precious assets. Always wear ear protection when you're exposed to loud sustained sounds! Loud sounds can damage your inner ear permanently, meaning you can start to close your ability to hear.
		\end{remark}
		All sound waves have following properties: {\it frequency, wavelength, \& amplitude.}
		\item {\sf3.2. Frequency.} Frequency refers to number of times a complete waveform passes through a single point over a period of time or how fast wave is vibrating. It is measured by cycles per sec in a unit called {\it hertz} (Hz). 1 cycle per sec is equivalent to 1 Hz, \& 1000 cycles are equivalent to 1000 Hz, or 1 kHz (pronounecd kilohertz). \fbox{Higher frequency, higher pitch of sound.} {\sf Fig. 3.1: Sound is made up of compression waves of air molecules that expand outward at a speed of around $343$ {\rm m{\tt/}s}. Frequency of a sound wave refers to how fast it vibrates: amplitude refers to intensity of sound; \& wavelength refers to length of 1 complete cycle of waveform.}
		\item {\sf3.3. Wavelength.} Wavelength refers to length of 1 complete cycle of a wave in physical space. This is distance from 1 peak or zero crossing to next. Can't actually see sound waves, but wavelength can be calculated by dividing speed of sound ($\approx343$ m{\tt/}s) by its frequency. So, for pitch of a {\it Concert A} note (440 Hz), length of waveform would be $\approx78$ cm or 2.6 ft.
		\begin{equation*}
			\frac{343\mbox{ m{\tt/}s}}{440\mbox{ Hz}} = 0.78\mbox{ m} = 78\mbox{ cm} = 2.56\mbox{ ft}.
		\end{equation*}
		On most pianos, wavelength of lowest bass note is almost 40 feet long! In contrast, wavelength of highest note is only around 3 inches. \fbox{Longer wavelength, lower the note.}
		
		Lower frequency sound also tends to travel longer distances. Think of a car playing loud music. As it approaches, you can hear fat sound of a bass guitar or a kick drum long before you can hear other instruments. Using this property, people in West Africa were able to transmit detailed messages over long distances using a language of deep drum sounds. A drummer called a ``carrier'' would drum out a rhythmic pattern on a huge log drum that carried messages like ``all people should gather at market place tomorrow morning''. All those within hearing range, which under ideal conditions could be as far as 7 miles, would receive message.
		
		-- Âm thanh tần số thấp hơn cũng có xu hướng truyền đi xa hơn. Hãy nghĩ đến 1 chiếc ô tô đang phát nhạc lớn. Khi nó đến gần, bạn có thể nghe thấy âm thanh to của 1 cây đàn ghi-ta bass hoặc trống đá rất lâu trước khi bạn có thể nghe thấy các nhạc cụ khác. Sử dụng đặc tính này, người dân ở Tây Phi có thể truyền tải các thông điệp chi tiết trên những khoảng cách xa bằng ngôn ngữ của âm thanh trống sâu. Một tay trống được gọi là ``người mang'' sẽ đánh 1 mẫu nhịp điệu trên 1 chiếc trống gỗ lớn mang theo các thông điệp như ``tất cả mọi người nên tập trung tại chợ vào sáng mai''. Tất cả những người trong phạm vi nghe được, trong điều kiện lý tưởng có thể cách xa tới 7 dặm, sẽ nhận được thông điệp.
		\item {\sf3.4. Amplitude.} Amplitude is related to volume of a sound, or how high peaks of waveform are Fig. 3.1. You can think of this as how much energy passes through a fixed amount of space over a fixed amount of time. Human ear perceives a vast range of sound levels, from sounds that are softer than a whisper to sounds that are louder than a pain-inducing jackhammer. In order to communicate volume of sound in a manageable way, music producers \& engineers use a unit of loudness called {\it decibels} (dB). Whispered voice level might be 30 dB, while jackhammer sound would be about 110 dB. Loud noises $> 120$ dB can cause immediate harm to ears.
		
		-- Biên độ liên quan đến âm lượng của âm thanh hoặc độ cao của các đỉnh sóng Hình 3.1. Bạn có thể nghĩ về điều này như lượng năng lượng đi qua 1 lượng không gian cố định trong 1 khoảng thời gian cố định. Tai người cảm nhận được 1 phạm vi rộng lớn các mức âm thanh, từ âm thanh nhẹ hơn tiếng thì thầm đến âm thanh to hơn tiếng búa khoan gây đau đớn. Để truyền đạt âm lượng âm thanh theo cách dễ quản lý, các nhà sản xuất âm nhạc \& kỹ sư sử dụng 1 đơn vị độ lớn gọi là {\it decibel} (dB). Mức giọng nói thì thầm có thể là 30 dB, trong khi âm thanh của búa khoan sẽ là khoảng 110 dB. Tiếng ồn lớn $> 120$ dB có thể gây hại ngay lập tức cho tai.
		\item {\sf3.5. Dynamics.} Variation of amplitude levels from low to high within a musical composition is referred to as dynamics. Difference between softest sound to loudest sound is called {\it dynamic range} of music. You can look at {\it waveform} of an audio signal to get a quick sense for its dynamic range. In general, lower heights mean lower amplitude \& higher heights mean higher amplitude. Loudness of a sound is also dependent on frequency. So, looking at a waveform alone won't tell you how loud sth will sound to listeners ({\sf Fig. 3.3: A waveform with varying amplitude}).
		
		-- {\sf Động lực học.} Sự thay đổi mức biên độ từ thấp đến cao trong 1 bản nhạc được gọi là động lực học. Sự khác biệt giữa âm thanh nhỏ nhất đến âm thanh to nhất được gọi là {\it dynamic range} của âm nhạc. Bạn có thể xem {\it waveform} của tín hiệu âm thanh để có cảm nhận nhanh về dải động của nó. Nhìn chung, độ cao thấp hơn có nghĩa là biên độ thấp hơn \& độ cao cao hơn có nghĩa là biên độ cao hơn. Độ to của âm thanh cũng phụ thuộc vào tần số. Vì vậy, chỉ xem dạng sóng sẽ không cho bạn biết âm thanh nào đó sẽ to như thế nào đối với người nghe ({\sf Hình 3.3: Dạng sóng có biên độ thay đổi}).
		\item {\sf3.6. Bandwidth.} Bandwidth refers to range of frequencies present in audio. As in case of dynamic range, can think of this as difference between highest \& lowest frequencies. Humans with good hearing can distinguish sounds between 20 Hz \& 20000 Hz. Most audio formats designed for music support frequencies up to 22 kHz (pronounced 22 kilohertz or 22000 hertz) so that they can capture full range of human hearing.
		
		-- Băng thông đề cập đến phạm vi tần số có trong âm thanh. Giống như trường hợp của phạm vi động, có thể coi đây là sự khác biệt giữa tần số cao nhất \& thấp nhất. Con người có thính giác tốt có thể phân biệt âm thanh giữa 20 Hz \& 20000 Hz. Hầu hết các định dạng âm thanh được thiết kế cho âm nhạc đều hỗ trợ tần số lên đến 22 kHz (phát âm là 22 kilohertz hoặc 220000 hertz) để chúng có thể thu được toàn bộ phạm vi thính giác của con người.
		
		Musical instruments naturally fall within range of human hearing at different places on frequency spectrum; this is referred to as instrument's bandwidth. {\it Instrument bandwidth} is important to music producers as they arrange a musical composition. In addition to quality of sound of instruments, those in different bandwidths can complement each other. Like a cello \& a flute, or a bass \& a saxophone. Music producers are keenly aware of influence of low- \& high-frequency instruments on their listeners. Musical instruments in bass register are often foundation of composition, holding everything together.
		
		-- Nhạc cụ tự nhiên nằm trong phạm vi nghe của con người ở các vị trí khác nhau trên phổ tần số; điều này được gọi là băng thông của nhạc cụ. {\it Băng thông nhạc cụ} rất quan trọng đối với các nhà sản xuất âm nhạc khi họ sắp xếp 1 bản nhạc. Ngoài chất lượng âm thanh của các nhạc cụ, những nhạc cụ có băng thông khác nhau có thể bổ sung cho nhau. Giống như đàn cello \& sáo, hoặc đàn bass \& saxophone. Các nhà sản xuất âm nhạc nhận thức sâu sắc về ảnh hưởng của các nhạc cụ có tần số thấp \& cao đến người nghe của họ. Nhạc cụ có âm trầm thường là nền tảng của bản nhạc, giữ mọi thứ lại với nhau.
		\item {\sf3.7. Pitch.} Within spectrum of human hearing, specific frequencies, ranges of frequencies, \& combinations of frequencies are essential for creating music. This sect covers some combinations of musical tones common in Western music culture. Then work in TunePad to try out different combinations \& explore those relationships through well-known musical compositions.
		
		-- Trong phổ thính giác của con người, các tần số cụ thể, phạm vi tần số, \& sự kết hợp của các tần số là điều cần thiết để tạo ra âm nhạc. Giáo phái này bao gồm 1 số sự kết hợp của các giai điệu âm nhạc phổ biến trong văn hóa âm nhạc phương Tây. Sau đó, hãy làm việc trong TunePad để thử các sự kết hợp khác nhau \& khám phá các mối quan hệ đó thông qua các tác phẩm âm nhạc nổi tiếng.
		
		While music producers \& engineers often think in terms of frequencies (hertz), musicians use pitch \& intervals to describe musical tones \& relationships between them. Pitches are individual notes like F, G, A, B, C, D, E as seen on piano keyboard. Interval between each adjacent note on a traditional keyboard is called a half step or a semitone. These base pitches can also have {\it accidentals}. Accidentals are like modifiers to notes that raise or lower base pitch. A note with a sharp \# applied has its pitch raised by a semitone, which a note with a flat $\flat$ applied is lowered by a semitone. Black notes on a piano are notes with accidentals. E.g., moving a C\# (black key) is a half step. Moving directly from a C to a D (both white keys) is called a whole step. Moving from a B to a C or an E to an F is also a half step because there's no black key in between ({\sf Fig. 3.4: A half step is distance between 2 adjacent piano keys, measured in semitones.})
		
		-- Trong khi các nhà sản xuất âm nhạc \& kỹ sư thường nghĩ theo tần số (hertz), thì các nhạc sĩ sử dụng cao độ \& khoảng cách để mô tả các cung bậc âm nhạc \& mối quan hệ giữa chúng. Cao độ là các nốt riêng lẻ như F, G, A, B, C, D, E như thấy trên bàn phím piano. Khoảng cách giữa mỗi nốt liền kề trên bàn phím truyền thống được gọi là nửa cung hoặc nửa cung. Các cao độ cơ bản này cũng có thể có {\it dấu hóa ngẫu nhiên}. Dấu hóa ngẫu nhiên giống như các dấu hiệu bổ nghĩa cho các nốt làm tăng hoặc giảm cao độ cơ bản. Một nốt có dấu thăng \# được áp dụng có cao độ được tăng lên 1 nửa cung, trong khi 1 nốt có dấu giáng $\flat$ được áp dụng sẽ hạ xuống 1 nửa cung. Các nốt đen trên đàn piano là các nốt có dấu hóa ngẫu nhiên. Ví dụ, di chuyển 1 C\# (phím đen) là nửa cung. Di chuyển trực tiếp từ C sang D (cả hai đều là phím trắng) được gọi là 1 cung trọn vẹn. Di chuyển từ B sang C hoặc từ E sang F cũng là nửa cung vì không có phím đen nào ở giữa ({\sf Hình 3.4: Nửa cung là khoảng cách giữa 2 phím đàn piano liền kề, được đo bằng nửa cung.})
		\item {\sf3.8. Musical Instrument Digital Interface.} 1 takeaway from prev sect: note names are confusing. There are multiple names for same pitch (G\# is same as A$\flat$), \& note names are repeated every octave. To help make things less ambiguous, computers \& digital musical instruments use a standardized format called {\it MIDI}, which stands for Musical Instrument Digital Interface. MIDI is a protocol, or set of rules, for how digital musical instruments communicate. Digital musical instruments send message to your computer or to other musical instruments. Typical MIDI controllers look like piano keyboards or drum pads but can take many other forms as well. When play a MIDI instrument, it sends information about a note's pitch, timing, \& volume along with other messages about vibrato, pitch bend, pressure, panning, \& clock signals. This table show 2 octaves of notes with their typical frequency values {\sf[Table]}. Appendix contains a complete table with note names, frequency values, \& MIDI numbers.
		
		TunePad uses MIDI numbers to designate pitch. To play a C0, lowest pitch on TunePad keyboard, use code {\tt playNote(12)}. To play a C4, a middle C in center of an 880key piano, use code {\tt playNote(60)}. MIDI notes go all way up to note G9 with note value 127.
		
		Now experiment with pitch in TunePad. Try creating a new piano instrument in TunePad \& adding this code:
		\begin{verbatim}
			# code for first piano cell
			playNote(48)
			playNote(55)
			playNote(60)
			playNote(55)
		\end{verbatim}
		This program plays 4 notes: 48 is a C, 55 is a G, \& 60 is a middle C. Now add a 2nd piano instrument to same project so that you have 2 cells. Add this code to 2nd cell:
		\begin{verbatim}
			# code for second piano cell
			playNote(72, beats = 4) # C5
			playNote(79, beats = 4) # G5
			playNote(76, beats = 4) # E5
			playNote(79, beats = 4) # G5
		\end{verbatim}
		This Python program looks similar to 1st one, but we've changed length of each note using {\it beats} parameter. In this case, asking TunePad to play 4 notes, each 4 beats long. Try playing both piano parts at same time. Can also make our notes shorter instead of longer. Add a 3rd piano instrument with notes that are each 1 half beat long. Try playing all 3 pianos together.
		\begin{verbatim}
			# code for third piano cell
			playNote(36, beats = 0.5)
			playNote(36, beats = 0.5)
			playNote(43, beats = 0.5)
			playNote(43, beats = 0.5)
			playNote(48, beats = 0.5)
			playNote(48, beats = 0.5)
			playNote(43, beats = 0.5)
			playNote(43, beats = 0.5)
		\end{verbatim}
		\item {\sf3.9. Harmony.} {\it Harmony} in music can be defined as a combination of notes that, when played together, have a pleasing sound. Although opinions about what sounds good in music are highly subjective, certain combinations of notes played together can \fbox{elicit predictable psychological responses} -- some combinations of notes sound {\it harmonious} while others some {\it discordant}. Musicians use this phenomenon to create an emotional tone for their compositions.
		
		-- {\it Harmony} trong âm nhạc có thể được định nghĩa là sự kết hợp các nốt nhạc, khi chơi cùng nhau, tạo ra âm thanh dễ chịu. Mặc dù ý kiến về những gì nghe hay trong âm nhạc là rất chủ quan, nhưng 1 số sự kết hợp các nốt nhạc chơi cùng nhau có thể \fbox{gợi ra những phản ứng tâm lý có thể dự đoán được} -- 1 số sự kết hợp các nốt nhạc nghe {\it hài hòa} trong khi 1 số khác lại {\it không hài hòa}. Các nhạc sĩ sử dụng hiện tượng này để tạo ra giai điệu cảm xúc cho các sáng tác của họ.
		
		In Western music, much of our conception of pitch is built on different mathematical ratios. Consider string of an instrument like a guitar or violin. Plucking open A (2nd lowest) string plays an A, which has a frequency of 110 Hz. Now if touch string at its midpoint, dividing it in half, still hear an A an octave above previous one -- twice frequency of 1st note, or 220 Hz. If touch string $\frac{1}{3}$ of way down \& pluck it, result is an E above higher A. This E is exactly 3 times our original frequency, or 330 Hz. Likewise, dividing string into 4ths multiplies original frequency by 4. Can continue this division on string as follows {\sf Fig. 3.5: harmonic series.}
		
		-- Trong âm nhạc phương Tây, phần lớn quan niệm của chúng ta về cao độ được xây dựng dựa trên các tỷ lệ toán học khác nhau. Hãy xem xét dây của 1 nhạc cụ như đàn ghi-ta hoặc đàn violin. Gảy mở dây A (dây thấp thứ 2) sẽ tạo ra nốt A, có tần số 110 Hz. Bây giờ nếu chạm vào dây ở điểm giữa của nó, chia nó thành hai nửa, vẫn nghe thấy nốt A cao hơn 1 quãng tám so với nốt trước đó -- gấp đôi tần số của nốt đầu tiên, hoặc 220 Hz. Nếu chạm vào dây $\frac{1}{3}$ xuống 1 khoảng \& gảy nó, kết quả là nốt E cao hơn nốt A cao hơn. Nốt E này chính xác gấp 3 lần tần số ban đầu của chúng ta, hoặc 330 Hz. Tương tự như vậy, chia dây thành 4 quãng sẽ nhân tần số ban đầu với 4. Có thể tiếp tục phép chia này trên dây như sau {\sf Hình 3.5: chuỗi điều hòa.}
		
		Resulting sequence of ascending pitches this produces is known as {\it harmonic series}. If 2 notes have a harmonic relationship, i.e., 2 frequencies s.t. result is a whole number. Harmonic series is simply set of frequencies that have a harmonic relationship to a {\it fundamental pitch} (initial note). Our initial experiment with string illustrates this relationship.
		
		To find frequencies that make up harmonic series for a given pitch, multiply its frequency by set of whole numbers. For A1, which is 55 Hz, 1st 8 harmonics would be {\sf[Table]}.
		
		In table, MIDI value is given for each harmonic of A1. Notice these values are given in decimal format. In TunePad, {\tt playNote} accepts both whole \& decimal values. Whole numbers are a data type referred to as {\it integer} values, or just as {\it ints}. Decimals are a separate data type referred to as {\it floating point} values, or just {\it floats}.
		
		Listen to an example \url{https://tunepad.com/examples/harmonic-series}. Notes with frequencies that form simple ratios, e.g. 2:1, 3:2, 4:3, or 5:4, tend to sound good together. E.g., can take note A4 (440 Hz) \& add a frequency that is 1.5 times its value, giving us an E4 (660 Hz). This results in a ratio of 3:2 \& a pleasant sound. However, if add a frequency that is 1.3 times value of 440 Hz, end up with 572 Hz, which creates a not-so-pleasant combination of tones. It's not an accident that there is no corresponding musical note to 572 Hz on piano keyboard.
		\item {\sf3.10. Intervals.} In music, an {\it interval} is distance between 2 notes. These notes can either be played simultaneously or not. If they are played simultaneously, pitches are called a {\it dyad} or a {\it chord}. Otherwise, they are a {\it melodic} interval. An interval is always measured from lowest note. Intervals have 2 different components: {\it generic interval} \& quality. Generic interval is distance from 1 note of a scale to another; this can also be described as number of letter names between 2 notes, including both notes in question. E.g., generic interval of C4 \& E4 has C4, D4, \& E4 in between. That's 3 notes, so we have a 3rd. Generic interval between F\#3 \& G3 is a second. Generic interval between G2 \& G3 in an 8th -- also known as an octave. Quality can be 1 of 5 options: Perfect, Major, Minor, Augmented, or Diminished. Each quality has a distinct sound \& can generate different emotional responses. Some common intervals in music along with their frequency ratios \& half steps. {\sf[Table]}
		
		These intervals are based on harmonic series, but this isn't exactly how most instruments are tuned. Talk more about this below. Also, naming of ratios (5th, 4th, Major 3rd, etc.) will make more sense in Chap. 5 where talk about scales \& keys. Notice that there's 1 particularly nasty-looking ratio called {\it Tritone interval} (45:32). This interval has historically been referred to as {\it Devil in Music} \& was frequently avoided in music composition for its dissonant qualities.
		
		-- Các khoảng này dựa trên chuỗi hài hòa, nhưng đây không phải là cách chính xác mà hầu hết các nhạc cụ được lên dây. Hãy nói thêm về điều này bên dưới. Ngoài ra, việc đặt tên cho các tỷ lệ (5, 4, 3 trưởng, v.v.) sẽ hợp lý hơn trong Chương 5, nơi nói về các thang âm \& cung. Lưu ý rằng có 1 tỷ lệ trông đặc biệt khó chịu được gọi là {\it Quãng ba cung} (45:32). Khoảng này trước đây được gọi là {\it Devil in Music} \& thường bị tránh trong sáng tác nhạc vì tính chất bất hòa của nó.
		
		1 of simplest \& most common intervals is octave, which has a frequency ratio of 2 to 1 (2:1) -- i.e., higher pitch completes 2 cycles in same amount of time that lower pitch completes 1 full cycle. Notes that are octave intervals from 1 another have same letter name \& are grouped together on a piano keyboard. Notice repeating patterns where C is highlighted note, C3, C4, C5 ({\sf Fig. 3.4: A half step is distance between 2 adjacent piano keys, measured in semitones.}). To illustrate, can begin with {\it middle C} (C4), which is $\approx262$ Hz, \& then move to a C5, which is an octave above it at 524 Hz. Can see C5 is double C4 frequency forming octave ratio, 2:1. Waveforms representing 2 notes forming this octave are plotted in {\sf Fig. 3.6: 2 waves at an interval of an octave.} Can see that for every single complete cycle of 262 Hz wave, C4, there are 2 full cycles of waveform for octave above it, 524 Hz C5. Easier to count cycles if look at 0-crossings.
		
		What does an octave look like in code? As have seen, each note on piano keyboard is a half step, \& there are 12 half steps between octaves. Try counting notes between C4 \& C5. Remember, black keys count!
		
		TunePad tracks notes on keyboard by half steps, so can easily play any octave interval without having to figure out exact note number. E.g., this code plays a middle C (60) \& a C 1 octave higher.
		\begin{verbatim}
			note = 60
			playNote(note)
			playNote(note + 12)
		\end{verbatim}
		This code assigned number 60 to variable {\tt note} on 1st line. 3rd line played a note 1 octave higher by adding 12 to original {\tt note} variable (not 72 is played). Expanding on this, can substitute any number you want for {\tt note} \& generate an octave above it by adding 12.
		
		Octaves sound good together in music \& are used in many popular songs. E.g., in song {\it Over the Rainbow} composed by {\sc Harold Arlen} from {\it Wizard of Oz}, beginning 2 notes are an octave apart.
		\begin{verbatim}
			# First two bars of "Over the Rainbow"
			# Composed by Henry Arlen
			playNote(60, beats = 2) # note C4
			playNote(60 + 12, beats = 2) # note C5
			playNote(71, beats = 1)
			playNote(67, beats = 0.5)
			playNote(69, beats = 0.5)
			playNote(71, beats = 0.5)
			rest(0.5)
			playNote(72, beats = 1)
		\end{verbatim}
		Try this example at \url{https://tunepad.com/examples/rainbow}.
		
		Another interval relationship important to Western music is ratio of 3:2, also known as perfect 5th, which has 7 half steps between notes. With this interval ratio, there are 3 complete cycles of higher frequency for every 2 periods of lower frequency ({\sf Fig. 3.7: Ratio between note C 262 Hz \& note G 393 Hz is considered a perfect 5th.})
		
		{\sc Henry Mancini} uses a perfect 5th (G3 392 Hz \& D4 587 Hz) in 1st 2 notes in melody for song {\it Moon River}. Code 1st few bars of {\it Moon River} using a variable called \verb|root_note| to set starting note. This allows us flexibility to easily play song beginning from any note on keyboard \& relationship between notes stays same no matter which note you start with. Try changing value of variable \verb|root_note| to another MIDI note. This can come in handy when you are composing for a singer who would rather have song in another key or octave.
		\begin{verbatim}
			# First bars of "Moon River"
			# Composed by Henry Mancini
			root note = 55
			playNote(root note, beats = 3)
			playNote(root note + 7, beats = 1)
			playNote(root note + 5, beats = 2)
			playNote(root note + 4, beats = 1.5)
			playNote(root note + 2, beats = 0.5)
			playNote(root note, beats = 0.5)
			playNote(root note - 2, beats = 0.5)
			playNote(root note, beats = 2)
		\end{verbatim}
		See this example at \url{https://tunepad.com/examples/moon}.
		\item {\sf3.11. Dissonance.} Dissonance refers to combinations of notes which when combined have an unpleasant sound that creates tension. Interval of a minor second (or 1 half step) is a complex frequency ratio of about 9.5:1. This combination gives you a sense of suspense. Can hear effect of dissonance used in composition by {\sc John Willimas} for movie {\it Jaws}. Use {\tt for} loop for this example, as 2 notes are repeated.
		\begin{verbatim}
			# bass line for the theme from Jaws
			# composed by John Williams
			for i in range(8):
			    playNote(40, beats = 0.5) # E2
			    playNote(41, beats = 0.5) # F2
		\end{verbatim}
		Intervals that are dissonant are unstable, leaving listener with impression that notes {\it want} to move elsewhere to resolve to more stable or {\it consonant} intervals.
		
		Can try this example in TunePad to hear how notes that are 1 half step apart crunch when played together.
		\begin{verbatim}
			# half step - the notes are just 1 number away
			playNote(41, beats=1, sustain=3)
			playNote(42, beats=1, sustain=2)
			rest(2)
			playNote([41, 42], beats=4)
			rest(2)
			
			# whole step - these notes are 2 numbers away
			playNote(41, beats=1, sustain=3)
			playNote(43, beats=1, sustain=2)
			rest(2)
			playNote([41, 43], beats=4)
			rest(2)
		\end{verbatim}
		Listen to this example \url{https://tunepad.com/examples/dissonance}.
		
		Another example of use of dissonant intervals comes from horror movie {\it Halloween} (1978). Theme song by {\sc John Carpenter} creates a sense of suspense \& deep unease with use of dissonant intervals e.g. Tritone (ratio 45:32).
		\item {\sf3.12. Temperaments \& Tuning.} Follow along at \url{https://tunepad.com/examples/temperaments} .
		
		Intervals in prev sects were based on ratios called perfect or pure intervals. Waves of so-called perfect intervals align at a simple integer ratio. If 2 tones form a perfect interval, it will result in a louder sound, as amplitudes are added. If 1 of tones is out of tune, then there will be interference between 2 waves. This interference manifests as an audible rhythmic swelling or ``wah-wah'' between waves, which we call {\it beating}. Farther 2 tones are from being perfect, faster beating. If tones are apart far enough, might even hear this beating as a 3rd tone, called a {\it combination tone}. Pure \& impure intervals are not a value judgment but a description of natural phenomena.
		
		Using notes based on these simple ratios seems to make a lot of sense -- it's based on simple mathematical relationships that we know sound good to human ear. But, it turns out: quickly run into problems using this system when start trying to tune an instrument like a piano. E.g., say trying to tune an A4 against a fixed lower tone on a keyboard using pure ratios. If tuning this A4 against an F4 at $\approx349$ Hz, our intervals form a major 3rd at a 5:4 ratio of frequencies. This results in our A4 being $\approx436.26$ Hz. But, if tune our A4 against an F\#4 at 370 Hz, this produces a minor 3rd, which is at a 6:5 ratio of frequencies. Now our A4 is 444 Hz instead of 436.25 Hz! How can it be that same note maps to different frequencies?
		
		Question of how to map frequency -- of which there are endless possible values -- to a fine set of notes means that we have to both arbitrarily choose a starting point \& also decide at what intervals to increment. This is basis for what are called {\it temperaments}. Temperaments are systems that define sizes of different intervals -- how tones relate to 1 another. In choosing tones in an octave, must compromise between our melodic intervals \& our harmony. Ideally, want a system with as consistent melodic intervals \& that is as close to perfect harmonic intervals as possible. In a system based on perfect ratios -- also referred to {\it Just Intonation} -- divisions, or semitones, of an octave are not evenly distributed. I.e., there are unique sets of tunings for every note we choose as base note of our octave. Just Intonation also does not form a closed loop of an octave. This is getting into weeds a bit, but if derive each note's frequency by tuning ratio of a perfect 5th (3:2) from prev note, do not end up at same place 1 octave higher. In fact, tuning ratio of 3:2 12 times brings us back to our exact starting note only after 7 octaves. Because Just Intonation has too many mathematical snares to be represented by 12 notes of keyboard, it's not a stable tuning system \& not a temperament. By definition, a temperament is a calculated deviation from Just Intonation that maps each note to exactly 1 frequency while still getting as close as possible to pure intervals.
		
		Most contemporary music, including TunePad, is based on a system called Equal Temperament. Octave at a pure 2:1 ratio serves as foundation, which is then divided into 12 equal half steps. Most often, Western harmony is built primarily from 3rds, 5ths, \& octaves. Every octave (\& unison) is a pure interval in Equal Temperament. Perfect 4ths \& 5ths are {\it nearly} pure intervals. Major \& minor 3rds are quite far from perfect, but because we have grown so accustomed to hearing these intervals, they do not sound off to our ears. Because Equal Temperament is, well, equal, every chord will have same sound in every key. Each semitone is equally sized, \& every note maps to exactly 1 frequency. Furthermore, each semitone is divided into 100 cents, which we can use to further specify intonation. With our intervals decided, now only have to choose a starting pitch from which to tune others. Most of time in North America, system is aligned to A440, i.e. A4 is equal to exactly 440 Hz.
		
		Keyboard instruments have fixed pitch, while singers \& instruments e.g. violin or flute have flexible tuning. In acoustic performance, pitch can vary due to many factors. No instrument is perfectly in tune. Tuning can be affected by factors e.g. air pressure \& temperature. Even a performer's physiology can affect tuning. Often, performers will tune harmonies using Just Intonation s.t. a chord uses pure intervals \& is more pleasing. Many musicians will do this without even being aware that they are doing it -- Just Intonation just {\it feels} in tune.
		
		Important to remember that decision to tune to A440 \& to divide octave into 12 equal semitones is only 1 possibility in response to debates about musical tuning that date back thousands of years, \& it's only 1 of a myriad of ways that music can be tuned. There are many alternate tuning systems, both historical \& contemporary from both Western \& non-Western cultures, which are still in use today.
		
		-- Điều quan trọng cần nhớ là quyết định lên dây A440 \& chia quãng tám thành 12 nửa cung bằng nhau chỉ là 1 khả năng để đáp lại các cuộc tranh luận về cách lên dây nhạc có từ hàng ngàn năm trước, \& đó chỉ là 1 trong vô số cách để lên dây nhạc. Có nhiều hệ thống lên dây thay thế, cả lịch sử \& đương đại từ cả nền văn hóa phương Tây \& không phải phương Tây, vẫn được sử dụng cho đến ngày nay.
	\end{itemize}
	\item {\sf Interlude 3: Melodies \& Lists.} For this interlude, code a short sect of a remix of {\sc Beethoven}'s {\it Für Elise} created by artist \& YouTuber {\sc Kyle Exum} (Bassthoven, 2020). Because this song has a more intricate melody, learn how to play sequences of notes written out as Python {\it lists}. Talk more about lists in next chap, but for now, can think of them as a way to hold $> 1$ note in a single variable.
	\begin{enumerate}
		\item {\bf Step 1: Variables.} Create a new Keyboard instrument \& add some variables for our different note names.
		\begin{verbatim}
			A = 69 # set variable A equal to 69
			B = 71
			C = 72
			D = 74
			E = 76
			Eb = 75 # E flat
			Gs = 68 # G sharp
			_ = None			
		\end{verbatim}
		Last line is a little strange. It defines a variable called \verb|_|
		\begin{itemize}
			\item In Python, underscore \verb|_| character is a valid variable name.
			\item Set this variable to have a special value called {\tt None}.
			\item Calling {\tt playNote} with this value is same thing as a rest. It plays nothing.
		\end{itemize}
		\item {\bf Step 2: Phrases.}
		\begin{itemize}
			\item For this song, going to define 4 musical phrases that get repeated to make melody.
			\item Each phrase gets its own variable.
			\item Each variable will hold lists of notes in order they should be played.
			\item You create a Python list by enclosing variables inside of square brackets.
			\item Use underscore character \verb|_| mean play nothing.
			\item Sometimes subtract 12 from a note, i.e., to play note on octave lower.
		\end{itemize}
		\begin{verbatim}
			# four basic phrases that repeat throughout
			p1 = [ E, Eb, E, Eb, E, B, D, C, A, _, _, _ ]
			p2 = [ A, C - 12, E - 12, A, B, _, _, _ ]
			p3 = [ B, E - 12, Gs, B, C, _, _, _, C, _, _, _ ]
			p4 = [ B, E - 12, C, B, A, _, _, _, A, _, _, _ ]
		\end{verbatim}
		\item {\bf Step 3: Playing Phrases.} Now have defined our variables, can start to play melody. 1 way to do this: use a Python {\it for loop} to iterate through every note. 1 cool thing about Python: can join lists together using plus sign $+$. Here's what everything looks like together.
		\begin{verbatim}
			_ = None
			A = 69
			B = 71
			C = 72
			D = 74
			E = 76
			Eb = E - 1 # E flat
			Gs = A - 1 # G sharp
			
			# 4 basic phrases that repeat throughout
			p1 = [ E, Eb, E, Eb, E, B, D, C, A, _, _, _ ]
			p2 = [ A, C - 12, E - 12, A, B, _, _, _ ]
			p3 = [ B, E - 12, Gs, B, C, _, _, _, C, _, _, _ ]
			p4 = [ B, E - 12, C, B, A, _, _, _, A, _, _, _ ]
			p5 = [ A,_,_,_,A,_,_,_,A,_, _, _, A, _, _, _ ]
			
			for note in p1 + p5 + p2 + p3 + p1 + p2 + p4:
			    playNote(note, beats = 0.5)
			    
			for note in p1 + p2 + p3 + p1 + p2 + p4:
			    playNote(note, beats = 0.5)
		\end{verbatim}
		\item {\bf Step 4: Bass!} Add a Bass to your project \& change voice to {\tt808 Bass}. Can copy code below for bass pattern {\sf Fig. 3.8: Select 808 Bass voice.}
		\item {\bf Step 5: Drums.} To finish up, layer in a simple drum pattern that works well with melody. Create a new {\tt Drum instrument} \& add this code.
		\begin{verbatim}
			rest(12)
			for i in range(4):
			    playNote(21)
			    rest(2)
			    playNote(21)
			    rest(1)
			    playNote(16, beats = 0.5)
			    rest(1)
			    playNote(16, beats = 0.5)
			    rest(1)
			    playNote(21)
			    rest(2)
			    playNote(21)
			    rest(1)
			    playNote(28, beats = 0.5)
			    rest(1)
			    playNote
			
			for i in range(16):
			    playNote(0)
			    playNote(2, beats = 0.5)
			    playNote(2, beats = 0.5)
			    playNote(10)
			    playNote(0)
		\end{verbatim}
		Can try this project online \url{https://tunepad.com/interlude/bassthoven}.
	\end{enumerate}
	\item {\sf4. Chords -- Hợp âm.} Chords are an essential building block of musical compositions. Skillful use of chords can set foundation of a song \& create a sense of emotional movement. However, even though basic ideas behind chords are easy to understand, there's an overwhelming amount of terminology \& technical detail that can take years to learn. Using code helps us cut through layers of complicated terminology to reveal elegant structures beneath. With code, chords are nothing more than lists of numbers that follow consistent patterns. Work through chords using Python {\it lists \& functions}. Learn some of traditional music terminology \& what it means, but also build your own toolkit of computer code to use for new compositions.
	\begin{itemize}
		\item {\sf4.1. Chords.} Can follow along with interactive online examples at \url{https://tunepad.com/examples/chord-basics}. In Chap. 3, introduced idea of harmony \& dissonance. $\ge2$ notes have a harmonic relationship if their frequencies have integer ratios. E.g., when 2 notes are 1 octave apart, higher note vibrates exactly 2 complete cycles for every 1 cycle of lower note (a 2:1 ratio). When 2 notes are a 5th apart, their frequencies have a 3:2 ratio. Higher note vibrates exactly 3 times for every 2 complete cycles of lower note.
		
		Building on this idea of harmonic relationships between notes, a {\it chord} is more than 1 note played together at same time. In Python can think of a chord as a {\it list} of numbers representing MIDI (Musical Instrument Digital Interface) note values. E.g., this code plays a C major chord in TunePad.
		\begin{verbatim}
			Cmaj = [ 48, 52, 55 ] # notes C, E, G
			playNote(Cmaj)
		\end{verbatim}
		Here {\tt Cmaj} is a variable. Instead of assigning that variable to a single number, we're assigning it a {\it list} of numbers. In Python, a list is a set of values enclosed in square brackets \& separated by commas. Then on 2nd line we play all 3 notes together using {\tt Cmaj} variable {\sf Fig. 4.1: C major chord with MIDI note numbers}.
		
		Can also play same chord using just a single line of code where pass list of numbers directly to {\tt playNote} function.
		\begin{verbatim}
			playNote([ 48, 52, 55 ])
		\end{verbatim}
		But, using variable is nice because it helps make our code easier to read \& understand. Here are a few other chord examples:
		\begin{verbatim}
			Cmaj = [ 48, 52, 55 ] # C major chord
			Fmaj = [ 53, 57, 60 ] # F major chord
			Gmaj = [ 55, 59, 62 ] # G major chord
		\end{verbatim}
		A chord's name comes from 2 parts. 1st part is {\it root} note of chord -- usually 1st note in list. E.g., a F major chord starts with note 53 (or an F on piano keyboard). \& G major chord starts with note 55, a G on keyboard.
		
		2nd part of a chord's name is its type or {\it quality}. In our examples, {\tt Cmaj, Fmaj, Gmaj} are all {\it major} chords. Later in this chap review several common chord types \& how to create them in code. Each chord type has a consistent pattern. E.g., all major chords follow exact same pattern: take root note, add 4 to get 2nd note, \& then add 7 to get last note. Can build a major chord up from any base note you want as long as it follows this same pattern {\sf Fig. 4.2: Creating chords as lists of numbers in Python. Each major chord follows same pattern.}
		
		Another way to write this in code: define a single root note variable \& then create chord based on root:
		\begin{verbatim}
			root = 48
			playNote([ root, root + 4, root + 7 ]) # C major
			root = 52
			playNote([ root, root + 4, root + 7 ]) # F major
		\end{verbatim}
		1 thing to notice about this code: 1st 2 lines \& last 2 lines are almost identical. Just changing root note value. In fact, all we need to make any chord we want is its root note \& pattern that defines its quality. Once we know patterns, rest is easy.
		
		But, it would be tedious \& error prone to write out {\tt[root, root + 4, root + 7]} every time we wanted to use a major chord. Fortunately, Python gives us a powerful tool for exactly this kind of situation: {\it user-defined functions}.
		\item {\sf4.2. User-defined functions.} In 1st few chaps of this book, using functions to play music: {\tt playNote, rest, moveTo} are all functions provided to TunePad. When want to use a function, just type its name \& list parameters to send it inside parentheses. With Python, can also create our own functions to build up a musical toolkit. Creating functions also helps make code shorter \& easier to understand because we'll be able to use same segments of code over \& over again without having to copy \& paste. A quick example that creates a major chord based on a root note.
		\begin{verbatim}
			def majorChord(root):
			    chord = [ root, root + 4, root + 7 ]
			    return chord
		\end{verbatim}
		Now that have defined function, can use it as a shortcut in TunePad.
		\begin{verbatim}
			Cmaj = majorChord(48)
			Fmaj = majorChord(53)
			playNote(Cmaj, beats = 2)
			playNote(Fmaj, beats = 2)
		\end{verbatim}
		There's a lot going on with these few lines of code. Break it down line by line.
		\begin{itemize}
			\item Line 1 starts with {\tt def} keyword. This is short for ``define'', \& it tells Python that we're about to define a function.
			\item Next is name of function we're defining. In this case, calling it {\tt majorChord}, but we could use any name want as long as it follows Python's naming rules.
			\item After function name, need to list out all of function's {\it parameters} enclosed inside parentheses. In this case, there's only 1 parameter called {\it root}. If need $> 1$ parameter, separate each parameter with commas. Can think of parameters as special kinds of variables that are only usable inside of a function.
			\item After parameter list, need colon character :. This tells Python: {\it body} of function is coming next {\sf Fig. 4.3: How to declare a user-defined function in Python}.
			\item Line 2 starts body of function. Here just creating a variable called {\it chord} \& assigning it to a list of numbers that define a major chord. Numbers are root note, root note $+4$, \& root note $+7$. An important thing to notice: this line of code is intended by 4 spaces. Just like with loops, indentation tells Python: these lines are part of function body. They're considered to be inside function, not outside.
			\item Line 3 is also indented by 4 spaces because it's also part of function body. This line uses special {\tt return} keyword to say what value function produces. In this case, returning {\tt chord} variable, list of 3 numbers that make up our major chord. When Python gets to {\tt return} keyword, it immediately exits function \& returns value given on that line.
			\item Also possible to define a function with no return value. In this case, Python provides a special return value called {\tt None}.
		\end{itemize}
		That's it for our function def. Now can see how it's used on lines 4--7. Notice can call our new function multiple times to generate different chords, letting us reuse code to create more readable \& elegant programs. In rest of chap, use this same template to begin to build up a library of chord types, each with its own function.
		
		Can define variables inside a function just like you might otherwise. A variable's {\it scope} refers to where we can use this variable or function. Can think of this as level of indentation for a function. Variables defined within def of a function can only be used in that function; their scope is within that function. Following code will result in an error on line 4.
		\begin{verbatim}
			def majorChord(root):
			    chord = [ root, root + 4, root + 7 ]
			majorChord(60)
			playNote(chord) # ERROR
		\end{verbatim}
		{\tt chord} variable lives only in our {\tt majorChord} function. Can refer to this as a local variable. Alternatively, there are global variables which can be used anywhere after they've been defined. In code below, {\tt root, chord} are global variables, \& therefore can be freely used in body of any functions:
		\begin{verbatim}
			root = 60
			chord = [ root, root + 4, root + 7 ]
			def songPar1():
			    playNote(chord)
			    playNote(root)
			songPart1()
		\end{verbatim}
		Before move on, let's define 1 more function that shows how we can use our new functions inside of other functions:
		\begin{verbatim}
			def playMajorChord(root, duration):
			    chord = majorChord(root)
			    playNote(chord, beats = duration)
		\end{verbatim}
		This function uses our {\tt majorChord} function to both build a chord from a root note \& then calls {\tt playNote} to play chord. This new {\tt playMajorChord} function takes 2 parameters, {\tt root} note \& a {\tt duration} that says how long to play note.
		\begin{note}
			By convention, Python function \& variable names use lowercase letters, with different words separated by underscore character, e.g., \verb|play_major_chord|. This style is referred to as ``\verb|snake_case|''. However, this book uses a different style called ``camelCase'' where names start with a lowercase letter \& uppercase letters are used to start new words (as in {\tt playMajorChord}). Use camelCase in this book to be consistent with other programming language conventions e.g. JavaScript, but should feel free to use \verb|snake_case| for your own Python programs if want to.
		\end{note}
		\item {\sf4.3. Common chord types.} This sect reviews some of most commonly used chord types in modern musical genres. For each chord, provide pattern of numbers that defines its quality, an example of a chord of that type on piano keyboard, \& a TunePad function that generates chords from a root note. Also describe chord in terms of musical intervals. Names for intervals can be a little confusing, especially when combined with note names or MIDI note values.
		
		{\bf Common chord types{\tt/}qualities.}
		\begin{itemize}
			\item Major triad: Major 7th: Suspended 2
			\item Minor: Minor 7th: Suspended 4
			\item Diminished: Dominant 7th: Augmented
		\end{itemize}
		To hear these chords in action, go to \url{https://tunepad.com/examples/chord-functions}.
		\begin{itemize}
			\item {\sf4.3.1. Major triad.} Major chords are commonly described as cheerful \& happy. They consist of a root note, root + 5, \& root + 7. In music theory, 2nd note of a major triad is called a {\it major 3rd}, \& 3rd note is called a {\it perfect 5th}. This can be referred to as a {\it triad} due to fact there there are 3 distinct notes {\sf Fig. 4.4: C major chord.}
			\begin{itemize}
				\item Pattern: [0, 4, 7]
				\item Intervals: Major 3rd, Perfect 5th
				\item Notation: C major, CMaj, CM, C
				\item Python Function:
				\begin{verbatim}
					def majorChord(root):
					    return [ root, root + 4, root + 7 ]
				\end{verbatim}
			\end{itemize}
			\item {\sf4.3.2. Minor triad.} Minor chords convey more of a somber tone. They're very similar to major chords except that 2nd note adds 3 to root note instead of 4. In music theory, 2nd note is called a {\it minor 3rd} (instead of a major 3rd). But, even with this small change, difference in mood is dramatic {\sf Fig. 4.5: D minor chord.}
			\begin{itemize}
				\item Pattern: [0, 3, 7]
				\item Intervals: Minor 3rd, Perfect 5th
				\item Notation: D minor, Dmin, Dm C
				\item Python Function:
				\begin{verbatim}
					def minorChord(root):
					    return [ root, root + 3, root + 7 ]
				\end{verbatim}
			\end{itemize}
			\item {\sf4.3.3. Diminished triad.} Diminished chords instill tension \& instability in music. They're often used as a way to transition between chords in a progression. There are a few different types of diminished chords, but simplest, 3-note variety, is almost identical to minor triad except last note is decreased (diminished) by 1. This is called a {\it diminished 5th interval} {\sf Fig. 4.6: B diminished chord.}
			\begin{itemize}
				\item Pattern: [0, 3, 6]
				\item Intervals: Minor 3rd, Diminished 5th
				\item Notation: B dim, $\rm B^\circ$
				\item Python Function:
				\begin{verbatim}
					def diminishedTriad(root):
					    return [ root, root + 3, root + 6 ]
				\end{verbatim}
			\end{itemize}
			\item {\sf4.3.4. Major 7th.} Major 7th chord starts with a major triad \& then adds a 4th note to end of list (root + 11). This addition is called a {\it major 7th interval}. Extra note creates a more sophisticated \& contemplative feeling {\sf Fig. 4.7: C major 7th chord.}
			\begin{itemize}
				\item Pattern: [0, 4, 7, 11]
				\item Intervals: Major 3rd, Perfect 5th, Major 7th
				\item Notation: $\rm Cmaj^7,CM^7,CMa^7$
				\item Python Function:
				\begin{verbatim}
					def major7th(root):
					    return [ root, root + 4, root + 7, root + 11 ]
				\end{verbatim}
			\end{itemize}
			\item {\sf4.3.5. Minor 7th.} A minor 7th starts with a minor triad \& adds a minor 7th (root + 10). This chord is a bit more moody than major 7th in feel {\sf Fig. 4.8: D minor 7th chord.}
			\begin{itemize}
				\item Pattern: [0, 3, 7, 10]
				\item Intervals: Minor 3rd, Perfect 5th, Minor 7th
				\item Notation: $\rm Dmin^7,Dm^7$
				\item Python Function:
				\begin{verbatim}
					def minor7th(root):
					    return [ root, root + 3, root + 7, root + 10 ]
				\end{verbatim}
			\end{itemize}
			\item {\sf4.3.6. Dominant 7th.} Just like major \& minor 7ths, can create a dominant 7th by combining some of our earlier building blocks. Dominant 7th starts with a major triad \& adds a minor 7th to get pattern $[0,4,7,10]$. Combination of major \& minor intervals can create a feeling of restlessness {\sf Fig. 4.9: G dominant 7th chord}.
			\begin{itemize}
				\item Pattern: [0, 4, 7, 10]
				\item Intervals: Minor 3rd, Perfect 5th, Minor 7th
				\item Notation: $\rm G^7$
				\item Python Function:
				\begin{verbatim}
					def dominant7th(root):
					    return [ root, root + 4, root + 7, root + 10 ]
				\end{verbatim}
			\end{itemize}
			\item {\sf4.3.7. Suspended 2 \& suspended 4.} Suspended triad chords start with a major triad, but shift middle note up or down. A sus2 chord combines a root note, a major 2nd (+2), \& a perfect 5th {\sf Fig. 4.10: Csus2 chord.}
			\begin{itemize}
				\item Pattern: [0, 2, 7]
				\item Intervals: Major 2nd, Perfect 5th
				\item Notation: Csus2, $\rm C^{sus2}$
				\item Python Function:
				\begin{verbatim}
					def sus2(root):
					    return [ root, root + 2, root + 7 ]
				\end{verbatim}
			\end{itemize}
			A sus4 chord shifts middle note in other direction to a major 4th (+5) {\sf Fig. 4.11: Csus4 chord.}
			\begin{itemize}
				\item Pattern: [0, 5, 7]
				\item Intervals: Major 4th, Perfect 5th
				\item Notation: Csus4, $\rm C^{sus4}$
				\item Python Function:
				\begin{verbatim}
					def sus4(root):
					return [ root, root + 5, root + 7 ]
				\end{verbatim}
			\end{itemize}
			Can try both versions of suspended chord in interactive tutorial.
			\item {\sf4.3.8. Augmented triad.} Last type of chord we'll cover here is called an {\it augmented triad}. This is just a major triad with a ``sharpened 5th'': last note is raised from a perfect 5th (+7) to an augmented 5th (+8). This chord can add a feeling of suspense or anxiety {\sf Fig. 4.12: C augmented chord.}
			\begin{itemize}
				\item Pattern: [0, 4, 8]
				\item Intervals: Minor 3rd, Perfect 5th
				\item Notation: Caug, C+
				\item Python Function:
				\begin{verbatim}
					def augmentedTriad(root):
					    return [ root, root + 4, root + 8 ]
				\end{verbatim}
			\end{itemize}
			There are many, many other kinds of chords we can explore that add extra notes or use notes in different patterns. Extended chords bring in intervals like 9ths, 11ths, \& 13ths, \& inverted chords shift position of root note. With 88 keys on a piano keyboard \& dozens of chord qualities to choose from, there are hundreds \& hundreds of possible chords we can use in a given song. How do we reduce this complexity to generate music that sounds good? There are 3 answers to this question:
			\begin{itemize}
				\item 1st, {\it musical keys} are like templates that give us a collection of chords \& notes that will sound good together. Once we know what key we're in, set of possible chords becomes much more manageable. Next chap will cover main ideas behind keys \& scales.
				\item 2nd, there are standard {\it chord progressions} that are used consistently in different genres of music. A chord progression is a sequence of chords that set up a compositional structure for a piece of music. In Chap. 6, show how to generate progression of chords from common templates.
				\item 3rd answer is simply hard-won experience. As develop your musical ear, will become more \& more familiar with chord types \& progressions \& how they're used in different genres. This experience will help you begin to innovate \& improvise.
			\end{itemize}
			Common interval names: {\sf[Table: Semitones: Name]}
		\end{itemize}
		\begin{note}
			Function names have to start with a letter (lowercase or uppercase). Names can include letters, numbers, \& underscore \verb|_| character. Unicode characters are also allowed.
		\end{note}
	\end{itemize}
	\item {\sf Interlude 4: Playing Chords.} In this interlude we're going to explore a few options for playing chords using TunePad \& Python. When composing harmony of your song, have more to consider than just what chords to choose. Also have to consider how to play these chords. Subtle variation in timbre, harmonics, \& timing can make a huge difference in sound that you ultimately produce. In Chap. 4, saw how to play chords using a list \& a single {\tt playNote} statement like this:
	\begin{verbatim}
		playNote([48, 53, 55], beats = 4.0)
	\end{verbatim}
	This is your most basic \& most mechanical sounding option. Here are a few other ideas \& techniques to experiment with. Can follow along online with this TunePad project \url{https://tunepad.com/interlude/play-chords}.
	\begin{itemize}
		\item {\sf Option 1: Block chords.} With block chords you play every note in a chord at exact same time \& for exact same duration. This approach is simple \& can add a strong rhythmic feel to your music. But in some situations using block chords can sound harsh \& overly mechanical. Here's a simple function that takes a chord (a list of numbers) \& plays each note at same time for an equal duration:
		\begin{verbatim}
			def block(chord, beats):
			    playNote(chord, beats)
		\end{verbatim}
		\item {\sf Option 2: Rolled chords.} Sometimes when humans play chords, they introduce subtle variations in timing between note onsets. This variation can be intentional \& exaggerated or simply a natural result of playing or strumming chords by hand. This style is called a {\it rolled} chord. Might roll a chord if want to bring out a change in harmony or if you want to emulate a strummed instrument. Artists like Dr. {\sc Dre} have used rolled chords on piano to create iconic sounds. Can also combine rolled chords \& block chords. If your chord progression is changing chords, can draw attention to this by rolling chord that changes. This function rolls a chord by adding a short, fixed delay between each note onset.
		\begin{verbatim}
			def rolled(chord, duration):
			    delay = 0.1 # how far to space out note start times
			    offset = 0 # accumulated delay
			    for note in chord:
			        playNote(note, beats = delay, sustain = duration - offset)
			        offset += delay # keep track of accumulated delay
			    fastForward(duration - offset)
		\end{verbatim}
		Function uses a couple of ``bookkeeping'' variables called {\tt delay, offset}. {\tt delay} variable just says how long to pause before each successive note in chord is played. {\tt offset} variable keeps track of total amount of delay that we've introduced in for loop. Use {\tt offset} variable in 2 places. 1st, on line 6, adjust {\tt sustain} parameters so that all of notes in chord are released at exact same time (sustain parameter lets not ring out longer than what gets passed in beats parameter). 2nd, on line 9, adjust playhead position after loop finishes. This makes it so that calling rolled function advances playhead forward by {\it exact amount} specified in {\tt duration} parameter. 1 quick note: line 7 uses plus-equal operator $+=$. This is a short hand way of saying {\tt offset = offset + delay}.		
		\item {\sf Option 3: Random rolled chords.} Can take this technique a step further by {\it randomly} varying note onset times. To do this, use 1 of Python's utilities from random module: {\tt uniform} function. This function takes an input of 2 numbers \& generates a random decimal number between those 2 numbers. Will use this to generate an offset between each successive note in chord. As with previous example, use {\tt sustain} parameter of {\tt playNote} to hold out each note for remaining duration of original inputted beats, subtracting total cumulative amount of offset each time.
		\begin{verbatim}
			from random import uniform
			def rolled(chord, duration):
			    max_delay = 0.15
			    offset = 0
			    for note in chord:
		        	next_delay = uniform(0, max_delay)
			        playNote(note, beats = next_delay, sustain = duration - offset)
			        offset += next_delay
			    fastForward(duration - offset)
		\end{verbatim}
		This code is a little more complicated than previous example, talk through it 1 line at a time. On 1st line, importing {\tt uniform} function. On line 2, defining our rolled function with 2 parameters: a list of notes in a chord \& number of total beats. On line 3, defining a constant value that defines maximum value that our offset between 2 notes can take. Higher values will create a more spaced-out sound, \& lower values will create more closed, tighter-sounding chords. On line 4, initialize a variable to track total offset at each step, which starts at 0. Starting on line 5, iterate through each note of chord. At each step, calculate offset to next note \& then call {\tt playNote}. Finally, on line 9, move playhead remaining number of beats.
		
		If this code seems confusing, that's okay. Dive more into understanding this kind of code in later chaps. Can treat this function as another tool in your coding toolkit.
		\item {\sf Option 4: Arpeggios.} Another way of playing chords: play 1 note at a time. This method is called an {\it arpeggio}. Order you play notes doesn't matter. Can start at any note \& play notes of chord in any order to get sound that's right for your track. In code below, starting with lowest note \& working up in increasing order of pitch.
		\begin{verbatim}
			def arpeggio(chord, total_beats):
			    duration = total_beats / len(chord)
			    for note in chord:
			        playNote(note, beats = duration)
		\end{verbatim}
		On 1st line, set up our function def, which takes 2 parameters: a list of notes in a chord \& total number of beats to play chord. On 2nd line, calculate duration of each individual note by evenly dividing total number fo beats by number of notes in chord. On lines 3 \& 4, use a for loop to iterate through list of notes in chord, playing each note for same number of beats.
		\item {\sf Option 5: Patterned arpeggios.} This arpeggio function is ok, but try to make sth a little more interesting. Remember, don't have to play each note evenly \& can switch up order of chord. Define a function that takes a 7th chord, which has exactly 4 notes, \& play it over duration of a measure. Going to use concept of {\it indexing}, which read more about in next chap. For now, just know that indexing is how we access specific elements of a list. To index a list in Python, use square brackets around position of element want to use. These positions start at 0, so if have a variable {\tt chord} \& want to access 1st element of list, root, would type {\tt chord[0]}. With this in mind, define a function. Have 4 beats to work with \& 4 notes -- that's indices 0--3. A quick example of what's possible, but you can experiment with different note variations \& note orders to get different effects.
		\begin{verbatim}
			def my_pattern(chord):
			    playNote(chord[0], 0.75)
			    playNote(chord[1], 0.25)
			    playNote(chord[2], 0.5)
			    playNote(chord[1], 0.5)
			    playNote(chord[3], 0.5)
			    playNote(chord[2], 0.5)
			    rest(1)
		\end{verbatim}
		In this function, we aren't doing anything fancy to iterate through chord, \& don't need to calculate our beats each time. Our beat values for lines 2--8 add up to exactly 4.0 beats. If following along, can try tweaking these to be different values that still up to 4.0 beats. Can also try tweaking indices of notes to change which chord tone plays.
	\end{itemize}
	\item {\sf5. Scales, keys, \& melody.} {\it Scales} are patterns of notes played 1 at a time in ascending or descending order of pitch. Most scales span 1 octave using some subset of 12 possible notes on piano keyboard. When scale completes octave, pattern starts over. {\it Keys} are similar to scales except ordering of notes doesn't matter, \& they contain all of notes in scale regardless of octave that you start on. Keys are like templates that help us select notes \& chords that we know will sound good together. Keys give harmonic \& melodic structure to music.
	\begin{itemize}
		\item {\sf5.1. Chromatic scale.} Building blocks of scales are half steps \& whole steps. Half steps are smallest interval commonly used in music \& distance between 2 notes that are next to each other in pitch \& on piano keyboard. Whole steps are made up of 2 half steps.
		
		Most basic scale is chromatic scale. In this scale, every note is exactly 1 half step up from previous note. This scale can start on any note \& spans an octave in 12 notes. Starting with a C on piano keyboard, would have following notes: C C\# D D\# E F F\# G G\# A A\# B. Or, using MIDI (Musical Instrument Digital Interface) note numbers we could also write: 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59.
		
		Playing a chromatic scale in TunePad is easy using a loop:
		\begin{verbatim}
			# loop from 48 up to (but not including) 60
			for note in range(48, 60):
			    playNote(note)
		\end{verbatim}
		If wanted to play a chromatic scale starting on a different root note, could just change numbers in {\tt range} function above.
		\item {\sf5.2. Major \& minor scales.} Perhaps most important scales in Western music are major \& minor scales. These scales each use 7 out of 12 possible notes in an octave. There are 12 major scales \& 12 minor scales, 1 for each possible starting pitch. After 7th note, next note would be 1st note -- or {\it tonic} -- an octave up. Scales are named by their tonic \& quality in same way that chords are named. A major scale starting on note D would be called {\it D Major}.
		
		Major scales are commonly described as cheerful \& happy (like major chords). Major scale is made up of following intervals: {\it whole step, whole step, half step, whole step, whole step, whole step, half step}. Major scale starting on C would have notes shown in {\sf Fig. 5.1: Whole step \& half step intervals of C major scale.}
		
		In MIDI version, can see whole steps skip up by 2 notes, which half steps skip up by 1 step. {\sf[Table: Note names: MIDI numbers: Intervals]}.
		
		Minor scales also use 7 notes out of each octave, but in a different order than major scales. This difference in intervals contributes to different emotional connotation of scale. Minor scales are commonly described as sad, melancholy, \& distant. A minor scale starting on C would have following notes: {\sf[Table: Note names: MIDI numbers: Intervals]}.
		
		Major \& minor scales are both examples of {\it modes}. Modes are simply different ways of ordering intervals of a scale.
		\item {\sf5.3. Pentatonic scales.} Pentatonic scales are a subset of notes of major \& minor scales. There are 5 notes in a pentatonic scale. These scales have no half step intervals, which results in less dissonance between notes. Many common melodies are based on pentatonic scales, especially in folk \& pop music. Melody of {\it Amazing Grace} is pentatonic, as is {\sc Ed Sheeran}'s {\it Shape of You}.
		
		There are both major \& minor pentatonic scales. Major pentatonic is created by omitting 4th \& 7th notes of major scale. Minor pentatonic omits 2nd \& 6th notse of minor scale. Can experiment with sound of pentatonic scale by playing only black keys of piano keyboard, which forms either an F\# major pentatonic scale or a D\# minor pentatonic scale {\sf Fig. 5.2: C Major Pentatonic Scale \& F\# Major Pentatonic Scale.} F\# Major pentatonic scale uses only black keys of keyboard. D\# Minor pentatonic scale starts with D\# \& uses only black keys as well.
		\item {\sf5.4. Building scales in TunePad.} Building scales in TunePad is similar to building chords. Because scales are just patterns of intervals (spaces between notes), can create short functions to generate scales. Every major scale has an identical pattern of intervals, \& same is true for minor scales as well. Only thing that changes is starting note. To generate scales in TunePad, all we need to do is decide what note to start on \& then apply pattern to this starting note.
		
		1 of advantages of thinking about music in terms of computer code: we don't have to memorize endless scales \& combinations of notes \& chords that make up different keys. Professional musicians train for years to learn how to play different scales without having to think about it so that they can fluidly switch from 1 key to another. This is part of what makes improvisational musicians so impressive. Playing a solo means knowing exactly which notes \& chords can be played \& how those notes \& chords relate to a genre or theme of a piece being performed.
		
		A quick example of generating a scale with Python code in TunePad:
		\begin{verbatim}
			def majorScale(tonic):
			    intervals = [ 0, 2, 4, 5, 7, 9, 11 ]
			    return [ i + tonic for i in intervals ]
		\end{verbatim}
		Example above uses a new Python concept called a {\it list comprehension}. A list comprehension is a shorthand way to create a list in Python. Line 2 uses a list comprehension to create a new list consisting each element of intervals list added to tonic value: {\tt[ i + tonic for i in intervals]} This is equivalent to writing:
		\begin{verbatim}
			result = [ ]
			for i in intervals:
			    result.append(i + tonic)
		\end{verbatim}
		2nd version is a little more cumbersome to write than 1st version using list comprehension, although either version is fine to use.
		\begin{itemize}
			\item {\sf5.4.1. Major scale.}
			\begin{itemize}
				\item Intervals: [ 0, 2, 4, 5, 7, 9, 11]
				\item Notation: C major, CMaj, CM, C
				\item Python Function with List Comprehension:
				\begin{verbatim}
					def majorScale(tonic):
					    intervals = [ 0, 2, 4, 5, 7, 9, 11 ]
					    return [ i + tonic for i in intervals ]
				\end{verbatim}
			\end{itemize}
			An alternative Python function with a loop instead of a list comprehension:
			\begin{verbatim}
				def majorScale(tonic):
				    intervals = [ 0, 2, 4, 5, 7, 9, 11 ]
				    scale = [ ]
				    for i in intervals:
				        scale.append(i + tonic)
				    return scale
			\end{verbatim}
			A 3rd variation with no loop \& no list comprehension:
			\begin{verbatim}
				def majorScale(tonic):
				    return [ tonic, tonic + 2, tonic + 4, tonic + 5, tonic + 7, tonic + 9, tonic + 11 ]
			\end{verbatim}
			\item {\sf5.4.2. Minor scale.}
			\begin{itemize}
				\item Intervals: [ 0, 2, 3, 5, 7, 8, 10 ]
				\item Notation: C minor, Cmin, Cm
				\item Python Function
				\begin{verbatim}
					def minorScale(tonic):
					    intervals = [ 0, 2, 3, 5, 7, 8, 10 ]
					    return [ i + tonic for i in intervals ]
				\end{verbatim}
			\end{itemize}
			\item {\sf5.4.3. Major pentatonic scale.}
			\begin{itemize}
				\item Intervals: [ 0, 2, 4, 7, 9 ]
				\item Python Function:
				\begin{verbatim}
					def majorPentScale(tonic):
					    intervals = [ 0, 2, 4, 7, 9 ]
					    return [ i + tonic for i in intervals ]
				\end{verbatim}
			\end{itemize}
			\item {\sf5.4.4. Minor pentatonic scale.}
			\begin{itemize}
				\item Intervals: [ 0, 3, 5, 7, 10 ]
				\item Python Function:
				\begin{verbatim}
					def minorPentScale(tonic):
					    intervals = [ 0, 3, 5, 7, 10 ]
					    return [ i + tonic for i in intervals ]
				\end{verbatim}
			\end{itemize}
			\item {\sf5.4.5. Chromatic scale.}
			\begin{itemize}
				\item Intervals:[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
				\item Python Function:
				\begin{verbatim}
					def minorPentScale(tonic):
					    intervals = [ 0, 3, 5, 7, 10 ]
					    return [ i + tonic for i in intervals ]
				\end{verbatim}
			\end{itemize}
			Try these functions at \url{https://tunepad.com/examples/build-scales}.
		\end{itemize}
		\item {\sf5.5. Playing scales in TunePad.} Now have seen how to build a scale, can use functions from prev sect to play music. Unlike with chords, notes of a scale aren't usually played all at once. Most basic way to play a scale: play 1 note at a time, in ascending order. Somehow we have to access each element of list individually \& pass that to {\tt playNote} {\sf Fig. 5.3: A representation of a list with values \& indices.}
		
		Each element can be accessed using its position in list -- called an {\it index}. In coding, 1st element of a list is at index 0; 2nd element of list is at index 1, 3rd at index 2; \& so on. In Python can also access last element of a list at index $-1$. Accessing individual elements of a list is referred to as {\it indexing}. In code, do this by using square brackets \& index number. Can also use this technique to change value of individual elements in a list.
		\begin{verbatim}
			notes = [ 60, 62, 64 ]
			notes[2] = 66 # replace the value 64 with 66
			playNote( notes[0] )
			playNote( notes[1] )
			playNote( notes[2] )
		\end{verbatim}
		In line 1, define a new list called {\it notes} with 3 values. In line 2, replace value at index 2 with new value of 66. In lines 3--5, play each note of list, 1 at a time. {\it1 of most confusing parts about computer programming for beginners: lists start at index 0 \& end at an index 1 less than length of list.} However, with a little practice this become less \& less confusing.
		\begin{note}
			If try to index into a list with an index that doesn't exist, Python will stop running \& complain with an {\tt IndexError}. Because indices start at 0, valid indices are 0 all way up to length of list minus 1.
		\end{note}
		Another way to iterate through a list is by using a {\it for loop}. Previously, when have seen loops, have used them to do exact same operation multiple times in a row. Recall syntax of a for loop:
		\begin{verbatim}
			for var in range(start, stop):
		\end{verbatim}
		Can replace {\tt range(start, stop)} part of a for loop with a list instead. This will execute body of loop once for every element in list. There's a special variable here called ``loop variable'' that gets set to value of each consecutive element in list every time the loop repeats. In code above, {\tt var} is loop variable, but can use any valid Python variable name. E.g., a loop that plays all notes of a major scale starting on note 60.
		\begin{verbatim}
			for note in majorScale(60):
			    playNote(note)
		\end{verbatim}
		In this example, {\tt note} is our loop variable. For every iteration of loop it gets set to next note in scale. Give this a try at \url{https://tunepad.com/examples/play-scales}.
		\item {\sf5.6. Other kinds of scales.} There are many other types of scales, but variants of major \& minor scale are most common in popular music. Other scales that we don't cover here include set of church modes, whole tone scales, diminished scales, \& modes of limited transposition.
		
		-- {\sf Các loại thang âm khác.} Có nhiều loại thang âm khác, nhưng các biến thể của thang âm trưởng \& thứ là phổ biến nhất trong âm nhạc đại chúng. Các thang âm khác mà chúng tôi không đề cập ở đây bao gồm bộ các chế độ nhà thờ, thang âm toàn cung, thang âm giảm, \& chế độ chuyển cung hạn chế.
		
		Above have discussed scales common to Western music, but concept of collections of notes is cross-cultural. Arabic maqam is system of melodic modes in traditional Arabic music used in both compositions \& improvisations. In Indian classical music Rage are collections of melodic modes \& motifs, each connoting a distinct personality or emotion. Gamelan music in Indonesia is organized by Pathet, which is a system of hierarchies of notes in which different notes have prominence. Composers from West have often borrowed -- or in some cases, stolen -- these scales for their own music. This raises many issues of appropriation \& exploitation within music industry. Music industry has a long history of marginalizing groups while also profiting off of cultural traditions without properly compensating or recognizing musical provenance.
		
		-- Ở trên đã thảo luận về các thang âm phổ biến trong âm nhạc phương Tây, nhưng khái niệm về tập hợp các nốt nhạc là liên văn hóa. Maqam tiếng Ả Rập là hệ thống các chế độ giai điệu trong âm nhạc Ả Rập truyền thống được sử dụng trong cả sáng tác \& ngẫu hứng. Trong âm nhạc cổ điển Ấn Độ, Rage là tập hợp các chế độ giai điệu \& họa tiết, mỗi chế độ biểu thị 1 tính cách hoặc cảm xúc riêng biệt. Âm nhạc Gamelan ở Indonesia được tổ chức theo Pathet, đây là 1 hệ thống phân cấp các nốt nhạc trong đó các nốt nhạc khác nhau có sự nổi bật. Các nhà soạn nhạc phương Tây thường mượn -- hoặc trong 1 số trường hợp, đánh cắp -- các thang âm này cho âm nhạc của riêng họ. Điều này làm nảy sinh nhiều vấn đề về chiếm đoạt \& khai thác trong ngành công nghiệp âm nhạc. Ngành công nghiệp âm nhạc có lịch sử lâu dài về việc gạt ra ngoài lề các nhóm trong khi cũng kiếm lợi từ các truyền thống văn hóa mà không đền bù hoặc công nhận nguồn gốc âm nhạc 1 cách thỏa đáng.
		\item {\sf5.7. Keys.} When writing music, there are seemingly endless notes to choose from. Keys are 1 way to narrow down question of what note to choose. Keys are underlying organizational framework of most music \& encode both melodic \& harmonic structures \& rules. Knowing these rules (\& how to break them) helps us to write music that listeners can easily comprehend \& appreciate.
		
		Concept of keys is closely related to that of scales. Keys are composed of all of notes in all of octaves that make up scale with same name. E.g., notes in key C major are same as notes in C major scale across all octaves. But, while scales are usually played in increasing or decreasing order of pitch, ordering of notes in a key doesn't matter. Notes that are part of a given key are called {\it diatonic}, \& remaining notes that are not part of that key are called {\it chromatic}.
		
		Hundreds of years ago, different keys used to be associated with different emotions, so composers would choose specific keys that reinforce mood of their composition. This is because intervals in each key were slightly different due to system of tuning; different keys were actually aurally distinct from 1 another. In modern times, each key is made up of exact same intervals.
		\item {\sf5.8. Circle of 5ths.} Keys are organized according to {\it Circle of 5ths}. Circle of 5ths is essentially a pattern of intervals. Moving clockwise around circle is moving tonic note up by a 5th from previous key. This adds 1 raised -- or sharp -- note as 7th note of scale. Alternatively, moving counterclockwise raises tonic by a 4th \& is often referred to as Circle of 4ths. This adds 1 flat note as 4th note of scale {\sf Fig. 5.4: Circle of 5ths arranges musical keys.}
		
		Major \& minor keys that share all of same notes are considered relative keys. For a minor scale, relative major scale starts on 3rd note; for a major scale, relative minor starts on 6th note. Relative minor key of C major is A minor, \& relative major of A minor is C major.
		
		Keys that are adjacent on Circle of 4ths or 5ths -- e.g., D major \& G major -- share nearly all of same notes \& are considered {\it closely related}. Relative major or minor key for a given key is also considered closely related. Generally, when a song changes keys -- also known as {\it modulating} -- it goes to 1 of closely related keys. Because closely related keys share most of same notes, modulating to 1 of these keys is less jarring to listener.
		\item {\sf5.9. Melody.} {\it Melody} is central component of much of music we listen to. It's part of a song that gets stuck in your head. Much of what goes into great melody writing is intuition \& practice, but knowing a bit of theory can help you get started. Melodies have 2 essential parameters: pitch \& rhythm. These elements are of equal importance, but in this sect, mostly be looking at pitch.
		
		When it comes to writing melodies, understanding that you are working within confines of keys, scales, \& a harmonic chord progression is a great place to start. Can use our song's harmony to provide a structural scaffold. Often, melodies place chord tones from harmony on strong beats of measure (beats 1 \& 3). These tones are consonant with harmony, i.e. they sound pleasing. Simplest melody might stick solely to these chord tones. In example below, only play chord tones of C major \& D minor.
		\begin{verbatim}
			# over C major
			playNote(55, 0.75)
			playNote(55, 0.25)
			playNote(52, 1)
			playNote(48, 0.5)
			rest(1.5)
			
			# over D minor
			playNote(57, 0.75)
			playNote(57, 0.25)
			playNote(53, 1)
			playNote(50, 0.5)
		\end{verbatim}
		Follow along with these examples at \url{https://tunepad.com/examples/simple-melody}.
		
		Dissonance is also a powerful tool in melody writing. This can add interest \& variation \& sometimes have an intense emotional impact on listeners. A melody with no dissonance, that only plays chord tones, becomes boring. 1 way to utilize dissonance: add notes in between our chord tones to fill in our melody. Can choose notes that correspond with scale based on our song's key. In example below, now filling in space between last 2 notes of each measure:
		\begin{verbatim}
			# over C major
			playNote(55, 0.75)
			playNote(55, 0.25)
			playNote(52, 0.5)
			playNote(50, 0.5) # passing tone
			playNote(48, 0.5)
			rest(1.5)
			
			# over D minor
			playNote(57, 0.75)
			playNote(57, 0.25)
			playNote(53, 0.5)
			playNote(52, 0.5) # passing tone
			playNote(50, 0.5)
		\end{verbatim}
		Sth to consider when writing a melody is {\it contour}. Contour describes shape melody takes: natural rises \& falls in pitch. A melody can either move stepwise to adjacent notes or leap to more distant notes. This motion can either decrease or increase in pitch. I.e., have 4 types of motion a melody might take, each with different connotations. E.g., might hear a melody that opens with a large leap as more emotional. Can expect most melodies to be within range of about an octave to an octave \& a half. In example below, combine idea of leaps to chord tones \& passing tones:
		\begin{verbatim}
			# over C major
			playNote(55, 1)
			playNote(64, 1.5) # large leap
			playNote(60, 1.5)
			
			# over D minor
			playNote(57, 1)
			playNote(65, 1.5) # large leap
			playNote(67, 1) # passing tone
			playNote(69, 0.5)
		\end{verbatim}
		If consider contour \& pitch content as a vertical phenomenon, can think of melodic form as a horizontal structure. Can break melodies into parts called {\it phrases}. If melodies are paragraphs, then phrases are like musical sentences. They are complete thoughts that are punctuated \& combined to form more complete \& cohesive ideas. Phrases are often 2, 4, or 8 bars in duration. These phrases are combined to form larger structures, which become overall song form. Explore this more in Chap. 9.
		
		Principles of repetition \& variation work in opposition to 1 another. In writing melodies, there generally needs to be enough repetition so that a listener has sth to latch onto. But with too much repetition, a melody becomes boring. A catchy melody is result of striking a balance between these 2 forces.
		
		1 way to build intuition about melody writing: analyze melodies from artists you like \& want to emulate. Critical listening skills that you develop from analyzing existing melodies is directly applicable to writing your own melodies. Experimentation \& improvisation are other great ways to build up this intuition. Can try tapping out rhythms to serve basis of a melody, or play around on a piano or another instrument. Try playing around with our automatic melody generator at \url{https://tunepad.com/examples/melody-gen}.
	\end{itemize}
	\item {\sf Interlude 5: Lean On Me. {\sc bill Withers}.} Practice using chords by recreating a small part of piano harmony from song {\it Lean on Me} by {\sc bill Withers} (1972), Columbia Records. A simplified version of chord structure that you can try entering into a TunePad project.
	\begin{verbatim}
		# Chord Variables
		Cmaj = [ 48, 52, 55 ]
		Dmin = [ 50, 53, 57 ]
		Emin = [ 52, 55, 59 ]
		Fmaj = [ 53, 57, 60 ]
		Bdim = [ 47, 50, 53 ]
	\end{verbatim}
	{\sf[Table: Chord: Beats: Python]}. Can see full code here: \url{https://tunepad.com/interlude/chord-progressions}. 1 thing to notice is how chord progression mirrors emotion of song as a whole. {\sc Withers} mixes upbeat (``I'll be your friend. I'll help you carry on'') with harsh reality of life (``We all have pain. We all have sorrow''). Harmony starts on a major chord (C major) but then passes through a succession of minor chords (D minor, E minor) before eventually landing on more encouraging major chords for prolonged notes (F major). It's as if harmony is also saying: we're going to go through some hard times, but it'll work out in end.
	
	Version above is slightly modified from original in that we're using simplified chords \& a diminished B chord at end that slides into a C major. As listen to it, notice how B diminished feels unstable as if it needs to resolve into C major to bring harmony full circle to signal a transition in song.
	
	{\bf More elegant code.} 1 temping way to code this up would be to just type all of {\tt playNote} functions, 1 after another. This works, but it's not necessarily most elegant way to express music. When you're coding, there's always $> 1$ way to solve a problem, so it's good to get into habit of asking if there are other, easier ways to accomplish things. E.g., what if wanted to change velocity of all of chords? We'd have to edit 1 line at a time or use find \& replace. As an alternative, what if we put all of chords into a list \& then iterated through that list with a for loop?
	\begin{verbatim}
		chords = [ Cmaj, Cmaj, Dmin, Emin, Fmaj, Fmaj, Emin, Dmin, Cmaj ]
		for chord in chords:
		    playNote(chord)
	\end{verbatim}
	This would be an improvement. If nothing else, would have reduced number of lines needed to play harmony. Obvious problem: it won't work because notes are different lengths. Some are long (4 beats) \& others are short (1 beat). But this code plays all chords with equal duration.
	
	If there were an easy way to iterate through 2 lists at same time, could make 1 list with chords \& another with durations. Python includes exactly this kind of feature with sth called {\tt zip} function. Think of it like a zipper that merges 2 Python lists together instead of 2 pieces of fabric. It walks through lists, element by element, \& merges them together into pairs of values. Result looks sth like this:
	\begin{verbatim}
		chords = [ Cmaj, Cmaj, Dmin, Emin, Fmaj, Fmaj, Emin, Dmin, Cmaj ]
		durations = [ 4, 1, 1, 1, 4, 1, 1, 1, 4 ]
		for chord, duration in zip(chords, durations):
		    playNote(chord, beats = duration)
	\end{verbatim}
	Complete example or you can try it online: \url{https://tunepad.com/interlude/lean-on-me}.
	\begin{verbatim}
		CM = [ 48, 52, 55, 55 + 12 ]
		Dm = [ 50, 53, 57, 57 + 12 ]
		Em = [ 52, 55, 59, 59 + 12 ]
		FM = [ 53, 57, 60, 60 + 12 ]
		Bd = [ 47, 50, 53, 53 + 12 ]
		
		chords = [ CM, CM, Dm, Em, FM, FM, Em, Dm, CM, CM, Dm, Em ]
		durations = [ 4, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 3, 4 ]
		
		for chord, duration in zip(chords + [ Em, Dm ], durations):
		    playNote(chord, beats = duration)
		    
		for chord, duration in zip(chords + [ Bd, CM ], durations):
		    playNote(chord, beats = duration)
	\end{verbatim}
	\item {\sf6. Diatonic chords \& chord progressions.} Now that have some familiarity with chords, question is how to use them. How can we reduce hundreds of chords \& thousands of combinations of chords down to a manageable set of options? How can we explore creative musical space that chords provide without feeling overwhelmed?
	
	-- {\sf Hợp âm diatonic \& tiến trình hợp âm.} Bây giờ đã quen với hợp âm, câu hỏi đặt ra là làm thế nào để sử dụng chúng. Làm thế nào chúng ta có thể giảm hàng trăm hợp âm \& hàng nghìn tổ hợp hợp âm xuống một tập hợp các tùy chọn có thể quản lý được? Làm thế nào chúng ta có thể khám phá không gian âm nhạc sáng tạo mà hợp âm cung cấp mà không cảm thấy choáng ngợp?
	
	1 answer to these questions: use keys to select subsets of chords that we know will sound good together. From there we can follow guidelines for arranging chords into sequential patterns called {\it progressions} that will support various harmonic elements that come together in a piece of music.
	
	This chap introduces traditional {\it Roman numeral} system for referring to chords that fit with a particular key along with methods for choosing chord progressions. Along way we'll code functions for creating chord progressions in any key using concept of Python {\it dictionaries}.
	\begin{itemize}
		\item {\sf6.1. Diatonic chords.} A {\it diatonic} chord is any chord that can be played using only 7 notes of current key. E.g., if working in key of C major, diatonic chords consist of all of chords you can play with only white keys on piano keyboard: C D E F G A B. Main diatonic chords we can make with just these 7 notes {\sf Fig. 6.1: 7 diatonic chords of C major.} Hear these chords at \url{https://tunepad.com/examples/diatonic-chords}.
		
		If have a piano keyboard handy, try playing with these 7 chords to get a feel for how they sound. What emotions do you feel as chords ring out? What patterns of chords sound good together? No matter what key we're in, there will always be 7 diatonic chords, 1 for each note in scale. To build a diatonic chord, just pick any note from scale as root of chord. Then go up 2 notes for ``3rd'' of chord, \& up 2 notes again for ``5th'' of chord. Can keep moving up by 2 notes of scale to get ``7th'' \& ``9th'' \& so on.
		
		Can refer to these chords by their scale degree (1st, 2nd, 3rd, 4th, 5th, 6th, \& 7th). A chord built on 5th note of a scale would be ``5'' chord for that key. If we're in any {\it major key}, 1st, 4th, \& 5th diatonic chords will have a major quality (regardless of starting note of key). Further, 2nd, 3rd, \& 6th chords will always have a minor quality, \& 7th chord is diminished. E.g., in C major, would have following diatonic chords: CMaj Dmin Emin FMaj GMaj Amin Bdim. This pattern of chord qualities is same for any major key because all of major keys have same pattern of note intervals. Same idea is true for minor keys. Because all minor keys have same pattern of intervals, quality of chords stays consistent. 1st, 4th, \& 5th diatonic chords are minor quality. 3rd, 6th, \& 7th chords are major quality. 2nd chord is diminished. In C minor, e.g., would have following diatonic chords: Cmin Ddim E$\flat$Maj Fmin Gmin A$\flat$Maj B$\flat$Maj. In popular music, chord progressions are made up almost entirely of diatonic chords. Recall from prev chap: melody of a song is also built on this harmonic scaffold. Melodies often use primarily notes from underlying chord progression, because these notes are more consonant \& pleasing. Notes from outside harmony are typically used in passing as decoration or as a neighboring tone.
		\item {\sf6.2. Roman numerals.} Each of 12 major keys \& 12 minor keys have 7 diatonic chords, giving us an overwhelming total of 168 diatonic chords. To reduce this complexity, musicians, producers, \& composers use a system of {\it Roman numerals} to refer to different diatonic chords by their scale degree rather than their specific name. If chord has a major quality, it gets an uppercase Roman numeral. If it has a minor quality, it gets a lowercase Roman numeral. Each key also has 1 diminished chord, which is both lowercase \& has an accompanied ${}^\circ$ symbol.
		\begin{note}
			Roman numerals are a system of numbering which originated in ancient Rome. In this system, numbers are composed of combinations of letters. In music, only numerals corresponding to numbers one through 7 are used: I, II, III, IV, V, VI, \& VII.
		\end{note}
		That gives us following Roman numerals $\forall$ of diatonic chords of any key. {\sf[Table: Scale degree (Major keys, Minor keys): 1st: 2nd: 3rd: 4th: 5th: 6th: 7th]}. With this system there are just these 7 symbols to focus on instead of 168. Roman numerals take some getting used to, but they give us a language for thinking about chord progressions without having tp refer to name of each specific chord. In coding or mathematics, this kind of generalization is called an {\it abstraction}. Abstraction means removing specific details of individual situations \& focusing instead on bigger picture patterns. In coding, use language constructs like variables, functions, \& parameters to create abstractions that reduce complexity of our code \& problems we're trying to solve.
		\item {\sf6.3. Tendency tones \& harmonic functions.} Now know how to refer to diatonic chords by their names, how do order them into pleasing chord progressions? Ordering of chords within a progression isn't random. Notes that make up chords have different tendencies relative to 1 another -- i.e. listeners hears them as wanting to resolve in expected ways when played with other notes in a chord progression. Strongest of these tendencies is pull of 7th note of a scale back to root note of scale (tonic). In most cases, hear this note as wanting to resolve upward by a half step back to tonic. If this note does not resolve, often hear progression as incomplete. 2nd \& 5th notes of a scale also have a strong pull back to tonic.
		
		Tendencies of individual notes give each chord a characteristic or {\it function}. Can think of a chord's function as its desire -- how it relates to prev chords \& how it {\it wants} to move music forward. Chords are described as having 3 primary functions: {\it tonic, predominant, \& dominant}. Most chord progressions typically progress from tonic to predominant to dominant back to tonic. {\sf[Table: Tonic chords: Predominant chords: Dominant chords]}.
		
		Chords that have same function also share many of same notes. E.g., iii (3) chords \& vi (6) chords share 2 of same notes with tonic chords (I), so they're grouped together. Predominant ii \& IV chords also share 2 notes in common, as do dominant V \& $\rm vii^\circ$ chords {\sf Fig. 6.2: Dominant V \& $\rm vii^\circ$ chords share 2 notes in common. This is easy to see when you line piano diagrams up vertically.}
		
		Chord functions might best be described as rough guidelines that help inform our decisions about composition. There are also many variations to basic chords. Chords can be inverted (i.e., root is no longer lowest note), extended with additional notes to add color, or combined with ``chromatic'' chords that include notes from outside of main key. Secondary dominant chords are dominant chords borrowed from other related keys. Knowing which chord to use in any given context comes down to musical experience, taste, \& conventions of different genres.
		\item {\sf6.4. Chord progressions.} Many common chord progressions follow scheme of {\it tonic $\to$ predominant $\to$ dominant}. Tonic brings stability \& grounding. Predominant is a departure from this stability that builds tension. Predominant pulls toward dominant, which eventually resolves back to tonic. After a chord progression finishes, it starts over. Different genres of music have different harmonic rules \& standard chord progressions, but here are flow charts that help visualize common chord progression patterns {\sf Fig. 6.3: Flowcharts for generating chord progressions in major \& minor keys.}
		
		An example of how to use these charts. If start with tonic I chord on top, might move down to dominant V chord, \& then slide up to tonic prolongation vi chord -- a subtle tease with resolution. Could then go to predominant IV chord before progression resolves back to tonic I chord \& repeats. This progression would be I $\to$ V $\to$ vi $\to$ IV (1, 5, 6, 4), which is an extremely common pattern in popular music {\sf Fig. 6.4: Example of using flowchart to generate a chord progression.}
		
		Contemporary pop music uses many of same progressions as early rock \& Roll \& Blues music -- much of early rock music came out of Blues. As a result, many early rock songs are built on Blues progressions, most notably I-IV-V. 1 of most ubiquitous progressions -- especially in early rock -- is ``doo wop'' progression I-vi-IV-V. Same chords can be reordered to form our I-V-vi-IV example. {\sf[Table: Common major progressions: Common minor progressions]}
		
		Hip-hop songs center more around rhythm \& vocals \& tend to use shorter progressions, often in a minor key with only 1 or 2 chords e.g. i-V, i-VI, \& i-ii${}^\circ$. Hip-hop developed alongside advances in recording technology that allowed early artists to remix samples from other songs, \&, as a result, genre also borrows progressions from pop \& rock music.
		
		When writing chord progressions, 1 tactic: borrow from existing songs to help you develop your own ear \& begin to think critically about harmony. Can also experiment on your own. Use harmonic conventions to narrow down some of options, but also try breaking rules as you become more confident.
		\item {\sf6.5. Chord inversions.} An {\it inverted} chord is just like an ordinary chord except that root note is no longer lowest pitch. Take C major as an example. When root note C is also lowest note of chord, say chord is in root position {\sf Fig. 6.5: C major chord in root position, 1st inversion \& 2nd inversion.}
		
		When 3rd of chord is lowest note, chord is in its 1st inversion. In case of C major, i.e., E is now lowest note. When 5th of chord is lowest, it's 2nd inversion, \& so on. Each inversion has exactly same notes as root chord, but ordering of notes by their pitch is different.
		\item {\sf6.6. Voice leading.} {\it Voice leading} deals with relationship between notes in consecutive chords in a progression. Principle behind voice leading: treat each note of a chord as an individual melodic voice. Imagine 3 human vocalists each singing 1 individual note of a chord. Because considering each voice independently, idea: minimize leaps 1 person's voice has to make between chords so that progression is smoother \& easier to sing. By considering different possible inversions of each chord we can create more of a dovetailing effect with subtle shifts between successive chords. Not only will this improve sound of your progressions, but it will also improve potential playability of music on instruments like guitar, piano, or vocal harmony. 2 figures show same progression with \& without voice leading {\sf Fig. 6.6: Chord progression I-V-vi-IV without voice leading \& with voice leading. Chords V, vi, \& IV are inverted to reduce pitch range \& to minimize movement of individual voices ``singing'' notes of chords.} Hear these examples at \url{https://tunepad.com/examples/voice-leading}.
		\item {\sf6.7. Python dictionaries.} In Python, {\it dictionaries} or {\it maps} are unordered sets of data consisting of values referenced by {\it keys}. These keys aren't same as musical keys. They're more like kind of keys that open locked doors. Each different key open its own door.
		
		Dictionaries are extremely useful in programming because they provide an easy way to store multiple data elements by name. E.g., if wanted to store information for a music streaming service, might need to save song name, artist, release date, genre, record label, song length, \& album artwork. A dictionary gives you an easy method for storing all of these elements in a single data object. (like *.bib files)
		\begin{verbatim}
			track_info = {
			    "artist" : "Herbie Hancock",
			    "album" : "Head Hunters",
			    "label" : "Columbia Records",
			    "genre" : "Jazz-Funk",
			    "year" : 1973,
			    "track" : "Chameleon",
			    "length" : 15.75 }
		\end{verbatim}
		Dictionaries are defined using curly braces with keys \& values separated by a colon. Different entries are separated by commas. After defining a dictionary, can change existing values or add new values using associated keys. Similar to way we access values in a list with an index, use square brackets \& a key to access elements in a dictionary.
		\begin{verbatim}
			track_info["artwork"] = "https://images.ssl-images-amz.com/
			images/81KRhL.jpg"
		\end{verbatim}
		In this line, because key ``artwork'' hasn't been used in dictionary yet, it creates a new key-value pair. If ``artwork'' had been added already, it would change existing value. 1 thing to notice: values in a dictionary can be any type including strings, numbers, lists, or even other dictionaries. Dictionary keys can also be strings or numerical values, but they must be unique for each value stored.
		\item {\sf6.8. Programming with diatonic chords.} With Python code, there are many different ways to determine diatonic chords for a given key. Here are {\tt majorChord, minorChord, diminishedChord} functions from Chap. 4 again.
		\begin{verbatim}
			def majorChord(root):
			    return [root, root + 4, root + 7]
			    
			def minorChord(root):
			    return [root, root + 3, root + 7]
			    
			def dimChord(root):
			    return [root, root + 3, root + 6]
		\end{verbatim}
		Can use these functions to define variables for each diatonic chord in C major:
		\begin{verbatim}
			I = majorChord(48)
			ii = minorChord(50)
			iii = minorChord(52)
			IV = majorChord(53)
			V = majorChord(55)
			vi = minorChord(57)
			vii0 = diminishedChord(59)
		\end{verbatim}
		This code is clear \& readable, but not as reusable as it could be. What if want to play in a different key? Or in a different octave? Would have to change {\it each} line of code. As an alternative, could write a function that takes tonic as an input \& returns a dictionary that maps Roman numerals to individual diatonic chords.
		\begin{verbatim}
			def buildChords(tonic):
			    numerals_lookup = { "I" : majorChord(tonic),
				                    "ii" : minorChord(tonic+2),
				                    "iii" : minorChord(tonic+4),
				                    "IV" : majorChord(tonic+5),
				                    "V" : majorChord(tonic+7),
				                    "vi" : minorChord(tonic+9),
				                    "vii0" : diminishedChord(tonic+11)}
		        return numerals_lookup
		\end{verbatim}
		Method above works for major keys, but what if wanted it to work with minor keys as well? Can add another parameter \& an {\tt if-else} statement to handle this as well.
		\begin{verbatim}
			def buildChords(tonic, mode):
			    if mode == "major":
			        numerals_lookup = {"I" : majorChord(tonic),
			                           "ii" : minorChord(tonic+2),
			                           "iii" : minorChord(tonic+4),
			                           "IV" : majorChord(tonic+5),
			                           "V" : majorChord(tonic+7),
			                           "vi" : minorChord(tonic+9),
			                           "vii0" : diminishedChord(tonic+11)}
			    else:
			        numerals_lookup = {"i" : minorChord(tonic),
			                           "ii0" : diminishedChord(tonic+2),
			                           "III" : majorChord(tonic+3),
			                           "iv" : minorChord(tonic+5),
			                           "v" : minorChord(tonic+7),
			                            "VI" : majorChord(tonic+8),
			                           "VII" : majorChord(tonic+10)}
			    return numerals_lookup			    
		\end{verbatim}
		These are far from only solution for creating diatonic chords for different keys. In general, there are almost an endless number of ways to solve complex problems in programming. Figuring out which approach is best for a given circumstance takes practice \& experience, but your goal is usually to write code that is as simple \& easy to understand as possible. Try this code at \url{https://tunepad.com/examples/chord-dictionary}.
	\end{itemize}
	\item {\sf Interlude 6: Random Chord Progressions.} A short Python example that generates \& then plays random chord progressions using charts in {\sf Fig. 6.3}. Can start with a table that maps each chord to a simplified set of possible transition chords. Table below on left uses Roman numerals, \& table on right uses Arabic numbers to show same thing. Note these tables don't include all of possibilities from flow charts above, but most of possible transitions are included. Now can turn this transition table into a computer algorithm using Python.
	\begin{itemize}
		\item {\bf Step 1: Random chord algorithm.} Create a new piano cell in a TunePad project \& add this code.
		\begin{verbatim}
			from random import choice # import the choice function
			
			progression = [ 1 ] # create a list with just one chord
			chord = choice([3, 4, 5, 6]) # choose a random next chord
			
			while chord != 1: # repeat while chord is not equal to 1
			    progression.append(chord) # add the next chord to the list
			    if chord == 2: # if the current chord is 2
			        chord = choice([1, 5]) # then choose a random next chord
			    elif chord == 3: # else if the current chord is 3
			        chord = 4 # ...
			    elif chord == 4:
			        chord = choice([1, 2, 5])
			    elif chord == 5:
			        chord = choice([1, 6])
			    else: # the chord is 6
			        chord = choice([ 2, 3, 4 ])
			print(progression)
		\end{verbatim}
		Break it down line by line. Line 1 imports a function called {\it choice} from Python's {\tt random} module. {\tt choice} function selects 1 element from a list at random. Can think of it as picking a random card from a deck. Line 3 creates a variable called {\tt progression} that consists of a list with only 1 element in it. This list will hold our finished chord progression, \& start it with tonic chord 1. Line 4 picks next chord at random. Use our transition table to select from 3--6 as possible next chords in sequence. Save random choice in a variable called {\tt chord}. Line 6 is a new kind of Python loop called a {\tt while loop}. This loop repeats indefinitely until a certain condition is met. In our case, going to repeat loop until our {\tt chord} variable $= 1$. Line 7 is part of while loop. It adds our new {\tt chord} to end of {\tt progression} list using {\tt append} function. 1st time through loop, {\tt progression} list will have 2 elements, 1 \& whatever random chord was selected on line 4. Each additional time through loop, line 7 will add another chord to list. Line 8 asks {\it if} our random chord $= 2$. If so, it selects a random next chord based on values in our transition table (on line 9). Line 10
	\end{itemize}
	\item {\sf7. Frequency, fourier, \& filters.}
	\item {\sf8. Note-based production effects.}
	\item {\sf9. Song composition \& EarSketch.}
	\item {\sf10. Modular synthesis.}
	\item {\sf11. History of music \& computing.}
	\item {\sf Appendix A: Python ref.}
	\item {\sf Appendix B: TunePad programming ref.}
	\item {\sf Appendix C: Music ref.}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{\cite{Valimaki_Pakarinen_Erkut_Karjalainen2006}. {\sc Vesa Välimäki, Jyri Pakarinen, Cumhur Erkut, Matti Karjalainen}. Discrete-Time Modeling of Musical Instruments}
{\sf[283 citations]}
\begin{question}[Cf. Continuous-time modeling vs. Discrete-time modeling]
	How about continuous-time modeling of musical instruments?
\end{question}

\begin{question}[Cf. Mathematical modeling technique vs. Physical modeling technique]
	Compare Mathematical modeling technique vs. Physical modeling technique.
\end{question}

\begin{itemize}
	\item {\bf Abstract.} This article describes physical modeling techniques that can be used for simulating musical instruments. Methods are closely related to digital signal processing. They discretize system w.r.t. time, because aim: run simulation using a computer. Physics-based modeling methods can be classified as mass-spring, modal, wave digital, finite difference, digital waveguide \& source-filter models. Present basic theory \& a discussion on possible extensions for each modeling technique. For some methods, a simple model example is chosen from existing literature demonstrating a typical use of method. E.g., in case of digital waveguide modeling technique a vibrating string model is discussed, \& in case of wave digital filter technique, present a classical piano hammer model. Tackle some nonlinear \& time-varying models \& include new results on digital waveguide modeling of a nonlinear string. Discuss current trends \& future directions in physical modeling of musical instruments.
	\item {\sf1. Introduction.} Musical instruments have historically been among most complicated mechanical systems made by humans. They have been a topic of interest for physicists \& acousticians for over a century. Modeling of musical instruments using computers is newest approach to understanding how these instruments work.
	
	This paper presents an overview of physics-based modeling of musical instruments. Specifically, this paper focuses on sound synthesis methods derived using physical modeling approach. Several previously published tutorial \& review papers discussed physical modeling synthesis techniques for musical instruments sounds [73, 129, 251, 255, 256, 274, 284, 294]. Purpose of this paper: give a unified introduction to 6 main classes of discrete-time physical modeling methods, namely mass--spring, modal, wave digital, finite difference, digital waveguide \& source--filter models. This review also tackles mixed \& hybrid models in which usually 2 different modeling techniques are combined.
	
	Physical models of musical instruments have been developed for 2 main purposes: research of acoustical properties \& sound synthesis. Methods discussed in this paper can be applied to both purpose, but here main focus is sound synthesis. Basic idea of physics-based sound synthesis: build a simulation model of sound production mechanism of a musical instrument \& to generate sound with a computer program or signal processing hardware that implements that model. Motto of physical modeling synthesis: when a model has been designed properly, so that it behaves much like actual acoustic instrument, synthetic sound will automatically be natural in response to performance. In practice, various simplifications of model cause sound output to be similar to, but still clearly different from, original sound. Simplifications may be caused by intentional approximations that reduce computational cost or by inadequate knowledge of what is actually happening in acoustic instrument. A typical \& desirable simplification: linearization of slightly nonlinear phenomena, which may avert unnecessary complexities, \& hence may improve computational efficiency.
	
	In speech technology, idea of accounting for physics of sound source, human voice production organs, is an old tradition, which has led to useful results in speech coding \& synthesis. While 1st experiments on physics-based musical sound synthesis were documented several decades ago, 1st commercial products based on physical modeling synthesis were introduced in 1990s. Thus, topic is still relatively young. Research in field has been very active in recent years.
	
	1 of motivations for developing a physically based sound synthesis: musicians, composers, \& other users of electronic musical instruments have a constant hunger for better digital instruments \& for new tools for organizing sonic events. A major problem in digital musical instruments has always been how to control them. For some time, researchers of physical models have hoped: these models would offer more intuitive, \& in some ways better, controllability than previous sound synthesis methods. In addition to its practical applications, physical modeling of musical instruments is an interesting research topic for other reasons. It helps to solve old open questions, e.g. which specific features in a musical instrument's sound make it recognizable to human listeners or why some musical instruments sound sophisticated while others sound cheap. Yet another fascinating aspect of this field: when physical principles are converted into computational methods, possible to discover new algorithms. This way, possible to learn new signal processing methods from nature.
	\item {\sf2. Brief history.} Modeling of musical instruments is fundamentally based on understanding of their sound production principles. 1st person attempting to understand how musical instruments work might have been Pythagoras, who lived in ancient Greece around 500 BC. At that time, understanding of musical acoustics was very limited \& investigations focused on tuning of string instruments. Only after late 18th century, when rigorous mathematical methods e.g. PDEs were developed, was it possible to build formal models of vibrating strings \& plates.
	
	Earliest work on physics-based discrete-time sound synthesis was probably conducted by {\sc Kelly \& Lochbaum} in context of vocal-tract modeling [145]. A famous early musical example is `Bicycle Built for 2' (1961), where singing voice was produced using a discrete-time model of human vocal tract. This was result of collaboration between {\sc Mathews, Kelly \& Lochbaum} [43]. 1st vibrating string simulations were conducted in early 1970s by {\sc Hiller \& Ruiz} [113, 114], who discretized wave equation to calculate waveform of a single point of a vibrating string. Computing 1 s of sampled waveform took minutes. A few years later, {\sc Cadoz} \& his colleagues developed discrete-time mass-spring models \& built dedicated computing hardware to run real-time simulations [38].
	
	In late 1970s \& early 1980s, {\sc McIntyre, Woodhouse, \& Schumacher} made important contributions by introducing simplified discrete-time models of bowed strings, clarinet \& flute [173, 174, 235], \& {\sc Karplus \& Strong} [144] invented a simple algorithm that produces string-instrument-like sounds with few arithmetic operations. Based on these ideas \& their generalizations, {\sc Smith \& Jaffe} introduced a signal-processing oriented simulation technique for vibrating strings [120, 244]. Soon thereafter, {\sc Smith} proposed term `digital waveguide' \& developed general theory [247, 249, 253].
	
	1st commercial product based on physical modeling synthesis, an electronic keyboard instrument by Yamaha, was introduced in 1994 [168]; it used digital waveguide techniques. More recently, digital waveguide techniques have been also employed in MIDI synthesizers on personal computer soundcards. Currently, much of practical sound synthesis is based on software, \& there are many commercial \& freely available pieces of synthesis software that apply 1 or more physical modeling methods.
	\item {\sf3. General concepts of physics-based modeling.} In this sect, discuss a number of physical \& signal processing concepts \& terminology that are important in understanding modeling paradigms discussed in subsequent sects. Each paradigm is also characterized briefly in end of this sect. A  reader familiar with basic concepts in context of physical modeling \& sound synthesis may go directly to Sect. 4.
	\begin{itemize}
		\item {\sf3.1. Physical domains, variables, \& parameters.} Physical phenomena can be categorized as belonging to different `physical domains'. Most important ones for sound sources e.g. musical instruments are acoustical \& mechanical domains. In addition, electrical domain is needed for electroacoustic instruments \& as a domain to which phenomena from other domains are often mapped. Domains may interact with one another, or they can be used as analogies (equivalent models) of each other. Electrical circuits \& networks are often applied as analogies to describe phenomena of other physical domains.
		
		Quantitative description of a physical system is obtained through measurable quantities that typically come in pairs of variables, e.g. force \& velocity in mechanical domain, pressure \& volume velocity in acoustical domain or voltage \& current in electrical domain. Members of such dual variable pairs are categorized generically as `across variable' or `potential variable', e.g. voltage, force or pressure, \& `through variable' or `kinetic variable', e.g. current, velocity or volume velocity. If there is a linear relationship between dual variables, this relation can be expressed as a parameter, e.g. impedance $Z = \frac{U}{I}$ being ratio of voltage $U$ \& current $I$, or by its inverse, admittance $Y = \frac{I}{U}$. An example from mechanical domain is mobility (mechanical admittance) defined as ratio of velocity \& force. When using such parameters, only 1 of dual variables is needed explicitly, because the other one is achieved through constraint rule.
		
		Modeling methods discussed in this paper use 2 types of variables for computation, `K-variables' \& `wave variables' (also denoted as `W-variables'). `K' comes from Kirchhoff \& refers to Kirchhoff continuity rules of quantities in electric circuits \& networks [185]. `W' is shortform for wave, referring to wave components of physical variables. Instead of pairs of across \& through as with K-variables, wave variables come in pairs of incident \& reflected wave components. Details of wave modeling are discussed in Sects. 7--8, while K-modeling is discussed particularly in Sects. 4 \& 10. It will become obvious: these are different formulations of same phenomenon, \& possibility to combine both approaches in hybrid modeling will be discussed in Sect. 10.
		
		Decomposition into wave components is prominent in such wave propagation phenomena where opposite-traveling waves add up to actual observable K-quantities. A wave quantity is directly observable only when there is no other counterpart. It is, however, a highly useful abstraction to apply wave components to any physical case, since this helps in solving computability (causality) problems in discrete-time modeling.
		\item {\sf3.2. Modeling of physical structure \& interaction.} Physical phenomena are observed as structures \& processes in space \& time. In sound source modeling, interested in dynamic behavior that is modeled by variables, while slowly varying or constant properties are parameters. Physical interaction between entities in space always propagates with a finite velocity, which may differ by orders of magnitude in different physical domains, speed of light being upper limit.
		
		`Causality' is a fundamental physical property that follows from finite velocity of interaction from a cause to corresponding effect. In many mathematical relations used in physical models causality is not directly observable. E.g., relation of voltage across \& current through an impedance is only a constraint, \& variables can be solved only within context of whole circuit. Requirement of causality (more precisely temporal order of cause preceding effect) introduces special computability problems in discrete-time simulation, because 2-way interaction with a delay shorter than a unit delay (sampling period) leads to `delay-free loop problem'. Use of wave variables is advantageous, since incident \& reflected waves have a causal relationship. In particular, wave digital filter (WDF) theory, discussed in Sect. 8, carefully treats this problem through use of wave variables \& specific scheduling of computation operations.
		
		Taking finite propagation speed into account requires using a spatially distributed model. Depending on case at hand, this can be a full 3D model e.g. used for room acoustics, a 2D model e.g. for a drum membrane (discarding air loading) or a 1D model e.g. for a vibrating sting. If object to be modeled behaves homogeneously enough as a whole, e.g. due to its small size compared with wavelength of wave propagation, it can be considered a lumped entity that does not need a description of spatial dimensions.
		
		-- Việc tính đến tốc độ lan truyền hữu hạn đòi hỏi phải sử dụng 1 mô hình phân bố không gian. Tùy thuộc vào trường hợp cụ thể, đây có thể là mô hình 3D đầy đủ, ví dụ như được sử dụng cho âm học phòng, mô hình 2D, ví dụ như cho màng trống (loại bỏ tải trọng không khí) hoặc mô hình 1D, ví dụ như cho 1 cú chích rung. Nếu vật thể được mô hình hóa hoạt động đủ đồng nhất như 1 tổng thể, ví dụ như do kích thước nhỏ so với bước sóng truyền sóng, thì nó có thể được coi là 1 thực thể tập trung không cần mô tả về kích thước không gian.
		\item {\sf3.3. Signals, signal processing, \& discrete-time modeling.} In signal processing, signal relationships are typically represented as 1-directional cause-effect chains. Contrary to this, bi-directional interaction is common in (passive) physical systems, e.g. in systems where reciprocity principle is valid. In true physics-based modeling, 2-way interaction must be taken into account. I.e., from signal processing viewpoint, such models are full of feedback loops, which further implicates: concepts of computability (causality) \& stability become crucial.
		
		In this paper, apply digital signal processing (DSP) approach to physics-based modeling whenever possible. Motivation for this: DSP is an advanced theory \& tool that emphasizes computational issues, particularly maximal efficiency. This efficiency is crucial for real-time simulation \& sound synthesis. Signal flow diagrams are also a good graphical means to illustrate algorithms underlying simulations. Assume: reader is fmailiar with fundamentals of DSP, e.g. sampling theorem [242] to avoid aliasing (also spatial aliasing) due to sampling in time \& space as well as quantization effects due to finite numerical precision.
		
		An important class of systems is those that are linear \& time invariant (LTI). They can be modeled \& simulated efficiently by digital filters. They can be analyzed \& processed in frequency domain through linear transforms, particularly by Z-transform \& discrete Fourier transform (DFT) in discrete-time case. While DFT processing through fast Fourier transform (FFT) is a powerful tool, it introduces a block delay \& does not easily fit to sample-by-sample simulation, particularly when bi-directional physical interaction is modeled.
		
		Nonlinear \& time-varying systems bring several complications to modeling. Nonlinearities create new signal frequencies that easily spread beyond Nyquist limit, thus causing aliasing, which is perceived as very disturbing distortion. In addition to aliasing, delay-free loop problem \& stability problems can become worse than they are in linear systems. If nonlinearities in a system to be modeled are spatially distributed, modeling task is even more difficult than with a localized nonlinearity. Nonlinearities will be discussed in several sects of this paper, most completely in Sect. 11.
		\item {\sf3.4. Energetic behavior \& stability.} Product of dual variables e.g. voltage \& current gives power, which, when integrated in time, yields energy. Conservation of energy in a closed system is a fundamental law of physics that should also be obeyed in true physics-based  modeling. In musical instruments, resonators are typically passive, i.e. they do not produce energy, while excitation (plucking, bowing, blowing, etc.) is an active process that injects energy to passive resonators.
		
		Stability of a physical system is closely related to its energetic behavior. Stability can be defined so that energy of system remains finite for finite energy excitations. From a signal processing viewpoint, stability may also be defined so that variables, e.g. voltages, remain within a linear operating range for possible inputs in order to avoid signal clipping \& distortion.
		
		In signal processing systems with 1-directional input--output connections between stable subblocks, an instability can appear only if there are feedback loops. In general, impossible to analyze such a system's stability without knowing its whole feedback structure. Contrary to this, in models with physical 2-way interaction, if each element is passive, then any arbitrary network of such elements remains stable.
		\item {\sf3.5. Modularity \& locality of computation.} For a computational realization, desirable to decompose a model systematically into blocks \& their interconnections. Such an object-based approach helps manage complex models through use of modularity principle. Abstractions to macro blocks on basis of more elementary ones helps hiding details when building excessively complex models.
		
		For 1-directional interactions used in signal processing, enough to provide input \& output terminals for connecting blocks. For physical interaction, connections need to be done through ports, with each port having a pair of K- or wave variables depending on modeling method used. This allows mathematical principles used for electrical networks [185]. Details on block-wise construction of models will be discussed in following sects for each modeling paradigm.
		
		Locality of interaction is a desirable modeling feature, which is also related to concept of causality. For a physical system with a finite propagation speed of waves, enough: a block interacts only with its nearest neighbors; it does not need global connections to compute its task \& effect automatically propagates throughout system.
		
		In a discrete-time simulation with bi-directional interactions, delays shorter than a unit delay (including 0 delay) introduce delay-free loop problem that we face several times in this paper. While possible to realize fractional delays [154], delayers shorter than unit delay contain a delay-free component. There are ways to make such `implicit' system computable, but cost in time (or accuracy) may become prohibitive for real-time processing.
		\item {\sf3.6. Physics-based discrete-time modeling paradigms.} This paper presents an overview of physics-based methods \& techniques for modeling \& synthesizing musical instruments. Have excluded some methods often used in acoustics, because they do not easily solve task of efficient discrete-time modeling \& synthesis. E.g., finite element \& boundary element methods (FEM \& BEM) are generic \& powerful for solving system behavior numerically, particularly for linear systems, but focus on inherently time-domain methods for sample-by-sample computation.
		
		Main paradigms in discrete-time modeling of musical instruments can be briefly characterized as follows.
		\begin{itemize}
			\item {\sf3.6.1. Finite difference models.} In Sect. 4, finite difference models are numerical replacement for solving PDEs. Differentials are approximated by finite differences so that time \& position will be discretized. Through proper selection of discretization to regular meshes, computational algorithms become simple \& relatively efficient. Finite difference time domain (FDTD) schemes are K-modeling methods, since wave components are not explicitly utilized in computation. FDTD schemes have been applied successfully to 1D, 2D, \& 3D systems, although in linear 1D cases digital waveguides are typically superior in computational efficiency \& robustness. In multidimensional mesh structures, FDTD approach is more efficient. It also shows potential to deal systematically with nonlinearities (Sect. 11). FDTD algorithms can be problematic due to lack of numerical robustness \& stability, unless carefully designed.
			\item {\sf3.6.2. Mass--spring networks.} In Sect. 5, mass--spring networks are a modeling approach, where intuitive basic elements in mechanics -- masses, springs, \& damping elements -- are used to construct vibrating structures. It is inherently a K-modeling methodology, which has been used to construct small- \& large-scale mesh-like \& other structures. It has resemblance to FDTD schemes in mesh structures \& to WDFs for lumped element modeling. Mass--spring networks can be realized systematically also by WDFs using wave variables (Sect. 8).
			\item {\sf3.6.3. Modal decomposition methods.} In Sect. 6 modal decomposition methods represent another approach to look at vibrating systems, conceptually from a frequency-domain viewpoint. Eigenmodes of a linear system are exponentially decaying sinusoids at eigenfrequencies in response of a system to impulse excitation. Although thinking by modes is normally related to frequency domain, time-domain simulation by modal methods can be relatively efficient, \& therefore suitable to discrete-time computation. Modal decomposition methods are inherently based on use of K-variables. Modal synthesis has been applied to make convincing sound synthesis of different musical instruments. Functional transform method (FTM) is a recent development of systematically exploiting idea of spatially distributed modal behavior, \& it has also been extended to nonlinear system modeling.
			\item {\sf3.6.4. Digital waveguides.} Digital waveguides (DWGs) in Sect. 7 are most popular physics-based method of modeling \& synthesizing musical instruments that are based on 1D resonators, e.g. strings \& wind instruments. Reason for this is their extreme computational efficiency in their basic formulations. DWGs have been used also in 2D \& 3D mesh structures, but in such cases wave-based DWGs are not superior in efficiency. Digital waveguides are based on use of traveling wave components; thus, they form a wave modeling (W-modeling) paradigm [Term digital waveguide is used also to denote K-modeling, e.g. FDTD mesh-structures, \& source-filter models derived from traveling wave solutions, which may cause methodological confusion.]. Therefore, they are also compatible with WDFs (Sect. 8), but in order to be compatible with K-modeling techniques, special conversion algorithms must be applied to construct hybrid models, as discussed in Sect. 10.
			\item {\sf3.6.5. Wave digital filters.} WDFs in Sect. 8 are another wave-based modeling technique, originally developed for discrete-time simulation of analog electric circuits \& networks. In their original form, WDFs are best suited for lumped element modeling; thus, they can be easily applied to wave-based mass--spring modeling. Due to their compatibility with digital waveguides, these methods complement each other. WDFs have also been extended to multidimensional networks \& to systematic \& energetically consistent modeling of nonlinearities. They have been applied particularly to deal with lumped \& nonlinear elements in models, where wave propagation parts are typically realized by digital waveguides.
			\item {\sf3.6.6. Source--filter models.} In Sect. 9 source--filter models form a paradigm between physics-based modeling \& signal processing models. True spatial structure \& bi-directional interactions are not visible, but are transformed into a transfer function that can be realized as a digital filter. Approach is attractive in sound synthesis because digital filters are optimized to implement transfer functions efficiently. Source part of a source--filter model is often a wavetable, consolidating different physical or synthetic signal components needed to feed filter part. Source--filter paradigm is frequently used in combination with other modeling paradigms in more or less ad hoc ways.
		\end{itemize}
	\end{itemize}
	\item {\sf4. Finite difference models.} Finite difference schemes can be used for solving PDEs, e.g. those describing vibration of a string, a membrane or an air column inside a tube [264]. Key idea in finite difference scheme: replace derivatives with finite difference approximations. An early example of this approach in physical modeling of musical instruments is work done by {\sc Hiller \& Ruiz} in early 1970s [113, 114]. This line of research has been continued \& extended by {\sc Chaigne} \& colleagues [45, 46, 48] \& recently by others [25, 26, 29, 30, 81, 103, 131].
	
	Finite difference approach leads to a simulation algorithm that is based on a difference equation, which can be easily programmed with a computer. E.g., how basic wave equation, which describes small-amplitude vibration of a lossless, ideally flexible string, is discretized using this principle. Here present a formulation after Smith [253] using an ideal string as a starting point for discrete-time modeling. A more thorough continuous-time analysis of physics of strings can be found in [96].
	\begin{itemize}
		\item {\sf4.1. Finite difference models for an ideal vibrating string.} {\sf Fig. 1: Part of an ideal vibrating string.} depicts a snapshot of an ideal (lossless, linear, flexible) vibrating string by showing displacement as a function of position. Wave equation for string is given by \fbox{$Ky'' = \epsilon\ddot{y}$}
		\item {\sf4.2. Boundary conditions \& string excitation.}
		\item {\sf4.3. Finite difference approximation of a lossy string.}
		\item {\sf4.4. Stiffness in finite difference strings.}
	\end{itemize}
	\item {\sf5. Mass-spring networks.}
	\begin{itemize}
		\item {\sf5.1. Basic theory.}
		\item {\sf5.2. CORDIS-ANIMA.}
		\item {\sf5.3. Other mass-spring systems.}
	\end{itemize}
	\item {\sf6. Modal decomposition methods.}
	\begin{itemize}
		\item {\sf6.1. Modal synthesis.}
		\item {\sf6.2. Filter-based modal methods.}
		\item {\sf6.3. Functional transform method.}
	\end{itemize}
	\item {\sf7. Digital waveguides.}
	\begin{itemize}
		\item {\sf7.1. From wave propagation to digital waveguides.}
		\item {\sf7.2. Modeling of losses \& dispersion.}
		\item {\sf7.3. Modeling of waveguide termination \& scattering.}
		\item {\sf7.4. Digital waveguide meshes \& networks.}
		\item {\sf7.5. Reduction of a DWG model to a single delay loop structure.}
		\item {\sf7.6. Commuted DWG synthesis.}
		\item {\sf7.7. Case study: modeling \& synthesis of acoustic guitar.} Acoustic guitar is an example of a musical instruments for which DWG modeling is found to be an efficient method, especially for real-time sound synthesis [134, 137, 142, 160, 286, 295]. DWG principle in {\sf Fig. 14: A DWG block diagram of 2 strings coupled through a common bridge impedance $Z_{\rm b}$ \& terminated at other end by nut impedances $Z_{\rm t1},Z_{\rm t2}$. Plucking points are for force insertion from wavetables ${\rm WT}_i$ into junctions in delay-lines ${\rm DL}_{ij}$. Output is taken as bridge velocity.} allows for true physically distributed modeling of strings \& their interaction, while SDL commuted synthesis ({\sf Fig. 17: Reduction of bi-directional delay-line waveguide model (top) to a single delay line loop structure (bottom).} \& {\sf Fig. 18: Principles of commuted DWG synthesis: (a) cascaded excitation, string \& body, (b) body \& string blocks commuted \& (c) excitation \& body blocks consolidated into a wavetable for feeding string model.}) allows for more efficient computation. In this subsect discuss principles of commuted waveguide synthesis as applied to high-quality synthesis of acoustic guitar.
		
		There are several features that must be added to simple commuted SDL structure in order to achieve natural sound \& control of playing features. {\sf Fig. 19: Degrees of freedom for string vibration in guitar: Torsional, Longitudinal, Vertical, Horizontal.} depicts degrees of freedom for vibration of strings in guitar. Transversal directions, i.e. vertical \& horizontal polarizations of vibration, are most prominent ones. Vertical vibration connects strongly to bridge, resulting in stronger initial sound \& faster decay than horizontal vibrations that start more weakly \& decay more slowly. Effect of longitudinal vibration is weak but can be observed in generation of some partials of sound [320]. Longitudinal effects are more prominent in piano [16, 58], but are particularly important in such instruments as kantele [82] through nonlinear effect of tension modulation (Sect. 11). Torsional vibration of strings in guitar is not shown to have a remarkable effect on sound. In violin it has a more prominent physical role, although it makes virtually no contribution to sound.
		
		In commuted waveguide synthesis, 2 transversal polarizations can be realized by 2 separate SDL string models, $S_{\rm v}(z)$ for vertical \& $S_{\rm h}(z)$ for horizontal polarization in {\sf Fig. 20: Dual-polarization string model with sympathetic vibration coupling between strings. Multiple wavetables are used for varying plucking styles. Filter $E(z)$ can control detailed timbre of plucking \& $P(z)$ is a plucking point comb filter.}, each one with slightly different delay \& decay parameters. Coefficient $m_{\rm p}$ is used to control relative excitation amplitudes of each polarization, depending on initial direction of string movement after plucking. Coefficients $m_{\rm o}$ can be used to mix vibration signal components at bridge.
		
		{\sf Fig. 20} also shows another inherent feature of guitar, sympathetic coupling between strings at bridge, which causes an undamped string to gain energy from another string set in vibration. While principle shown in {\sf Fig. 14} implements this automatically if string \& bridge admittances are correctly set, model in {\sf Fig. 20} requires special signal connections from point C to vertical polarization model of other strings. This is just a rough approximation of physical phenomenon that guarantees stability of model. There is also a connection through $g_{\rm c}$ that allows for simple coupling from horizontal polarization to excite vertical vibration, with a final result of a coupling between polarizations.
		
		Dual-polarization model in {\sf Fig. 20} is excited by wavetables containing commuted waveguide excitations for different plucking styles. Filter $E(z)$ can be used to control timbre details of selected excitation, \& filter $E(z)$ can be used to control timbre details of selected excitation, \& filter $P(z)$ is a plucking point comb filter, as prev discussed.
		
		-- Mô hình phân cực kép trong {\sf Hình 20} được kích thích bằng các bảng sóng chứa các kích thích ống dẫn sóng chuyển mạch cho các kiểu gảy khác nhau. Bộ lọc $E(z)$ có thể được sử dụng để kiểm soát các chi tiết âm sắc của kích thích đã chọn, \& bộ lọc $E(z)$ có thể được sử dụng để kiểm soát các chi tiết âm sắc của kích thích đã chọn, \& bộ lọc $P(z)$ là bộ lọc lược điểm gảy, như đã thảo luận trước đó.
		
		For solid body electric guitars, a magnetic pickup model is needed, but body effect can be neglected. Magnetic pickup can be modeled as a lowpass filter [124,137] in series with a comb filter similar to plucking point filter, but in this case corresponding to pickup position.
		
		Calibration of model parameters is an important task when simulating a particular instrument. Methods for calibrating a string instrument model are presented, e.g., in [8, 14, 24, 27, 137, 142, 211, 244, 286, 295, 320].
		
		-- Hiệu chuẩn các tham số mô hình là 1 nhiệm vụ quan trọng khi mô phỏng 1 nhạc cụ cụ thể. Các phương pháp hiệu chuẩn mô hình nhạc cụ dây được trình bày $\ldots$
		
		A typical procedure: apply time-frequency analysis to recorded sound of plucked or struck string, in order to estimate decay rate of each harmonic. Parametric models e.g. FZ-ARMA analysis [133, 138] may yield more complete information of modal components in string behavior. This information is used to design a low-order loop filter which approximates frequency-dependent losses in SDL loop structure [14,17,79,244,286]. A recent novel idea has been to design a sparse FIR loop filter, which is of high order but has few nonzero coefficients [163, 209, 293]. This approach offers a computationally efficient way to imitate large deviations in decay rates of harmonic components. Through implementing a slight difference in delays \& decay rates of 2 polarizations, beating or 2-stage decay of signal envelope can be approximated. for plucking point comb filter: required to estimate plucking point from a recorded tone [199, 276, 277, 286].
		
		{\sf Fig. 21: Detailed SDL loop structure for string instrument sound synthesis.} depicts a detailed structure used in practice to realize SDL loop. Fundamental frequency of string sound is inversely proportional to total delay of loop blocks. Accurate tuning requires application of a fractional delay, because an integral number of unit delays is not accurate enough when a fixed sampling rate is used. Fractional delays are typically approximated by 1st-order allpass filters or 1st- to 5th-order Lagrange interpolators as discussed in [154].
		
		When loop filter properties are estimated properly, excitation wavetable signal is obtained by inverse filtering (deconvolution) of recorded sound by SDL response. For practical synthesis, only initial transient part of inverse-filtered excitation is used, typically covering several 10s of milliseconds.
		
		After careful calibration of model, a highly realistic sounding synthesis can be obtained by parametric control \& modification of sound features. Synthesis is possible even in cases which are not achievable in practice in real acoustic instruments.		
		\item {\sf7.8. DWG modeling of various musical instruments.} Digital waveguide modeling has been applied to a variety of musical instruments other than acoustic guitar. In this subsect, present a brief overview of such models \& features that need special attention to each case. For an in-depth presentation on DWG modeling techniques applied to different instrument families, see [254].
		\begin{itemize}
			\item {\sf7.8.1. Other plucked string instruments.}
			\item {\sf7.8.2. Struck string instruments.}
			\item {\sf7.8.3. Bowed string instruments.}
			\item {\sf7.8.4. Wind instruments.}
			\item {\sf7.8.5. Percussion instruments.}
			\item {\sf7.8.6. Speech \& singing voice.}
			\item {\sf7.8.7. Inharmonic SDL type of DWG models.}
		\end{itemize}
	\end{itemize}
	\item {\sf8. Wave digital filters.} Purpose of this sect: provide a general overview of physical modeling using WDFs in context of musical instruments. Only essential basics of topic will be discussed in detail; the rest will be glossed over. For more information about project, reader is encouraged to refer to [254]. Also, another definitive work can be found in [94].
	\begin{itemize}
		\item {\sf8.1. What are wave digital filters?} WDFs were developed in late 1960s by {\sc Alfred Fettweis} [93] for digitizing lumped analog electrical circuits. Traveling-wave formulation of lumped electrical elements, where WDF approach is based, was introduced earlier by {\sc Belevitch} [21, 254].
		
		WDFs are certain types of digital filters with valid interpretations in physical world. I.e., can simulate behavior of a lumped physical system (hệ thống vật lý tập trung) using a digital filter whose coefficients depend on parameters of this physical system. Alternatively, WDFs can be seen as a particular type of finite difference schemes with excellent numerical properties [254]. As discussed in Sect. 4, task of finite difference schemes in general: provide discrete versions of PDEs for simulation \& analysis purposes.
		
		WDFs are useful for physical modeling in many respects. 1stly, they are modular: same building blocks can be used for modeling very different systems; all that needs to be changed: \fbox{topology of wave digital network}. 2ndly, preservation of energy \& hence also stability is usually addressed, since elementary blocks can be made passive, \& energy preservation between blocks are evaluated using Kirchhoff's laws. Finally, WDFs have good numerical properties, i.e., they do not experience artificial damping at high frequencies.
		
		Physical systems were originally considered to be lumped in basic wave digital formalism. I.e., system to be modeled, say a drum, will become a point-like black box, which has functionality of drum. However, its inner representation, as well as its spatial dimensions, is lost. Must bear in mind, however: question of whether a physical system can be considered lumped depends naturally not only on which of its aspects wish to model but also on frequency scale want to use in modeling (Sect. 3).
		\item {\sf8.2. Analog circuit theory.}
		\item {\sf8.3. Wave digital building blocks.}
		\item {\sf8.4. Interconnection \& adaptors.}
		\item {\sf8.5. Physical modeling using WDFs.}
		\item {\sf8.6. Current research.}
	\end{itemize}
	\item {\sf9. Source-filter models.}
	\begin{itemize}
		\item {\sf9.1. Subtractive synthesis in computer music.}
		\item {\sf9.2. Source-filter models in speech synthesis.}
		\item {\sf9.3. Instrument body modeling by digital filters.}
		\item {\sf9.4. Karplus--Strong algorithm.}
		\item {\sf9.5. Virtual analog synthesis.}
	\end{itemize}
	\item {\sf10. Hybrid models.}
	\begin{itemize}
		\item {\sf10.1. KW-hybrids.}
		\item {\sf10.2. KW-hybrids modeling examples.}
	\end{itemize}
	\item {\sf11. Modeling of nonlinear \& time-varying phenomena.}
	\begin{itemize}
		\item {\sf11.1. Modeling of nonlinearities in musical instruments.}
		\item {\sf11.2. Case study: nonlinear string model using generalized time-varying allpass filters.}
		\item {\sf11.3. Modeling of time-varying phenomena.}
	\end{itemize}
	\item {\sf12. Current trends \& further research.}
	\item {\sf13. Conclusions.}
\end{itemize}

%------------------------------------------------------------------------------%

\section{Wikipedia}

\subsection{Wikipedia{\tt/}computer music}
``{\it Computer music} is application of \href{https://en.wikipedia.org/wiki/Computing_technology}{computing technology} in \href{https://en.wikipedia.org/wiki/Musical_composition}{music composition}, to help human composers create new music or to have computers independently create music, e.g. with \href{https://en.wikipedia.org/wiki/Algorithmic_composition}{algorithmic composition} programs. it includes theory \& application of new \& existing computer software technologies \& basic aspects of music, e.g. \href{https://en.wikipedia.org/wiki/Sound_synthesis}{sound synthesis}, \href{https://en.wikipedia.org/wiki/Digital_signal_processing}{digital signal processing}, \href{https://en.wikipedia.org/wiki/Sound_design}{sound design}, sonic diffusion, \href{https://en.wikipedia.org/wiki/Acoustics}{acoustics}, \href{https://en.wikipedia.org/wiki/Electrical_engineering}{electrical engineering}, \& \href{https://en.wikipedia.org/wiki/Psychoacoustics}{psychoacoustics}. Field of computer music can trace its roots back to origins of \href{https://en.wikipedia.org/wiki/Electronic_music}{electric music}, \& 1st experiments \& innovations with electronic instruments at turn of 20th century.

\subsubsection{History}

\subsubsection{Advances}

\subsubsection{Research}

\subsubsection{Machine improvisation}

\subsubsection{Live coding}

'' -- \href{https://en.wikipedia.org/wiki/Computer_music}{Wikipedia{\tt/}computer music}

%------------------------------------------------------------------------------%

\subsection{Wikipedia{\tt/}transcription (music)}
``{\sf A {\sc J. S. Bach} keyboard piece transcribed for guitar.} In music, {\it transcription} is practice of \href{https://en.wikipedia.org/wiki/Musical_notation}{notating} a piece or a sound which was previously unnotated \&{\tt/}or unpopular as a written music, e.g., a \href{https://en.wikipedia.org/wiki/Jazz_improvisation}{jazz improvisation} or a \href{https://en.wikipedia.org/wiki/Video_game_soundtrack}{video game soundtrack}. When a musician is tasked with creating \href{https://en.wikipedia.org/wiki/Sheet_music}{sheet music} from a recording \& they write down notes that make up piece in \href{https://en.wikipedia.org/wiki/Music_notation}{music notation}, it is said: they created a {\it musical transcription} of that recording. Transcription may also mean rewriting a piece of music, either solo or \href{https://en.wikipedia.org/wiki/Musical_ensemble}{ensemble}, for another instrument or other instruments than which it was originally intended. \href{https://en.wikipedia.org/wiki/Beethoven_Symphonies_(Liszt)}{Beethoven Symphonies} transcribed for solo piano by \href{https://en.wikipedia.org/wiki/Franz_Liszt}{Franz Liszt} are an example. Transcription in this sense is sometimes called \href{https://en.wikipedia.org/wiki/Arrangement}{\it arrangement}, although strictly speaking transcriptions are faithful adaptations, whereas arrangements change significant aspects of original piece.

Further examples of music transcription include \href{https://en.wikipedia.org/wiki/Ethnomusicology}{ethnomusicological} notation of \href{https://en.wikipedia.org/wiki/Oral_tradition}{oral traditions} of folk music, e.g. Béla Bartók's \& Ralph Vaughan Williams' collections of national folk music of Hungary \& England resp. French composer Olivier Messiaen transcribed \href{https://en.wikipedia.org/wiki/Bird_song}{birdsong} in wild, \& incorporated it into many of his compositions, e.g. his \href{https://en.wikipedia.org/wiki/Catalogue_d%27oiseaux}{Catalogue d'oiseaux} for solo piano. Transcription of this nature involves scale degree recognition \& harmonic analysis, both of which transcriber will need \href{https://en.wikipedia.org/wiki/Relative_pitch}{relative} or \href{https://en.wikipedia.org/wiki/Perfect_pitch}{perfect pitch} to perform. 

In popular music \& rock, there are 2 forms of transcription. Individual performers copy a note-for-note guitar solo or other melodic line. As well, music publishers transcribe entire recordings of guitar solos \& bass lines \& sell sheet music in bound books. Music publishers also publish PVG (piano{\tt/}vocal{\tt/}guitar) transcriptions of popular music, where melody line is transcribed, \& then accompaniment on recording is arranged as a piano part. Guitar aspect of PVG label is achieved through guitar chords written above melody. Lyrics are also included below melody.

\subsubsection{Adaptation}
Some composers have rendered homage to other composers by creating ``identical'' versions of earlier composers' pieces while adding their own creativity through use of completely new sounds arising from difference in instrumentation. Most widely known example of this is {\sc Ravel}'s arrangement for orchestra of {\sc Mussorgsky}'s piano piece \href{https://en.wikipedia.org/wiki/Pictures_at_an_Exhibition}{\it Pictures at an Exhibition}. {\sc Webern} used his transcription for orchestra of 6-part \href{https://en.wikipedia.org/wiki/Ricercar}{ricercar} from {\sc Bach}'s \href{https://en.wikipedia.org/wiki/The_Musical_Offering}{\it The Musical Offering} to analyze structure of Bach piece, by using different instruments to play different subordinate \href{https://en.wikipedia.org/wiki/Motif_(music)}{motifs} of Bach's themes \& melodies.

In transcription of this form, new piece can simultaneously imitate original sounds while recomposing them with all technical skills of an expert composer in such a way that it seems: piece was originally written for new medium. But some transcriptions \& arrangements have been done for purely pragmatic or contextual reasons. E.g., in Mozart's time, overtures \& songs from this popular operas were transcribed for small \href{https://en.wikipedia.org/wiki/Wind_ensemble}{wind ensemble} simply because such ensembles were common ways of providing popular entertainment in public places. {\sc Mozart} himself did this in his own opera \href{https://en.wikipedia.org/wiki/The_Marriage_of_Figaro}{\it The Marriage of Figaro}. A more contemporary example is {\sc Stravinsky}'s transcription for 4 hands piano of \href{https://en.wikipedia.org/wiki/The_Rite_of_Spring}{\it The Rite of Spring}, to be used on ballet's rehearsals. Today musicians who play in cafes or restaurants will sometimes play transcriptions or arrangements of pieces written for a larger group of instruments.

Other examples of this type of transcription include {\sc Bach}'s arrangement of {\sc Vivaldi}'s 4-violin concerti for 4 keyboard instruments \& orchestra; {\sc Mozart}'s arrangement of some Bach \href{https://en.wikipedia.org/wiki/Fugue}{fugues} from \href{https://en.wikipedia.org/wiki/The_Well-Tempered_Clavier}{\it The Well-Tempered Clavier} for string \href{https://en.wikipedia.org/wiki/Trio_(music)}{trio}; {\sc Beethoven}'s arrangement of his \href{https://en.wikipedia.org/wiki/Gro%C3%9Fe_Fuge}{\it Gro$\beta$e Fuge}, originally written for \href{https://en.wikipedia.org/wiki/String_quartet}{string quartet}, for \href{https://en.wikipedia.org/wiki/Piano}{piano} duet, \& his arrangement of his \href{https://en.wikipedia.org/wiki/Violin_Concerto_(Beethoven)}{Violin Concerto} as a \href{https://en.wikipedia.org/wiki/Piano_concerto}{piano concerto}; Franz Liszt's piano arrangements of works of many composers, including \href{https://en.wikipedia.org/wiki/Beethoven_Symphonies_(Liszt)}{symphonies of Beethoven}; {\sc Tchaikovsky}'s arrangement of 4 Mozart piano pieces into an \href{https://en.wikipedia.org/wiki/Orchestral_suite}{orchestral suite} called ``\href{https://en.wikipedia.org/wiki/Orchestral_Suite_No._4_Mozartiana_(Tchaikovsky)}{Mozartiana}''; {\sc Mahler}'s re-orchestration of {\sc Schumann} symphonies; \& {\sc Schoenberg}'s arrangement for orchestra of {\sc Brahms}'s piano quintet \& {\sc Bach}'s ``St. Anne'' Prelude \& Fugue for organ.

Since piano became a popular instrument, a large literature has sprung up of transcriptions \& arrangements for piano of works for orchestra or chamber music ensemble. These are sometimes called ``\href{https://en.wikipedia.org/wiki/Reduction_(music)}{piano reductions}'', because multiplicity of orchestral parts -- in an orchestral piece there may be as many as 2 dozen separate instrumental parts being played simultaneously -- has to be reduced to what a single pianist (or occasionally 2 pianists, or 1 or 2 pianos, e.g. different arrangements for {\sc George Gershwin}'s \href{https://en.wikipedia.org/wiki/Rhapsody_in_Blue}{\it Rhapsody in Blue}) can manage to play.

Piano reductions are frequently made of orchestral accompaniments to choral works, for purposes of rehearsal or of performance with keyboard alone.

Many orchestral pieces have been transcribed for \href{https://en.wikipedia.org/wiki/Concert_band}{concert band}.

\subsubsection{Transcription aids}

\begin{itemize}
	\item {\bf Notation software.} Since advent of desktop publishing, musicians can acquire \href{https://en.wikipedia.org/wiki/Music_notation_software}{music notation software}, which can receive user's mental analysis of notes \& then store \& format those notes into standard music notation for personal printing or professional publishing of sheet music. Some notation software can accept a Standard \href{https://en.wikipedia.org/wiki/MIDI}{MIDI} File (SMF) or MIDI performance as input instead of manual note entry. These notation applications can export their scores in a variety of formats like \href{https://en.wikipedia.org/wiki/Encapsulated_PostScript}{EPS}, \href{https://en.wikipedia.org/wiki/Portable_Network_Graphics}{PNG}, \& \href{https://en.wikipedia.org/wiki/Scalable_Vector_Graphics}{SVG}. Often software contains a sound library that allows user's score to be played aloud by application for verification.
	\item {\bf Slow-down software.} Prior to invention of digital transcription aids, musicians would slow down a record or a tape recording to be able to hear melodic lines \& chords at a slower, more digestible pace. Problem with this approach was: it also changed pitches, so once a piece was transcribed, it would then have to be transposed into correct key. Software designed to slow down tempo of music without changing pitch of music can be very helpful for recognizing pitches, melodies, chords, rhythms, \& lyrics when transcribing music. However, unlike slow-down effect of a record player, pitch \& original octave of notes will stay same, \& not descend in pitch. This technology is simple enough that it is available in many free software applications.
	
	Software generally goes through a 2-step process to accomplish this. 1st, audio file is played back at a lower sample rate than that of original file. This has same effect as playing a tape or vinyl record at slower speed -- pitch is lowered meaning music can sound like it is in a different key. 2nd step: use \href{https://en.wikipedia.org/wiki/Digital_signal_processing}{Digital Signal Processing} (or DSP) to shift pitch back up to original pitch level or musical key.
	\item {\bf Pitch tracking software.} Main article: \href{https://en.wikipedia.org/wiki/Pitch_tracker}{Wikipedia{\tt/}pitch tracker}. As mentioned in the Automatic music transcription sect, some commercial software can roughly track pitch of dominant melodies in polyphonic musical recordings. Note scans are not exact, \& often need to be manually edited by user before saving to file in either a proprietary file format or in Standard MIDI File Format. Some pitch tracking software also allows scanned note lists to be animated during audio playback.
\end{itemize}

\subsubsection{Automatic music transcription (AMT)}
Term ``automatic music transcription'' was 1st used by audio researchers {\sc James A. Moorer,  Martin Piszczalski, \& Bernard Galler} in 1977. With their knowledge of digital audio engineering, these researchers believed: a computer could be programmed to analyze a \href{https://en.wikipedia.org/wiki/Digital_recording}{digital recording} of music s.t. pitches of melody lines \& chord patterns could be detected, along with rhythmic accents of percussion instruments. Task of AMT concerns 2 separate activities: making an analysis of a musical piece, \& printing out a score from that analysis.

This was not a simple goal, but one that would encourage academic research for at least another 3 decades. Because of close scientific relationship of speech to music, much academic \& commercial research that was directed toward more financially resourced \href{https://en.wikipedia.org/wiki/Speech_recognition}{speech recognitions} technology would be recycled into research about music recognition technology. While many musicians \& educators insist that manually doing transcriptions is a valuable exercise for developing musicians, motivation for AMT remains same as motivation for sheet music: musicians who do not have intuitive transcription skills will search for sheet music or a chord chart, so that they may quickly learn how to play a song. A collection of tools created by this ongoing research could be of great aid to musicians. Since much recorded music does not have available sheet music, an automatic transcription device could also offer transcriptions that are otherwise unavailable in sheet music. To date, no software application can yet completely fulfill {\sc James Moorer};s definition of AMT. However, pursuit of AMT has spawned creation of many software applications that can aid in manual transcription. Some can slow down music while maintaining original pitch \& octave, some can track pitch of melodies, some can track chord changes, \& others can track beat of music.

Automatic transcription most fundamentally involves identifying pitch \& duration of performed notes. This entails tracking pitch \& identifying note onsets. After capturing those physical measurements, this information is mapped into traditional music notation, i.e., sheet music.

\href{https://en.wikipedia.org/wiki/Digital_signal_processing}{Digital Signal Processing} is branch of engineering that provides software engineers with tools \& algorithms needed to analyze a digital recording in terms of pitch (note detection of melodic instruments), \& energy content of un-pitched sounds (detection of percussion instruments). Musical recordings are sampled at a given recording rate \& its frequency data is stored in any digital wave format in computer. Such format represents sound by \href{https://en.wikipedia.org/wiki/Sampling_(signal_processing)}{digital sampling}.
\begin{itemize}
	\item {\bf Pitch detection.} \href{https://en.wikipedia.org/wiki/Pitch_detection}{Pitch detection} is often detection of individual \href{https://en.wikipedia.org/wiki/Musical_note}{notes} that might make up a \href{https://en.wikipedia.org/wiki/Melody}{melody} in music, or notes in a \href{https://en.wikipedia.org/wiki/Chord_(music)}{chord}. When a single key is pressed upon a piano, what we hear is not just {\it1} \href{https://en.wikipedia.org/wiki/Frequency}{frequency} of sound vibration, but a {\it composite} of multiple sound vibrations occurring at different mathematically related frequencies. Elements of this composite of vibrations at differing frequencies are referred to as \href{https://en.wikipedia.org/wiki/Harmonic}{harmonics} or partials.
	
	E.g., if note $A_3$ (220 Hz) is played, individual \href{https://en.wikipedia.org/wiki/Frequency}{frequencies} of composite's \href{https://en.wikipedia.org/wiki/Harmonic_series_(music)}{harmonic series} will start at 220 Hz as \href{https://en.wikipedia.org/wiki/Fundamental_frequency}{fundamental frequency}: 440 Hz would be 2nd harmonic, 660 Hz would be 3rd harmonic, 880 Hz would be 4th harmonic, etc. These are integer multiples of fundamental frequency (e.g., $2\cdot220 = 440$, 2nd harmonic). While only about 8 harmonics are really needed to audibly recreate note, total number of harmonics in this mathematical series can be large, although higher harmonic's numerical weaker magnitude \& contribution of that harmonic. Contrary to intuition, a musical recording at its lowest physical level is not a collection of individual \href{https://en.wikipedia.org/wiki/Musical_note}{notes}, but is really a collection of individual harmonics. That is why very similar-sounding recordings can be created with differing collections of instruments \& their assigned notes. As long as total harmonics of recording are recreated to some degree, it does not really matter which instruments or which notes were used.
	\item {\bf Beat detection.}
	\item {\bf How ATM works.}
	\item {\bf Detailed computer steps behind AMT.}
\end{itemize}

'' -- \href{https://en.wikipedia.org/wiki/Transcription_(music)}{Wikipedia{\tt/}transcription (music)}

%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}