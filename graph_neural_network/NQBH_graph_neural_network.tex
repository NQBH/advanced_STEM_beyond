\documentclass{article}
\usepackage[backend=biber,natbib=true,style=alphabetic,maxbibnames=50]{biblatex}
\addbibresource{/home/nqbh/reference/bib.bib}
\usepackage[utf8]{vietnam}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,enumitem,fancyvrb,float,graphicx,mathtools,tikz}
\usetikzlibrary{angles,calc,intersections,matrix,patterns,quotes,shadings}
\allowdisplaybreaks
\newtheorem{assumption}{Assumption}
\newtheorem{baitoan}{Bài toán}
\newtheorem{cauhoi}{Câu hỏi}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{dangtoan}{Dạng toán}
\newtheorem{definition}{Definition}
\newtheorem{dinhly}{Định lý}
\newtheorem{dinhnghia}{Định nghĩa}
\newtheorem{example}{Example}
\newtheorem{ghichu}{Ghi chú}
\newtheorem{hequa}{Hệ quả}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{lemma}{Lemma}
\newtheorem{luuy}{Lưu ý}
\newtheorem{nhanxet}{Nhận xét}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}
\newtheorem{principle}{Principle}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{vidu}{Ví dụ}
\usepackage[left=1cm,right=1cm,top=5mm,bottom=5mm,footskip=4mm]{geometry}
\def\labelitemii{$\circ$}
\DeclareRobustCommand{\divby}{%
    \mathrel{\vbox{\baselineskip.65ex\lineskiplimit0pt\hbox{.}\hbox{.}\hbox{.}}}%
}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Graph Neural Networks (GNNs) -- Mạng Lưới Neuron Đồ Thị}
\author{Nguyễn Quản Bá Hồng\footnote{A scientist- {\it\&} creative artist wannabe, a mathematics {\it\&} computer science lecturer of Department of Artificial Intelligence {\it\&} Data Science (AIDS), School of Technology (SOT), UMT Trường Đại học Quản lý {\it\&} Công nghệ TP.HCM, Hồ Chí Minh City, Việt Nam.\\E-mail: {\sf nguyenquanbahong@gmail.com} {\it\&} {\sf hong.nguyenquanba@umt.edu.vn}. Website: \url{https://nqbh.github.io/}. GitHub: \url{https://github.com/NQBH}.}}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
    This text is a part of the series {\it Some Topics in Advanced STEM \& Beyond}:

    {\sc url}: \url{https://nqbh.github.io/advanced_STEM/}.

    Latest version:
    \begin{itemize}
        \item {\it Graph Neural Networks (GNNs) -- Mạng Lưới Neuron Đồ Thị}.

        PDF: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/graph_neural_network/NQBH_graph_neural_network.pdf}.

        \TeX: {\sc url}: \url{https://github.com/NQBH/advanced_STEM_beyond/blob/main/graph_neural_network/NQBH_graph_neural_network.tex}.
        \item {\it }.

        PDF: {\sc url}: \url{.pdf}.

        \TeX: {\sc url}: \url{.tex}.
    \end{itemize}
\end{abstract}
\tableofcontents

%------------------------------------------------------------------------------%

\section{Introduction to Graph Neural Networks}

%------------------------------------------------------------------------------%

\subsection{{\sc Keita Broadwater, Namid Stillmann}. Graph Neural Networks in Action}

\begin{itemize}
    \item {\sf Fig: Mental model of GNN project.}  The steps involved for a GNN project are similar to many conventional ML pipelines, but we need to use graph-specific tools to create them. Start with raw data, which is then transformed into a graph data model \& that can be stored in a graph database or used in a graph processing system. From the graph processing system (\& some graph database), we can do exploratory data analysis \& visualization. Finally, for graph ML, we preprocess data into a format that can be submitted for training \& then train our graph ML model, in our examples, these will be GNNs. \fbox{GNNs $\subset$ Graph ML models}.

    -- {\sf Hình: Mô hình tinh thần của dự án GNN.} Các bước cần thực hiện cho 1 dự án GNN tương tự như nhiều quy trình ML thông thường, nhưng chúng ta cần sử dụng các công cụ dành riêng cho đồ thị để tạo ra chúng. Bắt đầu với dữ liệu thô, sau đó được chuyển đổi thành mô hình dữ liệu đồ thị \& có thể được lưu trữ trong cơ sở dữ liệu đồ thị hoặc sử dụng trong hệ thống xử lý đồ thị. Từ hệ thống xử lý đồ thị (\& 1 số cơ sở dữ liệu đồ thị), chúng ta có thể thực hiện phân tích dữ liệu thăm dò \& trực quan hóa. Cuối cùng, đối với ML đồ thị, chúng ta xử lý trước dữ liệu thành 1 định dạng có thể được gửi để huấn luyện \& sau đó huấn luyện mô hình ML đồ thị của chúng ta, trong các ví dụ của chúng ta, đây sẽ là các GNN. \fbox{GNN $\subset$ Mô hình ML đồ thị}.
    \item {\sf Foreword.} Our world is highly rich in structure, comprising objects, their relations, \& hierarchies. Sentences can be represented as sequences of words, maps can be broken down into streets \& intersections, www connects websites via hyperlinks, \& chemical compounds can be described by a set of atoms \& their interactions. Despite prevalence of graph structures in our world, both traditional \& even modern ML methods struggle to properly handle such rich structural information: ML conventionally expects fixed-sized vectors as inputs \& is thus only applicable to simpler structures e.g. sequences or grids. Consequently, graph ML has long relied on labor-intensive \& error-prone handcrafted feature engineering techniques. Graph neural networks (GNNs) finally revolutionize this paradigm by breaking up with regularity restriction of conventional DL techniques. They unlock ability to learn representations from raw graph data with exceptional performance \& allow us to view DL as a much broader technique that can seamlessly generalize to complex \& rich topological structures.

    -- Thế giới của chúng ta vô cùng phong phú về cấu trúc, bao gồm các đối tượng, mối quan hệ của chúng, \& hệ thống phân cấp. Câu có thể được biểu diễn dưới dạng chuỗi từ, bản đồ có thể được chia nhỏ thành các con phố \& giao lộ, www kết nối các trang web thông qua siêu liên kết, \& hợp chất hóa học có thể được mô tả bằng 1 tập hợp các nguyên tử \& tương tác của chúng. Mặc dù cấu trúc đồ thị rất phổ biến trong thế giới của chúng ta, cả các phương pháp ML truyền thống \& thậm chí hiện đại đều gặp khó khăn trong việc xử lý đúng cách thông tin cấu trúc phong phú như vậy: ML thường mong đợi các vectơ có kích thước cố định làm đầu vào \& do đó chỉ áp dụng cho các cấu trúc đơn giản hơn, e.g. chuỗi hoặc lưới. Do đó, ML đồ thị từ lâu đã dựa vào các kỹ thuật thiết kế đặc trưng thủ công tốn nhiều công sức \& dễ xảy ra lỗi. Mạng nơ-ron đồ thị (GNN) cuối cùng đã cách mạng hóa mô hình này bằng cách phá vỡ sự hạn chế về quy tắc của các kỹ thuật DL thông thường. Chúng mở khóa khả năng học các biểu diễn từ dữ liệu đồ thị thô với hiệu suất vượt trội \& cho phép chúng ta xem DL như 1 kỹ thuật rộng hơn nhiều, có thể khái quát hóa liền mạch thành các cấu trúc tôpô phức tạp \& phong phú.

    When {\sc Matthias Fey} -- creator of PyTorch Geometric \& founding engineer Kumo.AI -- begin to dive into field of graph ML, DL on graphs was still in its early stages. Over time, dozens to hundreds of different methods were developed, contributing incremental insights \& refreshing ideas. Tools like our own PyTorch Geometric library have expanded significantly, offering cutting-edge graph-based building blocks, models, examples, \& scalability solutions. Reflecting on this growth, it is clear how overwhelming it can be for newcomers to navigate essentials \& best practices that have emerged over time, as valuable information is scattered across theoretical research papers or buried in implementations in GitHub repositories.

    -- Khi {\sc Matthias Fey} -- người sáng lập PyTorch Geometric \& kỹ sư sáng lập Kumo.AI -- bắt đầu dấn thân vào lĩnh vực học máy đồ thị, học máy trên đồ thị vẫn còn ở giai đoạn sơ khai. Theo thời gian, hàng chục đến hàng trăm phương pháp khác nhau đã được phát triển, đóng góp những hiểu biết sâu sắc \& những ý tưởng mới mẻ. Các công cụ như thư viện PyTorch Geometric của chúng tôi đã mở rộng đáng kể, cung cấp các khối xây dựng, mô hình, e.g., \& giải pháp khả năng mở rộng dựa trên đồ thị tiên tiến. Nhìn lại sự phát triển này, rõ ràng là những người mới bắt đầu có thể gặp khó khăn như thế nào khi tìm hiểu những điều cốt lõi \& các phương pháp hay nhất đã xuất hiện theo thời gian, khi thông tin giá trị nằm rải rác trong các bài báo nghiên cứu lý thuyết hoặc bị chôn vùi trong các triển khai trên kho lưu trữ GitHub.

    Now power of GNNs has been widely understood, this timely book provides a well-structured \& easy-to-follow overview of field, providing answers to many pain points of graph ML practitioners. Hands-on approach, with practical code examples embedded directly within each chap, invaluably demystifies complexities, making concepts tangible \& actionable. Despite success of GNNs across all kinds of domains in research, adoption in real-world applications remains limited to companies that have enough resources to acquire necessary knowledge for applying GNNs in practice. Confident: this book will serve as an invaluable resource to empower practitioners to over that gap \& unlock full potentials of GNNs.

    -- Giờ đây, sức mạnh của GNN đã được hiểu rộng rãi, cuốn sách kịp thời này cung cấp 1 cái nhìn tổng quan được cấu trúc tốt \& dễ hiểu về lĩnh vực này, giải đáp nhiều vấn đề khó khăn của các chuyên gia ML đồ thị. Phương pháp tiếp cận thực hành, với các ví dụ mã thực tế được nhúng trực tiếp trong mỗi chương, giúp làm sáng tỏ những điều phức tạp, biến các khái niệm thành hiện thực \& khả thi. Mặc dù GNN đã thành công trong nhiều lĩnh vực nghiên cứu, việc áp dụng vào các ứng dụng thực tế vẫn chỉ giới hạn ở các công ty có đủ nguồn lực để có được kiến thức cần thiết cho việc áp dụng GNN vào thực tế. Tự tin: cuốn sách này sẽ là 1 nguồn tài nguyên vô giá giúp các chuyên gia vượt qua khoảng cách đó \& khai phá toàn bộ tiềm năng của GNN.
    \item {\sf Preface.} My journey into world of graphs began unexpectedly, during an interview at LinkedIn. As session wrapped up, shown a visualization of network -- a mesmerizing structure that told stories without a single word. Organizations I had been part of appeared clustered, like constellations against a dark canvas. What surprised me most was that this structure was not built using metadata LinkedIn held about my connection; rather, it emerged organically from relationships between nodes \& edges.

    -- Hành trình khám phá thế giới đồ thị của tôi bắt đầu 1 cách bất ngờ, trong 1 buổi phỏng vấn tại LinkedIn. Khi buổi phỏng vấn kết thúc, 1 hình ảnh trực quan về mạng lưới được trình chiếu - 1 cấu trúc mê hoặc kể những câu chuyện mà không cần 1 lời nào. Các tổ chức mà tôi từng là thành viên hiện ra như những chòm sao trên nền vải tối. Điều khiến tôi ngạc nhiên nhất là cấu trúc này không được xây dựng bằng siêu dữ liệu mà LinkedIn nắm giữ về kết nối của tôi; thay vào đó, nó xuất hiện 1 cách tự nhiên từ các mối quan hệ giữa các nút \& cạnh.

    Years later, driven by curiosity, I recreated that visualization. I marveled once again at how underlying connections along could map out an intricate picture of my professional life. This deepened my appreciation for power inherent in graphs -- a fascination that only grew when I joined Cloudera \& encountered graph neural networks (GNNs). Their potential for solving complex problems was captivating, but diving into them was like trying to navigate an uncharted forest without a map. There were no comprehensive resources tailored for nonacademics; progress was slow, often cobbled together from fragments \& trial \& error.

    -- Nhiều năm sau, nhờ sự tò mò, tôi đã tái hiện lại hình ảnh đó. Tôi lại 1 lần nữa kinh ngạc trước cách các kết nối cơ bản có thể vẽ nên 1 bức tranh phức tạp về cuộc sống nghề nghiệp của mình. Điều này càng làm tôi trân trọng hơn sức mạnh tiềm ẩn của đồ thị -- 1 niềm đam mê chỉ lớn dần khi tôi gia nhập Cloudera \& gặp gỡ mạng nơ-ron đồ thị (GNN). Tiềm năng giải quyết các vấn đề phức tạp của chúng thật hấp dẫn, nhưng việc đào sâu vào chúng cũng giống như cố gắng khám phá 1 khu rừng chưa được khám phá mà không có bản đồ. Không có tài nguyên toàn diện nào được thiết kế riêng cho những người không chuyên; tiến độ rất chậm, thường được chắp vá từ những mảnh \& thử \& sai.

    This book is guide I wish I had during those early days. It aims to provide a clear \& accessible path for practitioners, enthusiasts, \& anyone looking to understand \& apply GNNs without wading through endless academic papers or fragmented online searches. Hop: it serves as a 1-stop resource to learn fundamentals \& paves way for deeper exploration. Whether you are here out of professional necessity, sheer curiosity, or same kind of amazement that 1st drew me in, invite to embark on this journey, bring potential of GNNs to life.

    -- Cuốn sách này chính là cẩm nang mà tôi ước mình đã có trong những ngày đầu ấy. Nó hướng đến việc cung cấp 1 lộ trình rõ ràng \& dễ tiếp cận cho các chuyên gia, người đam mê, \& bất kỳ ai muốn hiểu \& áp dụng GNN mà không cần phải lội qua vô số bài báo học thuật hay tìm kiếm trực tuyến rời rạc. Hop: nó là 1 nguồn tài nguyên tổng hợp để học các kiến thức cơ bản \& mở đường cho những khám phá sâu hơn. Cho dù bạn đến đây vì nhu cầu công việc, sự tò mò đơn thuần, hay cùng 1 sự ngạc nhiên như đã thu hút tôi lần đầu, tham gia vào hành trình này, khai phá tiềm năng của GNN.
    \item {\sf About this book.} GNNs in Action is a book designed for people to jump quickly into this new field \& start building applications. At same time, try to strike a balance by including just enough critical theory to make this book as standalone as possible. Also fill in implementation details that may not be obvious or are left unexplained in currently available online tutorials \& documents. In particular, information about new \& emerging topics is very likely to be fragmented. This fragmentation adds friction when implementing \& testing new technologies.

    -- GNNs in Action là 1 cuốn sách được thiết kế để mọi người có thể nhanh chóng bước vào lĩnh vực mới này \& bắt đầu xây dựng ứng dụng. Đồng thời, cố gắng cân bằng bằng cách đưa vào vừa đủ lý thuyết quan trọng để cuốn sách này trở nên độc lập nhất có thể. Đồng thời, bổ sung những chi tiết triển khai có thể chưa rõ ràng hoặc chưa được giải thích trong các hướng dẫn trực tuyến hiện có \& tài liệu. Đặc biệt, thông tin về các chủ đề mới \& đang nổi lên rất có thể sẽ bị phân mảnh. Sự phân mảnh này gây khó khăn khi triển khai \& thử nghiệm các công nghệ mới.

    With GNNs in Action, offer a book that can reduce that friction by filling in gaps \& answering key questions whose answers are likely scattered over internet or not covered at all. Done so in a way that \fbox{emphasizes approachability rather than high rigor}.

    -- Với GNNs in Action, cung cấp 1 cuốn sách có thể giảm thiểu sự khó khăn đó bằng cách lấp đầy những khoảng trống \& trả lời những câu hỏi quan trọng mà câu trả lời có thể nằm rải rác trên internet hoặc chưa được đề cập đến. Hãy làm điều này theo cách \fbox{nhấn mạnh tính dễ tiếp cận hơn là tính nghiêm ngặt cao}.
    \begin{itemize}
        \item {\sf Who should read this book.} This book is designed for ML engineers \& data scientists familiar with neural networks but new to graph learning. If have experience in OOP, find concepts particularly accessible \& applicable.

        -- Cuốn sách này được thiết kế dành cho các kỹ sư ML \& nhà khoa học dữ liệu đã quen thuộc với mạng nơ-ron nhưng chưa quen với học đồ thị. Nếu có kinh nghiệm về OOP, tìm các khái niệm đặc biệt dễ hiểu \& áp dụng.
        \item {\sf How this book is organized: A road map.} In Part 1 of this book, provide a motivation for exploring GNNs, as well as cover fundamental concepts of graphs \& graph-based ML. In Chap. 1, introduce concepts of graphs \& graph ML, providing guidelines for their use \& applications. Chap. 2 covers graph representations up to \& including node embeddings. This will be 1st programmic exposure to GNNs, which are used to create such embeddings.

        -- Trong Phần 1 của cuốn sách này, chúng tôi sẽ cung cấp động lực để khám phá GNN, cũng như đề cập đến các khái niệm cơ bản về đồ thị \& Học máy dựa trên đồ thị. Trong Chương 1, chúng tôi sẽ giới thiệu các khái niệm về đồ thị \& Học máy dựa trên đồ thị, đồng thời cung cấp hướng dẫn sử dụng \& ứng dụng của chúng. Chương 2 sẽ đề cập đến các biểu diễn đồ thị, bao gồm cả nhúng nút. Đây sẽ là lần đầu tiên chúng tôi tiếp xúc với GNN, được sử dụng để tạo ra các nhúng như vậy.

        In part 2, core of book, introduce major types of GNNs, including graph convolutional networks (GCNs) \& GraphSAGE in Chap. 3, graph attention networks (GATs) in Chap. 4, \& graph autoencoders (GAEs) in Chap. 5. These methods are bread \& butter for most GNN applications \& also cover a range of other DL concepts e.g. convolution, attention, \& autoencoders.

        -- Trong phần 2, cốt lõi của cuốn sách, giới thiệu các loại GNN chính, bao gồm mạng tích chập đồ thị (GCN) \& GraphSAGE trong Chương 3, mạng chú ý đồ thị (GAT) trong Chương 4, \& bộ mã hóa tự động đồ thị (GAE) trong Chương 5. Các phương pháp này là nền tảng cho hầu hết các ứng dụng GNN \& cũng bao gồm 1 loạt các khái niệm DL khác, e.g., tích chập, chú ý, \& bộ mã hóa tự động.

        In part 3, look at more advanced topics. Describe GNNs for dynamic graphs (spatio-temporal GNNs) in Chap. 6 \& give methods to train GNNs at scale in Chap. 7. Finally, end with some consideration for project \& system planning for graph learning projects in Chap. 8.

        -- Trong phần 3, xem xét các chủ đề nâng cao hơn. Mô tả GNN cho đồ thị động (GNN không gian-thời gian) trong Chương 6 \& đưa ra các phương pháp huấn luyện GNN ở quy mô lớn trong Chương 7. Cuối cùng, kết thúc bằng 1 số cân nhắc về dự án \& lập kế hoạch hệ thống cho các dự án học đồ thị trong Chương 8.
        \item {\sf About code.} Python is coding language of choice throughout this book. There are now several GNN libraries in Python ecosystem, including PyTorch Geometric (PyG), Deep Graph Library (DGL), GraphScope, \& Jraph. Focus on PyG, which is 1 of most popular \& easy-to-use frameworks, written on top of PyTorch. Want this book to be approachable by an audience with a wide set of hardware constraints, so with exception of some individual sects \& Chap. 7 on scalability, distributed systems \& GPU systems aren't required, although they can be used for some of coded examples.

        -- Python là ngôn ngữ lập trình được lựa chọn trong suốt cuốn sách này. Hiện nay, hệ sinh thái Python đã có 1 số thư viện GNN, bao gồm PyTorch Geometric (PyG), Thư viện Đồ thị Sâu (DGL), GraphScope, Jraph. Tập trung vào PyG, 1 trong những framework phổ biến nhất, dễ sử dụng, được viết trên nền tảng PyTorch. Tôi muốn cuốn sách này dễ tiếp cận với những độc giả có nhiều hạn chế về phần cứng, vì vậy, ngoại trừ 1 số điểm riêng biệt trong Chương 7 về khả năng mở rộng, các hệ thống phân tán \& GPU không bắt buộc, mặc dù chúng có thể được sử dụng cho 1 số ví dụ được mã hóa.

        Book provides a survey of most relevant implementations of GNNs, including graph convolutional networks (GCNs), graph autoencoders (GAEs), graph attention networks (GATs), \& graph long short-term memory (LSTM). Aim: cover GNN tasks mentioned earlier. In addition, touch on different types of graphs, including knowledge graphs.

        -- Sách cung cấp 1 bản tổng quan về các triển khai GNN phổ biến nhất, bao gồm mạng tích chập đồ thị (GCN), bộ mã hóa tự động đồ thị (GAE), mạng chú ý đồ thị (GAT), bộ nhớ dài hạn đồ thị (LSTM). Mục tiêu: bao quát các nhiệm vụ GNN đã đề cập trước đó. Ngoài ra, đề cập đến các loại đồ thị khác nhau, bao gồm đồ thị tri thức.

        This book contains many examples of source code both in numbered listings \& in line with normal text. In both case, source code is formatted in a {\tt fixed-width font like this} to separate it from ordinary text. Sometimes code is also \textbf{\texttt in bold} to highlight code
    \end{itemize}
    PART 1: 1ST STEPS. Graphs are 1 of most versatile \& powerful ways to represent complex, interconnected data. This 1st part introduces fundamental concepts of graph theory, explaining what graphs are, why they matter as a data type, \& how their structure captures relationships that traditional data formats miss. Explore building blocks of graphs \& different graph types.

    -- Đồ thị là 1 trong những phương pháp linh hoạt \& mạnh mẽ nhất để biểu diễn dữ liệu phức tạp, có liên kết với nhau. Phần 1 này giới thiệu các khái niệm cơ bản của lý thuyết đồ thị, giải thích đồ thị là gì, tại sao chúng quan trọng như 1 kiểu dữ liệu, \& cách cấu trúc của chúng nắm bắt các mối quan hệ mà các định dạng dữ liệu truyền thống bỏ sót. Khám phá các khối xây dựng của đồ thị \& các kiểu đồ thị khác nhau.

    Explore fundamental concepts about GNNs, beginning with what they are \& how they differ from traditional neural networks. With this foundation, study graph embeddings, uncovering how to represent graphs in a way that makes them useful for ML. These concepts set stage for mastering GNNs \& their transformative capabilities in later chaps. By end of this book, have a solid understanding of basics, preparing you to dive deeper into mechanics of GNNs.

    -- Khám phá các khái niệm cơ bản về GNN, bắt đầu với bản chất của chúng \& sự khác biệt so với mạng nơ-ron truyền thống. Với nền tảng này, nghiên cứu nhúng đồ thị, khám phá cách biểu diễn đồ thị sao cho hữu ích cho ML. Những khái niệm này đặt nền tảng cho việc nắm vững GNN \& khả năng biến đổi của chúng trong các chương sau. Khi đọc xong cuốn sách này, bạn sẽ có được kiến thức cơ bản vững chắc, sẵn sàng cho việc tìm hiểu sâu hơn về cơ chế hoạt động của GNN.
    \item {\sf1. Discovering graph neural networks.} Covers:
    \begin{itemize}
        \item Defining graphs \& GNNs
        \item Understanding why people are excited about GNNs
        \item Recognizing when to use GNNs
        \item Taking a big picture look at solving a problem with a GNN
    \end{itemize}
    For data practitioners, fields of ML \& DS initially excite us because of potential to draw nonintuitive \& useful insights from data. In particular, insights from ML \& DL promise to enhance our understanding of world. For working engineer, these tools promise to deliver business value in unprecedented ways.

    -- Đối với các chuyên gia dữ liệu, lĩnh vực Học máy \& Phân tích dữ liệu (ML \& DS) ban đầu khiến chúng ta hào hứng vì tiềm năng rút ra những hiểu biết phi trực quan \& hữu ích từ dữ liệu. Đặc biệt, những hiểu biết từ Học máy \& Phân tích dữ liệu (ML \& DL) hứa hẹn sẽ nâng cao hiểu biết của chúng ta về thế giới. Đối với các kỹ sư đang làm việc, những công cụ này hứa hẹn mang lại giá trị kinh doanh theo những cách chưa từng có.

    Experience deviates from this ideal. Real-world data is usually messy, dirty, \& biased. Furthermore, statistical methods \& learning systems come with their own set of limitations. An essential role of practitoners: comprehend these limitations \& bridge gap between real data \& a feasible solution. E.g., may want to predict fraudulent activity in a bank, but 1st need to make sure that our training data has been correctly labeled. Even more importantly, need to check that our models won't incorrectly assign fraudulent activity to normal behaviors, possibly due to some hidden confounders in data.

    -- Kinh nghiệm thực tế thường khác xa lý tưởng này. Dữ liệu thực tế thường lộn xộn, bẩn thỉu, \& thiên vị. Hơn nữa, các phương pháp thống kê \& hệ thống học tập cũng có những hạn chế riêng. 1 vai trò thiết yếu của người thực hành: hiểu rõ những hạn chế này \& thu hẹp khoảng cách giữa dữ liệu thực tế \& 1 giải pháp khả thi. Ví dụ: có thể muốn dự đoán hoạt động gian lận trong 1 ngân hàng, nhưng trước tiên cần đảm bảo rằng dữ liệu đào tạo của chúng ta đã được dán nhãn chính xác. Quan trọng hơn nữa, cần kiểm tra xem các mô hình của chúng ta có gán sai hoạt động gian lận cho các hành vi bình thường hay không, có thể do 1 số yếu tố gây nhiễu tiềm ẩn trong dữ liệu.

    For graph data, until recently, bridging this gap has been particularly challenging. Graphs are a data structure that is rich with information \& especially adept at capturing intricacies of data where relationships play a crucial role. Graphs are omnipresent, with relationship data appearing in different forms e.g. atoms in molecules (nature), social networks (society), \& even models connection of web pages on internet (technology). Important to note: term {\it relational} here does not refer to {\it relational databases}, but rather to data where relationships are of significance.

    -- Đối với dữ liệu đồ thị, cho đến gần đây, việc thu hẹp khoảng cách này đặc biệt khó khăn. Đồ thị là 1 cấu trúc dữ liệu giàu thông tin \& đặc biệt khéo léo trong việc nắm bắt những dữ liệu phức tạp, nơi các mối quan hệ đóng vai trò then chốt. Đồ thị hiện diện ở khắp mọi nơi, với dữ liệu mối quan hệ xuất hiện dưới nhiều dạng khác nhau, e.g.: nguyên tử trong phân tử (tự nhiên), mạng xã hội (xã hội), \& thậm chí cả mô hình kết nối các trang web trên internet (công nghệ). Điều quan trọng cần lưu ý: thuật ngữ {\it relational} ở đây không đề cập đến {\it relational databases}, mà là dữ liệu trong đó các mối quan hệ có ý nghĩa quan trọng.

    Previously, if you wanted to incorporate relational features from a graph into a DL model, it had to be done in an indirect way, with different models used to process, analyze, \& then use graph data. These separate models often couldn't be easily scaled \& had trouble taking into account all node \& edge properties of graph data. To make best use of this rich \& ubiquitous data type for ML, needed a specialized ML technique specifically designed for distinct qualities of graphs \& relational data. This is gap that GNNs fill.

    -- Trước đây, nếu muốn tích hợp các đặc điểm quan hệ từ đồ thị vào mô hình DL, việc này phải được thực hiện gián tiếp, sử dụng các mô hình khác nhau để xử lý, phân tích, \& sau đó sử dụng dữ liệu đồ thị. Các mô hình riêng biệt này thường không dễ dàng mở rộng \& gặp khó khăn trong việc tính đến tất cả các thuộc tính nút \& cạnh của dữ liệu đồ thị. Để tận dụng tối đa kiểu dữ liệu phong phú \& phổ biến này cho ML, cần có 1 kỹ thuật ML chuyên biệt được thiết kế riêng cho các đặc tính riêng biệt của đồ thị \& dữ liệu quan hệ. Đây chính là khoảng trống mà GNN lấp đầy.

    DP field often contains a lot of hype around new technologies \& methods. However, GNNs are widely recognized as a genuine leap forward for graph-based learning [2]. This does not mean: GNNs are a silver bullet. Careful comparisons should be done between predictive results derived from GNNs \& other ML \& DL methods.

    -- Lĩnh vực DP thường chứa đựng nhiều thông tin cường điệu về các công nghệ \& phương pháp mới. Tuy nhiên, GNN được công nhận rộng rãi là 1 bước tiến thực sự cho học tập dựa trên đồ thị [2]. Điều này không có nghĩa là: GNN là giải pháp hoàn hảo. Cần so sánh cẩn thận giữa các kết quả dự đoán thu được từ GNN \& các phương pháp ML \& DL khác.

    Key thing to remember: if your DS problem involves data that can be structured as a graph -- i.e., data is connected or relational -- then GNNs could offer a valuable approach, even if you weren't aware that sth was missing in your approach. GNNs can be designed to handle very large data, to scale, to adapt to graphs of different sizes \& shapes. This can make working with relationship-centric data easier \& more efficient, as well as yield richer results.

    -- Điều quan trọng cần nhớ: nếu bài toán DS của bạn liên quan đến dữ liệu có thể được cấu trúc dưới dạng đồ thị - i.e., dữ liệu được kết nối hoặc quan hệ - thì GNN có thể cung cấp 1 phương pháp tiếp cận hữu ích, ngay cả khi bạn không nhận ra rằng phương pháp của mình còn thiếu điều gì đó. GNN có thể được thiết kế để xử lý dữ liệu rất lớn, có khả năng mở rộng, thích ứng với các đồ thị có kích thước \& hình dạng khác nhau. Điều này có thể giúp việc xử lý dữ liệu tập trung vào mối quan hệ dễ dàng \& hiệu quả hơn, cũng như mang lại kết quả phong phú hơn.

    Standout advantages of GNNs are why data scientists \& engineers are increasingly recognizing importance of mastering them. GNNs have the ability to unveil unique insights from relational dat -- from identifying new drug candidates to optimizing ETA prediction accuracy in your Google Maps app -- acting as a catalyst for discovery \& innovation, \& empowering professionals to push boundaries of conventional data analysis. Their diverse applicability spans various fields, offering professionals a versatile tool that is as relevant in e-commerce (e.g., recommendation engines) as it is in bioinformatics (e.g., drug toxicity prediction). Proficiency in GNNs equips data professionals with a multifaceted tool for enhanced, accurate, \& innovative data analysis of graphs.

    -- Những lợi thế nổi bật của GNN là lý do tại sao các nhà khoa học dữ liệu \& kỹ sư ngày càng nhận thức được tầm quan trọng của việc thành thạo chúng. GNN có khả năng khám phá những hiểu biết độc đáo từ dữ liệu quan hệ -- từ việc xác định các ứng cử viên thuốc mới đến tối ưu hóa độ chính xác dự đoán ETA trong ứng dụng Google Maps -- đóng vai trò là chất xúc tác cho khám phá \& đổi mới, \& trao quyền cho các chuyên gia vượt qua các giới hạn của phân tích dữ liệu thông thường. Khả năng ứng dụng đa dạng của chúng trải rộng trên nhiều lĩnh vực, mang đến cho các chuyên gia 1 công cụ đa năng, vừa phù hợp trong thương mại điện tử (e.g.: công cụ đề xuất) vừa phù hợp trong tin sinh học (e.g.: dự đoán độc tính của thuốc). Thành thạo GNN trang bị cho các chuyên gia dữ liệu 1 công cụ đa năng để phân tích dữ liệu đồ thị chính xác, \& sáng tạo.

    For all these reasons, GNNs are now popular choice for recommender engines, analyzing social networks, detecting fraud, understanding how biomolecules behave, \& many other practical examples.

    -- Vì tất cả những lý do này, GNN hiện là lựa chọn phổ biến cho các công cụ đề xuất, phân tích mạng xã hội, phát hiện gian lận, hiểu cách các phân tử sinh học hoạt động, \& nhiều ví dụ thực tế khác.
    \begin{itemize}
        \item {\sf1.1. Goals of this book.} GNNs in Action is aimed at practitioners who want to begin to deploy GNNs to solve real problems. This could be a ML engineer not familiar with graph data structures, a data scientist who hasn't yet tried GNNs, or even a software engineer who may be unfamiliar with either. Throughout this book, cover topics from basics of graphs all way to more complex GNN models. Build up architecture of a GNN, step-by-step. This includes overall architecture of a GNN \& critical aspect of message passing. Then go on to add different features \& extensions to these basic aspects, e.g. introducing convolution \& sampling, attention mechanisms, a generative model, \& operating on dynamic graphs. When building our GNNs, work with Python \& use some standard libraries. GNNs libraries are either standalone or use TensorFlow or PyTorch as a backend. In this text, focus will be on PyTorch Geometric (PyG). Other popular libraries include Deep Graph Library (DGL, a standalone library) \& SPektral (which uses Keras \& TensorFlow as a backend). There is also Jraph for JAX users.

        -- GNNs in Action hướng đến những người thực hành muốn bắt đầu triển khai GNN để giải quyết các vấn đề thực tế. Họ có thể là 1 kỹ sư ML chưa quen thuộc với cấu trúc dữ liệu đồ thị, 1 nhà khoa học dữ liệu chưa từng thử GNN, hoặc thậm chí là 1 kỹ sư phần mềm chưa quen thuộc với cả hai. Xuyên suốt cuốn sách này, chúng tôi sẽ đề cập đến các chủ đề từ kiến thức cơ bản về đồ thị cho đến các mô hình GNN phức tạp hơn. Xây dựng kiến trúc của GNN, từng bước một. Điều này bao gồm kiến trúc tổng thể của GNN \& khía cạnh quan trọng của việc truyền thông điệp. Sau đó, tiếp tục thêm các tính năng khác nhau \& phần mở rộng cho các khía cạnh cơ bản này, e.g.: giới thiệu tích chập \& lấy mẫu, cơ chế chú ý, mô hình sinh, \& hoạt động trên đồ thị động. Khi xây dựng GNN, làm việc với Python \& sử dụng 1 số thư viện chuẩn. Thư viện GNN có thể độc lập hoặc sử dụng TensorFlow hoặc PyTorch làm nền tảng. Trong văn bản này, trọng tâm sẽ là PyTorch Geometric (PyG). Các thư viện phổ biến khác bao gồm Thư viện Deep Graph (DGL, 1 thư viện độc lập) \& SPektral (sử dụng Keras \& TensorFlow làm nền tảng). Ngoài ra còn có Jraph dành cho người dùng JAX.

        Our aim throughout this book is to enable you to:
        \begin{enumerate}
            \item access suitability of a GNN solution for your problem.
            \item understand when traditional neural networks won't perform as well as a GNN for graph structured data \& when GNNs may not be the best tool for tabular data.
            \item design \& implement a GNN architecture to solve problems specific to you.
            \item make clear limitations of GNNs.
        \end{enumerate}
        This book is weighted toward implementation using programming. Also devote some time on essential theory \& concepts, so that techniques covered can be sufficiently understood. These are covered in an ``Under Hood'' sect at end of most chaps to separate technical reasons from actual implementation. There are many different models \& packages that build on key concepts introduced in this book. So, this book should not be seen as a comprehensive review of all GNns methods \& models, which could run to several thousands of pages, but rather starting point for curious \& eager-to-learn practitioner.

        -- Mục tiêu của chúng tôi trong suốt cuốn sách này là giúp bạn:
        \begin{enumerate}
            \item đánh giá tính phù hợp của giải pháp GNN cho vấn đề của bạn.
            \item hiểu khi nào mạng nơ-ron truyền thống không hoạt động tốt bằng GNN đối với dữ liệu có cấu trúc đồ thị \& khi nào GNN có thể không phải là công cụ tốt nhất cho dữ liệu dạng bảng.
            \item thiết kế \& triển khai kiến trúc GNN để giải quyết các vấn đề cụ thể của bạn.
            \item làm rõ những hạn chế của GNN.
        \end{enumerate}
        Cuốn sách này thiên về việc triển khai bằng lập trình. Đồng thời dành thời gian cho các lý thuyết \& khái niệm thiết yếu, để các kỹ thuật được đề cập có thể được hiểu đầy đủ. Những điều này được trình bày trong phần ``Under Hood'' ở cuối hầu hết các chương để phân biệt lý do kỹ thuật với việc triển khai thực tế. Có rất nhiều mô hình \& gói khác nhau được xây dựng dựa trên các khái niệm chính được giới thiệu trong cuốn sách này. Vì vậy, cuốn sách này không nên được coi là 1 bài tổng quan toàn diện về tất cả các phương pháp \& mô hình GNN, có thể dài tới hàng nghìn trang, mà nên là điểm khởi đầu cho những người thực hành \& ham học hỏi.

        Book is divided into 3 parts. Part 1 covers basics of GNNs, especially ways in which they differ from other neural networks, e.g. {\it message passing \& embeddings}, which have specific meaning for GNNs. Part 2, heart of book, goes over models themselves, where we cover a handful of key model types. Then, in part 3, go into more detail with some of harder models \& concepts, including how to scale graphs \& deal with temporal data.

        -- Sách được chia thành 3 phần. Phần 1 trình bày những kiến thức cơ bản về GNN, đặc biệt là những điểm khác biệt giữa chúng với các mạng nơ-ron khác, e.g. {truyền thông điệp \& nhúng}, vốn có ý nghĩa riêng đối với GNN. Phần 2, trọng tâm của sách, sẽ đề cập đến bản thân các mô hình, trong đó chúng ta sẽ tìm hiểu 1 số loại mô hình chính. Sau đó, trong phần 3, chúng ta sẽ đi sâu hơn vào 1 số mô hình \& khái niệm khó hơn, bao gồm cách chia tỷ lệ đồ thị \& xử lý dữ liệu thời gian.

        GNNs in Action is designed for people to jump quickly into this new field \& start building applications. Aim for this book: reduce friction of implementing new technologies by filling in gaps \& answering key development questions whose answers may not be easy to find or may not be covered elsewhere at all. Each method is introduced through an example application so you can understand how GNNs are applied in practice.

        -- GNNs in Action được thiết kế để mọi người nhanh chóng tiếp cận lĩnh vực mới này \& bắt đầu xây dựng ứng dụng. Mục tiêu của cuốn sách này: giảm thiểu sự cản trở khi triển khai các công nghệ mới bằng cách lấp đầy những khoảng trống \& trả lời những câu hỏi phát triển quan trọng mà câu trả lời có thể không dễ tìm hoặc chưa được đề cập ở bất kỳ nơi nào khác. Mỗi phương pháp được giới thiệu thông qua 1 ứng dụng ví dụ để bạn có thể hiểu cách GNN được áp dụng trong thực tế.
        \begin{itemize}
            \item {\sf1.1.1. Catching up on graph fundamentals.} Do need to understand basics of graphs before you can understand GNNs. Goal for this book is to teach GNNs to DL practitioners \& builders for traditional neural networks who may not know much about graphs. At same time, also recognize: readers of this book may vary enormously in their knowledge of graphs. How to address these differences \& make sure everyone has what they need to make the most of this book? In this chap, provide an introduction to fundamental graph concepts that are most essential to understanding GNNs.

            -- Bạn cần nắm vững những kiến thức cơ bản về đồ thị trước khi có thể hiểu về GNN. Mục tiêu của cuốn sách này là hướng dẫn GNN cho những người thực hành DL \& những người xây dựng mạng nơ-ron truyền thống, những người có thể chưa biết nhiều về đồ thị. Đồng thời, cũng cần lưu ý: kiến thức về đồ thị của độc giả có thể rất khác nhau. Làm thế nào để giải quyết những khác biệt này \& đảm bảo mọi người đều có những kiến thức cần thiết để tận dụng tối đa cuốn sách này? Trong chương này, chúng tôi sẽ giới thiệu các khái niệm cơ bản về đồ thị, những khái niệm thiết yếu nhất để hiểu về GNN.

            After refresher on key concepts in graphs \& graph learning, look into some case studies in several fields where GNNs are being successfully applied. Then, break down those specific cases to see what makes a good case for using a GNN, as well as how to know if you have a GNN problem on your hands. At end of chap, introduce mechanics of GNNs, barebone skeleton that the rest of book will add to.

            -- Sau khi ôn lại các khái niệm chính về đồ thị \& học đồ thị, xem xét 1 số nghiên cứu điển hình trong 1 số lĩnh vực mà GNN đang được ứng dụng thành công. Sau đó, phân tích các trường hợp cụ thể đó để xem đâu là lý do tốt để sử dụng GNN, cũng như cách nhận biết liệu bạn có đang gặp vấn đề về GNN hay không. Cuối chương, giới thiệu cơ chế hoạt động của GNN, bộ khung xương cốt mà phần còn lại của cuốn sách sẽ bổ sung.
        \end{itemize}
        \item {\sf1.2. Graph-based learning.} This section defines graphs, graph-based learning, \& some fundamentals of GNNs, including basic structure of a graph \& a taxonomy of different types of graphs. Then, review graph-based learning, putting GNNs in context with other learning methods. Finally, explain value of graphs, ending with an example of data derived from Titanic dataset.

        -- Học tập dựa trên đồ thị. Phần này định nghĩa đồ thị, học tập dựa trên đồ thị, \& 1 số kiến thức cơ bản về mạng nơ-ron nhân tạo (GNN), bao gồm cấu trúc cơ bản của đồ thị \& phân loại các loại đồ thị khác nhau. Sau đó, xem xét lại học tập dựa trên đồ thị, đặt GNN vào bối cảnh của các phương pháp học tập khác. Cuối cùng, giải thích giá trị của đồ thị, kết thúc bằng 1 ví dụ về dữ liệu được lấy từ tập dữ liệu Titanic.
        \begin{itemize}
            \item {\sf1.2.1. What are graphs?} Graphs are data structures with elements, expressed as {\it nodes or vertices}, \& relationships between elements, expressed as {\it edges or links}. All nodes in graph will have additional {\it feature data}. This is node-specific data, relating to things e.g. names or ages of individuals in a social network. Links are key to power of relational data, as they allow us to learn more about system, give new tools for analyzing data, \& predict new properties from it. This is in contrast to tabular data e.g. a database table, dataframe, or spreadsheet, where data is fixed in rows \& columns.

            -- Đồ thị là cấu trúc dữ liệu với các phần tử, được biểu diễn dưới dạng {\it nút hoặc đỉnh}, \& mối quan hệ giữa các phần tử, được biểu diễn dưới dạng {\it cạnh hoặc liên kết}. Tất cả các nút trong đồ thị sẽ có thêm {\it dữ liệu đặc trưng}. Đây là dữ liệu cụ thể của từng nút, liên quan đến các thông tin như tên hoặc tuổi của các cá nhân trong mạng xã hội. Liên kết là chìa khóa cho sức mạnh của dữ liệu quan hệ, vì chúng cho phép chúng ta tìm hiểu thêm về hệ thống, cung cấp các công cụ mới để phân tích dữ liệu \& \& dự đoán các thuộc tính mới từ dữ liệu đó. Điều này trái ngược với dữ liệu dạng bảng, e.g.: bảng cơ sở dữ liệu, khung dữ liệu hoặc bảng tính, trong đó dữ liệu được cố định theo hàng \& cột.

            To describe \& learn from edges between nodes, we need a way to write them down. This can be explicitly, quickly, can see describing things in this way becomes unwieldy \& might be repeating redundant information. Luckily, there are many mathematical formalisms for describing relations in graphs. 1 of most common: describe {\it adjacency matrix}. Notice: adjacency matrix is symmetric across diagonal \& all values are 1s or 0s. Adjacency matrix of a graph is an important concept that makes it easy to observe all connections of a graph in a single table. Here assumed: there is no directionability in our graphs, i.e., if 0 is connected to 1, then 1 is also connected to 0. This is known as an {\it undirected graph}. Undirected graphs can be easily inferred from an adjacency matrix because, in this case, matrix is symmetric across diagonal, upper-right triangle is reflected onto bottom-left.

            -- Để mô tả \& học từ các cạnh giữa các nút, chúng ta cần 1 cách để viết chúng ra. Điều này có thể rõ ràng, nhanh chóng, có thể thấy việc mô tả mọi thứ theo cách này trở nên cồng kềnh \& có thể lặp lại thông tin dư thừa. May mắn thay, có nhiều công thức toán học để mô tả các mối quan hệ trong đồ thị. 1 trong những công thức phổ biến nhất: mô tả {\it adjacency matrix}. Lưu ý: ma trận kề là đối xứng qua đường chéo \& tất cả các giá trị là 1 hoặc 0. Ma trận kề của đồ thị là 1 khái niệm quan trọng giúp dễ dàng quan sát tất cả các kết nối của đồ thị trong 1 bảng duy nhất. Ở đây giả sử: không có tính định hướng trong đồ thị của chúng ta, i.e., nếu 0 được kết nối với 1, thì 1 cũng được kết nối với 0. Đây được gọi là {\it undirected graph}. Đồ thị vô hướng có thể dễ dàng suy ra từ ma trận kề vì, trong trường hợp này, ma trận đối xứng qua đường chéo, tam giác trên cùng bên phải được phản chiếu xuống dưới cùng bên trái.

            Also assume: all relations between nodes are identical. If we wanted relation of nodes B-E to mean more than relation of nodes B-A, then we could increase weight of this edge. This translates to increasing value in adjacency matrix, making entry for B-A edge equal to 10 instead of 1, e.g.

            -- Cũng giả sử: tất cả các mối quan hệ giữa các nút đều giống hệt nhau. Nếu chúng ta muốn mối quan hệ giữa các nút B-E có ý nghĩa hơn mối quan hệ giữa các nút B-A, thì chúng ta có thể tăng trọng số của cạnh này. Điều này tương đương với việc tăng giá trị trong ma trận kề, e.g., nhập giá trị cho cạnh B-A bằng 10 thay vì 1.

            Graphs where all relations are of equal importance are known as {\it unweighted graphs} \& can also be easily observed from adjacency matrix because all graph entries are either 1s or 0s. Graphs where edges have multiple values are known as {\it weighted}.

            -- Đồ thị mà tất cả các mối quan hệ đều có tầm quan trọng như nhau được gọi là {\it unweighted graphs} \& cũng có thể dễ dàng quan sát được từ ma trận kề vì tất cả các mục đồ thị đều là 1 hoặc 0. Đồ thị mà các cạnh có nhiều giá trị được gọi là {\it weighted}.

            If any of nodes in graph do not have an edge that connects to itself, then nodes will also have 0s at their own value in adjacency matrix (0s along diagonal), i.e., a graph does not have self-loops. A {\it self-loop} occurs when a node has an edge that connects to that same node. To add a self-loop, we just make value for that node nonzero at its position in diagonal.

            -- Nếu bất kỳ nút nào trong đồ thị không có cạnh nối với chính nó, thì các nút cũng sẽ có giá trị 0 tại chính nó trong ma trận kề (các giá trị 0 dọc theo đường chéo), i.e., đồ thị không có vòng lặp tự thân. 1 vòng lặp tự thân {\it} xảy ra khi 1 nút có cạnh nối với chính nút đó. Để thêm 1 vòng lặp tự thân, chúng ta chỉ cần gán giá trị khác không cho nút đó tại vị trí của nó trên đường chéo.

            In practice, an adjacency matrix is only 1 of many ways to describe relations in a graph. Others include adjacency lists, edge lists, or an incidence matrix. Understanding these types of data structures well is vital to graph-based learning. If you are unfamiliar with these terms, or need a refresher, recommend looking through appendix A, which has additional details \& explanations.

            -- Trên thực tế, ma trận kề chỉ là 1 trong nhiều cách để mô tả các mối quan hệ trong đồ thị. Các cách khác bao gồm danh sách kề, danh sách cạnh, hoặc ma trận liên quan. Việc hiểu rõ các loại cấu trúc dữ liệu này rất quan trọng đối với việc học tập dựa trên đồ thị. Nếu bạn chưa quen với các thuật ngữ này hoặc cần ôn tập lại, xem Phụ lục A, trong đó có thêm chi tiết \& giải thích.
            \item {\sf1.2.2. Different types of graphs.} Understanding many different types of graphs can help us work out what methods to use to analyze \& transform graph, \& what ML methods to apply. In following, give a very quick overview of some of most common properties for graphs to have.

            -- Hiểu biết về nhiều loại đồ thị khác nhau có thể giúp chúng ta tìm ra phương pháp nào cần sử dụng để phân tích \& biến đổi đồ thị, \& áp dụng phương pháp ML nào. Sau đây, chúng tôi sẽ giới thiệu sơ lược về 1 số thuộc tính phổ biến nhất của đồ thị.
            \begin{itemize}
                \item {\sf Homogeneous \& Heterogeneous Graphs.} Most basic graphs are {\it homogeneous graphs}, which are made up of 1 type of node \& 1 type of edge. Consider a homogeneous graph that describes a recruitment network. In this type of graph, nodes would represent job candidates, \& edges would represent relationships between candidates.

                -- {\sf Đồ thị Đồng nhất \& Đồ thị Không Đồng nhất.} Hầu hết các đồ thị cơ bản là {\it đồ thị đồng nhất}, được tạo thành từ 1 loại nút \& 1 loại cạnh. Xem xét 1 đồ thị đồng nhất mô tả 1 mạng lưới tuyển dụng. Trong loại đồ thị này, các nút sẽ đại diện cho các ứng viên xin việc, \& cạnh sẽ đại diện cho mối quan hệ giữa các ứng viên.

                If want to expand power of our graph to describe our recruitment network, could give it more types of nodes \& edges, making it a {\it heterogeneous graphs}. With this expansion, some nodes may be candidates \& others may be companies. Edges could now consist of relationships between candidates \& current or past employment of job candidates at companies. See {\sf Fig. 1.2: A homogeneous graph \& a heterogeneous graph. Here, shade of a node or edge represents its type or class. For homogeneous graph, all nodes are of same type, \& all edges are of same type. For heterogeneous graph, nodes \& edges have multiple types} for a comparison of a homogeneous graph (all nodes or edges have same shade) with a heterogeneous graph (nodes \& edges have a variety of shades).

                -- Nếu muốn mở rộng sức mạnh của đồ thị để mô tả mạng lưới tuyển dụng, có thể cung cấp cho nó nhiều loại nút \& cạnh hơn, biến nó thành {\it đồ thị không đồng nhất}. Với sự mở rộng này, 1 số nút có thể là ứng viên \& những nút khác có thể là công ty. Các cạnh bây giờ có thể bao gồm các mối quan hệ giữa ứng viên \& việc làm hiện tại hoặc trước đây của ứng viên tại các công ty. Xem {\sf Hình 1.2: Đồ thị đồng nhất \& đồ thị không đồng nhất. Ở đây, sắc thái của 1 nút hoặc cạnh biểu thị loại hoặc lớp của nó. Đối với đồ thị đồng nhất, tất cả các nút đều cùng loại, \& tất cả các cạnh đều cùng loại. Đối với đồ thị không đồng nhất, các nút \& cạnh có nhiều loại} để so sánh đồ thị đồng nhất (tất cả các nút hoặc cạnh có cùng sắc thái) với đồ thị không đồng nhất (các nút \& cạnh có nhiều sắc thái khác nhau).
                \item {\sf Bipartite graphs.} Similar to heterogeneous graphs, {\it bipartite graphs} also can be separated or partitioned into different subsets. However, bipartite graphs ({\sf Fig. 1.3: A bipartite graph. There are 2 types of nodes (2 shades of circles). In a bipartite graph, nodes cannot be connected to nodes of same type. This is also an example of a heterogeneous graph.}) have a very specific network structure s.t. nodes in each subset connect to nodes outside of their subset \& not inside. Later, discuss recommendation system \& Pinterest graph. This graph is bipartite because 1 set of nodes (pins) connects another set of nodes (boards) but not to nodes within their set (pins).

                -- {\sf Đồ thị hai phần.} Tương tự như đồ thị không đồng nhất, {\it Đồ thị 2 phần} cũng có thể được tách hoặc phân vùng thành các tập con khác nhau. Tuy nhiên, đồ thị hai phần ({\sf Hình 1.3: Đồ thị hai phần. Có 2 loại nút (2 sắc thái của hình tròn). Trong đồ thị hai phần, các nút không thể được kết nối với các nút cùng loại. Đây cũng là 1 ví dụ về đồ thị không đồng nhất.}) có cấu trúc mạng rất cụ thể, e.g.: các nút trong mỗi tập con kết nối với các nút bên ngoài tập con của chúng \& không kết nối với các nút bên trong. Sau đó, thảo luận về hệ thống đề xuất \& đồ thị Pinterest. Đồ thị này là hai phần vì 1 tập hợp các nút (ghim) kết nối với 1 tập hợp các nút khác (bảng) nhưng không kết nối với các nút trong tập hợp của chúng (ghim).
                \item {\sf Cyclic graphs, acyclic graphs, \& directed acyclic graphs.} A graph is {\it cyclic} if it allows you to start at a node, travel along its edge, \& return to starting node without retracing any steps, creating a circular path within graph. In contrast, in an {\it acyclic} graph, no matter which path you take from any starting node, you cannot return to starting point without backtracking. These graphs, as shown in {\sf Fig. 1.4: A cyclic graph, an acyclic graph, \& a DAG. In cyclic graph, cycle is shown by arrows (directed edges) connecting nodes A-E-D-C-B-A. Note 2 nodes, F \& G are part of graph, but not part of its defining cycle. Acyclic graph is composed of undirected edges, \& no cycle is possible. In DAG, all directed edges flow in 1 direction, from A to F.}, often resemble tree-like structures or paths that do not loop back on themselves.

                -- {\sf Đồ thị tuần hoàn, đồ thị phi chu trình, \& đồ thị phi chu trình có hướng.} 1 đồ thị là {\it tuần hoàn} nếu nó cho phép bạn bắt đầu tại 1 nút, di chuyển dọc theo cạnh của nó, \& quay lại nút bắt đầu mà không cần quay lại bất kỳ bước nào, tạo ra 1 đường tròn trong đồ thị. Ngược lại, trong đồ thị {\it phi chu trình}, bất kể bạn đi theo đường nào từ bất kỳ nút bắt đầu nào, bạn không thể quay lại điểm bắt đầu mà không quay lại. Các đồ thị này, như được thể hiện trong {\sf Hình 1.4: Đồ thị tuần hoàn, đồ thị phi chu trình, \& DAG. Trong đồ thị tuần hoàn, chu trình được biểu diễn bằng các mũi tên (cạnh có hướng) nối các nút A-E-D-C-B-A. Lưu ý 2 nút, F \& G là 1 phần của đồ thị, nhưng không phải là 1 phần của chu trình xác định của nó. Đồ thị phi chu trình bao gồm các cạnh không có hướng, \& không thể có chu trình. Trong DAG, tất cả các cạnh có hướng đều chảy theo 1 hướng, từ A đến F.}, thường giống các cấu trúc giống cây hoặc các đường dẫn không vòng lại với chính chúng.

                While both cyclic \& acyclic graphs can be either undirected or directed, a {\it directed acyclic graph (DAG)} is a specific type of acyclic graph that is exclusively directed. In a DAG, all edges have a direction, \& no cycles are allowed. DAGs represent 1-way relationships where we can't follow arrows \& end up back at starting point. This characteristic makes DAGs essential in causal analysis, as they reflect causal structures where causality is assumed to be undirectional. E.g., A can cause B, but B can't simultaneously cause A. This unidirectional nature aligns perfectly with structure of DAGs, making them ideal for modeling workflow processes, dependency chains, \& causal relationships in various fields.

                -- Trong khi cả đồ thị tuần hoàn \ \& đồ thị phi tuần hoàn đều có thể vô hướng hoặc có hướng, {\it đồ thị phi tuần hoàn có hướng (DAG)} là 1 loại đồ thị phi tuần hoàn cụ thể chỉ có hướng. Trong DAG, tất cả các cạnh đều có hướng, \& không cho phép chu trình. DAG biểu diễn các mối quan hệ 1 chiều, trong đó chúng ta không thể theo mũi tên \& kết thúc ở điểm xuất phát. Đặc điểm này làm cho DAG trở nên thiết yếu trong phân tích nhân quả, vì chúng phản ánh các cấu trúc nhân quả trong đó quan hệ nhân quả được coi là vô hướng. Ví dụ: A có thể gây ra B, nhưng B không thể đồng thời gây ra A. Bản chất đơn hướng này hoàn toàn phù hợp với cấu trúc của DAG, khiến chúng trở nên lý tưởng để mô hình hóa các quy trình công việc, chuỗi phụ thuộc, \& các mối quan hệ nhân quả trong nhiều lĩnh vực khác nhau.
                \item {\sf Knowledge graphs.} A {\it knowledge graph} is a specialized type of heterogeneous graph that represents data with enriched semantic meaning, capturing not only relationships between different entities but also context \& nature of these relationships. Unlike conventional graphs, which primarily emphasize structure \& connectivity, a knowledge graph incorporates metadata \& follow specific schemas to provide deeper contextual information. This allows for advanced reasoning \& querying capabilities, e.g. identifying patterns, uncovering specific types of connections, or inferring new relationships.

                -- {\sf Đồ thị tri thức.} Đồ thị tri thức {\it} là 1 loại đồ thị không đồng nhất chuyên biệt, biểu diễn dữ liệu với ý nghĩa ngữ nghĩa phong phú, không chỉ nắm bắt mối quan hệ giữa các thực thể khác nhau mà còn cả bối cảnh \& bản chất của các mối quan hệ này. Không giống như đồ thị thông thường, chủ yếu nhấn mạnh vào cấu trúc \& kết nối, đồ thị tri thức kết hợp siêu dữ liệu \& tuân theo các lược đồ cụ thể để cung cấp thông tin ngữ cảnh sâu hơn. Điều này cho phép khả năng suy luận \& truy vấn nâng cao, e.g.: xác định các mẫu, khám phá các loại kết nối cụ thể hoặc suy ra các mối quan hệ mới.

                In example of an academic research network at a university, a knowledge graph might represent various entities e.g. Professors, Students, Papers, \& Research Topics, \& explicitly define relationships between them. E.g., Professors \& Students could be associated with Papers through an Authorship relationship, while Professors might also Supervise Students. Furthermore, graph would reflect hierarchical structures, e.g., Professors \& Students being categorized under Departments. Can see this knowledge graph depicted in {\sf Fig. 1.5: A knowledge graph representing an academic research network within a university's physics department. Graph illustrates both hierarchical relationships, e.g., professors \& students as members of department, \& behavioral relationships, e.g., professors supervising students \& authoring papers. Entities e.g. Professors, Students, Papers, \& Topics are connected through semantically meaningful relationships (Supervises, Wrote, Inspires). Entities also have detailed features (Name, Department, Type) providing further context. Semantic connections \& features enable advanced querying \& analysis of complex academic interactions.}

                -- Trong ví dụ về mạng lưới nghiên cứu học thuật tại 1 trường đại học, biểu đồ kiến thức có thể biểu diễn nhiều thực thể khác nhau, e.g.: Giáo sư, Sinh viên, Bài báo, \& Chủ đề nghiên cứu, \& định nghĩa rõ ràng mối quan hệ giữa chúng. Ví dụ: Giáo sư \& Sinh viên có thể được liên kết với Bài báo thông qua mối quan hệ Tác giả, trong khi Giáo sư cũng có thể Giám sát Sinh viên. Hơn nữa, biểu đồ sẽ phản ánh các cấu trúc phân cấp, e.g.: Giáo sư \& Sinh viên được phân loại theo Khoa. Có thể xem biểu đồ kiến thức này được mô tả trong {\sf Hình 1.5: Biểu đồ kiến thức biểu diễn mạng lưới nghiên cứu học thuật trong khoa vật lý của 1 trường đại học. Biểu đồ minh họa cả các mối quan hệ phân cấp, e.g.: giáo sư \& sinh viên là thành viên của khoa, \& các mối quan hệ hành vi, e.g.: giáo sư hướng dẫn sinh viên \& biên soạn bài báo. Các thực thể, e.g.: Giáo sư, Sinh viên, Bài báo, \& Chủ đề được kết nối thông qua các mối quan hệ có ý nghĩa ngữ nghĩa (Giám sát, Viết, Truyền cảm hứng). Các thực thể cũng có các tính năng chi tiết (Tên, Khoa, Loại) cung cấp thêm ngữ cảnh. Kết nối ngữ nghĩa \& các tính năng cho phép truy vấn nâng cao \& phân tích các tương tác học thuật phức tạp.}
                \begin{note}
                    Should use multigraphs to further represent knowledge graphs.
                \end{note}
                A key feature of knowledge graphs is their ability to provide explicit context. Unlike conventional heterogeneous graphs, which display different types of entities \& their basic connections without detailed semantic meaning, knowledge graphs go further by defining specific types \& meanings of relationships. E.g., while a traditional graph might show that Professors are connected to Departments or that Students are linked to Papers, a knowledge graph would specify that Professors supervise Students or that Students \& Professors Wrote Papers. This added layer of meaning enables more powerful querying \& analysis, making knowledge graphs particularly valuable in fields e.g. NLP, recommendation systems, \& academic research analysis.

                -- 1 đặc điểm quan trọng của đồ thị tri thức là khả năng cung cấp ngữ cảnh rõ ràng. Không giống như các đồ thị không đồng nhất thông thường, vốn hiển thị các loại thực thể khác nhau \& các kết nối cơ bản của chúng mà không có ý nghĩa ngữ nghĩa chi tiết, đồ thị tri thức tiến xa hơn bằng cách xác định các loại \& ý nghĩa cụ thể của các mối quan hệ. Ví dụ: trong khi đồ thị truyền thống có thể hiển thị Giáo sư được kết nối với Khoa hoặc Sinh viên được liên kết với Bài báo, đồ thị tri thức sẽ chỉ rõ Giáo sư hướng dẫn Sinh viên hoặc Sinh viên \& Giáo sư viết Bài báo. Lớp ý nghĩa bổ sung này cho phép truy vấn \& phân tích mạnh mẽ hơn, khiến đồ thị tri thức đặc biệt có giá trị trong các lĩnh vực như NLP, hệ thống đề xuất, \& phân tích nghiên cứu học thuật.
                \item {\sf Hypergraphs.} 1 of more complex \& difficult graphs to work with is hypergraph. {\it Hypergraphs} are those where a single edge can be connected to multiple different nodes. For graphs that are not hypergraphs, edges are used to connected exactly 2 nodes (or a node to itself for self-loops). As shown in {\sf Fig. 1.6: 1 undirected hypergraph, illustrated in 2 ways. On left, have a graph whose edges are represented by shaded areas, marked by letters, \& whose vertices are dots, marked by numbers. On right, have a graph whose edge lines (marked by letters) connect up to 3 nodes (circles marked by numbers).}, edges in a hypergraph can connect between any number of nodes. Complexity of a hypergraph is reflected in its adjacency data. For typical graphs, network connectivity is represented by a 2D adjacency matrix. For hypergraphs, adjacency matrix extends to a higher dimensional tensor, referred to as an {\it incidence tensor}. This tensor is $N$-dimensional, where $N$ is maximum number of nodes connected by a single edge. An example of a hypergraph might be a communication platform that allows for group chats as well as single person conversations. In an ordinary graph, edges would only connect 2 people. In a hypergraph, 1 hyperedge could connect multiple people, representing a group chat.

                -- {\sf Siêu đồ thị.} 1 trong những đồ thị phức tạp hơn \& khó làm việc hơn là siêu đồ thị. {\t Siêu đồ thị} là những đồ thị mà 1 cạnh đơn có thể được kết nối với nhiều nút khác nhau. Đối với những đồ thị không phải là siêu đồ thị, các cạnh được sử dụng để kết nối chính xác 2 nút (hoặc 1 nút với chính nó đối với các vòng lặp tự thân). Như thể hiện trong {\sf Hình 1.6: 1 siêu đồ thị vô hướng, được minh họa theo 2 cách. Bên trái, có 1 đồ thị mà các cạnh được biểu diễn bằng các vùng tô bóng, được đánh dấu bằng các chữ cái, \& có các đỉnh là các dấu chấm, được đánh dấu bằng các số. Bên phải, có 1 đồ thị mà các đường cạnh (được đánh dấu bằng các chữ cái) kết nối tối đa 3 nút (các vòng tròn được đánh dấu bằng các số).}, các cạnh trong siêu đồ thị có thể kết nối giữa bất kỳ số lượng nút nào. Độ phức tạp của siêu đồ thị được phản ánh trong dữ liệu kề của nó. Đối với các đồ thị thông thường, kết nối mạng được biểu diễn bằng ma trận kề 2D. Đối với siêu đồ thị, ma trận kề mở rộng đến 1 tenxơ có chiều cao hơn, được gọi là tenxơ {\it incidence tenxơ}. Tenxơ này là tenxơ N chiều, trong đó N là số lượng nút tối đa được kết nối bởi 1 cạnh duy nhất. 1 ví dụ về siêu đồ thị có thể là 1 nền tảng giao tiếp cho phép trò chuyện nhóm cũng như trò chuyện cá nhân. Trong 1 đồ thị thông thường, các cạnh chỉ kết nối 2 người. Trong 1 siêu đồ thị, 1 siêu cạnh có thể kết nối nhiều người, đại diện cho 1 cuộc trò chuyện nhóm.
            \end{itemize}
            \item {\sf1.2.3. Graph-based learning.} Graphs are ubiquitous in our everyday life. {\it Graph-based learning} takes graphs as input data to build models that give insight into questions about this data. Later in this chap, look at different examples of graph data as well as at sort of questions \& tasks we can use graph-based learning to answer.

            -- {\sf Học tập dựa trên đồ thị.} Đồ thị hiện diện khắp nơi trong cuộc sống hàng ngày của chúng ta. {\it Học tập dựa trên đồ thị} sử dụng đồ thị làm dữ liệu đầu vào để xây dựng các mô hình cung cấp thông tin chi tiết về các câu hỏi liên quan đến dữ liệu này. Ở phần sau của chương này, xem xét các ví dụ khác nhau về dữ liệu đồ thị cũng như các loại câu hỏi \& bài tập mà chúng ta có thể sử dụng học tập dựa trên đồ thị để trả lời.

            Graph-based learning uses a variety of ML methods to build {\it representations} of graphs. These representations are then used for downstream tasks e.g. node or link prediction or graph classification. In Chap. 2, learn about 1 of essential tools in graph-based learning, building embeddings. Briefly, embeddings are {\it low-dimensional} vector representations. Can build an embedding of different nodes, edges, or entire graphs, \& there are a number of different ways to do this e.g. Node2Vec (N2V) or DeepWalk algorithms.

            -- Học tập dựa trên đồ thị sử dụng nhiều phương pháp học máy khác nhau để xây dựng {\it biểu diễn} của đồ thị. Các biểu diễn này sau đó được sử dụng cho các tác vụ hạ nguồn, e.g. dự đoán nút hoặc liên kết hoặc phân loại đồ thị. Trong Chương 2, tìm hiểu về 1 trong những công cụ thiết yếu trong học tập dựa trên đồ thị, đó là xây dựng nhúng. Nói 1 cách ngắn gọn, nhúng là biểu diễn vectơ {\it chiều thấp}. Có thể xây dựng nhúng của các nút, cạnh hoặc toàn bộ đồ thị khác nhau, \& có 1 số cách khác nhau để thực hiện việc này, e.g. thuật toán Node2Vec (N2V) hoặc DeepWalk.

            Methods for analysis on graph data have been around for a long time, at least as early as 1950s when {\it clique methods} used certain features of a graph to identify subsets or communities in graph data [4].

            -- Các phương pháp phân tích dữ liệu đồ thị đã có từ rất lâu, ít nhất là từ những năm 1950 khi {\it clique methods} sử dụng 1 số tính năng nhất định của đồ thị để xác định các tập hợp con hoặc cộng đồng trong dữ liệu đồ thị.

            1 of most famous graph-based algorithms is PageRank, which was developed by {\sc Larry Page \& Sergey Brin} in 1996 \& formed basis for Google's search algorithms. Some believe: this algorithm was a key element in company's meteoric rise in following years. This highlights that a successful graph-based learning algorithm can have a huge effect.

            -- 1 trong những thuật toán dựa trên đồ thị nổi tiếng nhất là PageRank, được phát triển bởi Larry Page \& Sergey Brin vào năm 1996 \& tạo nền tảng cho các thuật toán tìm kiếm của Google. 1 số người tin rằng: thuật toán này là yếu tố then chốt cho sự phát triển vượt bậc của công ty trong những năm tiếp theo. Điều này nhấn mạnh rằng 1 thuật toán học dựa trên đồ thị thành công có thể mang lại hiệu quả to lớn.

            These methods are only a small subset of graph-based learning \& analysis techniques. Others include belief propagation [5], graph kernel methods [6], label propagation [7], \& isomaps [8]. However, in this book, focus on 1 of newest \& most exciting additions to family of graph-based learning techniques: GNNs.

            -- Những phương pháp này chỉ là 1 tập hợp con nhỏ của các kỹ thuật học tập dựa trên đồ thị \& phân tích. Các phương pháp khác bao gồm truyền bá niềm tin [5], phương pháp hạt nhân đồ thị [6], truyền bá nhãn [7], \& isomaps [8]. Tuy nhiên, trong cuốn sách này, chúng tôi tập trung vào 1 trong những bổ sung mới nhất \& thú vị nhất cho nhóm kỹ thuật học tập dựa trên đồ thị: GNN.
            \item {\sf1.2.4. What is a GNN?} GNNs combine graph-based learning with DL, i.e., neural networks are used to build embeddings \& process relational data. An overview of inner workings of a GNN is shown in {\sf Fig. 1.7: An overview of how GNNs work. An input graph is passed to a GNN. GNN then uses neural networks to transform graph features e.g. nodes or edges into nonlinear embeddings through a process known as message passing. These embeddings are then tuned to specific unknown properties using training data. After GNN is trained, it can predict unknown features of a graph.}

            -- GNN kết hợp học tập dựa trên đồ thị với DL, i.e., mạng nơ-ron được sử dụng để xây dựng các nhúng \& xử lý dữ liệu quan hệ. Tổng quan về hoạt động bên trong của GNN được thể hiện trong {\sf Hình 1.7: Tổng quan về cách thức hoạt động của GNN. 1 đồ thị đầu vào được truyền đến GNN. Sau đó, GNN sử dụng mạng nơ-ron để chuyển đổi các đặc trưng đồ thị, e.g. các nút hoặc cạnh, thành các nhúng phi tuyến tính thông qua 1 quá trình được gọi là truyền thông điệp. Các nhúng này sau đó được điều chỉnh theo các thuộc tính cụ thể chưa biết bằng cách sử dụng dữ liệu huấn luyện. Sau khi GNN được huấn luyện, nó có thể dự đoán các đặc trưng chưa biết của đồ thị.}

            GNNs allows you to represent \& learn from graphs, including their constituent nodes, edges, \& features. In particular, many methods of GNNs are built specifically to scale effectively with size \& complexity of a graph, i.e., GNNs can operate on huge graphs. In this sense, GNNs provide analogous advantages to relational data as convolutional neural networks have given for image-based data \& computer vision.

            -- GNN cho phép bạn biểu diễn \& học từ các đồ thị, bao gồm các nút, cạnh, \& đặc trưng cấu thành của chúng. Đặc biệt, nhiều phương pháp GNN được xây dựng chuyên biệt để mở rộng hiệu quả theo kích thước \& độ phức tạp của đồ thị, i.e., GNN có thể hoạt động trên các đồ thị rất lớn. Theo nghĩa này, GNN mang lại những lợi thế tương tự cho dữ liệu quan hệ như mạng nơ-ron tích chập đã mang lại cho dữ liệu dựa trên hình ảnh \& thị giác máy tính.

            Historically, applying traditional ML methods to graph data structures has been challenging because graph data, when represented in grid-like formats \& data structures, can lead to massive repetitions of data. To address this, graph-based learning focuses on approaches that are {\it permutation invariant}, i.e., ML method is uninfluenced by ordering of graph representation. In concrete terms, it means: we can shuffle rows \& columns of adjacency matrix without affecting our algorithm's performance. Whenever we are working with data that contains relational data, i.e., has an adjacency matrix, then we want to use a ML method that is permutation invariant to make our method more general \& efficient. Although GNNs can be applied to all graph data, GNNs are especially useful because they can deal with huge graph datasets \& typically perform better than other ML methods.

            -- Theo truyền thống, việc áp dụng các phương pháp ML truyền thống vào cấu trúc dữ liệu đồ thị là 1 thách thức vì dữ liệu đồ thị, khi được biểu diễn ở định dạng dạng lưới \& cấu trúc dữ liệu, có thể dẫn đến sự lặp lại dữ liệu rất lớn. Để giải quyết vấn đề này, học dựa trên đồ thị tập trung vào các phương pháp {\it permutation invariant}, i.e., phương pháp ML không bị ảnh hưởng bởi thứ tự biểu diễn đồ thị. Nói 1 cách cụ thể, điều này có nghĩa là: chúng ta có thể xáo trộn các hàng \& cột của ma trận kề mà không ảnh hưởng đến hiệu suất của thuật toán. Bất cứ khi nào chúng ta làm việc với dữ liệu có chứa dữ liệu quan hệ, i.e., có ma trận kề, thì chúng ta muốn sử dụng phương pháp ML không thay đổi hoán vị để làm cho phương pháp của chúng ta tổng quát hơn \& hiệu quả hơn. Mặc dù GNN có thể được áp dụng cho tất cả dữ liệu đồ thị, nhưng GNN đặc biệt hữu ích vì chúng có thể xử lý các tập dữ liệu đồ thị khổng lồ \& thường hoạt động tốt hơn các phương pháp ML khác.

            Permutation invariances are a type of {\it inductive bias}, or an algorithm's learning bias, \& are powerful tools for designing ML algorithms [1]. Need for permutation-invariant approaches is 1 of central reasons that graph-based learning has increased in popularity in recent years.

            -- Bất biến hoán vị là 1 loại {\it thiên vị quy nạp}, hay thiên vị học tập của thuật toán, \& là những công cụ mạnh mẽ để thiết kế các thuật toán ML [1]. Nhu cầu về các phương pháp bất biến hoán vị là 1 trong những lý do chính khiến học tập dựa trên đồ thị ngày càng phổ biến trong những năm gần đây.

            \fbox{Insights.} Being designed for permutation-invariant data comes with some drawbacks along with its advantages. \fbox{GNNs are not as well suited for other data, e.g. images or tables.} While this might seem obvious, images \& tables are not permutation invariant \& therefore not a good fit for GNNs. If we shuffle rows \& columns of an image, then we scramble input. Instead, ML algorithms for images seek {\it translational invariance}, i.e., we can translate (shift) object in an image, \& it won't affect performance of algorithm. Other neural networks, e.g. convolutional neural networks (CNNs) typically perform much better on images.

            -- Việc được thiết kế cho dữ liệu bất biến hoán vị đi kèm với 1 số nhược điểm bên cạnh những ưu điểm của nó. \fbox{GNN không phù hợp lắm với các dữ liệu khác, e.g. hình ảnh hoặc bảng.} Mặc dù điều này có vẻ hiển nhiên, nhưng hình ảnh \& bảng không bất biến hoán vị \& do đó không phù hợp với GNN. Nếu chúng ta xáo trộn các hàng \& cột của 1 hình ảnh, thì chúng ta sẽ xáo trộn dữ liệu đầu vào. Thay vào đó, các thuật toán ML cho hình ảnh tìm kiếm {\it translational invariance}, i.e., chúng ta có thể dịch chuyển (shift) đối tượng trong 1 hình ảnh, \& điều này sẽ không ảnh hưởng đến hiệu suất của thuật toán. Các mạng nơ-ron khác, e.g. mạng nơ-ron tích chập (CNN) thường hoạt động tốt hơn nhiều trên hình ảnh.
            \item {\sf1.2.5. Differences between tabular \& graph data.} Graph data includes all data with some relational content, making it a powerful way to represent complex connections. While graph data might initially seem distinct from traditional tabular data, many datasets that are typically represented in tables can be recreated as graphs with some data engineering \& imagination. Take a closer look at Titanic dataset, a classic example in ML, \& explore how it can be transformed from a table format to a graph format.

            -- {\sf Sự khác biệt giữa dữ liệu dạng bảng \& dữ liệu đồ thị.} Dữ liệu đồ thị bao gồm tất cả dữ liệu có nội dung quan hệ, khiến nó trở thành 1 phương pháp mạnh mẽ để biểu diễn các kết nối phức tạp. Mặc dù ban đầu dữ liệu đồ thị có vẻ khác biệt so với dữ liệu dạng bảng truyền thống, nhưng nhiều tập dữ liệu thường được biểu diễn dưới dạng bảng có thể được tái tạo dưới dạng đồ thị với 1 chút kỹ thuật dữ liệu \& trí tưởng tượng. Hãy xem xét kỹ hơn tập dữ liệu Titanic, 1 ví dụ kinh điển trong ML, \& khám phá cách nó có thể được chuyển đổi từ định dạng bảng sang định dạng đồ thị.

            Titanic dataset describes passengers on Titanic, a ship that famously met an untimely end when it collided with an iceberg. Historically, this datasets has been analyzed in tabular format, containing rows for each passenger with columns representing features e.g. age, gender, fare, class, \& survival status. However, dataset also contains rich, unexplored relationships that are not immediately visible in a table format {\sf Fig. 1.8: Titanic Dataset is usually displayed \& analyzed using a table format.}

            -- Bộ dữ liệu Titanic mô tả hành khách trên Titanic, 1 con tàu nổi tiếng đã gặp phải cái chết bất ngờ khi va chạm với 1 tảng băng trôi. Trước đây, bộ dữ liệu này được phân tích theo định dạng bảng, bao gồm các hàng cho mỗi hành khách \& các cột biểu diễn các đặc điểm như tuổi, giới tính, giá vé, hạng ghế, \& tình trạng sống sót. Tuy nhiên, bộ dữ liệu cũng chứa các mối quan hệ phong phú, chưa được khám phá mà không thể hiển thị ngay lập tức ở định dạng bảng {\sf Hình 1.8: Bộ dữ liệu Titanic thường được hiển thị \& phân tích bằng định dạng bảng.}
            \begin{itemize}
                \item {\sf Recasting Titanic dataset as a graph.} To transform Titanic dataset into a graph, need to consider how to represent underlying relationships between passengers as nodes \& edges:
                \begin{enumerate}
                    \item Nodes: In graph, each passenger can be represented as a node. Can also introduce nodes for other entities, e.g. cabins, families, or even groups e.g. ``3rd-class passengers''.
                    \item Edges represent relationships or connections between these nodes, e.g.: Passengers who are family members (siblings, spouses, parents, or children) based on available data; Passengers who share a cabin or were traveling together; Social or business relationships that might be inferred from shared ticket numbers, last names, or other identifying features.
                \end{enumerate}
                -- {\sf Tái cấu trúc tập dữ liệu Titanic dưới dạng đồ thị.} Để chuyển đổi tập dữ liệu Titanic thành đồ thị, cần xem xét cách biểu diễn các mối quan hệ cơ bản giữa hành khách dưới dạng các nút \& cạnh:
                \begin{enumerate}
                    \item Nút: Trong đồ thị, mỗi hành khách có thể được biểu diễn dưới dạng 1 nút. Cũng có thể giới thiệu các nút cho các thực thể khác, e.g.: cabin, gia đình hoặc thậm chí các nhóm, e.g.: ``hành khách hạng 3''.
                    \item Cạnh biểu diễn các mối quan hệ hoặc kết nối giữa các nút này, e.g.: Hành khách là thành viên gia đình (anh chị em ruột, vợ/chồng, cha mẹ hoặc con cái) dựa trên dữ liệu có sẵn; Hành khách ở chung cabin hoặc đi cùng nhau; Các mối quan hệ xã hội hoặc kinh doanh có thể được suy ra từ số vé, họ hoặc các đặc điểm nhận dạng chung khác.
                \end{enumerate}
                To construct this graph, need to use existing information in table \& potentially enrich it with secondary data sources or assumptions (e.g., linking last names to create family groups). This process converts tabular data into a graph-based structure, shown in {\sf Fig. 1.9: Titanic dataset, showing family relationships of people on Titanic visualized as a graph. Here, can see that there was a rich social network as well as many passengers with unknown family ties.}, where each edge \& node encapsulates meaningful relational data.

                -- Để xây dựng biểu đồ này, cần sử dụng thông tin hiện có trong bảng \& có thể làm giàu nó bằng các nguồn dữ liệu thứ cấp hoặc giả định (e.g.: liên kết họ để tạo nhóm gia đình). Quá trình này chuyển đổi dữ liệu dạng bảng thành cấu trúc dạng biểu đồ, được hiển thị trong {\sf Hình 1.9: Bộ dữ liệu Titanic, thể hiện mối quan hệ gia đình của những người trên Titanic được trực quan hóa dưới dạng biểu đồ. Ở đây, có thể thấy rằng có 1 mạng lưới xã hội phong phú cũng như nhiều hành khách có mối quan hệ gia đình chưa rõ ràng.}, trong đó mỗi cạnh \& nút đóng gói dữ liệu quan hệ có ý nghĩa.
                \item {\sf How graph data adds depth \& meaning.} Once dataset is represented as a graph, it provides a much deeper view of social \& familial connections between passengers, e.g.,:
                \begin{enumerate}
                    \item {\it Family relationships}: Graph clearly shows how certain passengers were related (e.g., as parents, children, or siblings). This could help us understand survival patterns, as family members might have behaved differently in a crisis than individuals traveling alone.
                    \item {\it Social networks}: Beyond families, graph could reveal broader social networks (e.g., friendships or business connections), which could be important factors in analyzing behavior \& outcomes.
                    \item {\it Community insights}: Graph structure also allows for community detection algorithms to identify clusters of related or connected passengers, which may reveal new insights into survival rates, rescue patterns, or other behaviors.
                \end{enumerate}
                Graph representations add depth by specifying connections that might not be obvious in a tabular format. E.g., understanding who traveled together, who shared a cabin, or who had social or family ties can provide more context on survival rates \& passenger behavior. This is crucial for tasks e.g. node prediction, where we want to predict attributes or outcomes based on relationships represented in graph.

                -- {\sf Cách dữ liệu đồ thị tăng thêm chiều sâu \& ý nghĩa.} Khi tập dữ liệu được biểu diễn dưới dạng đồ thị, nó cung cấp cái nhìn sâu sắc hơn nhiều về các mối quan hệ xã hội \& gia đình giữa các hành khách, e.g.:
                \begin{enumerate}
                    \item {\it Mối quan hệ gia đình}: Đồ thị cho thấy rõ mối quan hệ của 1 số hành khách nhất định (e.g.: cha mẹ, con cái hoặc anh chị em ruột). Điều này có thể giúp chúng ta hiểu được các mô hình sinh tồn, vì các thành viên trong gia đình có thể đã hành xử khác nhau trong khủng hoảng so với những cá nhân đi du lịch 1 mình.
                    \item {\it Mạng xã hội}: Ngoài gia đình, đồ thị có thể tiết lộ các mạng xã hội rộng hơn (e.g.: tình bạn hoặc kết nối kinh doanh), đây có thể là những yếu tố quan trọng trong việc phân tích hành vi \& kết quả.
                    \item {\it Thông tin chi tiết về cộng đồng}: Cấu trúc đồ thị cũng cho phép các thuật toán phát hiện cộng đồng xác định các cụm hành khách có liên quan hoặc kết nối, điều này có thể tiết lộ những hiểu biết mới về tỷ lệ sống sót, mô hình cứu hộ hoặc các hành vi khác.
                \end{enumerate}
                Biểu diễn đồ thị tăng thêm chiều sâu bằng cách chỉ định các kết nối có thể không rõ ràng ở định dạng bảng. E.g., việc hiểu rõ ai đi cùng nhau, ai ở chung cabin, hoặc ai có mối quan hệ xã hội hoặc gia đình có thể cung cấp thêm bối cảnh về tỷ lệ sống sót \& hành vi của hành khách. Điều này rất quan trọng đối với các tác vụ như dự đoán nút, trong đó chúng ta muốn dự đoán các thuộc tính hoặc kết quả dựa trên các mối quan hệ được biểu diễn trên đồ thị.

                By creating an adjacency matrix or defining graph edges \& nodes based on relationships in dataset, can transition from simple data analysis to more sophisticated graph-based learning methods.

                -- Bằng cách tạo ma trận kề hoặc xác định các cạnh \& nút đồ thị dựa trên các mối quan hệ trong tập dữ liệu, có thể chuyển đổi từ phân tích dữ liệu đơn giản sang các phương pháp học dựa trên đồ thị phức tạp hơn.
            \end{itemize}
        \end{itemize}
        \item {\sf1.3. GNN applications: Case studies.} GNNs are neural networks designed to work on relational data. They give new ways for relational data to be transformed \& manipulated, by being easier to scale \& more accurate than previous graph-based learning methods. In following, discuss some exciting applications of GNNs, to see, at a high level, how this class of models are solving real-world problems.

        -- {\sf Ứng dụng GNN: Nghiên cứu điển hình.} GNN là mạng nơ-ron được thiết kế để hoạt động trên dữ liệu quan hệ. Chúng cung cấp những cách thức mới để chuyển đổi \& thao tác dữ liệu quan hệ, nhờ khả năng mở rộng dễ dàng \& chính xác hơn so với các phương pháp học dựa trên đồ thị trước đây. Tiếp theo, thảo luận về 1 số ứng dụng thú vị của GNN, để xem xét ở cấp độ tổng quan, cách lớp mô hình này đang giải quyết các vấn đề thực tế.
        \begin{itemize}
            \item {\sf1.3.1. Recommendation engines.} Enterprise graphs can exceed billions of nodes \& many billions of edges. On other hand, many GNNs are benchmarked on datasets that consist of fewer than a million nodes. When applying GNNs to large graphs, adjustments of training \& inference algorithms \& storage techniques all have to be made. (Can learn more about specifics of scaling GNNs in Chap. 7.)

            -- {\sf Công cụ đề xuất.} Đồ thị doanh nghiệp có thể vượt quá hàng tỷ nút \& hàng tỷ cạnh. Mặt khác, nhiều GNN được đánh giá chuẩn trên các tập dữ liệu có ít hơn 1 triệu nút. Khi áp dụng GNN cho đồ thị lớn, cần phải điều chỉnh các thuật toán huấn luyện \& suy luận \& kỹ thuật lưu trữ. (Bạn có thể tìm hiểu thêm về các chi tiết cụ thể về việc mở rộng GNN trong Chương 7.)

            1 of most well-known industry examples of GNNs is their use as recommendation engines. E.g., Pinterest is a social media platform for finding \& sharing images \& ideas. there are 2 major concepts to Pinterest's users: collections or categories of ideas, called {\it boards}  (like a bulletin board); \& objects a user wants to bookmark called {\it pins}. Pins include images, videos, \& websites URLs. A user board focused on dogs might then include pins of pet photos, puppy videos, or dog-related website links. A board's pins aren't exclusive to it; a pet drawing that was pinned to Dogs board could also be pinned to a Puppies board, as shown in {\sf Fig. 1.10: A bipartite graph that is like Pinterest graph. Nodes in this case are pins \& boards}.

            -- 1 trong những ví dụ nổi tiếng nhất về GNN trong ngành là việc sử dụng chúng làm công cụ đề xuất. E.g., Pinterest là 1 nền tảng truyền thông xã hội để tìm kiếm \& chia sẻ hình ảnh \& ý tưởng. Có 2 khái niệm chính đối với người dùng Pinterest: bộ sưu tập hoặc danh mục ý tưởng, được gọi là {\it boards} (giống như bảng tin); \& đối tượng mà người dùng muốn đánh dấu được gọi là {\it pins}. Ghim bao gồm hình ảnh, video, \& URL trang web. 1 bảng người dùng tập trung vào chó có thể bao gồm các ghim ảnh thú cưng, video về chó con hoặc các liên kết trang web liên quan đến chó. Ghim của 1 bảng không chỉ giới hạn ở đó; 1 bức vẽ thú cưng được ghim vào bảng Chó cũng có thể được ghim vào bảng Chó con, như thể hiện trong {\sf Hình 1.10: Đồ thị hai phần giống như đồ thị Pinterest. Các nút trong trường hợp này là ghim \& boards}.

            1 way to interpret relationships between pins \& boards is as a {\it biparite graph}. For Pinterest graph, all pins are connected to boards, but no pin is connected to another pin, \& no board is connected to another board. Pins \& boards are 2 classes of nodes. Members of these classes can be linked to members of other class, but not to member of same class. Pinterest graph was reported to have 3 billion nodes \& 18 billion edges.

            -- 1 cách để diễn giải mối quan hệ giữa các chân \& bảng là sử dụng đồ thị lưỡng đối. Đối với đồ thị Pinterest, tất cả các chân được kết nối với các bảng, nhưng không có chân nào được kết nối với chân khác, \& không có bảng nào được kết nối với bảng khác. Chân \& bảng là 2 lớp nút. Các thành viên của các lớp này có thể được liên kết với các thành viên của lớp khác, nhưng không thể liên kết với các thành viên của cùng lớp. Đồ thị Pinterest được báo cáo có 3 tỷ nút \& 18 tỷ cạnh.

            PinSage, a graph convolutional network (GCN), was 1 of 1st documented highly scaled GNNs used in an enterprise system [9]. This was used in Pinterest's recommendation systems to overcome past challenges of applying graph-learning models to massive graphs. Compared to baseline methods, tests on this system showed it improved user engagement by 30\%. Specifically, PinSage was used to predict which objects should be recommended to be included in a user's graph. However, GNNs can also be used to predict what an object is, e.g. whether it contains a dog or mountain, based on the rest of nodes in graph \& how they are connected. Do a deep dive on GCNs, of which PinSage is an extension, in Chap. 3.

            -- PinSage, 1 mạng tích chập đồ thị (GCN), là 1 trong những GNN có quy mô lớn đầu tiên được ghi nhận sử dụng trong hệ thống doanh nghiệp [9]. Mạng này được sử dụng trong các hệ thống đề xuất của Pinterest để vượt qua những thách thức trước đây khi áp dụng các mô hình học đồ thị vào các đồ thị lớn. So với các phương pháp cơ sở, các thử nghiệm trên hệ thống này cho thấy nó đã cải thiện mức độ tương tác của người dùng lên 30\%. Cụ thể, PinSage được sử dụng để dự đoán đối tượng nào nên được đề xuất đưa vào đồ thị của người dùng. Tuy nhiên, GNN cũng có thể được sử dụng để dự đoán 1 đối tượng là gì, e.g.: nó chứa 1 con chó hay 1 ngọn núi, dựa trên các nút còn lại trong đồ thị \& cách chúng được kết nối. Hãy tìm hiểu sâu hơn về GCN, trong đó PinSage là 1 phần mở rộng, trong Chương 3.
            \item {\sf1.3.2. Drug discovery \& molecular science.} In chemistry \& molecular sciences, a prominent problem has been representing molecules in a general, application-agnostic way, \& inferring possible interfaces between molecules, e.g. proteins. For molecule representation, can see: drawings of molecules that are common in high school chemistry classes bear resemblance to a graph structure, consisting of nodes (atoms) \& edges (atomic bonds), as shown in {\sf Fig. 1.11: In this molecule, can see individual atoms as nodes \& atomic bonds as edges.}

            -- {\sf Khám phá thuốc \& khoa học phân tử.} Trong hóa học \& khoa học phân tử, 1 vấn đề nổi cộm là biểu diễn phân tử theo cách tổng quát, không phụ thuộc vào ứng dụng, \& suy ra các giao diện khả dĩ giữa các phân tử, e.g. protein. Về biểu diễn phân tử, có thể thấy: các hình vẽ phân tử thường thấy trong các lớp hóa học trung học phổ thông có cấu trúc đồ thị, bao gồm các nút (nguyên tử) \& các cạnh (liên kết nguyên tử), như thể hiện trong {\sf Hình 1.11: Trong phân tử này, có thể thấy các nguyên tử riêng lẻ là các nút \& các liên kết nguyên tử là các cạnh.}

            Applying GNNs to these structures can, in certain circumstances, outperform traditional ``fingerprint'' methods for determining properties of a molecule. These traditional methods involve creation of features by domain experts to capture a molecule's properties, e.g. interpreting presence or absence of certain molecules or atoms [10]. GNNs learn new data-driven features that can be used to group certain molecules together in new \& unexpected ways or even to propose new molecules for synthesis. This is extremely important for predicting whether a chemical is toxic or safe for use or whether it has some downstream effects that can affect disease progression. Therefore, GNNs have shown themselves to be incredibly useful in field of drug discovery.

            -- Việc áp dụng GNN vào các cấu trúc này, trong 1 số trường hợp, có thể vượt trội hơn các phương pháp ``dấu vân tay'' truyền thống để xác định các đặc tính của 1 phân tử. Các phương pháp truyền thống này liên quan đến việc tạo ra các đặc điểm bởi các chuyên gia trong lĩnh vực để nắm bắt các đặc tính của 1 phân tử, e.g.: diễn giải sự hiện diện hoặc vắng mặt của 1 số phân tử hoặc nguyên tử nhất định [10]. GNN học các đặc điểm mới dựa trên dữ liệu, có thể được sử dụng để nhóm các phân tử nhất định lại với nhau theo những cách mới \& bất ngờ, hoặc thậm chí đề xuất các phân tử mới để tổng hợp. Điều này cực kỳ quan trọng để dự đoán liệu 1 hóa chất có độc hại hay an toàn để sử dụng hay liệu nó có 1 số tác động hạ lưu có thể ảnh hưởng đến sự tiến triển của bệnh hay không. Do đó, GNN đã chứng tỏ mình cực kỳ hữu ích trong lĩnh vực khám phá thuốc.

            Drug discovery, especially for GNNs, can be understood as a graph prediction problem. {\it Graph prediction} tasks are those that require learning \& predicting properties about entire graph. For drug discovery, aim: predict properties e.g. toxicity or treatment effectiveness (discriminative) or to suggest entirely new graphs that should be synthesized \& tested  (generative). To suggest these new graphs, drug discovery methods often combine GNNs with other generative models e.g. variational graph autoencoders (VGAEs), as shown, e.g., in {\sf Fig. 1.12: A GNN system used to predict new molecules [11]. Workflow here starts on left with a representation of a molecule as a graph. In middle parts of figure, this graph representation is transformed via a GNN into a latent representation. Latent representation is then transformed back to molecule to ensure: latent space can be decoded (right).} Describe VGAEs in more detail in Chap. 5 \& show how we can use these to predict molecules.

            -- Khám phá thuốc, đặc biệt đối với GNN, có thể được hiểu là 1 bài toán dự đoán đồ thị. Nhiệm vụ {\it Dự đoán đồ thị} là những nhiệm vụ yêu cầu học \& dự đoán các thuộc tính về toàn bộ đồ thị. Đối với khám phá thuốc, mục tiêu: dự đoán các thuộc tính, e.g. độc tính hoặc hiệu quả điều trị (phân biệt) hoặc đề xuất các đồ thị hoàn toàn mới cần được tổng hợp \& thử nghiệm (sinh). Để đề xuất các đồ thị mới này, các phương pháp khám phá thuốc thường kết hợp GNN với các mô hình sinh khác, e.g. bộ mã hóa tự động đồ thị biến phân (VGAE), như được hiển thị, e.g., trong {\sf Hình 1.12: Hệ thống GNN được sử dụng để dự đoán các phân tử mới [11]. Quy trình làm việc ở đây bắt đầu ở bên trái với biểu diễn của 1 phân tử dưới dạng đồ thị. Ở phần giữa của hình, biểu diễn đồ thị này được chuyển đổi thông qua GNN thành biểu diễn tiềm ẩn. Biểu diễn tiềm ẩn sau đó được chuyển đổi trở lại thành phân tử để đảm bảo: không gian tiềm ẩn có thể được giải mã (bên phải).} Mô tả VGAE chi tiết hơn trong Chương 5 \& chỉ ra cách chúng ta có thể sử dụng chúng để dự đoán các phân tử.
            \item {\sf1.3.3. Mechanical reasoning.} Develop rudimentary intuition about mechanics \& physics of world around us at a remarkably young age \& without any formal training in subject. Do not need to write down a set of equations to know how to catch a bouncing ball. Do not even have to be in presence of a physical ball. Given a series of snapshots of a bouncing ball, can predict reasonably well where ball is going to end up.

            -- {\sf Lý luận cơ học.} Phát triển trực giác cơ bản về cơ học \& vật lý của thế giới xung quanh chúng ta ngay từ khi còn rất nhỏ \& mà không cần bất kỳ sự đào tạo chính thức nào về môn học này. Không cần phải viết ra 1 loạt phương trình để biết cách bắt 1 quả bóng đang nảy. Thậm chí không cần phải ở gần 1 quả bóng thực tế. Chỉ cần 1 loạt ảnh chụp nhanh về 1 quả bóng đang nảy, bạn có thể dự đoán khá chính xác vị trí quả bóng sẽ rơi.

            While these problems might seem trivial for us, they are critical for many physical industries, including manufacturing \& autonomous driving. E.g., autonomous driving systems need to anticipate what will happen in a traffic scene consisting of many moving objects. Until recently, this task was typically treated as a problem of computer vision. However, more recent approaches have begun to use GNNs [12]. These GNN-based methods demonstrate: including relational information, e.g. how limbs are connected, can enable algorithms to develop physical intuition about how a person or animal moves with higher accuracy \& less data.

            -- Mặc dù những vấn đề này có vẻ tầm thường đối với chúng ta, nhưng chúng lại rất quan trọng đối với nhiều ngành công nghiệp vật lý, bao gồm sản xuất \& lái xe tự động. E.g., hệ thống lái xe tự động cần dự đoán những gì sẽ xảy ra trong 1 cảnh giao thông gồm nhiều vật thể chuyển động. Cho đến gần đây, nhiệm vụ này thường được coi là 1 vấn đề của thị giác máy tính. Tuy nhiên, các phương pháp tiếp cận gần đây hơn đã bắt đầu sử dụng GNN [12]. Các phương pháp dựa trên GNN này chứng minh: việc bao gồm thông tin quan hệ, e.g. cách các chi được kết nối, có thể cho phép các thuật toán phát triển trực giác vật lý về cách 1 người hoặc động vật di chuyển với độ chính xác cao hơn \& ít dữ liệu hơn.

            In {\sf Fig. 1.13: A graph representation of a mechanical body, taken from Sanchez-Gonzalez [13]. Body's segments are represented as nodes, \& mechanical forces binding them are edges.}, give an example of how a body can be thought of as a ``mechanical'' graph. Input graphs for these physical reasoning systems have elements that reflect problem. E.g., when reasoning about a human or animal body, a graph could consist of nodes that represent points on body where limbs connect. For systems of free bodies, nodes of a graph could be individual objects e.g. bouncing balls. Edges of graph then represent physical relationship (e.g., gravitational forces, elastic springs, or rigid connections) between nodes. Given these inputs, GNNs learn to predict future states of a set of objects without explicitly calling on physical{\tt/}mechanical laws [13]. These methods are a form of {\it edge prediction}, i.e., they predict how nodes connect over time. Furthermore, these models have to be dynamic to account for temporal evolution of system. Consider these problems in detail in Chap. 6.

            -- Trong {\sf Hình 1.13: Biểu diễn đồ thị của 1 vật thể cơ học, lấy từ Sanchez-Gonzalez [13]. Các đoạn của vật thể được biểu diễn là các nút, \& lực cơ học liên kết chúng là các cạnh.}, đưa ra 1 ví dụ về cách 1 vật thể có thể được coi là 1 đồ thị ``cơ học''. Đồ thị đầu vào cho các hệ thống suy luận vật lý này có các thành phần phản ánh vấn đề. Ví dụ: khi suy luận về cơ thể người hoặc động vật, đồ thị có thể bao gồm các nút biểu diễn các điểm trên cơ thể nơi các chi kết nối. Đối với các hệ thống vật thể tự do, các nút của đồ thị có thể là các vật thể riêng lẻ, e.g.: quả bóng nảy. Các cạnh của đồ thị sau đó biểu diễn mối quan hệ vật lý (e.g.: lực hấp dẫn, lò xo đàn hồi hoặc kết nối cứng) giữa các nút. Với các đầu vào này, GNN học cách dự đoán trạng thái tương lai của 1 tập hợp các vật thể mà không cần gọi rõ ràng các định luật cơ học vật lý [13]. Các phương pháp này là 1 dạng {\it dự đoán cạnh}, i.e., chúng dự đoán cách các nút kết nối theo thời gian. Hơn nữa, các mô hình này phải mang tính động để tính đến sự tiến hóa theo thời gian của hệ thống. Hãy xem xét chi tiết những vấn đề này trong Chương 6.
        \end{itemize}
        \item {\sf1.4. When to use a GNN?} Have explored real-world applications of GNNs, identify some underlying characteristics that make problems suitable for graph-based solutions. While cases of previous section clearly involved data that was naturally modeled as a graph, crucial to recognize that GNNs can also be effectively applied to problems where graph-like nature may not be immediately obvious.

        -- {\sf Khi nào nên sử dụng GNN?} Đã khám phá các ứng dụng thực tế của GNN, xác định 1 số đặc điểm cơ bản giúp bài toán phù hợp với các giải pháp dựa trên đồ thị. Mặc dù các trường hợp trong phần trước rõ ràng liên quan đến dữ liệu được mô hình hóa tự nhiên dưới dạng đồ thị, nhưng điều quan trọng là phải nhận ra rằng GNN cũng có thể được áp dụng hiệu quả cho các bài toán mà bản chất giống đồ thị có thể không rõ ràng ngay lập tức.

        So, instead of simply stating GNNs are useful for graph problems, this section will help you recognize patterns \& relationships within your data that could benefit from graph-based modeling, even if those relationships aren't immediately apparent. Essentially, there are 3 types of criteria for identifying GNN problems: implicit relationships \& interdependencies; high dimensionality \& sparsity; \& complex nonlocal interactions.

        -- Vì vậy, thay vì chỉ đơn thuần nêu rằng GNN hữu ích cho các bài toán đồ thị, phần này sẽ giúp bạn nhận ra các mẫu \& mối quan hệ trong dữ liệu của mình mà mô hình dựa trên đồ thị có thể mang lại lợi ích, ngay cả khi những mối quan hệ đó không rõ ràng ngay lập tức. Về cơ bản, có 3 loại tiêu chí để xác định các bài toán GNN: mối quan hệ ngầm \& phụ thuộc lẫn nhau; tính đa chiều cao \& thưa thớt; \& tương tác phi cục bộ phức tạp.
        \begin{itemize}
            \item {\sf1.4.1. Implicit relationships \& interdependencies.} Graphs are versatile data structures that can model a wide range of relationships. Even when a problem doesn't initially appear to be graph-like, even if your dataset is tabular, it is beneficial to explore whether implicit relationships or interdependencies might exist that could be represented explicitly. Implicit relationships are connections that are not immediately documented or obvious within data but can still play a significant role in understanding underlying patterns \& behaviors.

            -- {\sf Mối quan hệ ngầm \& sự phụ thuộc lẫn nhau.} Đồ thị là cấu trúc dữ liệu đa năng có thể mô hình hóa 1 loạt các mối quan hệ. Ngay cả khi 1 vấn đề ban đầu có vẻ không giống đồ thị, ngay cả khi tập dữ liệu của bạn ở dạng bảng, việc khám phá xem liệu có tồn tại các mối quan hệ ngầm hoặc sự phụ thuộc lẫn nhau có thể được biểu diễn 1 cách rõ ràng hay không vẫn rất hữu ích. Mối quan hệ ngầm là những kết nối không được ghi chép ngay lập tức hoặc hiển nhiên trong dữ liệu nhưng vẫn có thể đóng 1 vai trò quan trọng trong việc hiểu các mô hình \& hành vi cơ bản.

            {\bf Key indicators.} To determine if your problem might benefit from modeling implicit relationships with graphs, consider whether there are hidden or indirect connections between entities in your dataset. E.g., in customer behavior analysis, customers may appear as independent entities in a tabular dataset containing their purchases, demographics, \& other details. However, they could be connected through social media influence, peer recommendations, or shared purchasing patterns, forming an underlying network of interactions.

            -- {\bf Các chỉ số chính.} Để xác định xem vấn đề của bạn có thể được hưởng lợi từ việc mô hình hóa các mối quan hệ ngầm định bằng biểu đồ hay không, xem xét liệu có các kết nối ẩn hoặc gián tiếp giữa các thực thể trong tập dữ liệu của bạn hay không. Ví dụ: trong phân tích hành vi khách hàng, khách hàng có thể xuất hiện dưới dạng các thực thể độc lập trong 1 tập dữ liệu dạng bảng chứa thông tin mua hàng, nhân khẩu học \& các chi tiết khác của họ. Tuy nhiên, họ có thể được kết nối thông qua ảnh hưởng trên mạng xã hội, khuyến nghị của đồng nghiệp hoặc các mô hình mua sắm chung, tạo thành 1 mạng lưới tương tác cơ bản.

            Another indicator is presence of entities that share common attributes or activities without a direct or documented relationship. In case of investors, e.g., 2 or more investors may not have any formal connection but might frequently co-invest in same companies under similar conditions. Such patterns of co-investment could indicate a shared strategy or influence. In this scenario, a graph representation can be created where nodes represent individual investors, \& edges are formed between nodes when 2 or more investors co-invest in the same company. Additional attributes, e.g., investment size, timing, or types of companies invested in can be added to nodes or edges, allowing GNNs to identify patterns, trends, or even potential collaboration opportunities.

            -- 1 chỉ báo khác là sự hiện diện của các thực thể có chung thuộc tính hoặc hoạt động mà không có mối quan hệ trực tiếp hoặc được ghi chép lại. E.g., trong trường hợp nhà đầu tư, 2 hoặc nhiều nhà đầu tư có thể không có bất kỳ mối liên hệ chính thức nào nhưng thường xuyên cùng đầu tư vào cùng 1 công ty trong các điều kiện tương tự. Các mô hình đồng đầu tư như vậy có thể chỉ ra 1 chiến lược hoặc ảnh hưởng chung. Trong trường hợp này, có thể tạo biểu đồ biểu diễn, trong đó các nút đại diện cho các nhà đầu tư cá nhân, \& các cạnh được hình thành giữa các nút khi 2 hoặc nhiều nhà đầu tư cùng đầu tư vào cùng 1 công ty. Các thuộc tính bổ sung, e.g.: quy mô đầu tư, thời điểm đầu tư hoặc loại hình công ty được đầu tư, có thể được thêm vào các nút hoặc cạnh, cho phép GNN xác định các mô hình, xu hướng hoặc thậm chí các cơ hội hợp tác tiềm năng.

            Additionally, consider whether data involves entities that are interconnected through shared references or co-occurrence patterns. Document \& text data may not immediately suggest a graph structure, but if documents cite each other or share common topics or authors, they can be represented as nodes in a graph, with edges reflecting these relationships. Similarly, terms within documents can form co-occurrence networks, which are useful for tasks e.g. keyword extraction, document classification, or topic modeling.

            -- Ngoài ra, cân nhắc xem dữ liệu có bao gồm các thực thể được kết nối với nhau thông qua các tham chiếu chung hay các mẫu đồng hiện diện hay không. Dữ liệu tài liệu \& văn bản có thể không gợi ý ngay lập tức 1 cấu trúc đồ thị, nhưng nếu các tài liệu trích dẫn lẫn nhau hoặc có chung chủ đề hoặc tác giả, chúng có thể được biểu diễn dưới dạng các nút trong đồ thị, với các cạnh phản ánh các mối quan hệ này. Tương tự, các thuật ngữ trong tài liệu có thể tạo thành các mạng đồng hiện diện, hữu ích cho các tác vụ như trích xuất từ khóa, phân loại tài liệu hoặc mô hình hóa chủ đề.

            By identifying these key indicators in your data, you can uncover hidden or implicit relationships that can be represented explicitly through graphs. Such representations allow for more advanced analyses using GNNs, which can effectively capture \& model these relationships, leading to more accurate predictions \& deeper insights into data.

            -- Bằng cách xác định các chỉ số chính này trong dữ liệu, bạn có thể khám phá các mối quan hệ ẩn hoặc ngầm định có thể được biểu diễn rõ ràng thông qua biểu đồ. Các biểu diễn như vậy cho phép phân tích nâng cao hơn bằng cách sử dụng GNN, có thể nắm bắt hiệu quả \& mô hình hóa các mối quan hệ này, dẫn đến dự đoán chính xác hơn \& hiểu biết sâu sắc hơn về dữ liệu.
            \item {\sf1.4.2. High dimensionality \& sparsity.} Graph-based models are particularly effective in handling high-dimensional data where many features may be sparse or missing. These models excel in situations where there are underlying structure connecting sparse entities, allowing for more meaningful analysis \& improved performance.

            -- Các mô hình dựa trên đồ thị đặc biệt hiệu quả trong việc xử lý dữ liệu đa chiều, trong đó nhiều đặc điểm có thể thưa thớt hoặc bị thiếu. Các mô hình này đặc biệt hiệu quả trong các tình huống có cấu trúc cơ bản kết nối các thực thể thưa thớt, cho phép phân tích có ý nghĩa hơn \& cải thiện hiệu suất.

            {\bf Key indicators.} To determine if your problem involves high-dimensional \& sparse data suitable for GNNs, consider whether your dataset contains numerous entities with limited direct interactions or relationships. E.g., in recommender systems, user-item interaction data may appear tabular, but it is inherently sparse -- most users only interact with a small subset of available items. By representing users \& items as nodes \& representing their interactions (e.g., purchases or clicks) as edges, GNNs can exploit network effects to make more accurate recommendations. These models can also address cold-start problem by uncovering both explicit \& implicit relationships, leading to better performance in recommending new items to users or engaging new users with existing items.

            -- {\bf Các chỉ số chính.} Để xác định xem vấn đề của bạn có liên quan đến dữ liệu đa chiều \& thưa thớt phù hợp với GNN hay không, xem xét liệu tập dữ liệu của bạn có chứa nhiều thực thể với các tương tác hoặc mối quan hệ trực tiếp hạn chế hay không. Ví dụ: trong các hệ thống đề xuất, dữ liệu tương tác giữa người dùng \& sản phẩm có thể xuất hiện dưới dạng bảng, nhưng bản chất của nó là thưa thớt -- hầu hết người dùng chỉ tương tác với 1 tập hợp con nhỏ các sản phẩm khả dụng. Bằng cách biểu diễn người dùng \& sản phẩm dưới dạng các nút \& biểu diễn các tương tác của họ (e.g.: mua hàng hoặc nhấp chuột) dưới dạng các cạnh, GNN có thể khai thác hiệu ứng mạng để đưa ra các đề xuất chính xác hơn. Các mô hình này cũng có thể giải quyết vấn đề khởi động nguội bằng cách khám phá cả các mối quan hệ rõ ràng \& ngầm định, dẫn đến hiệu suất tốt hơn trong việc đề xuất sản phẩm mới cho người dùng hoặc thu hút người dùng mới sử dụng các sản phẩm hiện có.

            Another indicator that your problem may be suitable for graph-based models is when data represents entities that are sparsely connected but share significant characteristics. In drug discovery, e.g., molecules are represented as graphs, with atoms as nodes \& chemical bonds as edges. This representation captures inherent sparsity of molecular structures, where most atoms form only a few bonds, \& large portions of molecule may be distant from each other in graph. Traditional ML methods often struggle to predict properties of new molecules due to this sparsity, as they don't account for full structural context.

            -- 1 dấu hiệu khác cho thấy vấn đề của bạn có thể phù hợp với các mô hình dựa trên đồ thị là khi dữ liệu biểu diễn các thực thể được kết nối thưa thớt nhưng có chung các đặc điểm quan trọng. E.g., trong khám phá thuốc, các phân tử được biểu diễn dưới dạng đồ thị, với các nguyên tử là nút \& các liên kết hóa học là cạnh. Cách biểu diễn này nắm bắt được tính thưa thớt vốn có của các cấu trúc phân tử, trong đó hầu hết các nguyên tử chỉ tạo thành 1 vài liên kết, \& các phần lớn phân tử có thể nằm cách xa nhau trên đồ thị. Các phương pháp ML truyền thống thường gặp khó khăn trong việc dự đoán các đặc tính của các phân tử mới do tính thưa thớt này, vì chúng không tính đến toàn bộ bối cảnh cấu trúc.

            Graph-based models, particularly GNNs, overcome these challenges by capturing both local atomic environments \& global molecular structures. GNNs learn hierarchical features from fine-grained atomic interactions to broader molecular properties, \& their ability to remain invariant to ordering of atoms ensures consistent predictions. By using graph structure of molecules, GNNs make accurate predictions from sparse, connected data, thereby accelerating drug discovery process.

            -- Các mô hình dựa trên đồ thị, đặc biệt là GNN, khắc phục những thách thức này bằng cách nắm bắt cả môi trường nguyên tử cục bộ \& cấu trúc phân tử toàn cục. GNN học các đặc điểm phân cấp từ các tương tác nguyên tử chi tiết đến các đặc tính phân tử rộng hơn, \& khả năng duy trì tính bất biến theo thứ tự nguyên tử đảm bảo các dự đoán nhất quán. Bằng cách sử dụng cấu trúc đồ thị của phân tử, GNN đưa ra các dự đoán chính xác từ dữ liệu thưa thớt \& kết nối, do đó đẩy nhanh quá trình khám phá thuốc.

            By recognizing these key indicators in your data, you can identify situations where graph-based models can effectively handle high-dimensional \& sparse datasets. Representing such data as graphs allows GNNs to capture \& use underlying structures, resulting in more accurate predictions \& deeper insights across various applications.

            -- Bằng cách nhận diện các chỉ số chính này trong dữ liệu, bạn có thể xác định các tình huống mà mô hình dựa trên đồ thị có thể xử lý hiệu quả các tập dữ liệu đa chiều \& thưa thớt. Việc biểu diễn dữ liệu dưới dạng đồ thị cho phép GNN nắm bắt \& sử dụng các cấu trúc cơ bản, mang lại dự đoán chính xác hơn \& hiểu biết sâu sắc hơn trên nhiều ứng dụng khác nhau.
            \item {\sf1.4.3. Complex, nonlocal interactions.} Certain problems require underlying how distant elements in a dataset influence each other. In these cases, GNNs provide a framework to capture these complex interactions, where predicted value or label of a particular data point depends not just on features of its immediate neighbors but also on those of other related data points. This capability is especially useful when relationships extend beyond direct connections to involve multiple levels or degrees of separation.

            -- 1 số vấn đề đòi hỏi phải hiểu rõ cách các phần tử ở xa trong 1 tập dữ liệu ảnh hưởng lẫn nhau. Trong những trường hợp này, mạng nơ-ron nhân tạo (GNN) cung cấp 1 khuôn khổ để nắm bắt những tương tác phức tạp này, trong đó giá trị dự đoán hoặc nhãn của 1 điểm dữ liệu cụ thể không chỉ phụ thuộc vào các đặc điểm của các điểm lân cận mà còn phụ thuộc vào các đặc điểm của các điểm dữ liệu liên quan khác. Khả năng này đặc biệt hữu ích khi các mối quan hệ vượt ra ngoài các kết nối trực tiếp, bao gồm nhiều cấp độ hoặc mức độ phân tách.

            However, some standard GNNs, which rely primarily on local message passing, may struggle to capture long-range dependencies effectively. Advanced architectures or modifications, e.g. those incorporating global attention, nonlocal aggregation, or hierarchical message-passing, can be better address these challenges [14].

            {\bf Key indicators.} To determine if your problem involves complex, nonlocal interactions suitable for GNNs, consider whether outcome or behavior of 1 entity depends on attributes or actions of identities that are not directly connected to it but may be indirectly connected through other entities. E.g., in supply chain optimization, a delay in 1 supplier may not only affect its immediate downstream customers but could cascade through multiple levels of network, influencing distributors \& final consumers.

            -- {\bf Các chỉ số chính.} Để xác định xem vấn đề của bạn có liên quan đến các tương tác phức tạp, phi cục bộ phù hợp với GNN hay không, xem xét liệu kết quả hoặc hành vi của 1 thực thể có phụ thuộc vào các thuộc tính hoặc hành động của các danh tính không được kết nối trực tiếp với nó nhưng có thể được kết nối gián tiếp thông qua các thực thể khác hay không. E.g., trong tối ưu hóa chuỗi cung ứng, sự chậm trễ của 1 nhà cung cấp không chỉ ảnh hưởng đến các khách hàng hạ nguồn trực tiếp mà còn có thể lan truyền qua nhiều cấp độ mạng lưới, ảnh hưởng đến các nhà phân phối \& người tiêu dùng cuối cùng.

            Another indicator is whether problem involves scenarios where information, influence, or effects propagate through a network over time. In healthcare \& epidemiology, e.g., a disease outbreak might spread from a small cluster of patients through their interactions with shared healthcare providers, common environments, or overlapping social networks. Such propagation requires an approach that captures indirect transmission pathways of information or effects.

            -- 1 chỉ báo khác là liệu vấn đề có liên quan đến các kịch bản mà thông tin, ảnh hưởng hoặc tác động lan truyền qua mạng lưới theo thời gian hay không. Trong chăm sóc sức khỏe \& dịch tễ học, e.g., 1 đợt bùng phát dịch bệnh có thể lây lan từ 1 nhóm nhỏ bệnh nhân thông qua tương tác của họ với các nhà cung cấp dịch vụ chăm sóc sức khỏe chung, môi trường chung hoặc các mạng lưới xã hội chồng chéo. Sự lan truyền như vậy đòi hỏi 1 phương pháp tiếp cận nắm bắt các con đường truyền thông tin hoặc tác động gián tiếp.

            To close this section, in determining whether your problem is a good candidate for a GNN, ask yourself these questions:
            \begin{enumerate}
                \item Are there implicit relationships or interdependencies in my data that I could model?
                \item Do interactions between entities exhibit complex, nonlocal dependencies that go beyond immediate connections?
                \item Is data high-dimensional \& sparse, with a need to capture underlying relational structures?
            \end{enumerate}
            If answer to any of these questions is yes, consider framing your problem as a graph \& applying GNNs to unlock new insights \& predictive capabilities.

            -- Để kết thúc phần này, khi xác định xem vấn đề của bạn có phù hợp để áp dụng mô hình mạng nơ-ron nhân tạo (GNN) hay không, tự hỏi mình những câu hỏi sau:
            \begin{enumerate}
                \item Liệu có mối quan hệ ngầm định hoặc phụ thuộc lẫn nhau nào trong dữ liệu của tôi mà tôi có thể mô hình hóa không?
                \item Liệu các tương tác giữa các thực thể có biểu hiện sự phụ thuộc phức tạp, phi cục bộ, vượt ra ngoài các kết nối tức thời không?
                \item Liệu dữ liệu có đa chiều \& thưa thớt, cần phải nắm bắt các cấu trúc quan hệ cơ bản không?
            \end{enumerate}
            Nếu câu trả lời cho bất kỳ câu hỏi nào trong số này là có, cân nhắc việc định hình vấn đề của bạn dưới dạng biểu đồ \& áp dụng GNN để khám phá những hiểu biết mới \& khả năng dự đoán.
        \end{itemize}
        \item {\sf1.5. Understanding how GNNs operate.} In this section, explore how GNNs work, starting from initial collection of raw data to final deployment of trained models. Examine each step, highlighting processes of data handling, model building, \& unique message-passing technique that sets GNNs apart from traditional DL models.
        \begin{itemize}
            \item {\sf1.5.1. Mental model for training a GNN.} Our mental model covers data sourcing, graph representation, preprocessing, \& model development workflow. Start with raw data \& end up with a trained GNN model \& its outputs. {\sf Fig. 1.14: Mental model of GNN project. Start with raw data, which is transformed into a graph data model that can be stored in a graph database or used in a graph processing system. From graph processing system (\& some graph databases), exploratory data analysis \& visualization can be done. Finally, for graph ML, data is preprocessed into a form that can be submitted for training} illustrates \& visualizes topics related to these stages, annotated with chaps in which these topics appear.

            -- {\sf Mô hình tinh thần để huấn luyện GNN.} Mô hình tinh thần của chúng tôi bao gồm việc tìm nguồn dữ liệu, biểu diễn đồ thị, tiền xử lý \& quy trình phát triển mô hình. Bắt đầu với dữ liệu thô \& kết thúc bằng 1 mô hình GNN đã được huấn luyện \& kết quả của nó. {\sf Hình 1.14: Mô hình tinh thần của dự án GNN. Bắt đầu với dữ liệu thô, được chuyển đổi thành mô hình dữ liệu đồ thị có thể được lưu trữ trong cơ sở dữ liệu đồ thị hoặc được sử dụng trong hệ thống xử lý đồ thị. Từ hệ thống xử lý đồ thị (và 1 số cơ sở dữ liệu đồ thị), có thể thực hiện phân tích dữ liệu thăm dò \& trực quan hóa. Cuối cùng, đối với học máy đồ thị, dữ liệu được tiền xử lý thành 1 biểu mẫu có thể được gửi để huấn luyện} minh họa \& trực quan hóa các chủ đề liên quan đến các giai đoạn này, được chú thích bằng các chương trong đó các chủ đề này xuất hiện.

            While not all workflows include every step or stage of this process, most will incorporate at least some elements. At different stages of a model development project, different parts of this process will typically be used. E.g., when {\it training} a model, data analysis \& visualization may be needed to make design decisions, but when {\it deploying} a model, it may only be necessary to stream raw data \& quickly preprocess it for ingestion into a model. Though this book touches on earlier stages in this mental model, bulk of book is focused on how to train different types of GNNs. When other topics are discussed, they serve to support this main focus.

            -- Mặc dù không phải tất cả quy trình làm việc đều bao gồm mọi bước hoặc giai đoạn của quy trình này, nhưng hầu hết đều sẽ kết hợp ít nhất 1 số yếu tố. Ở các giai đoạn khác nhau của 1 dự án phát triển mô hình, các phần khác nhau của quy trình này thường sẽ được sử dụng. Ví dụ: khi {\it training} 1 mô hình, phân tích dữ liệu \& trực quan hóa có thể cần thiết để đưa ra quyết định thiết kế, nhưng khi {\it deployment} 1 mô hình, có thể chỉ cần truyền dữ liệu thô \& xử lý nhanh dữ liệu trước để đưa vào mô hình. Mặc dù cuốn sách này đề cập đến các giai đoạn trước đó của mô hình tư duy này, phần lớn nội dung sách tập trung vào cách huấn luyện các loại GNN khác nhau. Khi các chủ đề khác được thảo luận, chúng sẽ hỗ trợ cho trọng tâm chính này.

            Mental model shows core tasks of applying GNNs to ML problems, \& we return to this process repeatedly through rest of book. Examine this diagram from end to end.

            -- Mô hình tinh thần cho thấy các nhiệm vụ cốt lõi của việc áp dụng mạng nơ-ron nhân tạo (GNN) vào các bài toán ML, \& chúng ta sẽ quay lại quá trình này nhiều lần trong suốt phần còn lại của cuốn sách. Hãy xem xét sơ đồ này từ đầu đến cuối.

            1st step in training a GNN in structuring this raw data into a graph format, if it is not already. This requires deciding which entities in data to represent as nodes \& edges, as well as determining features to assign to them. Decision must also be made about data storage -- whether to use a graph database, processing system, or other formats.

            -- Bước đầu tiên trong quá trình huấn luyện GNN là cấu trúc dữ liệu thô này thành định dạng đồ thị, nếu dữ liệu chưa được định dạng. Điều này đòi hỏi phải quyết định những thực thể nào trong dữ liệu sẽ được biểu diễn dưới dạng nút \& cạnh, cũng như xác định các đặc điểm cần gán cho chúng. Quyết định cũng cần được đưa ra về lưu trữ dữ liệu -- sử dụng cơ sở dữ liệu đồ thị, hệ thống xử lý hay các định dạng khác.

            For ML, data must be preprocessed for training \& inference, involving tasks e.g. sampling, batching, \& splitting data into training, validation, \& test sets. Throughout this book, use PyTorch Geometric (PyG), which offers specialized classes for preprocessing \& data splitting while preserving graph's structure. Preprocessing is covered in most chaps, with more-in-depth explanations available in Appendix B.

            -- Đối với ML, dữ liệu phải được xử lý trước để huấn luyện \& suy luận, bao gồm các tác vụ như lấy mẫu, xử lý hàng loạt, \& chia dữ liệu thành các tập huấn luyện, xác thực, \& kiểm tra. Trong suốt cuốn sách này, sử dụng PyTorch Geometric (PyG), cung cấp các lớp chuyên biệt để xử lý trước \& chia tách dữ liệu trong khi vẫn bảo toàn cấu trúc đồ thị. Phần lớn các chương đều đề cập đến tiền xử lý, với các giải thích chi tiết hơn có sẵn trong Phụ lục B.

            After processing data, can then move on to model training. In this book, cover several architectures \& training types:
            \begin{itemize}
                \item Chaps. 2--3 discuss convolutional GNNs, where 1st use a GCN layer to produce graph embeddings (Chap. 2) \& then train a full GCN \& GraphSAGE models (Chap. 3).
                \item Chap. 4 explains graph attention networks (GATs), which adds attention to our GNNs.
                \item Chap. 5 introduces GNNs for unsupervised \& generative problems, where we train \& use a variational graph autoencoder (VGAE).
                \item Chap. 6 then explores advanced concept of spatiotemporal GNNs, based on graphs that evolve over time. Train a neural relational inference (NRI) model, which combines an autoencoder structure with a RNN.

                Most of examples provided for GNNs mentioned so far are illustrated with code examples which use small-scale graphs that can fit into memory on a laptop or desktop computer.
                \item In Chap. 7, delve into strategies for handling data that exceeds processing capacity of a single machine.
                \item In Chap. 8, close with some considerations for graph \& GNN projects, e.g. practical aspects of working with graph data, as well as how to convert nongraph data into a graph format.
            \end{itemize}
            -- Sau khi xử lý dữ liệu, có thể chuyển sang huấn luyện mô hình. Trong cuốn sách này, chúng tôi sẽ đề cập đến 1 số kiến trúc \& loại huấn luyện:
            \begin{itemize}
                \item Chương 2-3 thảo luận về mạng GNN tích chập, trong đó đầu tiên sử dụng 1 lớp GCN để tạo nhúng đồ thị (Chương 2) \& sau đó huấn luyện 1 mô hình GCN đầy đủ \& GraphSAGE (Chương 3).
                \item Chương 4 giải thích về mạng chú ý đồ thị (GAT), giúp tăng cường sự chú ý cho các GNN của chúng tôi.
                \item Chương 5 giới thiệu GNN cho các bài toán không giám sát \& sinh, trong đó chúng tôi huấn luyện \& sử dụng bộ mã hóa tự động đồ thị biến phân (VGAE).
                \item Chương 6 sau đó khám phá khái niệm nâng cao về mạng GNN không gian - thời gian, dựa trên các đồ thị phát triển theo thời gian. Huấn luyện 1 mô hình suy luận quan hệ thần kinh (NRI), kết hợp cấu trúc bộ mã hóa tự động với RNN.

                Hầu hết các ví dụ được cung cấp cho GNN đã đề cập cho đến nay đều được minh họa bằng các ví dụ mã sử dụng đồ thị quy mô nhỏ, có thể chứa trong bộ nhớ của máy tính xách tay hoặc máy tính để bàn.
                \item Trong Chương 7, đi sâu vào các chiến lược xử lý dữ liệu vượt quá khả năng xử lý của 1 máy tính.
                \item Trong Chương 8, kết thúc bằng 1 số cân nhắc cho các dự án đồ thị \& GNN, e.g.: các khía cạnh thực tế khi làm việc với dữ liệu đồ thị, cũng như cách chuyển đổi dữ liệu không phải đồ thị sang định dạng đồ thị.
            \end{itemize}
            \item {\sf1.5.2. Unique mechanisms of a GNN model.} Although there are a variety of GNN architectures at this point, they all tackle same problem of dealing with graph data in a way that is permutation invariant. They do this via encoding \& exchanging information across graph structure during learning process.

            -- {\sf Cơ chế độc đáo của mô hình GNN.} Mặc dù hiện tại có nhiều kiến trúc GNN khác nhau, tất cả đều giải quyết cùng 1 vấn đề là xử lý dữ liệu đồ thị theo cách bất biến hoán vị. Chúng thực hiện điều này thông qua việc mã hóa \& trao đổi thông tin trên toàn bộ cấu trúc đồ thị trong quá trình học.

            In a conventional neural network CNN, 1st need to initialize a set of parameters \& functions. These include number of layers, size of layers, learning rate, loss function, batch size, \& other hyperparameters. (These are all treated in detail in other books on DL, so assume familiar with those terms). Once defined these features, then train our network by iteratively updating weights of network, as shown in {\sf Fig. 1.15: Process for training a GNN, which is similar to training most other DL models.}

            -- Trong 1 mạng nơ-ron nhân tạo thông thường (CNN), trước tiên cần khởi tạo 1 tập hợp các tham số \& hàm. Chúng bao gồm số lớp, kích thước lớp, tốc độ học, hàm mất mát, kích thước lô, \& các siêu tham số khác. (Tất cả những điều này đều được trình bày chi tiết trong các sách khác về DL, vì vậy coi như bạn đã quen thuộc với các thuật ngữ đó). Sau khi xác định các đặc điểm này, huấn luyện mạng bằng cách cập nhật trọng số của mạng theo từng bước, như thể hiện trong {\sf Hình 1.15: Quy trình huấn luyện GNN, tương tự như huấn luyện hầu hết các mô hình DL khác.}

            Explicitly, perform following steps:
            \begin{enumerate}
                \item Input our data.
                \item Pass data through neural network layers that transform data according to parameters of layer \& an activation rule.
                \item Output a representation from final layer of network.
                \item Backpropagation error, \& adjust parameters accordingly.
                \item Repeat these steps a fixed number of {\it epochs} (process by which data is passed forward \& backward to train a neural network).
            \end{enumerate}
            For tabular data, these steps are exactly as listed, as shown in {\sf Fig. 1.16: Comparison of (simple) non-GNN \& GNN. GNNs have a layer that distributes data among its vertices.} For graph-based or relational data, these steps are similar except that each epoch relates to 1 iteration of message passing.

            -- Thực hiện các bước sau 1 cách rõ ràng:
            \begin{enumerate}
                \item Nhập dữ liệu.
                \item Truyền dữ liệu qua các lớp mạng nơ-ron để biến đổi dữ liệu theo các tham số của lớp \& 1 quy tắc kích hoạt.
                \item Xuất ra 1 biểu diễn từ lớp cuối cùng của mạng.
                \item Lỗi lan truyền ngược, \& điều chỉnh các tham số cho phù hợp.
                \item Lặp lại các bước này với số lượng cố định {\it epoch} (quy trình mà dữ liệu được truyền tới \& lùi để huấn luyện mạng nơ-ron).
            \end{enumerate}
            Đối với dữ liệu dạng bảng, các bước này chính xác như được liệt kê trong {\sf Hình 1.16: So sánh (đơn giản) không phải GNN \& GNN. GNN có 1 lớp phân phối dữ liệu giữa các đỉnh của nó.} Đối với dữ liệu dựa trên đồ thị hoặc dữ liệu quan hệ, các bước này tương tự nhau, ngoại trừ việc mỗi epoch liên quan đến 1 lần lặp truyền thông điệp.
            \item {\sf1.5.3. Message passing.} {\it Message passing}, which is touched on throughout book, is a central mechanism in GNNs that enables nodes to communicate \& share information across a graph [15]. This process allows GNNs to learn rich, informative representations of graph-structured data, which is essential for tasks e.g. node classification, link prediction, \& graph-level prediction. {\sf Fig. 1.17: Elements of our message passing layer. Each message passing layer consists of an aggregation, a transformation, \& an update step: 1. Input initial graph with node, edges, \& features. 2. Collect all features from neighboring nodes, known as messages, for each node. 3. Aggregate messages using invariant functions e.g. sum, max, or mean. 4. Transform messages using a neural network to create new node features. 5. Update all features in graph with new node features.} illustrates steps involved in typical message-passing layer.

            -- {\sf Truyền tin nhắn.} {\it Truyền tin nhắn}, được đề cập trong toàn bộ cuốn sách, là 1 cơ chế trung tâm trong GNN cho phép các nút giao tiếp \& chia sẻ thông tin trên 1 đồ thị [15]. Quá trình này cho phép GNN học các biểu diễn phong phú, nhiều thông tin về dữ liệu có cấu trúc đồ thị, điều này rất cần thiết cho các tác vụ e.g. phân loại nút, dự đoán liên kết, \& dự đoán cấp đồ thị. {\sf Hình 1.17: Các thành phần của lớp truyền tin nhắn của chúng tôi. Mỗi lớp truyền tin nhắn bao gồm 1 phép tổng hợp, 1 phép biến đổi, \& 1 bước cập nhật: 1. Đầu vào đồ thị ban đầu với nút, cạnh, \& các đặc điểm. 2. Thu thập tất cả các đặc điểm từ các nút lân cận, được gọi là các thông điệp, cho mỗi nút. 3. Tổng hợp các thông điệp bằng các hàm bất biến e.g. tổng, cực đại hoặc trung bình. 4. Biến đổi các thông điệp bằng mạng nơ-ron để tạo các đặc điểm nút mới. 5. Cập nhật tất cả các đặc điểm trong đồ thị bằng các đặc điểm nút mới.} minh họa các bước liên quan đến lớp truyền tin nhắn thông thường.

            Message-passing process begins with Input (step 1) of initial graph, where every node \& edge have their own features. In Collect step (step 2), each node gathers information from its immediate neighbors -- these pieces of information are referred to as ``messages''. This step ensures that each node has access to features of its neighbors, which are crucial for understanding local graph structure. Next, in Aggregate step (step 3), collected messages from neighboring nodes are combined using an invariant function, e.g. sum, mean, or max. This aggregation consolidates information from a node's neighborhood into a single vector, capturing most relevant details about its local environment.

            -- Quá trình truyền thông điệp bắt đầu với Đầu vào (bước 1) của đồ thị khởi tạo, trong đó mỗi nút \& cạnh đều có các đặc trưng riêng. Trong bước Thu thập (bước 2), mỗi nút thu thập thông tin từ các nút lân cận trực tiếp của nó -- những thông tin này được gọi là ``thông điệp''. Bước này đảm bảo rằng mỗi nút có quyền truy cập vào các đặc trưng của các nút lân cận, điều này rất quan trọng để hiểu cấu trúc đồ thị cục bộ. Tiếp theo, trong bước Tổng hợp (bước 3), các thông điệp được thu thập từ các nút lân cận được kết hợp bằng 1 hàm bất biến, e.g.: tổng, trung bình hoặc tối đa. Quá trình tổng hợp này hợp nhất thông tin từ vùng lân cận của 1 nút thành 1 vectơ duy nhất, nắm bắt các chi tiết quan trọng nhất về môi trường cục bộ của nó.

            In Transform step (step 4), aggregated messages are processed by a neural network to produce a new representation for each node. This transformation allows GNN to learn complex interactions \& patterns within graph by applying nonlinear functions to aggregated information.

            -- Trong bước Biến đổi (bước 4), các thông điệp tổng hợp được xử lý bởi mạng nơ-ron để tạo ra 1 biểu diễn mới cho mỗi nút. Phép biến đổi này cho phép GNN học các tương tác phức tạp \& các mẫu trong đồ thị bằng cách áp dụng các hàm phi tuyến tính vào thông tin tổng hợp.

            Finally, during Update step (step 5), features of each node in graph are replaced or updated with these new representations. This completes 1 round of message passing, incorporating information from neighboring nodes to refine each node's features.

            -- Cuối cùng, trong bước Cập nhật (bước 5), các đặc trưng của mỗi nút trong đồ thị được thay thế hoặc cập nhật bằng các biểu diễn mới này. Điều này hoàn tất 1 vòng truyền thông điệp, kết hợp thông tin từ các nút lân cận để tinh chỉnh các đặc trưng của từng nút.

            Each message-passing layer in a GNN allows nodes to gather information from nodes that are further away, or more ``hops'' away, in graph. Repeating these steps over multiple layers enables GNN to capture more complex dependencies \& long-range interactions within graph.

            -- Mỗi lớp truyền thông điệp trong GNN cho phép các nút thu thập thông tin từ các nút ở xa hơn, hoặc cách xa hơn ``các bước nhảy'', trong đồ thị. Việc lặp lại các bước này trên nhiều lớp cho phép GNN nắm bắt các mối quan hệ phụ thuộc phức tạp hơn \& các tương tác tầm xa trong đồ thị.

            By using message passing, GNNs efficiently encode graph structure \& data into useful representations for a variety of downstream tasks. Advanced architectures, e.g. those incorporating global attention or hierarchical message passing, further enhance model's ability to capture long-range dependencies across graph, enabling more robust performance on diverse applications.

            -- Bằng cách sử dụng kỹ thuật truyền thông điệp, GNN mã hóa hiệu quả cấu trúc đồ thị \& dữ liệu thành các biểu diễn hữu ích cho nhiều tác vụ hạ nguồn. Các kiến trúc tiên tiến, e.g. kiến trúc tích hợp sự chú ý toàn cục hoặc truyền thông điệp phân cấp, sẽ nâng cao hơn nữa khả năng của mô hình trong việc nắm bắt các phụ thuộc tầm xa trên toàn bộ đồ thị, cho phép hiệu suất mạnh mẽ hơn trên nhiều ứng dụng khác nhau.
        \end{itemize}
        \item {\sf Summary.}
        \begin{itemize}
            \item GNNs are specialized tools for handling relational, or relationship-centric, data, particularly in scenarios where traditional neural networks struggle due to complexity \& diversity of graph structures.

            -- GNN là công cụ chuyên dụng để xử lý dữ liệu quan hệ hoặc dữ liệu tập trung vào mối quan hệ, đặc biệt là trong các tình huống mà mạng nơ-ron truyền thống gặp khó khăn do tính phức tạp \& đa dạng của cấu trúc đồ thị.
            \item GNNs have found significant applications in areas e.g. recommendation engines, drug discovery, \& mechanical reasoning, showcasing their versatility in handling large \& complex relational data for enhanced insights \& predictions.

            -- GNN đã tìm thấy những ứng dụng quan trọng trong các lĩnh vực như công cụ đề xuất, khám phá thuốc, \& suy luận cơ học, thể hiện tính linh hoạt của chúng trong việc xử lý dữ liệu quan hệ phức tạp \& lớn để có được những hiểu biết sâu sắc \& dự đoán tốt hơn.
            \item Specific GNN tasks include node prediction, edge prediction, graph prediction, \& graph representation through embedding techniques.

            -- Các nhiệm vụ cụ thể của GNN bao gồm dự đoán nút, dự đoán cạnh, dự đoán đồ thị \& biểu diễn đồ thị thông qua các kỹ thuật nhúng.
            \item Specific GNN tasks include node prediction, edge prediction, graph prediction, \& graph representation through embedding techniques.

            -- Các nhiệm vụ cụ thể của GNN bao gồm dự đoán nút, dự đoán cạnh, dự đoán đồ thị \& biểu diễn đồ thị thông qua các kỹ thuật nhúng.
            \item GNNs are best used when data is represented as a graph, indicating a strong emphasis on relationships \& connections between data points. They are not ideal for individual, standalone data entries where relational information is insignificant.

            -- GNN được sử dụng tốt nhất khi dữ liệu được biểu diễn dưới dạng đồ thị, thể hiện sự nhấn mạnh vào mối quan hệ \& kết nối giữa các điểm dữ liệu. Chúng không lý tưởng cho các mục dữ liệu riêng lẻ, độc lập, nơi thông tin quan hệ không đáng kể.
            \item When deciding if a GNN solution is a good fit for your problem, consider cases that have characteristics e.g. implicit relationships, high-dimensionality, sparsity, \& complex nonlocal interactions. By understanding these fundamentals, practitioners can evaluate suitability of GNNs for their specific problems, implement them effectively, \& recognize their tradeoffs \& limitations in real-world applications.

            -- Khi quyết định xem giải pháp GNN có phù hợp với vấn đề của bạn hay không, xem xét các trường hợp có đặc điểm như mối quan hệ ngầm định, đa chiều, thưa thớt, \& tương tác phi cục bộ phức tạp. Bằng cách hiểu những nguyên tắc cơ bản này, người thực hành có thể đánh giá tính phù hợp của GNN cho các vấn đề cụ thể, triển khai chúng 1 cách hiệu quả, \& nhận ra những đánh đổi \& hạn chế của chúng trong các ứng dụng thực tế.
            \item Messages passing is a core mechanism of GNN,s which enables them to encode \& exchange information across a graph's structure, allowing for meaningful node, edge, \& graph-level predictions. Each layer of a GNN represents 1 step of message passing, with various aggregation functions to combine messages effectively, providing insights \& representations useful for ML tasks.

            -- Truyền thông điệp là 1 cơ chế cốt lõi của GNN, cho phép chúng mã hóa \& trao đổi thông tin trên toàn bộ cấu trúc đồ thị, cho phép đưa ra các dự đoán có ý nghĩa ở cấp độ nút, cạnh \& đồ thị. Mỗi lớp của GNN đại diện cho 1 bước truyền thông điệp, với nhiều hàm tổng hợp khác nhau để kết hợp các thông điệp 1 cách hiệu quả, cung cấp thông tin chi tiết \& biểu diễn hữu ích cho các tác vụ ML.
        \end{itemize}
    \end{itemize}
    \item {\sf2. Graph embeddings.} Covers:
    \begin{enumerate}
        \item Exploring graph embeddings \& their importance
        \item Creating node embeddings using non-GNN \& GNN methods
        \item Comparing node embeddings on a semi-supervised problem
        \item Taking a deeper dive into embedding methods
    \end{enumerate}
    Graph embeddings are essential tools in graph-based ML. They transform intricate structure of graphs -- be it the entire graph, individual nodes, or edges -- into a more manageable, lower-dimensional space. Do this to compress a complex dataset into a form that is easier to work with, without losing its inherent patterns \& relationships, information to which we will apply a GNN or other ML method.

    -- Bao gồm:
    \begin{enumerate}
        \item Khám phá nhúng đồ thị \& tầm quan trọng của chúng
        \item Tạo nhúng nút bằng phương pháp không phải GNN \& GNN
        \item So sánh nhúng nút trong bài toán bán giám sát
        \item Đi sâu hơn vào các phương pháp nhúng
    \end{enumerate}
    Nhúng đồ thị là công cụ thiết yếu trong học máy dựa trên đồ thị. Chúng chuyển đổi cấu trúc phức tạp của đồ thị -- có thể là toàn bộ đồ thị, từng nút hoặc cạnh -- thành 1 không gian ít chiều hơn, dễ quản lý hơn. Thực hiện điều này để nén 1 tập dữ liệu phức tạp thành 1 dạng dễ làm việc hơn, mà không làm mất các mẫu \& mối quan hệ vốn có của nó, thông tin mà chúng ta sẽ áp dụng GNN hoặc phương pháp học máy khác.

    Graphs encapsulate relationships \& interactions within networks, whether they are social networks, biological networks, or any system where entities are interconnected. Embeddings capture these real-life relationships in a compact from, facilitating tasks e.g. visualization, clustering, or predictive modeling.

    -- Đồ thị gói gọn các mối quan hệ \& tương tác trong các mạng lưới, dù là mạng xã hội, mạng sinh học hay bất kỳ hệ thống nào mà các thực thể được kết nối với nhau. Nhúng nắm bắt các mối quan hệ thực tế này 1 cách cô đọng, tạo điều kiện thuận lợi cho các tác vụ như trực quan hóa, phân cụm hoặc mô hình hóa dự đoán.

    There are numerous strategies to derive these embeddings, each with its unique approach \& application: from classical graph algorithms that use network's topology, to linear algebra techniques that decompose matrices representing graph, \& more advanced methods e.g. GNNs [1]. GNNs stand out because they can integrate embedding process directly into learning algorithm itself.

    -- Có nhiều chiến lược để tạo ra các nhúng này, mỗi chiến lược có cách tiếp cận \& ứng dụng riêng: từ các thuật toán đồ thị cổ điển sử dụng cấu trúc mạng, đến các kỹ thuật đại số tuyến tính phân tích các ma trận biểu diễn đồ thị, \& các phương pháp tiên tiến hơn, e.g. GNN [1]. GNN nổi bật vì chúng có thể tích hợp quá trình nhúng trực tiếp vào chính thuật toán học.

    In traditional ML workflows, embeddings are generated as a separate step, serving as a dimensionality-reduction technique in tasks e.g. regression or classification. However, GNNs merge embedding generation with model's learning process. As network processes inputs through its layers, embeddings are refined \& updated, making learning phase \& embedding phase inseparable. I.e., GNNs learn most informative representative of graph data during training time.

    -- Trong quy trình làm việc ML truyền thống, nhúng được tạo ra như 1 bước riêng biệt, đóng vai trò là kỹ thuật giảm chiều trong các tác vụ như hồi quy hoặc phân loại. Tuy nhiên, GNN kết hợp việc tạo nhúng với quy trình học của mô hình. Khi mạng xử lý dữ liệu đầu vào qua các lớp của nó, các nhúng được tinh chỉnh \& cập nhật, khiến giai đoạn học \& giai đoạn nhúng trở nên không thể tách rời. Tức là, GNN học dữ liệu biểu diễn đồ thị mang tính thông tin nhất trong thời gian đào tạo.

    Using graph embeddings can significantly enhance your DS \& ML projects, especially when dealing with complex networked data. By capturing essence of graph in a lower-dimensional space, embeddings make it feasible to apply a variety of other ML techniques to graph data, opening up a world of possibilities for analysis \& model building.

    -- Việc sử dụng nhúng đồ thị có thể cải thiện đáng kể các dự án DS \& ML của bạn, đặc biệt là khi xử lý dữ liệu mạng phức tạp. Bằng cách nắm bắt bản chất của đồ thị trong không gian ít chiều hơn, nhúng cho phép áp dụng nhiều kỹ thuật ML khác vào dữ liệu đồ thị, mở ra 1 thế giới khả thi cho việc phân tích \& xây dựng mô hình.

    In this chap, begin with an introduction to graph embeddings \& a case study on a graph of political book purchases. Start with Node2Vec (N2V) to establish a baseline with a non-GNN approach, guiding you through its practical application. In Sect. 2.2, shift to GNNs, offering a hands-on introduction to GNN-based embeddings, including setup, preprocessing, \& visualization. Sect. 2.3 provides a comparative analysis of N2V \& GNN embeddings, highlighting their applications. Chap then rounds off with a discussion of theoretical aspects of these embedding methods, with a special focus on principles behind N2V \& message-passing mechanism in GNNs. Process we take in this chap is illustrated in {\sf Fig. 2.1: Summary of process \& objectives in Chap. 2: 1. Preprocess Political Books dataset for embedding. 2. Use N2V \& GCN to create embeddings from preprocessed data. 3. Prepare N2V embeddings \& GCN embeddings for semi-supervised classification. 4. Embeddings are used as features in a random forest classifier (tabular features) \&a GCN classifier (node features).}

    -- Trong chương này, bắt đầu bằng phần giới thiệu về nhúng đồ thị \& nghiên cứu điển hình về đồ thị mua sách chính trị. Bắt đầu với Node2Vec (N2V) để thiết lập đường cơ sở với phương pháp không phải GNN, hướng dẫn bạn ứng dụng thực tế. Trong Phần 2.2, chuyển sang GNN, cung cấp phần giới thiệu thực hành về nhúng dựa trên GNN, bao gồm thiết lập, tiền xử lý, \& trực quan hóa. Phần 2.3 cung cấp phân tích so sánh về nhúng N2V \& GNN, làm nổi bật các ứng dụng của chúng. Sau đó, chương kết thúc bằng phần thảo luận về các khía cạnh lý thuyết của các phương pháp nhúng này, đặc biệt tập trung vào các nguyên tắc đằng sau cơ chế truyền tin nhắn N2V \& trong GNN. Quy trình chúng tôi thực hiện trong chương này được minh họa trong {\sf Hình 2.1: Tóm tắt các mục tiêu của quy trình \& trong Chương 2: 1. Tiền xử lý tập dữ liệu Sách chính trị để nhúng. 2. Sử dụng N2V \& GCN để tạo nhúng từ dữ liệu đã được xử lý trước. 3. Chuẩn bị nhúng N2V \& nhúng GCN cho phân loại bán giám sát. 4. Nhúng được sử dụng làm các đặc trưng trong bộ phân loại rừng ngẫu nhiên (các đặc trưng dạng bảng) \& bộ phân loại GCN (các đặc trưng dạng nút).}
    \begin{itemize}
        \item {\sf2.1. Creating embeddings with Node2Vec.} Understanding relationships within a network is a core task in many fields, from social network analysis to biology \& recommendation systems. In this section, explor how to create node embeddings using {\tt Node2Vec (N2V)}, a technique inspired by {\tt Word2Vec} from NLP [2]. N2V captures context of nodes within a graph by simulating random walks, allowing us to understand neighborhood relationships between nodes in a low-dimensional space. This approach is effective for identifying patterns, clustering similar nodes, \& preparing data for ML tasks.

        -- {\sf Tạo nhúng với Node2Vec.} Hiểu các mối quan hệ trong mạng là 1 nhiệm vụ cốt lõi trong nhiều lĩnh vực, từ phân tích mạng xã hội đến sinh học \& hệ thống khuyến nghị. Trong phần này, khám phá cách tạo nhúng nút bằng {\tt Node2Vec (N2V)}, 1 kỹ thuật lấy cảm hứng từ {\tt Word2Vec} trong NLP [2]. N2V nắm bắt ngữ cảnh của các nút trong đồ thị bằng cách mô phỏng các bước ngẫu nhiên, cho phép chúng ta hiểu các mối quan hệ lân cận giữa các nút trong không gian ít chiều. Phương pháp này hiệu quả trong việc xác định các mẫu, phân cụm các nút tương tự \& chuẩn bị dữ liệu cho các tác vụ ML.

        To make this process accessible, use {\tt Node2Vec} Python library, which is beginner-friendly, although it may be slower on larger graphs. N2V helps create embeddings that capture structural relationships between nodes, which we can then visualize to uncover insights about graph's structure. Our workflow involves several steps:
        \begin{enumerate}
            \item {\it Load data \& set N2V parameters.} Start by loading our graph data \& initializing N2V with specific parameters to control random walks, e.g. walk length \& number of walks per node.
            \item {\it Create embeddings.} N2V generates node embeddings by performing random walks on graph, effective summarizing each node's local neighborhood into a vector format.
            \item {\it Transform embeddings.} Resulting embeddings are saved \& then transformed into a format suitable for visualization.
            \item {\it Visualize embeddings in 2D.} Use UMAP, a dimensionality reduction technique, to project these embeddings into 2D, making it easier to visualize \& interpret results.
        \end{enumerate}
        -- Để quá trình này dễ hiểu hơn, sử dụng thư viện Python {\tt Node2Vec}, thư viện này thân thiện với người mới bắt đầu, mặc dù có thể chậm hơn trên các đồ thị lớn hơn. N2V giúp tạo các nhúng nắm bắt mối quan hệ cấu trúc giữa các nút, sau đó chúng ta có thể trực quan hóa để khám phá những hiểu biết sâu sắc về cấu trúc của đồ thị. Quy trình làm việc của chúng tôi bao gồm 1 số bước:
        \begin{enumerate}
            \item {\it Tải dữ liệu \& đặt tham số N2V.} Bắt đầu bằng cách tải dữ liệu đồ thị \& khởi tạo N2V với các tham số cụ thể để kiểm soát các bước ngẫu nhiên, e.g.: độ dài bước \& số lần bước trên mỗi nút.
            \item {\it Tạo nhúng.} N2V tạo nhúng nút bằng cách thực hiện các bước ngẫu nhiên trên đồ thị, tóm tắt hiệu quả vùng lân cận cục bộ của mỗi nút thành định dạng vector.
            \item {\it Biến đổi nhúng.} Các nhúng kết quả được lưu \& sau đó được chuyển đổi thành định dạng phù hợp để trực quan hóa.
            \item {\it Hình dung các nhúng trong 2D.} Sử dụng UMAP, 1 kỹ thuật giảm chiều, để chiếu các nhúng này vào 2D, giúp hình dung \& diễn giải kết quả dễ dàng hơn.
        \end{enumerate}
        Our data is Political Books dataset, which comprises books (nodes) connected by frequent co-purchases on Amazon.com during 2004 US election period (edges) [3]. Using this dataset provides a compelling example of how N2V can reveal underlying patterns in co-purchasing behavior, potentially reflecting broader ideological groupings among book buyers [4]. {\sf Table 2.1: Overview of Political Books dataset.} provides key information about Political Books graph. The Political Books dataset contains the following:
        \begin{enumerate}
            \item Nodes: represent books about US politics sold by Amazon.com.
            \item Edges: indicate frequent co-purchasing by same buyers, as suggested by Amazon's ``customers who bought this book also bought these other books'' feature.
        \end{enumerate}
        In {\sf Fig. 2.2: Graph visualization of Political Books dataset. Right-leaning books (nodes) are in a lighter shade \& are clustered in top half of figure, left-leaning are darker shaded circles \& clustered in lower half of figure, \& neural political stance are dark squares \& appear in middle. When 2 nodes are connected, it indicates that they have been purchased together frequently on Amazon.com.}, books are shaded based on their political alignment -- darker shade for liberal, lighter shade for conservative, \& striped for neural. Categories were assigned by {\sc Mark Newman} through a qualitative analysis of book descriptions \& reviews posted on Amazon.

        -- Dữ liệu của chúng tôi là tập dữ liệu Sách Chính trị, bao gồm các cuốn sách (nút) được kết nối bởi các giao dịch mua chung thường xuyên trên Amazon.com trong giai đoạn bầu cử Hoa Kỳ năm 2004 (cạnh) [3]. Việc sử dụng tập dữ liệu này cung cấp 1 ví dụ thuyết phục về cách N2V có thể tiết lộ các mô hình cơ bản trong hành vi mua chung, có khả năng phản ánh các nhóm ý thức hệ rộng hơn giữa những người mua sách [4]. {\sf Bảng 2.1: Tổng quan về tập dữ liệu Sách Chính trị.} cung cấp thông tin chính về biểu đồ Sách Chính trị. Tập dữ liệu Sách Chính trị chứa các thông tin sau:

        \begin{enumerate}
            \item Nút: biểu diễn các cuốn sách về chính trị Hoa Kỳ do Amazon.com bán.
            \item Cạnh: biểu thị việc mua chung thường xuyên của cùng 1 người mua, như được gợi ý bởi tính năng ``khách hàng đã mua cuốn sách này cũng đã mua những cuốn sách khác này'' của Amazon.

        \end{enumerate}
        Trong {\sf Hình 2.2: Biểu đồ trực quan của tập dữ liệu Sách Chính trị. Sách thiên hữu (nút) được tô sáng hơn \& tập trung ở nửa trên của hình, sách thiên tả là các vòng tròn tô đậm hơn \& tập trung ở nửa dưới của hình, \& lập trường chính trị thần kinh là các ô vuông đậm \& xuất hiện ở giữa. Khi 2 nút được kết nối, điều đó cho thấy chúng đã được mua cùng nhau thường xuyên trên Amazon.com.} Sách được tô màu dựa trên khuynh hướng chính trị của chúng -- tô đậm hơn cho khuynh hướng tự do, tô nhạt hơn cho khuynh hướng bảo thủ, \& có sọc cho khuynh hướng thần kinh. Các danh mục được phân loại bởi {\sc Mark Newman} thông qua phân tích định tính các mô tả sách \& bài đánh giá được đăng trên Amazon.

        This dataset, compiled by {\sc Valdis Krebs} \& available through GNN in Action repository or Carnegie Mellon University website, contains 105 books (nodes) \& 441 edges (co-purchases). If want to learn more about background of this dataset, {\sc Krebs} has written an article with this information [4].

        -- Bộ dữ liệu này, do {\sc Valdis Krebs} biên soạn \& có sẵn trên kho lưu trữ GNN in Action hoặc trang web của Đại học Carnegie Mellon, chứa 105 sách (nút) \& 441 cạnh (mua chung). Nếu muốn tìm hiểu thêm về bối cảnh của bộ dữ liệu này, {\sc Krebs} đã viết 1 bài báo với thông tin này [4].

        Using N2V, aim to explore structure of this collection of books, uncovering insights based on political learnings \& potential associations between different book categories. By visualizing embeddings created by N2V, can gain a better understanding of how books are grouped \& which ones might share a common audience, providing valuable insights into consumer behavior during a politically changed period.

        -- Sử dụng N2V, chúng tôi hướng đến việc khám phá cấu trúc của bộ sưu tập sách này, tìm ra những hiểu biết sâu sắc dựa trên các bài học chính trị \& những mối liên hệ tiềm năng giữa các thể loại sách khác nhau. Bằng cách trực quan hóa các phần nhúng do N2V tạo ra, chúng tôi có thể hiểu rõ hơn về cách sách được nhóm lại \& những cuốn nào có thể có chung đối tượng độc giả, từ đó cung cấp những hiểu biết giá trị về hành vi người tiêu dùng trong giai đoạn biến động chính trị.

        From visualization, data is already clustered in a logical way. This is thanks to {\it Kamada-Kawai algorithm} graph algorithm, which exploits topological data only without metadata \& is useful for visualizing graph. This graph visualization technique positions nodes in a way that reflects their connections, aiming for an arrangement where closely connected nodes are near each other but less connected nodes are farther apart. It achieves this by treating nodes like points connected by springs, iteratively adjusting their positions until ``tension'' in springs is minimized. This results in a layout that naturally reveals clusters \& relationships within graph based purely on its structure.

        -- Từ trực quan hóa, dữ liệu đã được phân cụm theo 1 cách logic. Điều này là nhờ thuật toán đồ thị {\it Kamada-Kawai}, thuật toán này chỉ khai thác dữ liệu tôpô mà không cần siêu dữ liệu \& rất hữu ích cho việc trực quan hóa đồ thị. Kỹ thuật trực quan hóa đồ thị này định vị các nút theo cách phản ánh kết nối của chúng, hướng đến 1 sự sắp xếp trong đó các nút có kết nối chặt chẽ sẽ ở gần nhau nhưng các nút ít kết nối hơn sẽ ở xa nhau hơn. Nó đạt được điều này bằng cách coi các nút như các điểm được kết nối bởi lò xo, điều chỉnh vị trí của chúng theo từng bước cho đến khi ``lực căng'' trong lò xo được giảm thiểu. Điều này dẫn đến 1 bố cục tự nhiên thể hiện các cụm \& mối quan hệ trong đồ thị chỉ dựa trên cấu trúc của nó.

        For Political Books dataset, Kamada-Kawai algorithm helps us visualize books (nodes) based on how often they are co-purchased on Amazon, without using any external information e.g. political alignment or book titles. This gives us an initial view of how books are grouped together by buying behavior. In next steps, use methods e.g. N2V to create embeddings that capture more detailed patterns \& further distinguish different book groups.

        -- Đối với tập dữ liệu Sách Chính trị, thuật toán Kamada-Kawai giúp chúng tôi trực quan hóa các cuốn sách (nút) dựa trên tần suất chúng được mua chung trên Amazon, mà không cần sử dụng bất kỳ thông tin bên ngoài nào, e.g. liên kết chính trị hoặc tiêu đề sách. Điều này cung cấp cho chúng tôi cái nhìn ban đầu về cách sách được nhóm lại với nhau theo hành vi mua. Trong các bước tiếp theo, sử dụng các phương pháp, e.g. N2V, để tạo các nhúng nắm bắt các mẫu chi tiết hơn \& phân biệt rõ hơn các nhóm sách khác nhau.
       \begin{itemize}
           \item {\sf2.1.1. Loading data, setting parameters, \& creating embeddings.} Use {\tt Node2Vec, NetworkX} libraries for our 1st hands-on encounter with graph embeddings. After installing these packages using {\tt pip}, load our dataset's graph data, which is stored in {\tt.gml} format (Graph Modeling Language, GML), using {\tt NetworkX} library \& generate embeddings with {\tt Node2Vec} library.

           -- {\sf Đang tải dữ liệu, thiết lập tham số, \& tạo nhúng.} Sử dụng thư viện {\tt Node2Vec, NetworkX} cho lần thực hành đầu tiên với nhúng đồ thị. Sau khi cài đặt các gói này bằng {\tt pip}, tải dữ liệu đồ thị của tập dữ liệu, được lưu trữ ở định dạng {\tt.gml} (Ngôn ngữ Mô hình Đồ thị, GML), bằng thư viện {\tt NetworkX} \& tạo nhúng bằng thư viện {\tt Node2Vec}.

           GML is a simple, human-readable plain text file format used to represent graph structures. It stores information about nodes, edges, \& their attributes in a structured way, making it easy to read \& write graph data. E.g., a {\tt.gml} file might contain a list of nodes (e.g., books in our dataset) \& edges (connections representing co-purchases) along with additional properties e.g. labels or weights. This format is widely used for exchanging graph data between different software \& tools. By loading {\tt.gml} file with {\tt NetworkX}, can easily manipulate \& analyze graph in Python.

           -- GML là 1 định dạng tệp văn bản thuần túy đơn giản, dễ đọc, được sử dụng để biểu diễn các cấu trúc đồ thị. Nó lưu trữ thông tin về các nút, cạnh \& thuộc tính của chúng theo 1 cấu trúc nhất định, giúp việc đọc \& ghi dữ liệu đồ thị trở nên dễ dàng. Ví dụ: tệp {\tt.gml} có thể chứa danh sách các nút (e.g.: sách trong tập dữ liệu của chúng tôi) \& các cạnh (các kết nối biểu diễn các giao dịch mua chung) cùng với các thuộc tính bổ sung, e.g.: nhãn hoặc trọng số. Định dạng này được sử dụng rộng rãi để trao đổi dữ liệu đồ thị giữa các phần mềm \& công cụ khác nhau. Bằng cách tải tệp {\tt.gml} với {\tt NetworkX}, bạn có thể dễ dàng thao tác \& phân tích đồ thị trong Python.

           In {\tt Node2Vec} library's {\tt Node2Vec} function, can use following parameters to specify calculations done \& properties of output embedding:
           \begin{itemize}
               \item {\it Size of embedding} ({\tt dimensions}): Think of this as how detailed each node's profile is, as in how many different traits you are noting down. Standard detail level is 128 traits, but you can tweak this based on how complex you want each node's profile to be.
               \item {\it Length of each walk} ({\tt Walk Length}): This is about how far each random walk through your graph goes, with 80 steps being usual journey. If want to see more of neighborhood around a node, increase this number.
               \item {\it Number of walks per node} ({\tt Num Walks}): This tells us how many times we will take a walk starting from each node. Starting with 10 walks gives a good overview, but if you want a fuller picture of a node's surroundings, consider going on more walks.
               \item {\it Backtracking control (Return Parameter $p$)}: This setting helps decide if our walk should circle back to where it has been. Setting it at 1 keeps things balanced, but adjusting it can make your walks more or less exploratory.
               \item {\it Exploration Depth (In-Out Parameter $q$)}: This one is about choosing between taking in broader neighborhood scene (e.g.,a BFS with $q > 1$) or diving deep into specific paths (e.g., a DFS with $q < 1$), with 1 being a mix of both.
           \end{itemize}
           Adjust these settings based on what you are looking to understand about your nodes \& their connections. Want more depth? Tweak exploration depth. Looking for broader context? Adjust walk length \& number of walks. In addition, keep in mind: size of your embeddings should match level of detail you need. In general, it is a good idea to try different combinations of these parameters to see effect on embeddings.

           -- Trong hàm {\tt Node2Vec} của thư viện {\tt Node2Vec}, bạn có thể sử dụng các tham số sau để chỉ định các phép tính được thực hiện \& các thuộc tính của nhúng đầu ra:
           \begin{itemize}
               \item {\tt Kích thước nhúng} ({\tt kích thước}): Hãy coi đây là mức độ chi tiết của hồ sơ từng nút, i.e., số lượng đặc điểm khác nhau mà bạn đang ghi lại. Mức độ chi tiết tiêu chuẩn là 128 đặc điểm, nhưng bạn có thể điều chỉnh tùy theo độ phức tạp mà bạn muốn hồ sơ của mỗi nút đạt được.
               \item {\tt Độ dài mỗi lần đi} ({\tt Độ dài lần đi}): Đây là khoảng cách mà mỗi lần đi ngẫu nhiên qua đồ thị của bạn đi được, với 80 bước là hành trình thông thường. Nếu muốn xem thêm về vùng lân cận xung quanh 1 nút, tăng con số này.
               \item {\tt Số lần đi trên mỗi nút} ({\tt Số lần đi}): Con số này cho chúng ta biết chúng ta sẽ đi bao nhiêu lần bắt đầu từ mỗi nút. Bắt đầu với 10 lần đi bộ sẽ cho 1 cái nhìn tổng quan tốt, nhưng nếu bạn muốn có cái nhìn đầy đủ hơn về môi trường xung quanh của 1 nút, cân nhắc đi bộ nhiều lần hơn.
               \item {\it Điều khiển quay lui (Tham số trả về $p$)}: Thiết lập này giúp quyết định xem việc đi bộ của chúng ta có nên quay lại vị trí ban đầu hay không. Đặt giá trị này ở mức 1 sẽ giữ mọi thứ cân bằng, nhưng việc điều chỉnh nó có thể khiến việc đi bộ của bạn mang tính khám phá nhiều hơn hoặc ít hơn.
               \item {\it Độ sâu khám phá (Tham số vào-ra $q$)}: Thiết lập này liên quan đến việc lựa chọn giữa việc xem xét toàn cảnh khu vực lân cận rộng hơn (e.g.: BFS có $q > 1$) hoặc đi sâu vào các đường dẫn cụ thể (e.g.: DFS có $q < 1$), với 1 là sự kết hợp của cả hai.
           \end{itemize}
           Điều chỉnh các thiết lập này dựa trên những gì bạn muốn hiểu về các nút \& kết nối của chúng. Muốn có thêm chiều sâu? Hãy tinh chỉnh độ sâu khám phá. Muốn có bối cảnh rộng hơn? Hãy điều chỉnh độ dài \& số lần đi bộ. Ngoài ra, nhớ rằng: kích thước nhúng phải phù hợp với mức độ chi tiết bạn cần. Nhìn chung, bạn nên thử kết hợp các thông số này theo nhiều cách khác nhau để xem hiệu quả của việc nhúng.

           For this exercise, use 1st 4 parameters. Deeper details on these parameters are found in Sect. 2.4. Code in {\sf Listing 2.1: Generating N2V embeddings} begins by loading graph into a variable called \verb|book_graph|, using \verb|read_gml| method from {\tt NetworkX} library. Next, a N2V {\tt model} is initialized with loaded graph. This model is set up with specific parameters: it will create 64-dimensional embeddings for each node, use walks of 30 steps along, perform 200 walks starting from each node to gather context, \& run these operations in parallel across 4 workers to speed up process.

           -- Với bài tập này, sử dụng 4 tham số đầu tiên. Chi tiết sâu hơn về các tham số này được tìm thấy trong Mục 2.4. Mã trong {\sf Liệt kê 2.1: Tạo nhúng N2V} bắt đầu bằng cách tải đồ thị vào 1 biến có tên là \verb|book_graph|, sử dụng phương thức \verb|read_gml| từ thư viện {\tt NetworkX}. Tiếp theo, 1 mô hình N2V {\tt} được khởi tạo với đồ thị đã tải. Mô hình này được thiết lập với các tham số cụ thể: nó sẽ tạo nhúng 64 chiều cho mỗi nút, sử dụng các bước đi 30 bước dọc theo, thực hiện 200 bước đi bắt đầu từ mỗi nút để thu thập ngữ cảnh, \& chạy các thao tác này song song trên 4 worker để tăng tốc quá trình.

           N2V model is then trained with additional parameters defined in {\tt fit} method. This involves setting a context window size of 10 nodes around each target node to learn embeddings, considering all nodes at least once \verb|min_count = 1|, \& processing 4 words (nodes, in this context) each time during training.

           -- Mô hình N2V sau đó được huấn luyện với các tham số bổ sung được xác định trong phương thức {\tt fit}. Phương thức này bao gồm việc thiết lập kích thước cửa sổ ngữ cảnh là 10 nút xung quanh mỗi nút mục tiêu để học các phép nhúng, xem xét tất cả các nút ít nhất 1 lần \verb|min_count = 1|, \& xử lý 4 từ (nút, trong ngữ cảnh này) mỗi lần trong quá trình huấn luyện.

           Once trained, access node embeddings using {\tt model}'s {\tt mv} method (reflecting its NLP heritage, {\tt wv} stands for word vectors). For our downstream tasks, we map each node to its embedding using a dictionary comprehension.

           -- Sau khi được đào tạo, truy cập các nhúng nút bằng phương pháp {\tt mv} của {\tt model} (phản ánh di sản NLP của nó, {\tt wv} là viết tắt của các vectơ từ). Đối với các tác vụ hạ nguồn, chúng tôi ánh xạ từng nút vào nhúng của nó bằng cách sử dụng 1 thuật toán hiểu từ điển.
           \begin{Verbatim}[numbers=left,xleftmargin=5mm]
import NetworkX as nx
from Node2Vec import Node2Vec
book_graph = nx.read_gml('polbooks.gml')
node2vec = Node2Vec(book_graph, dimensions = 64, walk_length = 30, num_walks = 200, workers = 4)
model = node2vec.fit(window = 10, min_count = 1, batch_words = 4)
embeddings = {str(node) : model.wv[str(node)] for node in gml_graph.nodes()}
           \end{Verbatim}
           \begin{enumerate}
               \item Loads graph data from a GML file into a NetworkX graph object
               \item Initializes N2V model with specified parameters for input graph
               \item Trains N2V model
               \item Extracts \& stores node embeddings generated by N2V model in a dictionary.
           \end{enumerate}
           \item {\sf2.1.2. Demystifying embeddings.} Explore what these embeddings are \& why they are valuable. An {\it embedding} is a dense numerical vector that represents identity of a node, edge, or graph in way that captures essential information about its structure \& relationships. In our context, an embedding created by N2V captures a node's position \& neighborhood within graph using topological information, i.e., it summarizes how node is connected to others, effectively capturing its role \& importance in network. Later, when use GNNs to create embeddings, they will also encapsulate node's features, providing an even richer representation that includes both structure \& attributes. Get deeper into theoretical aspects of embedding in Sect. 2.4.

           -- {\sf Giải mã các phép nhúng.} Khám phá các phép nhúng này là gì \& tại sao chúng có giá trị. 1 phép nhúng {\it} là 1 vectơ số dày đặc biểu diễn danh tính của 1 nút, cạnh hoặc đồ thị theo cách nắm bắt thông tin thiết yếu về cấu trúc \& các mối quan hệ của nó. Trong ngữ cảnh của chúng ta, 1 phép nhúng do N2V tạo ra nắm bắt vị trí \& lân cận của 1 nút trong đồ thị bằng thông tin tôpô, i.e., nó tóm tắt cách nút được kết nối với các nút khác, từ đó nắm bắt hiệu quả vai trò \& tầm quan trọng của nút đó trong mạng. Sau này, khi sử dụng GNN để tạo các phép nhúng, chúng cũng sẽ đóng gói các đặc trưng của nút, cung cấp 1 biểu diễn phong phú hơn, bao gồm cả cấu trúc \& các thuộc tính. Tìm hiểu sâu hơn về các khía cạnh lý thuyết của phép nhúng trong Phần 2.4.

           These embeddings are powerful because they transform complex, high-dimensional graph data into a fixed-size vector format that can be easily in various analyses \& ML tasks. E.g., they allow us to perform exploratory data analysis by revealing patterns, clusters, \& relationships within graph. Beyond this, embeddings can be directly used as features in ML models, where each dimension of vector represents a distinct feature. This is particularly useful in applications where understanding structure \& connections between data points, e.g. in social networks or recommendation systems, can significantly improve model performance.

           -- Các phép nhúng này rất mạnh mẽ vì chúng chuyển đổi dữ liệu đồ thị phức tạp, nhiều chiều thành định dạng vector có kích thước cố định, có thể dễ dàng thực hiện trong nhiều phân tích \& tác vụ ML. Ví dụ: chúng cho phép chúng ta thực hiện phân tích dữ liệu thăm dò bằng cách khám phá các mẫu, cụm, \& mối quan hệ trong đồ thị. Hơn thế nữa, các phép nhúng có thể được sử dụng trực tiếp làm đặc trưng trong các mô hình ML, trong đó mỗi chiều của vector đại diện cho 1 đặc trưng riêng biệt. Điều này đặc biệt hữu ích trong các ứng dụng mà việc hiểu cấu trúc \& kết nối giữa các điểm dữ liệu, e.g. trong mạng xã hội hoặc hệ thống khuyến nghị, có thể cải thiện đáng kể hiệu suất mô hình.

           To illustrate, consider node representing book {\it Losing Bin Laden} in our Political Books dataset. Using command {\tt model.wv['Losing Bin Laden']}, we retrieve its dense vector embedding. This vector, shown in {\sf Fig. 2.3: Extracting embedding for nodes associated with political book {\it Losing Bin Laden}. Output is a dense vector represented as a Python list.}, captures various aspects of book's role within network of co-purchased books, providing a compact, informative representation that can be used for further analysis or as input to other models.

           -- Để minh họa, xem xét nút biểu diễn cuốn sách {\tt "Losing Bin Laden"} trong tập dữ liệu Sách Chính trị của chúng tôi. Sử dụng lệnh {\tt model.wv['Losing Bin Laden']}, chúng tôi lấy được nhúng vector dày đặc của nó. Vector này, được hiển thị trong {\sf Hình 2.3: Trích xuất nhúng cho các nút liên kết với cuốn sách chính trị {\tt "Losing Bin Laden"}. Đầu ra là 1 vector dày đặc được biểu diễn dưới dạng danh sách Python.}, nắm bắt các khía cạnh khác nhau về vai trò của cuốn sách trong mạng lưới các cuốn sách được mua chung, cung cấp 1 biểu diễn cô đọng, giàu thông tin, có thể được sử dụng để phân tích sâu hơn hoặc làm đầu vào cho các mô hình khác.

           These embeddings can be used for exploratory data analysis to see patterns \& relationships in a graph. However, their usage extends further. 1 common application is to use these vectors as features in a ML problem that uses tabular data. In that case, each element in our embedding array will become a distinct feature column in tabular data. This can add a rich representation of each node to complement other attributes in model training. In next sect, look at how to visualize these embeddings to gain deeper insights into patterns \& relationships they represent.

           -- Các nhúng này có thể được sử dụng để phân tích dữ liệu thăm dò nhằm xem xét các mẫu \& mối quan hệ trong biểu đồ. Tuy nhiên, ứng dụng của chúng còn mở rộng hơn nữa. 1 ứng dụng phổ biến là sử dụng các vectơ này làm đặc trưng trong bài toán học máy sử dụng dữ liệu bảng. Trong trường hợp đó, mỗi phần tử trong mảng nhúng của chúng ta sẽ trở thành 1 cột đặc trưng riêng biệt trong dữ liệu bảng. Điều này có thể bổ sung thêm 1 biểu diễn phong phú cho mỗi nút để bổ sung cho các thuộc tính khác trong quá trình huấn luyện mô hình. Trong phần tiếp theo, xem cách trực quan hóa các nhúng này để hiểu sâu hơn về các mẫu \& mối quan hệ mà chúng biểu diễn.
           \item {\sf2.1.3. Transforming \& visualizing embeddings.} Visualization methods e.g. Uniform Manifold Approximation \& Projection (UMAP) are powerful tools for reducing high-dimensional datasets into lower-dimensional space [5]. UMAP is particularly effective for identifying inherent clusters \& visualizing complex structures that are difficult to perceive in high-dimensional data. Compared to other methods, e.g. t-SNE, UMAP excels in preserving both local \& global structures, making it ideal for revealing patterns \& relationships across different scales in data.

           -- {\sf Biến đổi \& trực quan hóa nhúng.} Các phương pháp trực quan hóa, e.g. Xấp xỉ Đa tạp Đồng nhất \& Chiếu (UMAP) là những công cụ mạnh mẽ để thu gọn các tập dữ liệu nhiều chiều thành không gian ít chiều hơn [5]. UMAP đặc biệt hiệu quả trong việc xác định các cụm cố hữu \& trực quan hóa các cấu trúc phức tạp khó nhận biết trong dữ liệu nhiều chiều. So với các phương pháp khác, e.g. t-SNE, UMAP vượt trội trong việc bảo toàn cả cấu trúc cục bộ \& toàn cục, khiến nó trở nên lý tưởng để khám phá các mẫu \& mối quan hệ trên các quy mô khác nhau trong dữ liệu.

           While N2V generates embeddings by capturing network structure of our data, UMAP takes these high-dimensional embeddings \& maps them onto a lower-dimensional space (typically 2D or 3D). This mapping aims to keep similar nodes close together while also preserving broader structural relationships, providing a more comprehensive visualization of graph's topology. After obtaining our N2V embeddings \& converting them into a numerical array, we initialize UMAP model with 2 components to project our data onto a 2D plane. By carefully selecting parameters e.g. number of neighbors \& minimum distance. UMAP can balance between revealing fine-grained local relationships \& maintaining global distances between clusters.

           -- Trong khi N2V tạo ra các nhúng bằng cách nắm bắt cấu trúc mạng của dữ liệu, UMAP sử dụng các nhúng đa chiều này \& ánh xạ chúng vào không gian đa chiều thấp hơn (thường là 2D hoặc 3D). Việc ánh xạ này nhằm mục đích giữ các nút tương tự gần nhau đồng thời bảo toàn các mối quan hệ cấu trúc rộng hơn, cung cấp hình ảnh trực quan toàn diện hơn về cấu trúc đồ thị. Sau khi thu thập các nhúng N2V \& chuyển đổi chúng thành 1 mảng số, chúng tôi khởi tạo mô hình UMAP với 2 thành phần để chiếu dữ liệu lên mặt phẳng 2D. Bằng cách lựa chọn cẩn thận các tham số, e.g.: số lượng lân cận \& khoảng cách tối thiểu, UMAP có thể cân bằng giữa việc thể hiện các mối quan hệ cục bộ chi tiết \& duy trì khoảng cách toàn cục giữa các cụm.

           By using UMAP, gain a more accurate \& interpretable visualization of our graph embeddings as shown in {\sf Listing 2.2. Visualizing embeddings using UMAP: 1. Transforms embeddings into a list of vectors for UMAP. 2. Initializes \& fits UMAP. 3. Plots nodes with UMAP embeddings \& color by their value.} allowing us to explore \& analyze patterns, clusters, \& structures more effectively than with traditional methods e.g. t-SNE.

           -- Bằng cách sử dụng UMAP, bạn có thể có được hình ảnh trực quan chính xác hơn \& dễ hiểu hơn về các nhúng đồ thị của mình như được hiển thị trong {\sf Liệt kê 2.2. Hình ảnh trực quan các nhúng bằng UMAP: 1. Chuyển đổi các nhúng thành danh sách các vectơ cho UMAP. 2. Khởi tạo \& phù hợp với UMAP. 3. Vẽ các nút bằng các nhúng UMAP \& tô màu theo giá trị của chúng.} cho phép chúng ta khám phá \& phân tích các mẫu, cụm, \& cấu trúc hiệu quả hơn so với các phương pháp truyền thống, e.g.: t-SNE.

           Resultant {\sf Fig. 2.4: Embeddings of Political Books dataset graph generated by N2V \& visualized using UMAP. Shape \& shading variations distinguish 3 political classes.} encapsulates political book graph's embeddings as distilled by N2V \& subsequently visualized through UMAP. Nodes appear in different shades according to their political alignment. Visualization unfolds a discernible structure, with potential clusters that correspond to various political learnings.

           -- Kết quả {\sf Hình 2.4: Đồ thị nhúng của tập dữ liệu Sách Chính trị được tạo bởi N2V \& trực quan hóa bằng UMAP. Hình dạng \& các biến thể tô bóng phân biệt 3 lớp chính trị.} đóng gói các nhúng của đồ thị sách chính trị được tinh lọc bởi N2V \& sau đó được trực quan hóa bằng UMAP. Các nút xuất hiện với các sắc thái khác nhau tùy theo sự liên kết chính trị của chúng. Trực quan hóa mở ra 1 cấu trúc rõ ràng, với các cụm tiềm năng tương ứng với các bài học chính trị khác nhau.

           Might wonder why don't just reduce dimensions of N2V embeddings from 64 to 2 \& visualize them directly, by passing UMAP altogether? In {\sf Listing 2.3: Visualizing 2D N2V embeddings without t-SNE}, show this approach, applying a 2D N2V transformation directly to our \verb|book_graph| object.

           -- Có thể bạn đang thắc mắc tại sao không giảm kích thước của các nhúng N2V từ 64 xuống còn 2 \& trực quan hóa chúng trực tiếp bằng cách truyền hoàn toàn UMAP? Trong {\sf Liệt kê 2.3: Trực quan hóa các nhúng N2V 2D mà không cần t-SNE}, trình bày cách tiếp cận này, áp dụng phép biến đổi N2V 2D trực tiếp vào đối tượng \verb|book_graph| của chúng ta.

           {\tt dimensions} parameter is set to 2, aiming for a direct 2D representation  suitable for immediate visualization without further dimensionality reduction. Other parameters are kept the same.

           -- Tham số {\tt dimensions} được đặt thành 2, hướng đến biểu diễn 2D trực tiếp, phù hợp để trực quan hóa ngay lập tức mà không cần giảm kích thước thêm. Các tham số khác được giữ nguyên.

           Once model is fitted with specified window \& word batch settings, extract 2D embeddings \& store them in a dictionary keyed by string representation of each node. This enables a direct mapping from node to its embedding vector.

           -- Sau khi mô hình được điều chỉnh với các thiết lập hàng loạt cửa sổ \& từ được chỉ định, trích xuất các nhúng 2D \& lưu trữ chúng trong 1 từ điển được khóa bằng chuỗi biểu diễn của mỗi nút. Điều này cho phép ánh xạ trực tiếp từ nút đến vectơ nhúng của nó.

           Extracted 2D points are compiled into a NumPy array \& plotted. Use standard Matplotlib library to create a scatterplot of these points using prepared color scheme to represent political leaning of each node visually.

           -- Các điểm 2D được trích xuất sẽ được biên dịch thành 1 mảng NumPy \& vẽ đồ thị. Sử dụng thư viện Matplotlib chuẩn để tạo biểu đồ phân tán các điểm này bằng cách sử dụng bảng màu đã chuẩn bị sẵn để thể hiện trực quan khuynh hướng chính trị của từng nút.

           Outcome shows how books are separated by political leanings, similar to UMAP result, but where books are more bunched together {\sf Fig. 2.5: Embeddings of Political Books dataset graph generated \& visualized by N2V for 2D. Shape \& shading variations distinguish 3 political classes. Here, see a similar clustering by political leaning as earlier in Fig. 2.5 but more bunched together.} 2 embeddings are then shown in {\sf Fig. 2.6: Comparison of embeddings generated by N2V \& t-SNE \& a direct visualization of 2D Node2Vec.}

           -- Kết quả cho thấy sách được phân loại theo khuynh hướng chính trị, tương tự như kết quả UMAP, nhưng sách được gom lại với nhau nhiều hơn {\sf Hình 2.5: Đồ thị nhúng của tập dữ liệu Sách Chính trị được tạo \& trực quan hóa bằng N2V cho 2D. Các biến thể hình \& tô bóng phân biệt 3 lớp chính trị. Ở đây, xem 1 phân cụm tương tự theo khuynh hướng chính trị như trước đó trong Hình 2.5 nhưng được gom lại với nhau nhiều hơn.} 2 nhúng sau đó được hiển thị trong {\sf Hình 2.6: So sánh các nhúng được tạo bởi N2V \& t-SNE \& trực quan hóa trực tiếp của Node2Vec 2D.}

           Clear: both methods know to separate books into groups based on political leanings. N2V is less expressive in how it separates books, bunching them together across 2D. Meanwhile, UMAP is better for spreading out books in 2D. Relevant benefit or information contained within these dimensions depends on task at hand.

           -- Rõ ràng: cả hai phương pháp đều biết cách phân loại sách thành các nhóm dựa trên khuynh hướng chính trị. N2V kém biểu cảm hơn trong cách phân loại sách, gom chúng lại với nhau trên không gian 2 chiều. Trong khi đó, UMAP tốt hơn trong việc phân bổ sách trên không gian 2 chiều. Lợi ích hoặc thông tin liên quan chứa trong các chiều này phụ thuộc vào nhiệm vụ được giao.
           \item {\sf2.1.4. Beyond visualization: Applications \& considerations of N2V embeddings.} While visualizing N2V embeddings offers intuitive insights into dataset's structure, their usage extends far beyond graphical representation. N2V is an embedding method designed specifically for graphs; it captures both local \& global structural properties of nodes by simulating random walks through graph. This process allows N2V to create dense, numerical vectors that summarize position \& context of each node within overall network.

           -- {\sf Vượt ra ngoài trực quan hóa: Ứng dụng \& cân nhắc về nhúng N2V.} Mặc dù trực quan hóa nhúng N2V mang lại cái nhìn sâu sắc trực quan về cấu trúc của tập dữ liệu, nhưng ứng dụng của chúng còn vượt xa việc biểu diễn đồ họa. N2V là 1 phương pháp nhúng được thiết kế riêng cho đồ thị; nó nắm bắt cả các thuộc tính cấu trúc cục bộ \& toàn cục của các nút bằng cách mô phỏng các bước ngẫu nhiên qua đồ thị. Quá trình này cho phép N2V tạo ra các vectơ số dày đặc, tóm tắt vị trí \& ngữ cảnh của mỗi nút trong toàn bộ mạng.

           These embeddings can then serve as feature-rich inputs for a variety of ML tasks, e.g. classification, recommendation, or clustering. E.g. in our Political Books dataset, embeddings could help predict a book's political learning based on its co-purchase patterns or could recommend books to users with similar political interests. They might even be used to forecast future sales based on content of a book.

           -- Những nhúng này sau đó có thể đóng vai trò là dữ liệu đầu vào giàu tính năng cho nhiều tác vụ ML, e.g. phân loại, đề xuất hoặc phân cụm. Ví dụ: trong tập dữ liệu Sách Chính trị của chúng tôi, các nhúng có thể giúp dự đoán khả năng học hỏi chính trị của 1 cuốn sách dựa trên các mô hình mua chung hoặc có thể đề xuất sách cho người dùng có cùng sở thích chính trị. Chúng thậm chí có thể được sử dụng để dự báo doanh số bán hàng trong tương lai dựa trên nội dung của 1 cuốn sách.

           However, important to understand nature of N2V's learning approach, which is {\it transductive}. Transductive learning is designed to work only with specific dataset it was trained on \& cannot generalize to new, unseen nodes without retraining model. This characteristic makes \fbox{N2V highly effective for static datasets} where all nodes \& edges are known up front but less suitable for dynamic settings where new data points or connections frequently appear. Essentially, N2V focuses on extracting detailed patterns \& relationships from existing graph rather than developing a model that can easily adapt to new data.

           -- Tuy nhiên, điều quan trọng là phải hiểu bản chất của phương pháp học tập của N2V, đó là {\it transductive}. Học tập transductive được thiết kế để chỉ hoạt động với tập dữ liệu cụ thể mà nó được huấn luyện \& không thể khái quát hóa sang các nút mới, chưa được biết đến mà không cần huấn luyện lại mô hình. Đặc điểm này khiến \fbox{N2V rất hiệu quả đối với các tập dữ liệu tĩnh}, trong đó tất cả các nút \& cạnh đều được biết trước nhưng ít phù hợp hơn với các thiết lập động, nơi các điểm dữ liệu hoặc kết nối mới thường xuyên xuất hiện. Về cơ bản, N2V tập trung vào việc trích xuất các mẫu \& mối quan hệ chi tiết từ đồ thị hiện có thay vì phát triển 1 mô hình có thể dễ dàng thích ứng với dữ liệu mới.

           While this transductive nature has its limitations, it also offer significant advantages. Because n2V uses full structure of graph during training, it can capture intricate relationships \& dependencies that might be missed by more generalized methods. This makes N2V particularly powerful for tasks where complete, fixed structure of data is known \& stable. However, to apply N2V effectively, it is crucial to ensure that graph data is represented in a way that captures all relevant features. In some cases, additional edges or nodes may need to be added to graph to fully represent underlying relationships.

           -- Mặc dù bản chất chuyển đổi này có những hạn chế, nhưng nó cũng mang lại những lợi thế đáng kể. Vì n2V sử dụng toàn bộ cấu trúc đồ thị trong quá trình huấn luyện, nó có thể nắm bắt được các mối quan hệ phức tạp \& các mối phụ thuộc mà các phương pháp tổng quát hơn có thể bỏ sót. Điều này làm cho N2V đặc biệt mạnh mẽ đối với các tác vụ mà cấu trúc dữ liệu hoàn chỉnh, cố định đã biết \& ổn định. Tuy nhiên, để áp dụng N2V hiệu quả, điều quan trọng là phải đảm bảo dữ liệu đồ thị được biểu diễn theo cách nắm bắt được tất cả các đặc điểm liên quan. Trong 1 số trường hợp, có thể cần thêm các cạnh hoặc nút bổ sung vào đồ thị để biểu diễn đầy đủ các mối quan hệ cơ bản.

           For those interested in a deeper understanding of transductive models \& how N2V's approach compares to other methods, further details are provided in Sect. 2.4.2, which explore tradeoffs between transductive \& inductive learning in greater depth [6, 7], helping you understand when each approach is most appropriate.

           -- Đối với những người quan tâm đến việc hiểu sâu hơn về các mô hình chuyển đổi \& cách tiếp cận của N2V so sánh với các phương pháp khác, các chi tiết bổ sung được cung cấp trong Phần 2.4.2, khám phá sự đánh đổi giữa học chuyển đổi \& quy nạp sâu hơn [6, 7], giúp bạn hiểu khi nào thì mỗi cách tiếp cận là phù hợp nhất.

           While N2V is effective for generating embeddings that capture structure of a fixed graph, real-world data often demands a more flexible \& generalizable approach. This need brings us to our 1st GNN architecture for creating node embeddings. Unlike N2V, which is a transductive method limited to specific nodes \& edges in training data, GNNs can learn in an {\it inductive} manner, i.e., GNNs are capable of generalizing to new, unseen nodes or edges without requiring retraining on entire graph.

           -- Mặc dù N2V hiệu quả trong việc tạo ra các phép nhúng nắm bắt cấu trúc của 1 đồ thị cố định, dữ liệu thực tế thường đòi hỏi 1 phương pháp linh hoạt hơn \& có thể khái quát hóa. Nhu cầu này đưa chúng ta đến kiến trúc GNN đầu tiên để tạo ra các phép nhúng nút. Không giống như N2V, vốn là phương pháp chuyển đổi giới hạn ở các nút \& cạnh cụ thể trong dữ liệu huấn luyện, GNN có thể học theo cách {\it quy nạp}, i.e., GNN có khả năng khái quát hóa sang các nút hoặc cạnh mới, chưa được biết đến mà không cần phải huấn luyện lại toàn bộ đồ thị.

           GNNs achieve this by not only understanding network's complex structure but also by incorporating node features \& relationships into learning process. This approach allows GNNs to adapt dynamically to changes in graph, making them well-suited for applications where data is continually evolving. Shift from N2V to GNNs represents a key transition from focusing on deep analysis within a static dataset to a broader applicability across diverse, evolving networks. This adaptability sets stage for a wider range of graph-based ML applications that require flexibility \& scalability. In next sect, explore how GNNs go beyond capabilities of N2V \& other transductive methods, allowing for ore versatile \& powerful models that can \fbox{handle dynamic nature of real-world data}.

           -- GNN đạt được điều này không chỉ bằng cách hiểu cấu trúc phức tạp của mạng mà còn bằng cách kết hợp các đặc điểm nút \& mối quan hệ vào quá trình học. Cách tiếp cận này cho phép GNN thích ứng linh hoạt với những thay đổi trong đồ thị, khiến chúng phù hợp với các ứng dụng mà dữ liệu liên tục phát triển. Sự chuyển đổi từ N2V sang GNN thể hiện 1 bước chuyển đổi quan trọng từ việc tập trung vào phân tích sâu trong 1 tập dữ liệu tĩnh sang khả năng ứng dụng rộng rãi hơn trên các mạng lưới đa dạng \& đang phát triển. Khả năng thích ứng này đặt nền tảng cho 1 loạt các ứng dụng học máy dựa trên đồ thị đòi hỏi tính linh hoạt \& khả năng mở rộng. Trong phần tiếp theo, khám phá cách GNN vượt ra ngoài khả năng của N2V \& các phương pháp chuyển đổi khác, cho phép tạo ra các mô hình linh hoạt \& mạnh mẽ hơn có thể \fbox{xử lý bản chất động của dữ liệu thực tế}.
       \end{itemize}
       \item {\sf2.2. Creating embeddings with a GNN.} While N2V provides a powerful method for generating embeddings by capturing local \& global structure of a graph, it is fundamentally a transductive approach, i.e., it cannot easily generalize to unseen nodes or edges without retraining. Although there have been extensions to N2V that enable it to work in inductive settings, GNNs are inherently designed for inductive learning. I.e., they can learn general patterns from graph data that allow them to make predictions or to generate embeddings for new nodes or edges without need to retrain entire model. This gives GNNs a significant edge in scenarios where flexibility \& adaptability are crucial.

       -- {\sf Tạo nhúng với GNN.} Mặc dù N2V cung cấp 1 phương pháp mạnh mẽ để tạo nhúng bằng cách nắm bắt cấu trúc cục bộ \& toàn cục của đồ thị, nhưng về cơ bản, nó là 1 phương pháp chuyển đổi, i.e., nó không thể dễ dàng khái quát hóa sang các nút hoặc cạnh chưa biết mà không cần đào tạo lại. Mặc dù đã có các phần mở rộng cho N2V cho phép nó hoạt động trong các thiết lập quy nạp, GNN vốn được thiết kế cho việc học quy nạp. Tức là, chúng có thể học các mẫu chung từ dữ liệu đồ thị, cho phép chúng đưa ra dự đoán hoặc tạo nhúng cho các nút hoặc cạnh mới mà không cần đào tạo lại toàn bộ mô hình. Điều này mang lại cho GNN 1 lợi thế đáng kể trong các tình huống mà tính linh hoạt \& khả năng thích ứng là rất quan trọng.

       GNNs not only incorporate structural information of graph, like N2V, but they also use node features to create richer representations. This dual capacity allows GNNs to learn both complex relationships within graph \& specific characteristics of individual nodes, enabling them to excel in tasks where both types of information are important.

       -- GNN không chỉ kết hợp thông tin cấu trúc của đồ thị, như N2V, mà còn sử dụng các đặc điểm của nút để tạo ra các biểu diễn phong phú hơn. Khả năng kép này cho phép GNN học cả các mối quan hệ phức tạp trong đồ thị \& các đặc điểm cụ thể của từng nút, cho phép chúng vượt trội trong các tác vụ mà cả hai loại thông tin đều quan trọng.

       That said, while GNNs have demonstrated impressive performance across many applications, they do not universally outperform methods e.g. N2V in all cases. E.g., N2V \& other random walk-based methods can sometimes perform better in scenarios where labeled data is scarce or noisy, thanks to their ability to work with just graph structure without needing additional node features.

       -- Tuy nhiên, mặc dù GNN đã chứng minh hiệu suất ấn tượng trong nhiều ứng dụng, chúng không phải lúc nào cũng vượt trội hơn các phương pháp như N2V trong mọi trường hợp. E.g., N2V \& các phương pháp dựa trên bước đi ngẫu nhiên khác đôi khi có thể hoạt động tốt hơn trong các tình huống dữ liệu được gắn nhãn khan hiếm hoặc nhiễu, nhờ khả năng hoạt động chỉ với cấu trúc đồ thị mà không cần thêm các đặc trưng nút.
       \begin{itemize}
           \item {\sf2.2.1. Constructing embeddings.} Unlike N2V, GNNs learn graph representations \& perform tasks e.g. node classification or link prediction simultaneously during training. Information from entire is processed through successive GNN layers, each refining node embeddings without requiring a separate step for their creation.

           -- {\sf Xây dựng nhúng.} Không giống như N2V, GNN học các biểu diễn đồ thị \& thực hiện đồng thời các tác vụ như phân loại nút hoặc dự đoán liên kết trong quá trình huấn luyện. Thông tin từ toàn bộ được xử lý qua các lớp GNN kế tiếp, mỗi lớp tinh chỉnh các nhúng nút mà không cần 1 bước riêng biệt để tạo chúng.

           To demonstrate how a GNN extracts features from graph data, perform a straightforward pass-through using an untrained model to generate preliminary embeddings. Even without optimization typically involved in training, this approach will show how GNNs use message passing to update embeddings, capturing both graph's structure \& its node features. When optimization is added, these embeddings become tailored to specific tasks e.g. node classification or link prediction.

           -- Để minh họa cách GNN trích xuất các đặc điểm từ dữ liệu đồ thị, thực hiện 1 phép truyền trực tiếp đơn giản bằng 1 mô hình chưa được huấn luyện để tạo ra các nhúng sơ bộ. Ngay cả khi không có quá trình tối ưu hóa thường được sử dụng trong huấn luyện, phương pháp này vẫn sẽ cho thấy cách GNN sử dụng việc truyền thông điệp để cập nhật các nhúng, nắm bắt cả cấu trúc của đồ thị \ \& các đặc điểm nút của nó. Khi được tối ưu hóa, các nhúng này sẽ được điều chỉnh cho phù hợp với các tác vụ cụ thể, e.g. phân loại nút hoặc dự đoán liên kết.
           \begin{itemize}
               \item {\sf Defining our GNN architecture.} Initiate our process by defining a siple GCN architecture, as shown in {\sf Listing 2.4: {\tt SimpleGNN} class}. Our {\tt SimpleGNN} class inherits from {\tt torch.nn.Module} \& is composed of 2 {\tt GCN-Conv} layers, which are building blocks of our GNN. This architecture is shown in {\sf Fig. 2.7: Architecture diagram of {\tt SimpleGNN} model}, consisting of 1st layer, a message passing layer {\tt self.conv1}, an activation {\tt torch.relu}, a dropout layer {\tt torch.dropout}, \& a 2nd message passing layer.

               -- {\sf Định nghĩa kiến trúc GNN của chúng ta.} Bắt đầu quy trình bằng cách định nghĩa 1 kiến trúc GCN đơn, như được hiển thị trong {\sf Liệt kê 2.4: {\tt SimpleGNN} class}. Lớp {\tt SimpleGNN} của chúng ta kế thừa từ {\tt torch.nn.Module} \& bao gồm 2 lớp {\tt GCN-Conv}, là các khối xây dựng nên GNN của chúng ta. Kiến trúc này được hiển thị trong {\sf Hình 2.7: Sơ đồ kiến trúc của mô hình {\tt SimpleGNN}}, bao gồm lớp thứ nhất, lớp truyền tin nhắn {\tt self.conv1}, lớp kích hoạt {\tt torch.relu}, lớp dropout {\tt torch.dropout}, \& lớp truyền tin nhắn thứ hai.

               Talk about architecture aspects specific to GNNs. Activation \& dropout are common in many DL scenarios. GNN layers, however, are different from conventional DL layers in a fundamental way. Core principle that allows GNNs to learn from graph data is message passing. For each GNN layer, in addition to updating layer's weights, a ``message'' is gathered from every node or edge neighborhood \& used to update an embedding. Essentially, each node sends messages to its neighbors \& simultaneously receives messages from them. For every node, its new embedding is computed by combining its own features with aggregated messages from its neighbors, through a combination of nonlinear transformations.

               -- Nói về các khía cạnh kiến trúc đặc thù của GNN. Activation \& dropout là phổ biến trong nhiều kịch bản DL. Tuy nhiên, các lớp GNN khác biệt cơ bản so với các lớp DL thông thường. Nguyên lý cốt lõi cho phép GNN học từ dữ liệu đồ thị là truyền thông điệp. Đối với mỗi lớp GNN, ngoài việc cập nhật trọng số của lớp, 1 ``thông điệp'' được thu thập từ mọi nút hoặc lân cận cạnh \& được sử dụng để cập nhật nhúng. Về cơ bản, mỗi nút gửi thông điệp đến các nút lân cận \& đồng thời nhận thông điệp từ chúng. Đối với mỗi nút, nhúng mới của nó được tính toán bằng cách kết hợp các đặc trưng của chính nó với các thông điệp tổng hợp từ các nút lân cận, thông qua sự kết hợp của các phép biến đổi phi tuyến tính.

               In this example, we are going to be using a graph convolutional network (GCN) to act as our message-passing GNN layers. Describe GCNs in much more detail i Chap. 3. For now, just need to know that GCNs act as message-passing layers that are critical in constructing embeddings.

               -- Trong ví dụ này, chúng ta sẽ sử dụng mạng tích chập đồ thị (GCN) để hoạt động như các lớp GNN truyền thông điệp. Mô tả chi tiết hơn về GCN trong Chương 3. Hiện tại, chúng ta chỉ cần biết rằng GCN hoạt động như các lớp truyền thông điệp, đóng vai trò quan trọng trong việc xây dựng các hàm nhúng.
               \item {\sf Data preparation.} Next, prepare our data. Start with same graph from previous section, \verb|books_gml|, in its {\tt NetworkX} form. Have to convert this {\tt NetworkX} object into a tensor form that is suitable to use with PyTorch operations. Because PyTorch Geometric (PyG) has many functions that convert graph objects, we can do this quite simply with \verb|data = from_NetworkX(gml_graph)|. Method \verb|from_NetworkX| specifically translates edge lists \& node{\tt.}edge attributes into PyTorch tensors.

               -- {\sf Chuẩn bị dữ liệu.} Tiếp theo, chuẩn bị dữ liệu. Bắt đầu với cùng 1 đồ thị từ phần trước, \verb|books_gml|, ở dạng {\tt NetworkX}. Phải chuyển đổi đối tượng {\tt NetworkX} này sang dạng tensor phù hợp để sử dụng với các phép toán PyTorch. Vì PyTorch Geometric (PyG) có nhiều hàm chuyển đổi đối tượng đồ thị, chúng ta có thể thực hiện việc này khá đơn giản với \verb|data = from_NetworkX(gml_graph)|. Phương thức \verb|from_NetworkX| đặc biệt chuyển đổi các thuộc tính danh sách cạnh \& node{\tt.}edge thành tensor PyTorch.

               For GNNs, generating node embeddings requires initializing node features. In our case, we do not have any predefined node features. When no node features are available or they are not informative, it is common practice to initialize node features randomly. A more effective approach: use {\it Xavier initialization}, which sets initial node features with values drawn from a distribution that keeps variety of activations consistent across layers. This technique ensures: model starts with a balanced representation, preventing problems e.g. vanishing or exploding gradients.

               -- Đối với GNN, việc tạo nhúng nút đòi hỏi phải khởi tạo các đặc trưng nút. Trong trường hợp của chúng tôi, chúng tôi không có bất kỳ đặc trưng nút nào được xác định trước. Khi không có đặc trưng nút nào khả dụng hoặc chúng không cung cấp thông tin, việc khởi tạo các đặc trưng nút 1 cách ngẫu nhiên là 1 phương pháp phổ biến. 1 cách tiếp cận hiệu quả hơn: sử dụng {\it Xavier initialization}, phương pháp này đặt các đặc trưng nút ban đầu với các giá trị được lấy từ 1 phân phối duy trì tính nhất quán của các phép kích hoạt trên các lớp. Kỹ thuật này đảm bảo: mô hình bắt đầu với 1 biểu diễn cân bằng, ngăn ngừa các vấn đề như gradient biến mất hoặc bùng nổ.

               By initializing {\tt data.x} with Xavier initialization, provide GNN with a starting point that allows it to learn meaningful node embeddings from noninformative features. During training, network adjusts these initial values to minimize loss function. When loss function is aligned with a specific target, e.g. node prediction, embeddings learned from initial random features will become tailored to task at hand, resulting in more effective representations. Randomize node features using following:

               -- Khởi tạo, cung cấp cho GNN 1 điểm khởi đầu cho phép nó học các phép nhúng nút có ý nghĩa từ các đặc trưng không mang tính thông tin. Trong quá trình huấn luyện, mạng sẽ điều chỉnh các giá trị ban đầu này để giảm thiểu hàm mất mát. Khi hàm mất mát được căn chỉnh với 1 mục tiêu cụ thể, e.g.: dự đoán nút, các phép nhúng học được từ các đặc trưng ngẫu nhiên ban đầu sẽ được điều chỉnh cho phù hợp với tác vụ hiện tại, mang lại hiệu quả biểu diễn cao hơn. Ngẫu nhiên hóa các đặc trưng nút bằng cách sử dụng các bước sau:
               \begin{Verbatim}[numbers=left,xleftmargin=5mm]
data.x = torch.randn((data.num_nodes, 64), dtype = torch.float)
'nn.init.xavier_uniform_(data.x) '
               \end{Verbatim}
               We could have also used embeddings from N2V exercise to use as node features. Recall \verb|node_embeddings| object from Sect. 2.1.3:

               -- Chúng ta cũng có thể sử dụng các nhúng từ bài tập N2V để làm đặc trưng nút. Hãy nhớ lại đối tượng \verb|node_embeddings| từ Mục 2.1.3:
               \begin{verbatim}
node_embeddings = [embeddings[str(node)] for node in gml_graph.nodes()]
               \end{verbatim}
               From this, can convert node embedding to a PyTorch tensor object \& assign it to node feature object, {\tt data.x}:

               -- Từ đó, có thể chuyển đổi nhúng nút thành đối tượng tenxơ PyTorch \& gán nó cho đối tượng tính năng nút, {\tt data.x}:
               \begin{verbatim}
node_features = torch.tensor(node_embedding, dtype = torch.float)
data.x = node_features
               \end{verbatim}
               \item {\sf Passing graph through GNN.} With structure of our GNN model defined \& our graph data formatted for PyG, proceed to embedding generation step. Initialize our model, {\tt SimpleGNN}, specifying number of features for each node \& size of hidden channels within network.

               -- {\sf Truyền đồ thị qua GNN.} Với cấu trúc mô hình GNN đã được xác định \& dữ liệu đồ thị được định dạng cho PyG, tiến hành bước tạo nhúng. Khởi tạo mô hình {\tt SimpleGNN}, chỉ định số lượng đặc trưng cho mỗi nút \& kích thước của các kênh ẩn trong mạng.
               \begin{verbatim}
model = SimpleGNN(num_features = data.x.shape[1], hidden_channels = 64)
               \end{verbatim}
               Here, specify 64 hidden channels because we want to compare resulting embeddings to the ones we produced using {\tt node2vec} method, which had 64 dimensions. Because 2nd GNN layer is last layer, output will be a 64-element vector.

               -- Ở đây, ta chỉ định 64 kênh ẩn vì chúng ta muốn so sánh các nhúng kết quả với các nhúng chúng ta tạo ra bằng phương pháp {\tt node2vec}, có 64 chiều. Vì lớp GNN thứ 2 là lớp cuối cùng, đầu ra sẽ là 1 vectơ 64 phần tử.

               Once initialized, we switch model to evaluation mode using {\tt model.eval()}. This mode is used during inference or validation phases when we want to make predictions or assess model performance without modifying model's parameters. Specifically, {\tt model.eval()} turns off certain behaviors specific to training, e.g. {\it dropout}, which randomly deactivates some neurons to prevent overfitting, \& {\it batch normalization}, which normalizes inputs across a mini-batch. By disabling these features, model provides consistent \& deterministic outputs, ensuring: evaluation accurately reflects its true performance on unseen data.

               -- Sau khi khởi tạo, chúng tôi chuyển mô hình sang chế độ đánh giá bằng {\tt model.eval()}. Chế độ này được sử dụng trong các giai đoạn suy luận hoặc xác thực khi chúng tôi muốn đưa ra dự đoán hoặc đánh giá hiệu suất mô hình mà không cần sửa đổi các tham số của mô hình. Cụ thể, {\tt model.eval()} sẽ tắt 1 số hành vi cụ thể trong quá trình huấn luyện, e.g.: {\tt dropout}, vô hiệu hóa ngẫu nhiên 1 số nơ-ron để ngăn ngừa quá khớp, \& {\tt batch normalization}, chuẩn hóa đầu vào trên 1 mini-batch. Bằng cách vô hiệu hóa các tính năng này, mô hình cung cấp đầu ra nhất quán \& xác định, đảm bảo: đánh giá phản ánh chính xác hiệu suất thực tế của nó trên dữ liệu chưa được biết đến.

               Important to disable gradient computations because they are not necessary for forward pass \& embedding generation. So, we employ \verb|torch.no_grad()|, which ensures: computational graph that records operations for backpropagation is not constructed, preventing us from accidentally changing performance.

               -- Điều quan trọng là phải tắt tính toán gradient vì chúng không cần thiết cho việc tạo nhúng \& truyền tiếp. Vì vậy, chúng tôi sử dụng \verb|torch.no_grad()|, đảm bảo: đồ thị tính toán ghi lại các thao tác cho lan truyền ngược không được xây dựng, ngăn chúng tôi vô tình thay đổi hiệu suất.

               Next, pass our node-feature matrix {\tt data.x} \& edge index \verb|data.edge_index| through model. Result is \verb|gnn_embeddings|, a tensor where each row corresponds to embedding of a node in our graph -- a numerical representation learned by our GNN, ready for downstream tasks e.g. visualization or classification:

               -- Tiếp theo, truyền ma trận đặc trưng nút {\tt data.x} \& chỉ số cạnh \verb|data.edge_index| qua mô hình. Kết quả là \verb|gnn_embeddings|, 1 tenxơ trong đó mỗi hàng tương ứng với việc nhúng 1 nút vào đồ thị của chúng ta -- 1 biểu diễn số được học bởi GNN, sẵn sàng cho các tác vụ tiếp theo, e.g. trực quan hóa hoặc phân loại:
               \begin{Verbatim}
model.eval()
with torch.no_grad():
    gnn_embeddings = model(data.x, data.edge_index)
               \end{Verbatim}
               After producing these embeddings, use UMAP to visualize them. Since we have been working with PyTorch tensor data types running on a GPU, need to convert our embeddings to a NumPy array data type to use analysis methods outside of PyTorch, which are done on a CPU:

               -- Sau khi tạo các nhúng này, sử dụng UMAP để trực quan hóa chúng. Vì chúng ta đang làm việc với các kiểu dữ liệu tensor PyTorch chạy trên GPU, cần chuyển đổi các nhúng sang kiểu dữ liệu mảng NumPy để sử dụng các phương pháp phân tích bên ngoài PyTorch, vốn được thực hiện trên CPU:
               \begin{verbatim}
gnn_embeddings_np = gnn_embeddings.detach().cpu().numpy()
               \end{verbatim}
               With this convention, we can produce UMAP calculations \& visualization following process we used in N2V case. Resulting scatterplot ({\sf Fig. 2.8: Visualization of embeddings generated from passing a graph through a GNN.}) is a 1st glimpse at clusters within our graph. Add different shadings based on each node's label (left-, right-, or neural-leaning) to see that similar leaning books are fairly well grouped, given that these embeddings were constructed from topology alone.

               -- Với quy ước này, chúng ta có thể tạo ra các phép tính UMAP \& trực quan hóa theo quy trình đã sử dụng trong trường hợp N2V. Biểu đồ phân tán kết quả ({\sf Hình 2.8: Trực quan hóa các nhúng được tạo ra khi truyền đồ thị qua mạng GNN.}) là cái nhìn đầu tiên về các cụm trong đồ thị của chúng ta. Hãy thêm các hiệu ứng tô bóng khác nhau dựa trên nhãn của mỗi nút (nghiêng trái, phải hoặc nghiêng nơ-ron) để thấy rằng các sách nghiêng tương tự được nhóm khá tốt, vì các nhúng này chỉ được xây dựng từ tôpô.

               Next, discuss both how GNN embeddings are used \& how they differ from those produced with N2V.

               -- Tiếp theo, thảo luận về cách sử dụng nhúng GNN \& cách chúng khác với nhúng được tạo bằng N2V như thế nào.
           \end{itemize}
           \item {\sf2.2.2. GNN vs. N2V embeddings.} Through this book, predominantly use GNNs to generate embeddings because this embedding process is intrinsic to a GNN's architecture. While embeddings play a pivotal role in methodologies \& applications we explore in rest of book, their presence is often subtle \& not always highlighted. This approach allows us to focus on broader concepts \& applications of GNN-based ML without getting slowed down by technicalities. Nonetheless, important to acknowledge: underlying power \& adaptability of embeddings are central to advanced techniques \& insights we get into throughout text.

           -- {\sf GNN so với nhúng N2V.} Trong cuốn sách này, chúng tôi chủ yếu sử dụng GNN để tạo nhúng vì quá trình nhúng này là 1 phần không thể thiếu trong kiến trúc của GNN. Mặc dù nhúng đóng vai trò then chốt trong các phương pháp luận \& ứng dụng mà chúng tôi sẽ đề cập trong phần còn lại của cuốn sách, nhưng sự hiện diện của chúng thường rất tinh tế \& không phải lúc nào cũng được nhấn mạnh. Cách tiếp cận này cho phép chúng tôi tập trung vào các khái niệm rộng hơn \& ứng dụng của ML dựa trên GNN mà không bị chậm lại bởi các vấn đề kỹ thuật. Tuy nhiên, điều quan trọng cần lưu ý là: sức mạnh tiềm ẩn \& khả năng thích ứng của nhúng là trọng tâm của các kỹ thuật tiên tiến \& những hiểu biết sâu sắc mà chúng tôi sẽ tìm hiểu trong suốt cuốn sách.

           GNN-produced node embeddings are particularly powerful because they enable us to tackle a broad range of graph-related tasks by using their inductive nature. Inductive learning allows these embeddings to generalize to new, unseen nodes or even entirely new graphs without needing to retrain model. In contrast, N2V embeddings are limited to specific graphs they were trained on \& can't easily adapt to new data. Reiterate key ways in which GNN embeddings differ from other embedding methods, e.g. N2V [1, 3].

           -- Nhúng nút do GNN tạo ra đặc biệt mạnh mẽ vì chúng cho phép chúng ta giải quyết 1 loạt các tác vụ liên quan đến đồ thị bằng cách sử dụng tính chất quy nạp của chúng. Học quy nạp cho phép các nhúng này tổng quát hóa sang các nút mới, chưa từng thấy hoặc thậm chí là các đồ thị hoàn toàn mới mà không cần phải đào tạo lại mô hình. Ngược lại, nhúng N2V bị giới hạn trong các đồ thị cụ thể mà chúng được đào tạo \& không thể dễ dàng thích ứng với dữ liệu mới. Nhắc lại những điểm chính mà nhúng GNN khác với các phương pháp nhúng khác, e.g.: N2V [1, 3].
           \begin{itemize}
               \item {\sf Adaptability of new graphs.} 1 of critical features of GNN embeddings is their adaptability. Because GNNs learn a function that maps node features to embeddings, this function can be applied to nodes in new graphs without needing to be retrained, provided nodes have similar feature spaces. This inductive capability is particularly valuable in dynamic environments where graph may evolve over time or in applications where model needs to be applied to different but structurally similar graphs. N2V, on other hand, needs to be reapplied for each new graph or set of nodes.

               -- {\sf Khả năng thích ứng của đồ thị mới.} 1 trong những đặc điểm quan trọng của nhúng GNN là khả năng thích ứng. Vì GNN học 1 hàm ánh xạ các đặc trưng của nút với các nhúng, hàm này có thể được áp dụng cho các nút trong đồ thị mới mà không cần phải đào tạo lại, miễn là các nút có không gian đặc trưng tương tự. Khả năng quy nạp này đặc biệt hữu ích trong các môi trường động, nơi đồ thị có thể phát triển theo thời gian hoặc trong các ứng dụng cần áp dụng mô hình cho các đồ thị khác nhau nhưng có cấu trúc tương tự. Mặt khác, N2V cần được áp dụng lại cho mỗi đồ thị hoặc tập hợp nút mới.
               \item {\sf Enhanced feature integration.} GNNs inherently consider node features during embedding process, allowing for a complex \& nuanced representation of each node. This integration of node features, alongside structural information, offers a more comprehensive view compared to N2V \& other methods that focus on a graph's topology. This capability makes GNN embeddings particularly suited for tasks where node features contain significant additional information.

               -- {\sf Tích hợp tính năng nâng cao.} GNN vốn đã xem xét các tính năng nút trong quá trình nhúng, cho phép biểu diễn phức tạp \& sắc thái của từng nút. Việc tích hợp các tính năng nút này, cùng với thông tin cấu trúc, mang lại cái nhìn toàn diện hơn so với các phương pháp N2V \& khác tập trung vào cấu trúc của đồ thị. Khả năng này làm cho nhúng GNN đặc biệt phù hợp cho các tác vụ mà các tính năng nút chứa thông tin bổ sung đáng kể.
               \item {\sf Task-specific optimization.} GNNs embeddings are trained alongside specific tasks, e.g. node classification, link prediction, or even graph classification. Through end-to-end training, GNN model learns to optimize embeddings for task at hand, leading to potentially higher performance \& efficiency compared to using pre-generated embeddings e.g. those from N2V.

               -- {\sf Tối ưu hóa theo tác vụ.} Các nhúng GNN được huấn luyện cùng với các tác vụ cụ thể, e.g.: phân loại nút, dự đoán liên kết, hoặc thậm chí phân loại đồ thị. Thông qua quá trình huấn luyện đầu cuối, mô hình GNN học cách tối ưu hóa các nhúng cho tác vụ đang thực hiện, dẫn đến hiệu suất \& hiệu quả cao hơn so với việc sử dụng các nhúng được tạo sẵn, e.g. từ N2V.

               That said, while GNN embeddings offer clear advantages in terms of adaptability \& applicability to new data, N2V embeddings have their strengths, particularly in capturing nuanced patterns within a specific graph's structure. In practice, choice between GNN \& N2V embeddings may depend on specific requirements of task, nature of graph data, \& constraints of computational environment.

               -- Tuy nhiên, trong khi nhúng GNN mang lại những lợi thế rõ ràng về khả năng thích ứng \& khả năng ứng dụng cho dữ liệu mới, nhúng N2V cũng có những điểm mạnh riêng, đặc biệt là trong việc nắm bắt các mẫu hình phức tạp trong cấu trúc đồ thị cụ thể. Trên thực tế, việc lựa chọn giữa nhúng GNN \& N2V có thể phụ thuộc vào các yêu cầu cụ thể của tác vụ, bản chất của dữ liệu đồ thị, \& các ràng buộc của môi trường tính toán.

               For tasks where graph structure is static \& well-defined, N2V might provide a simpler \& computational efficient solution. Conversely, for dynamic graphs, large-scale applications, or scenarios requiring incorporation of node features, GNNs will often be the more robust \& versatile choice. Additionally, when task itself is not well-defined \& work is exploratory, N2V is likely faster \& easier to use.

               -- Đối với các tác vụ có cấu trúc đồ thị tĩnh \& được xác định rõ ràng, N2V có thể cung cấp 1 giải pháp đơn giản hơn \& hiệu quả về mặt tính toán. Ngược lại, đối với các đồ thị động, ứng dụng quy mô lớn hoặc các tình huống yêu cầu tích hợp các đặc điểm nút, GNN thường là lựa chọn mạnh mẽ \& linh hoạt hơn. Ngoài ra, khi bản thân tác vụ không được xác định rõ \& công việc mang tính khám phá, N2V có thể nhanh hơn \& dễ sử dụng hơn.

               Have now successfully built our 1st GNN embedding. This is key 1st step for all GNN models, \& everything from this point will build on it. In next sect, give an example of some of these next steps \& show how to use embeddings to solve a ML problem.

               -- Chúng ta đã xây dựng thành công mô hình nhúng GNN đầu tiên. Đây là bước đầu tiên quan trọng cho tất cả các mô hình GNN, \& mọi thứ từ đây sẽ được xây dựng dựa trên nó. Trong phần tiếp theo, đưa ra ví dụ về 1 số bước tiếp theo \& trình bày cách sử dụng nhúng để giải quyết bài toán học máy.
           \end{itemize}
       \end{itemize}
       \item {\sf2.3. Using Node Embeddings.} Semi-supervised learning, which involves a combination of labeled \& unlabeled data, provides a valuable opportunity to compare different embedding techniques. In this chap, explore how GNN \& N2V embeddings can be used to predict labels when majority of data lacks labels.

       -- {\sf Sử dụng Nhúng Nút.} Học bán giám sát, bao gồm sự kết hợp giữa dữ liệu có nhãn \& không có nhãn, mang đến 1 cơ hội quý giá để so sánh các kỹ thuật nhúng khác nhau. Trong chương này, khám phá cách sử dụng nhúng GNN \& N2V để dự đoán nhãn khi phần lớn dữ liệu thiếu nhãn.

       Our task involves Political Books dataset \verb|books_graph|, where nodes represent political books \& edges indicate co-purchase relationships. To make process clearer, review steps taken so far \& outline our next steps, as illustrated in {\sf Fig. 2.9: Overview of steps taken in Chap. 2: 1. Preprocess Political Books dataset for embedding. 2. Use N2V \& GCN to create embeddings from preprocessed data. 3. Prepare N2V embeddings \& GCN embeddings for semi-supervised classification. 4. Embeddings are used as features in a random forest classifier (tabular features) \& a GCN classifier (node features).}

       -- Nhiệm vụ của chúng tôi liên quan đến tập dữ liệu Sách Chính trị \verb|books_graph|, trong đó các nút biểu diễn các cuốn sách chính trị \& các cạnh biểu diễn mối quan hệ đồng mua. Để làm rõ quy trình, xem lại các bước đã thực hiện cho đến nay \& phác thảo các bước tiếp theo, như minh họa trong {\sf Hình 2.9: Tổng quan các bước đã thực hiện trong Chương 2: 1. Tiền xử lý tập dữ liệu Sách Chính trị để nhúng. 2. Sử dụng N2V \& GCN để tạo nhúng từ dữ liệu đã được xử lý trước. 3. Chuẩn bị nhúng N2V \& nhúng GCN cho phân loại bán giám sát. 4. Nhúng được sử dụng làm đặc trưng trong bộ phân loại rừng ngẫu nhiên (đặc trưng dạng bảng) \& bộ phân loại GCN (đặc trưng nút).}

       Began with \verb|books_graph| dataset in graph format \& performed light preprocessing to prepare data for embedding. For N2V, this involved converting dataset from a {\tt.gml} file to a {\tt NetworkX} format. For GNN-based embeddings, converted {\tt NetworkX} graph into a PyTorch tensor \& initialized node features using Xavier initialization to ensure balanced variability across layers.

       -- Bắt đầu với tập dữ liệu \verb|books_graph| ở định dạng đồ thị \& thực hiện tiền xử lý nhẹ để chuẩn bị dữ liệu cho nhúng. Đối với N2V, việc này bao gồm chuyển đổi tập dữ liệu từ tệp {\tt.gml} sang định dạng {\tt NetworkX}. Đối với nhúng dựa trên GNN, chuyển đổi đồ thị {\tt NetworkX} thành tensor PyTorch \& khởi tạo các đặc trưng nút bằng khởi tạo Xavier để đảm bảo tính biến thiên cân bằng giữa các lớp.

       After preparing data, we generated embeddings using both N2V \& GCNs. Now, in this sect, apply these embeddings to a semi-supervised classification problem. This involves further processing to define classification task, where only 20\% of book labels are retained, simulating a realistic scenario with sparse labeled data.

       -- Sau khi chuẩn bị dữ liệu, chúng tôi đã tạo ra các nhúng sử dụng cả N2V \& GCN. Trong phần này, chúng tôi áp dụng các nhúng này vào 1 bài toán phân loại bán giám sát. Điều này bao gồm việc xử lý thêm để xác định tác vụ phân loại, trong đó chỉ giữ lại 20\% nhãn sách, mô phỏng 1 kịch bản thực tế với dữ liệu được gắn nhãn thưa thớt.

       Use 2 sets of embeddings (N2V \& GCN) with 2 different classifiers: a random forest classifier (to use embeddings as tabular features) \& a GCN classifier (to use graph structure \& node features). Goal: predict political orientation of books, with remaining 80\% of labels inferred based on given embeddings.

       -- Sử dụng 2 bộ nhúng (N2V \& GCN) với 2 bộ phân loại khác nhau: 1 bộ phân loại rừng ngẫu nhiên (để sử dụng nhúng làm đặc trưng dạng bảng) \& 1 bộ phân loại GCN (để sử dụng cấu trúc đồ thị \& đặc trưng nút). Mục tiêu: dự đoán khuynh hướng chính trị của sách, với 80\% nhãn còn lại được suy ra dựa trên các nhúng đã cho.
       \begin{itemize}
           \item {\sf2.3.1. Data preprocessing.} To start, we do a little more preprocessing to our \verb|books_gml| dataset {\sf Listing 2.5: Preprocessing for semi-supervised problem}. Must format labels in a suitable way for learning process. Because all nodes are labeled, we also have to set up semi-supervised problem by randomly selecting nodes from which we hide labels.

           -- {\sf Tiền xử lý dữ liệu.} Để bắt đầu, chúng ta thực hiện thêm 1 chút tiền xử lý cho tập dữ liệu \verb|books_gml| {\sf Liệt kê 2.5: Tiền xử lý cho bài toán bán giám sát}. Phải định dạng nhãn theo cách phù hợp cho quá trình học. Vì tất cả các nút đều được gắn nhãn, chúng ta cũng phải thiết lập bài toán bán giám sát bằng cách chọn ngẫu nhiên các nút mà chúng ta ẩn nhãn.

           Nodes associated with attribute {\tt'c'} are classified as {\tt'right'}, while those with {\tt'l'} are classified as {\tt'left'}. Nodes that do not fit these criteria, including those with neutral or unspecified attributes, are categorized as {\tt'neutral'}. These
       \end{itemize}
       \item {\sf2.4. Under Hood.}
    \end{itemize}

    PART 3: GRAPH NEURAL NETWORKS GNNs
    \item {\sf3. Graph convolutional networks \& GraphSAGE.}
    \item {\sf4. Graph attention networks.}
    \item {\sf5. Graph autoencoders.}

    PART 3: ADVANCED TOPICS.
    \item {\sf6. Dynamic graphs: Spatiotemporal GNNs.}
    \item {\sf7. Learning \& inference at scale.}
    \item {\sf8. Considerations for GNN projects.}
    \item {\sf A. Discovering graphs.}
    \item {\sf B. Installing \& configuring PyTorch Geometric.}

\end{itemize}

%------------------------------------------------------------------------------%

\subsection{{\sc Maxime Labonne}. Hands-On Graph Neural Networks Using Python: Practical Techniques \& Architectures for Building Powerful Graph \& DL Apps with PyTorch. 2023}

\begin{itemize}
    \item {\sf Preface.} In just 10 years, GNNs have become an essential \& popular DL architecture. They have already had a significant impact various industries, e.g. in drug discovery, where GNNs predicted a new antibiotic, named halicin, \& have improved estimated time of arrival calculations on Google Maps. Tech companies \& universities are exploring potential of GNNs in various applications, including recommender systems, fake news detection, \& chip design. GNNs have enormous potential \& many yet-to-be-discovered applications, making them a critical tool for solving global problems.

    -- Chỉ trong 10 năm, GNN đã trở thành 1 kiến trúc DL thiết yếu \& phổ biến. Chúng đã có tác động đáng kể đến nhiều ngành công nghiệp, e.g. trong lĩnh vực khám phá thuốc, nơi GNN dự đoán 1 loại kháng sinh mới, tên là halicin, \& đã cải thiện khả năng tính toán thời gian đến ước tính trên Google Maps. Các công ty công nghệ \& các trường đại học đang khám phá tiềm năng của GNN trong nhiều ứng dụng khác nhau, bao gồm hệ thống đề xuất, phát hiện tin giả, \& thiết kế chip. GNN có tiềm năng to lớn \& nhiều ứng dụng chưa được khám phá, khiến chúng trở thành 1 công cụ quan trọng để giải quyết các vấn đề toàn cầu.

    In this book, aim to provide a comprehensive \& practical overview of world of GNNs. Begin by exploring fundamental concepts of graph theory \& graph learning \& then delve into most widely used \& well-established GNN architectures. As we progress, also cover latest advances in GNNs \& introduce specialized architectures that are designed to tackle specific tasks, e.g. graph generation, link prediction, \& more.

    -- Trong cuốn sách này, chúng tôi đặt mục tiêu cung cấp 1 cái nhìn tổng quan toàn diện \& thực tiễn về thế giới GNN. Bắt đầu bằng việc khám phá các khái niệm cơ bản của lý thuyết đồ thị \& học đồ thị \& sau đó đi sâu vào các kiến trúc GNN được sử dụng rộng rãi nhất \& đã được thiết lập tốt. Khi chúng ta tiến triển, chúng tôi cũng sẽ đề cập đến những tiến bộ mới nhất trong GNN \& giới thiệu các kiến trúc chuyên biệt được thiết kế để giải quyết các nhiệm vụ cụ thể, e.g.: tạo đồ thị, dự đoán liên kết, \& nhiều hơn nữa.

    In addition to these specialized chaps, provide hands-on experience through 3 practical projects. These projects will cover critical real-world applications of GNNs, including traffic forecasting, anomaly detection, \& recommender systems. Through these projects, gain a deeper understanding of how GNNs work \& also develop skills to implement them in practical scenarios.

    -- Ngoài các chuyên gia này, chương trình còn cung cấp kinh nghiệm thực tế thông qua 3 dự án thực tế. Các dự án này sẽ đề cập đến các ứng dụng quan trọng của GNN trong thế giới thực, bao gồm dự báo lưu lượng truy cập, phát hiện bất thường \& hệ thống đề xuất. Thông qua các dự án này, bạn sẽ hiểu sâu hơn về cách thức hoạt động của GNN \& phát triển kỹ năng triển khai chúng trong các tình huống thực tế.

    Finally, this book provides a hands-on learning experience with readable code for every chap's techniques \& relevant applications, which are readily accessible on GitHub \& Google Colab. By end of this book, have a comprehensive understanding of field of graph learning \& GNNs \& will be well-equipped to design \& implement these models for a wide range of applications.

    -- Cuối cùng, cuốn sách này cung cấp trải nghiệm học tập thực hành với mã nguồn dễ đọc cho mọi kỹ thuật \& các ứng dụng liên quan, có thể dễ dàng truy cập trên GitHub \& Google Colab. Khi hoàn thành cuốn sách này, bạn sẽ có hiểu biết toàn diện về lĩnh vực học đồ thị \& Mạng lưới Mạng Thần kinh Nhân tạo (GNN) \& được trang bị tốt để thiết kế \& triển khai các mô hình này cho nhiều ứng dụng khác nhau.
    \begin{itemize}
        \item {\sf Who this book is for.} This book is intended for individuals interested in learning about GNNs \& how they can be applied to various real-world problems. This book is ideal for {\it data scientists, ML engineers, \& AI} professionals who want to gain practical experience in designing \& implementing GNNs. This book is written for individuals with prior knowledge of DL \& ML. However, it provides a comprehensive introduction to fundamental concepts of graph theory \& graph learning for those new to field. Also be useful for researchers \& students in computer science, mathematics, \& engineering who want to expand their knowledge in this rapidly growing area of research.

        --Cuốn sách này dành cho những cá nhân quan tâm đến việc tìm hiểu về mạng lưới thần kinh nhân tạo (GNN) \& cách chúng có thể được áp dụng cho các vấn đề thực tế khác nhau. Cuốn sách này lý tưởng cho các nhà khoa học dữ liệu, kỹ sư ML, \& AI chuyên gia muốn tích lũy kinh nghiệm thực tế trong việc thiết kế \& triển khai GNN. Cuốn sách này được viết cho những cá nhân đã có kiến thức về DL \& ML. Tuy nhiên, nó cung cấp 1 giới thiệu toàn diện về các khái niệm cơ bản của lý thuyết đồ thị \& học đồ thị cho những người mới bắt đầu. Nó cũng hữu ích cho các nhà nghiên cứu \& sinh viên ngành khoa học máy tính, toán học, \& kỹ thuật muốn mở rộng kiến thức trong lĩnh vực nghiên cứu đang phát triển nhanh chóng này.
        \item {\sf What this book covers.} Chap. 1: Getting Started with Graph Learning, provides a comprehensive introduction to GNNs, including their importance in modern data analysis \& ML. Chap starts by exploring relevance of graphs as a representation of data \& their widespread use in various domains. It then delves into importance of graph learning, including different applications \& techniques. Finally, chap focuses on GNN architecture \& highlights its unique features \& performance compared to other methods.

        -- Chương 1: Bắt đầu với Học đồ thị, cung cấp phần giới thiệu toàn diện về GNN, bao gồm tầm quan trọng của chúng trong phân tích dữ liệu hiện đại \& Học máy (ML). Chương này bắt đầu bằng việc khám phá tính liên quan của đồ thị như 1 phương tiện biểu diễn dữ liệu \& ứng dụng rộng rãi của chúng trong nhiều lĩnh vực. Sau đó, chương đi sâu vào tầm quan trọng của học đồ thị, bao gồm các ứng dụng \& kỹ thuật khác nhau. Cuối cùng, chương tập trung vào kiến trúc GNN \& làm nổi bật các tính năng độc đáo \& hiệu suất của nó so với các phương pháp khác.

        Chap. 2: Graph Theory for Graph Neural Networks, covers basics of graph theory \& introduces various types of graphs, including their properties \& applications. This chap also covers fundamental graph concepts, e.g. adjacency matrix, graph measures, e.g. centrality, \& graph algorithms, BFS \& DFS.

        -- Chương 2: Lý thuyết Đồ thị cho Mạng Nơ-ron Đồ thị, bao gồm những kiến thức cơ bản về lý thuyết đồ thị \& giới thiệu các loại đồ thị khác nhau, bao gồm các tính chất \& ứng dụng của chúng. Chương này cũng đề cập đến các khái niệm cơ bản về đồ thị, e.g. ma trận kề, độ đo đồ thị, e.g. độ tập trung, \& thuật toán đồ thị, BFS \& DFS.

        Chap. 3: Creating Node Representations with DeepWalk, focused on DeepWalk, a pioneer in applying ML to graph data. Main objective of DeepWalk architecture: generate node representations that other models can utilize for downstream tasks e.g. node classification. Chap covers 2 key components of DeepWalk -- Word2Vec \& random walks -- with a particular emphasis on Word2Vec skip-gram model.

        -- Chương 3: Tạo Biểu diễn Nút với DeepWalk, tập trung vào DeepWalk, 1 công nghệ tiên phong trong việc áp dụng Học máy (ML) vào dữ liệu đồ thị. Mục tiêu chính của kiến trúc DeepWalk: tạo ra các biểu diễn nút mà các mô hình khác có thể sử dụng cho các tác vụ hạ nguồn, e.g. phân loại nút. Chương này đề cập đến 2 thành phần chính của DeepWalk -- Word2Vec \& random walks -- đặc biệt nhấn mạnh vào mô hình Word2Vec skip-gram.

        Chap. 4: Improving Embeddings with Biased Random Walks in Node2Vec, focused on Node2Vec architecture, which is based on DeepWalk architecture covered in previous chap. Chap covers modifications made to random walk generation in Node2Vec \& how to select best parameters for a specific graph. Implementation of Node2Vec is compared to DeepWalk on Zachary's Karate Club to highlight differences between 2 architectures. Chap concludes with a practical application of Node2Vec, building a movie recommendation system.

        -- Chương 4: Cải thiện nhúng với bước ngẫu nhiên có thiên vị trong Node2Vec, tập trung vào kiến trúc Node2Vec, dựa trên kiến trúc DeepWalk đã được đề cập trong chương trước. Chương này đề cập đến những thay đổi được thực hiện đối với việc tạo bước ngẫu nhiên trong Node2Vec \& cách chọn tham số tốt nhất cho 1 đồ thị cụ thể. Việc triển khai Node2Vec được so sánh với DeepWalk trên Zachary's Karate Club để làm nổi bật sự khác biệt giữa hai kiến trúc. Chương kết thúc bằng 1 ứng dụng thực tế của Node2Vec, xây dựng hệ thống đề xuất phim.

        Chap. 5: Including Node Features with Vanilla Neural Networks, explores integration of additional information, e.g. node \& edge features, into graph embeddings to produce more accurate results. Chap starts with a comparison of vanilla neural network's performance on node features only, treated as tabular datasets. Then, experiment with adding topological information to neural networks, leading to creation of a simple vanilla GNN architecture.

        -- Chương 5: Bao gồm các đặc điểm nút với mạng nơ-ron nhân tạo thuần túy, khám phá việc tích hợp thông tin bổ sung, e.g. đặc điểm nút \& cạnh, vào nhúng đồ thị để tạo ra kết quả chính xác hơn. Chương này bắt đầu bằng việc so sánh hiệu suất của mạng nơ-ron nhân tạo thuần túy chỉ trên các đặc điểm nút, được xử lý dưới dạng tập dữ liệu bảng. Sau đó, thử nghiệm thêm thông tin tôpô vào mạng nơ-ron, dẫn đến việc tạo ra 1 kiến trúc mạng nơ-ron nhân tạo thuần túy đơn giản.

        Chap. 6: Introducing Graph Convolutional Networks, focuses on Graph Convolutional Network (GCN) architecture \& its importance as a blueprint for GNNs. It covers limitations of previous vanilla GNN layers \& explains motivation behind GCNs. Chap details how GCN layer works, its performance improvements over vanilla GNN layer, \& its implementation on Cora \& Facebook Page-Page datasets using PyTorch Geometric. Chap also touches upon task of node regression \& benefits of transforming tabular data into a graph.

        -- Chương 6: Giới thiệu về Mạng Tích chập Đồ thị, tập trung vào kiến trúc Mạng Tích chập Đồ thị (GCN) \& tầm quan trọng của nó như 1 bản thiết kế cho GNN. Chương này đề cập đến những hạn chế của các lớp GNN thuần túy trước đây \& giải thích động lực đằng sau GCN. Chương này trình bày chi tiết cách thức hoạt động của lớp GCN, những cải tiến về hiệu suất so với lớp GNN thuần túy, \& triển khai nó trên Cora \& bộ dữ liệu Facebook Page-Page bằng PyTorch Geometric. Chương cũng đề cập đến nhiệm vụ của hồi quy nút \& lợi ích của việc chuyển đổi dữ liệu dạng bảng thành dạng đồ thị.

        Chap. 7: Graph Attention Networks, focuses on Graph Attention Networks (GATs), which are an improvements over GCNs. Chap explains how GATs work by using concepts of self-attention \& provides a step-by-step understanding of graph attention layer. Chap also implements a graph attention layer from scratch using NumPy. Final section of chap discusses use of a GAT on 2 node classification datasets, Cora \& CiteSeer, \& compares accuracy with that of a GCN.

        -- Chương 7: Mạng lưới Chú ý Đồ thị, tập trung vào Mạng lưới Chú ý Đồ thị (GAT), 1 cải tiến so với GCN. Chương này giải thích cách thức hoạt động của GAT bằng cách sử dụng các khái niệm tự chú ý \& cung cấp hiểu biết từng bước về lớp chú ý đồ thị. Chương cũng triển khai 1 lớp chú ý đồ thị từ đầu bằng NumPy. Phần cuối của chương thảo luận về việc sử dụng GAT trên các tập dữ liệu phân loại 2 nút, Cora \& CiteSeer, \& so sánh độ chính xác với GCN.

        Chap. 8: Scaling up GNNs with GraphSAGE, focuses on GraphSAGE architecture \& its ability to handle large graphs effectively. Chap covers 2 main ideas behind GraphSAGE, including its neighbor sampling technique \& aggregation operators. Learn about variants proposed by tech companies e.g. Uber Eats \& Pinterest, as well as benefits of GraphSAGE's inductive approach. Chap concludes by implementing GraphSAGE for node classification \& multi-label classification tasks.

        -- Chương 8: Mở rộng mạng lưới mô hình dữ liệu lớn (GNN) với GraphSAGE, tập trung vào kiến trúc GraphSAGE \& khả năng xử lý đồ thị lớn hiệu quả. Chương này đề cập đến 2 ý tưởng chính đằng sau GraphSAGE, bao gồm kỹ thuật lấy mẫu lân cận \& các toán tử tổng hợp. Tìm hiểu về các biến thể được đề xuất bởi các công ty công nghệ, e.g. Uber Eats \& Pinterest, cũng như lợi ích của phương pháp quy nạp của GraphSAGE. Chương kết thúc bằng việc triển khai GraphSAGE cho các tác vụ phân loại nút \& phân loại đa nhãn.

        Chap. 9: Defining Expressiveness for Graph Classification, explores concept of expressiveness in GNNs \& how it can be used to design better models. It introduces Weisfeiler-Leman (WL) test, which provides framework for understanding expressiveness in GNNs. Chap uses WL test to compare different GNN layers \& determine most expressive one. Based on this result, a more powerful GNN is designed \& implemented by PyTorch Geometric. Chap concludes with a comparison of different methods for graph classification on PROTEINs dataset.

        -- Chương 9: Định nghĩa tính biểu cảm cho phân loại đồ thị, khám phá khái niệm tính biểu cảm trong mạng lưới biểu diễn dữ liệu (GNN) \& cách sử dụng nó để thiết kế các mô hình tốt hơn. Chương này giới thiệu kiểm định Weisfeiler-Leman (WL), cung cấp khuôn khổ để hiểu tính biểu cảm trong GNN. Chương sử dụng kiểm định WL để so sánh các lớp GNN khác nhau \& xác định lớp biểu cảm nhất. Dựa trên kết quả này, 1 GNN mạnh mẽ hơn đã được thiết kế \& triển khai bởi PyTorch Geometric. Chương kết thúc bằng việc so sánh các phương pháp phân loại đồ thị khác nhau trên tập dữ liệu PROTEIN.

        Chap. 10: Predicting Links with GNNs, focuses on link prediction in graphs. It covers traditional techniques, e.g. matrix factorization \& GNN-based methods. Chap explains concept of link prediction \& its importance in social networks \& recommender systems. Learn about limitations of traditional techniques \& benefits of using GNN-based methods. Exploree 3 GNN-based techniques from 2 different families, including node embeddings \& subgraph representation. Finally, implement various link prediction techniques in PyTorch Geometric \& choose best method for a given problem.

        -- Chương 10: Dự đoán Liên kết với GNN, tập trung vào dự đoán liên kết trong đồ thị. Chương này đề cập đến các kỹ thuật truyền thống, e.g. phân tích ma trận \& các phương pháp dựa trên GNN. Chương này giải thích khái niệm dự đoán liên kết \& tầm quan trọng của nó trong mạng xã hội \& các hệ thống đề xuất. Tìm hiểu về những hạn chế của các kỹ thuật truyền thống \& lợi ích của việc sử dụng các phương pháp dựa trên GNN. Khám phá 3 kỹ thuật dựa trên GNN từ 2 họ khác nhau, bao gồm nhúng nút \& biểu diễn đồ thị con. Cuối cùng, triển khai các kỹ thuật dự đoán liên kết khác nhau trong PyTorch Geometric \& chọn phương pháp tốt nhất cho 1 bài toán nhất định.

        Chap. 11: Generating Graphs Using GNNs, explores field of graph generation, which involves finding methods to create new graphs. Chap 1st introduces you to traditional techniques e.g. Erd\"os--R\'enyi \& small-world models. Then focus on 3 families of solutions for GNN-based graph generation: VAE-based, autoregressive, \& GAN-based models. Chap concludes with an implementation of a GAN-based framework with Reinforcement Learning (RL) to generate new chemical compounds using DeepChem library with TensorFlow.

        -- Chương 11: Tạo đồ thị bằng GNN, khám phá lĩnh vực tạo đồ thị, bao gồm việc tìm kiếm các phương pháp để tạo đồ thị mới. Chương 1 giới thiệu các kỹ thuật truyền thống, e.g. mô hình Erdos--Renyi \& mô hình thế giới nhỏ. Sau đó, tập trung vào 3 nhóm giải pháp tạo đồ thị dựa trên GNN: mô hình dựa trên VAE, mô hình tự hồi quy \& mô hình GAN. Chương kết thúc bằng việc triển khai 1 khuôn khổ dựa trên GAN với Học Tăng cường (RL) để tạo ra các hợp chất hóa học mới bằng thư viện DeepChem với TensorFlow.

        Chap. 12: Learning from Heterogeneous Graphs, focuses on heterogeneous GNNs. Heterogeneous graphs contain different types of nodes \& edges, in contrast in homogeneous graphs, which only involve 1 type of node \& 1 type of edge. Chap begins by reviewing {\it Message Passing Neural Network} (MPNN) framework for homogeneous GNNs, then expands framework to heterogeneous networks. Finally, introduce a technique for creating a heterogeneous dataset, transforming homogeneous architectures into heterogeneous ones, \& discussing an architecture specifically designed for processing heterogeneous networks.

        -- Chương 12: Học từ Đồ thị Không đồng nhất, tập trung vào Mạng nơ-ron nhân tạo không đồng nhất (GNN) không đồng nhất. Đồ thị không đồng nhất chứa các loại nút \& cạnh khác nhau, trái ngược với đồ thị đồng nhất, chỉ bao gồm 1 loại nút \& 1 loại cạnh. Chương bắt đầu bằng việc xem xét khuôn khổ Mạng nơ-ron truyền thông điệp (MPNN) cho GNN đồng nhất, sau đó mở rộng khuôn khổ sang các mạng không đồng nhất. Cuối cùng, giới thiệu 1 kỹ thuật để tạo 1 tập dữ liệu không đồng nhất, chuyển đổi các kiến trúc đồng nhất thành kiến trúc không đồng nhất, \& thảo luận về 1 kiến trúc được thiết kế riêng để xử lý các mạng không đồng nhất.

        Chap. 13: Temporal GNNs, focuses on Temporal GNNs, or Spatio-Temporal GNNs, which are a type of GNN that can handle graphs with changing edges \& features over time. Chap 1st explains concept of dynamic graphs \& applications of temporal GNNs, focusing on time series forecasting. Chap then moves on to application of temporal GNNs to web traffic forecasting to improve results using temporal information. Finally, chap describes another temporal GNN architecture specifically designed for dynamic graphs \& applies it to task of epidemic forecasting.

        -- Chương 13: Mạng GNN thời gian, tập trung vào Mạng GNN thời gian, hay Mạng GNN không gian-thời gian, là 1 loại Mạng GNN có thể xử lý đồ thị với các cạnh \& đặc trưng thay đổi theo thời gian. Chương 1 giải thích khái niệm về đồ thị động \& ứng dụng của Mạng GNN thời gian, tập trung vào dự báo chuỗi thời gian. Sau đó, chương chuyển sang ứng dụng Mạng GNN thời gian vào dự báo lưu lượng truy cập web để cải thiện kết quả bằng cách sử dụng thông tin thời gian. Cuối cùng, chương này mô tả 1 kiến trúc Mạng GNN thời gian khác được thiết kế riêng cho đồ thị động \& ứng dụng nó vào nhiệm vụ dự báo dịch bệnh.
        \item Chap. 14: Explaining GNNs, covers various techniques to better understands predictions \& behavior of a GNN model. Chap highlights 2 popular explanation methods: GNNExplainer \& integrated gradients. Then, see application of these techniques on a graph classification task using MUTAG dataset \& a node classification task using Twitch social network.

        -- Chương 14: Giải thích về mạng GNN, bao gồm các kỹ thuật khác nhau để hiểu rõ hơn về dự đoán \& hành vi của mô hình GNN. Chương này nêu bật 2 phương pháp giải thích phổ biến: GNNExplainer \& gradient tích hợp. Sau đó, xem xét ứng dụng của các kỹ thuật này trên 1 bài toán phân loại đồ thị sử dụng tập dữ liệu MUTAG \& 1 bài toán phân loại nút sử dụng mạng xã hội Twitch.
        \item Chap. 15: Forecasting Traffic Using A3T-GCN, focuses on application of Temporal Graph Neural Networks in field of traffic forecasting. It highlights importance of accurate traffic forecasts in smart cities \& challenges of traffic forecasting due to complex spatial \& temporal dependencies. Chap covers steps involved in processing a new dataset to create a temporal graph \& implementation of a new type of temporal GNN to predict future traffic speed. Finally, results are compared to a baseline solution to verity relevance of architecture.

        -- Chương 15: Dự báo lưu lượng giao thông bằng A3T-GCN, tập trung vào ứng dụng của Mạng nơ-ron đồ thị thời gian trong lĩnh vực dự báo giao thông. Bài viết nhấn mạnh tầm quan trọng của việc dự báo giao thông chính xác trong các thành phố thông minh \& những thách thức của việc dự báo giao thông do sự phụ thuộc phức tạp về không gian \& thời gian. Chương này trình bày các bước xử lý tập dữ liệu mới để tạo đồ thị thời gian \& triển khai 1 loại Mạng nơ-ron đồ thị thời gian mới để dự đoán tốc độ giao thông trong tương lai. Cuối cùng, kết quả được so sánh với giải pháp cơ sở để đánh giá tính phù hợp thực tế của kiến trúc.
        \item Chap. 16: Recommending Books Using LightGCN, focuses on application of GNNs in recommender systems. Goal of recommender systems: provide personalized recommendations to users based on their interests \& past interactions. GNNs are well-suited for this task as they can effectively incorporate complex relationships between users \& items. In this chap, LightGCN architecture is introduced as a GNN specifically designed for recommender systems. Using Book-Crossing dataset, chap demonstrates how to build a book recommender system with collaborative filtering using LightGCN architecture.

        -- Chương 16: Đề xuất sách bằng LightGCN, tập trung vào ứng dụng của GNN trong hệ thống đề xuất. Mục tiêu của hệ thống đề xuất: cung cấp các đề xuất được cá nhân hóa cho người dùng dựa trên sở thích \& các tương tác trước đây của họ. GNN rất phù hợp cho nhiệm vụ này vì chúng có thể kết hợp hiệu quả các mối quan hệ phức tạp giữa người dùng \& các mục. Trong chương này, kiến trúc LightGCN được giới thiệu như 1 GNN được thiết kế riêng cho hệ thống đề xuất. Sử dụng tập dữ liệu Book-Crossing, chương này trình bày cách xây dựng 1 hệ thống đề xuất sách với lọc cộng tác bằng kiến trúc LightGCN.
        \item Chap. 17: Recommending Books Using LightGCN, focuses on application of GNNs in recommender systems. Goal of recommender systems: provide personalized recommendations to users based on their interests \& past interactions. GNNs are well-suited for this tasks as they can effectively incorporate complex relationships between users \& items. In this chap, LightGCN architecture is introduced as a GNN specifically designed for recommender systems. Using Book-Crossing dataset, chap demonstrates how to build a book recommender system with collaborative filtering using LightGCN architecture.

        -- Chương 17: Đề xuất sách bằng LightGCN, tập trung vào ứng dụng của GNN trong các hệ thống đề xuất. Mục tiêu của hệ thống đề xuất: cung cấp các đề xuất được cá nhân hóa cho người dùng dựa trên sở thích \& các tương tác trước đây của họ. GNN rất phù hợp cho nhiệm vụ này vì chúng có thể kết hợp hiệu quả các mối quan hệ phức tạp giữa người dùng \& các mục. Trong chương này, kiến trúc LightGCN được giới thiệu như 1 GNN được thiết kế riêng cho các hệ thống đề xuất. Sử dụng tập dữ liệu Book-Crossing, chương này trình bày cách xây dựng 1 hệ thống đề xuất sách với lọc cộng tác bằng kiến trúc LightGCN.
        \item Chap. 18: Unlocking Potential of GNNs for Real-World Applications, summarizes what we have learned throughout book, \& looks ahead to future of GNNs.

        -- Chương 18: Khai phá tiềm năng của GNN cho các ứng dụng thực tế, tóm tắt những gì chúng ta đã học được trong suốt cuốn sách, \& hướng tới tương lai của GNN.
        \item {\sf To get most out of this book.} Should have a basic understanding of graph theory \& ML concepts, e.g. supervised \& unsupervised learning, training, \& evaluation of models to maximize your learning experience. Familiarity with DL frameworks, e.g. PyTorch, will also be useful, although not essential, as book will provide a comprehensive introduction to mathematical concepts \& their implementation.

        -- {\sf Để tận dụng tối đa cuốn sách này.} Bạn nên có hiểu biết cơ bản về lý thuyết đồ thị \& các khái niệm ML, e.g.: học có giám sát \& học không giám sát, đào tạo, \& đánh giá các mô hình để tối đa hóa trải nghiệm học tập của mình. Việc quen thuộc với các nền tảng DL, e.g.: PyTorch, cũng sẽ hữu ích, mặc dù không bắt buộc, vì cuốn sách sẽ cung cấp phần giới thiệu toàn diện về các khái niệm toán học \& cách triển khai chúng.

        Software covered in book: Python 3.8.15, PyTorch 1.13.1, PyTorch Geometric 2.2.0.
    \end{itemize}

    PART 1: INTRODUCTION TO GRAPH LEARNING.

    In recent years, graph representation of data has become increasingly prevalent across various domains, from social networks to molecular biology. It is crucial to have a deep understanding of GNNs, which are designed specifically to handle graph-structured data, to unlock full potential of this representation.

    -- Trong những năm gần đây, biểu diễn dữ liệu dạng đồ thị ngày càng trở nên phổ biến trong nhiều lĩnh vực, từ mạng xã hội đến sinh học phân tử. Việc hiểu sâu sắc về mạng lưới mô hình dữ liệu (GNN), được thiết kế chuyên biệt để xử lý dữ liệu có cấu trúc đồ thị, là rất quan trọng để khai thác toàn bộ tiềm năng của biểu diễn này.

    This 1st part part consists of 2 chaps \& serves as a solid foundation for rest of book. It introduces concepts of graph learning \& GNNs \& their relevance in numerous tasks \& industries. It also covers fundamental concepts of graph theory \& its applications in graph learning, e.g. graph centrality measures. This part also highlights unique features \& performance of GNN architecture compared to other methods.

    -- Phần 1 này bao gồm 2 chương \& đóng vai trò là nền tảng vững chắc cho phần còn lại của cuốn sách. Phần này giới thiệu các khái niệm về học đồ thị \& Mạng lưới Mạng Toàn cầu (GNN) \& sự liên quan của chúng trong nhiều tác vụ \& ngành công nghiệp. Phần này cũng đề cập đến các khái niệm cơ bản của lý thuyết đồ thị \& ứng dụng của nó trong học đồ thị, e.g. các phép đo tính trung tâm của đồ thị. Phần này cũng làm nổi bật các tính năng độc đáo \& hiệu suất của kiến trúc GNN so với các phương pháp khác.

    By end of this part, have a solid understanding of importance of GNNs in solving many real-world problems. Will be acquainted with essentials of graph learning \& how it is used in various domains. Furthermore, will have a comprehensive overview of main concepts of graph theory used in later chaps. With this solid foundation, will be well equipped to move on to more advanced concepts in graph learning \& GNNs in following parts of book.

    -- Đến cuối phần này, bạn sẽ hiểu rõ tầm quan trọng của mạng GNN trong việc giải quyết nhiều vấn đề thực tế. Bạn sẽ được làm quen với những kiến thức cơ bản về học đồ thị \& cách thức ứng dụng nó trong nhiều lĩnh vực khác nhau. Hơn nữa, bạn sẽ có cái nhìn tổng quan toàn diện về các khái niệm chính của lý thuyết đồ thị được sử dụng trong các chương sau. Với nền tảng vững chắc này, bạn sẽ được trang bị tốt để tiếp tục học các khái niệm nâng cao hơn về học đồ thị \& mạng GNN trong các phần tiếp theo của cuốn sách.
    \item {\sf1. Getting Started with Graph Learning.} In this chap, delve into foundations of GNNs \& understand why they are crucial tools in modern data analysis \& ML. To that end, answer 3 essential questions providing us with a comprehensive understanding of GNNs.

    -- Trong chương này, chúng ta sẽ đi sâu vào nền tảng của GNN \& hiểu tại sao chúng là công cụ quan trọng trong phân tích dữ liệu hiện đại \& ML. Để đạt được mục tiêu đó, trả lời 3 câu hỏi thiết yếu, giúp chúng ta hiểu toàn diện về GNN.

    1st, explore significance of graphs as a representation of data, \& why they are widely used in various domains e.g. CS, biology, \& finance. Next, delve into importance of graph learning, where we will understand different applications of graph learning \& different families of graph learning techniques. Finally, focus on GNN family, highlighting its unique features, performance, \& how it stands out compared to other methods.

    -- Đầu tiên, khám phá ý nghĩa của đồ thị trong việc biểu diễn dữ liệu, \& lý do tại sao chúng được sử dụng rộng rãi trong nhiều lĩnh vực khác nhau, e.g. Khoa học Máy tính, Sinh học, \& Tài chính. Tiếp theo, đi sâu vào tầm quan trọng của học đồ thị, nơi chúng ta sẽ tìm hiểu các ứng dụng khác nhau của học đồ thị \& các nhóm kỹ thuật học đồ thị khác nhau. Cuối cùng, tập trung vào họ GNN, làm nổi bật các tính năng độc đáo, hiệu suất của nó, \& cách nó nổi bật so với các phương pháp khác.

    By end of this chap, have a clear understanding of why GNns are important \& how they can be used to solve real-world problems. Will also be equipped with knowledge \& skills you need to dive deeper into more advanced topics.
    \begin{itemize}
        \item {\sf1.1. Why graphs?} 1st question we need to address: why interested in graphs in 1st place? Graph theory, mathematical study of graphs, has emerged as a fundamental tool for understanding complex systems \& relationships. A graph is a visual representation of a collection of nodes (also called vertices) \& edges that connect these nodes, providing a structure to represent entities \& their relationships.

        -- {\sf Tại sao lại là đồ thị?} Câu hỏi đầu tiên chúng ta cần giải quyết: tại sao lại quan tâm đến đồ thị ngay từ đầu? Lý thuyết đồ thị, nghiên cứu toán học về đồ thị, đã nổi lên như 1 công cụ cơ bản để hiểu các hệ thống phức tạp \& các mối quan hệ. Đồ thị là biểu diễn trực quan của 1 tập hợp các nút (còn gọi là đỉnh) \& các cạnh kết nối các nút này, cung cấp 1 cấu trúc để biểu diễn các thực thể \& các mối quan hệ của chúng.

        By representing a complex system as a network of entities with interactions, can analyze their relationships, allowing us to gain a deeper understanding of their underlying structures \& patterns. Versatility of graphs makes them a popular choice in various domains, including:
        \begin{itemize}
            \item CS, where graphs can be used to model structure of computer programs, making it easier to understand how different components of a system interact with each other
            \item Physics, where graphs can be used to model physical systems \& their interactions, e.g. relationship between particles \& their properties
            \item Biology, where graphs can be used to model biological systems, e.g. metabolic pathways, as a network of interconnected entities
            \item Social sciences, where graphs can be used to study \& understand complex social networks, including relationships between individuals in a community
            \item Finance, where graphs can be used to analyze stock market trends \& relationships between different financial instruments
            \item Engineering, where graphs can be used to model \& analyze complex systems, e.g. transportation networks \& electrical power grids
        \end{itemize}
        These domains naturally exhibit a relational structure. E.g., graphs are a natural representation of social networks: nodes are users, \& edges represent friendships. But graphs are so versatile they can also be applied to domains where relationship structure is less natural, unlocking new insights \& understanding.

        -- Bằng cách biểu diễn 1 hệ thống phức tạp như 1 mạng lưới các thực thể có tương tác, chúng ta có thể phân tích các mối quan hệ của chúng, cho phép chúng ta hiểu sâu hơn về cấu trúc \& các mô hình cơ bản của chúng. Tính linh hoạt của đồ thị khiến chúng trở thành lựa chọn phổ biến trong nhiều lĩnh vực, bao gồm:
        \begin{itemize}
            \item Khoa học máy tính, trong đó đồ thị có thể được sử dụng để mô hình hóa cấu trúc của các chương trình máy tính, giúp dễ dàng hiểu được cách các thành phần khác nhau của 1 hệ thống tương tác với nhau.
            \item Vật lý, trong đó đồ thị có thể được sử dụng để mô hình hóa các hệ thống vật lý \& các tương tác của chúng, e.g.: mối quan hệ giữa các hạt \& các tính chất của chúng
            \item Sinh học, trong đó đồ thị có thể được sử dụng để mô hình hóa các hệ thống sinh học, e.g.: các con đường trao đổi chất, như 1 mạng lưới các thực thể được kết nối với nhau
            \item Khoa học xã hội, trong đó đồ thị có thể được sử dụng để nghiên cứu \& hiểu các mạng lưới xã hội phức tạp, bao gồm các mối quan hệ giữa các cá nhân trong 1 cộng đồng
            \item Tài chính, trong đó đồ thị có thể được sử dụng để phân tích xu hướng thị trường chứng khoán \& mối quan hệ giữa các công cụ tài chính khác nhau
            \item Kỹ thuật, trong đó đồ thị có thể được sử dụng để mô hình hóa \& phân tích các hệ thống phức tạp, e.g.: Mạng lưới giao thông \& lưới điện
        \end{itemize}
        Các miền này tự nhiên thể hiện 1 cấu trúc quan hệ. E.g., đồ thị là 1 biểu diễn tự nhiên của mạng xã hội: các nút là người dùng, \& các cạnh biểu diễn tình bạn. Tuy nhiên, đồ thị rất linh hoạt nên chúng cũng có thể được áp dụng cho các miền mà cấu trúc quan hệ ít tự nhiên hơn, mở ra những hiểu biết mới \& hiểu biết sâu sắc.

        E.g., images can be represented as a graph, as in {\sf Fig. 1.2: Original image vs. graph representation of this image.} Each pixel is a node, \& edges represent relationships between neighboring pixels. This allows for application of graph-based algorithms to image processing \& computer vision tasks.

        -- E.g., hình ảnh có thể được biểu diễn dưới dạng đồ thị, như trong {\sf Hình 1.2: Ảnh gốc so với biểu diễn đồ thị của ảnh này.} Mỗi pixel là 1 nút, \& các cạnh biểu diễn mối quan hệ giữa các pixel lân cận. Điều này cho phép áp dụng các thuật toán dựa trên đồ thị vào xử lý ảnh \& các tác vụ thị giác máy tính.

        Similarly, a sentence can be transformed into a graph, where nodes are words \& edges represent relationships between adjacent words. This approach is useful in natural language processing \& information retrieval tasks, where context \& meaning of words are critical factors.

        -- {\sf Tại sao lại học đồ thị?} Tương tự, 1 câu có thể được chuyển đổi thành đồ thị, trong đó các nút là các từ \& các cạnh biểu diễn mối quan hệ giữa các từ liền kề. Cách tiếp cận này hữu ích trong xử lý ngôn ngữ tự nhiên \& các tác vụ truy xuất thông tin, trong đó ngữ cảnh \& ý nghĩa của từ là các yếu tố quan trọng.

        Unlike text \& images, graphs do not have a fixed structure. However, this flexibility also makes graphs more challenging to handle. Absence of a fixed structure means they can have an arbitrary number of nodes \& edges, with no specific ordering. In addition, graphs can represent dynamic data, where connections between entities can change over time. E.g., relationships between users \& products can change as they interact with each other. In this scenario, nodes \& edges are updated to reflect changes in real world, e.g. new users, new products, \& new relationships. In next sect, delve deeper into how to use graphs with ML to create valuable applications.

        -- Không giống như văn bản \& hình ảnh, đồ thị không có cấu trúc cố định. Tuy nhiên, tính linh hoạt này cũng khiến việc xử lý đồ thị trở nên khó khăn hơn. Việc thiếu cấu trúc cố định đồng nghĩa với việc chúng có thể có số lượng nút \& cạnh tùy ý, không theo thứ tự cụ thể. Ngoài ra, đồ thị có thể biểu diễn dữ liệu động, trong đó các kết nối giữa các thực thể có thể thay đổi theo thời gian. Ví dụ: mối quan hệ giữa người dùng \& sản phẩm có thể thay đổi khi chúng tương tác với nhau. Trong trường hợp này, các nút \& cạnh được cập nhật để phản ánh những thay đổi trong thế giới thực, e.g.: người dùng mới, sản phẩm mới, \& mối quan hệ mới. Trong phần tiếp theo, tìm hiểu sâu hơn về cách sử dụng đồ thị với Học máy (ML) để tạo ra các ứng dụng có giá trị.
        \item {\sf1.2. Why graph learning?} Graph learning is application of ML techniques to graph data. This study area encompasses a range of tasks aimed at understanding \& manipulating graph-structured data. There are many graphs learning tasks, including:
        \begin{itemize}
            \item {\bf Node classification} is a task that involves predicting category (class) of a node in a graph. E.g., it can categorize online users or items based on their characteristics. In this task, model is trained on a set of labeled nodes \& their attributes, \& it uses this information to predict class of unlabeled nodes.
            \item {\bf Link prediction} is a task that involves predicting missing links between pairs of nodes in a graph. This is useful in knowledge graph completion, where goal: complete a graph of entities \& their relationships. E.g., it can be used to predict relationships between people based on their social network connections (friend recommendation).
            \item {\bf Graph classification} is a task that involves categorizing different graphs into predefined categories. 1 example of this is in molecular biology, where molecular structures can be represented as graphs, \& goal: predict their properties for drug design. In this task, model is trained on a set of labeled graphs \& their attributes, \& it uses this information to categorize unseen graphs.
            \item {\bf Graph generation} is a task that involves generating new graphs based on a set of desired properties. 1 of main applications is generating novel molecular structures for drug discovery. This is achieved by training a model on a set of existing molecular structures \& then using it to generate new, unseen structures. Generated structures can be evaluated for their potential as drug candidates \& further studied.
        \end{itemize}
        -- Học đồ thị là ứng dụng các kỹ thuật ML vào dữ liệu đồ thị. Lĩnh vực nghiên cứu này bao gồm 1 loạt các nhiệm vụ nhằm mục đích hiểu \& thao tác dữ liệu có cấu trúc đồ thị. Có nhiều nhiệm vụ học đồ thị, bao gồm:
        \begin{itemize}
            \item {\bf Phân loại nút} là 1 nhiệm vụ liên quan đến việc dự đoán loại (lớp) của 1 nút trong đồ thị. Ví dụ: nó có thể phân loại người dùng hoặc mục trực tuyến dựa trên các đặc điểm của họ. Trong nhiệm vụ này, mô hình được huấn luyện trên 1 tập hợp các nút được gắn nhãn \& thuộc tính của chúng, \& nó sử dụng thông tin này để dự đoán loại của các nút chưa được gắn nhãn.
            \item {\bf Dự đoán liên kết} là 1 nhiệm vụ liên quan đến việc dự đoán các liên kết bị thiếu giữa các cặp nút trong đồ thị. Điều này hữu ích trong việc hoàn thành đồ thị tri thức, với mục tiêu: hoàn thành đồ thị các thực thể \& mối quan hệ của chúng. Ví dụ: nó có thể được sử dụng để dự đoán mối quan hệ giữa mọi người dựa trên kết nối mạng xã hội của họ (khuyến nghị bạn bè).
            \item {\bf Phân loại đồ thị} là 1 nhiệm vụ liên quan đến việc phân loại các đồ thị khác nhau thành các loại được xác định trước. 1 ví dụ về điều này là trong sinh học phân tử, nơi các cấu trúc phân tử có thể được biểu diễn dưới dạng đồ thị, \& mục tiêu: dự đoán các đặc tính của chúng để thiết kế thuốc. Trong nhiệm vụ này, mô hình được huấn luyện trên 1 tập hợp các đồ thị được gắn nhãn \& các thuộc tính của chúng, \& nó sử dụng thông tin này để phân loại các đồ thị chưa được biết đến.
            \item {\bf Tạo đồ thị} là 1 nhiệm vụ liên quan đến việc tạo ra các đồ thị mới dựa trên 1 tập hợp các đặc tính mong muốn. 1 trong những ứng dụng chính là tạo ra các cấu trúc phân tử mới để khám phá thuốc. Điều này đạt được bằng cách huấn luyện 1 mô hình trên 1 tập hợp các cấu trúc phân tử hiện có \& sau đó sử dụng nó để tạo ra các cấu trúc mới, chưa được biết đến. Các cấu trúc được tạo ra có thể được đánh giá về tiềm năng của chúng như các ứng cử viên thuốc \& nghiên cứu thêm.
        \end{itemize}
        Graph learning has many other practical applications that can have a significant impact. 1 of most well-known applications is {\it recommender systems}, where graph learning algorithms recommend relevant items to users based on their previous interactions \& relationships with other items. Another important application is {\it traffic forecasting}, where graph learning can improve travel time predictions by considering complex relationships between different routes \& modes of transportation.

        -- Học đồ thị có nhiều ứng dụng thực tế khác có thể mang lại tác động đáng kể. 1 trong những ứng dụng nổi tiếng nhất là hệ thống đề xuất, trong đó các thuật toán học đồ thị đề xuất các mục liên quan cho người dùng dựa trên các tương tác trước đó của họ \& mối quan hệ với các mục khác. 1 ứng dụng quan trọng khác là dự báo giao thông, trong đó học đồ thị có thể cải thiện dự đoán thời gian di chuyển bằng cách xem xét các mối quan hệ phức tạp giữa các tuyến đường \& phương thức vận tải khác nhau.

        Versatility \& potential of graph learning make it an exciting field of research \& development. Study of graphs have advanced rapidly in recent years, driven by availability of large datasets, powerful computing resources, \& advancements in ML \& AI. As a result, can list 4 prominent families of graph learning techniques [1]:
        \begin{enumerate}
            \item {\bf Graph signal processing}, which applies traditional signal processing methods to graphs, e.g. graph Fourier transform \& spectral analysis. These techniques reveal intrinsic properties of graph, e.g. its connectivity \& structure.
            \item {\bf Matrix factorization}, which seeks to find low-dimensional representations of large matrices. Goal of matrix factorization: identify latent factors or patterns that explain observed relationships in original matrix. This approach can provide a compact \& interpretable representation of data.
            \item {\bf Random walk}, which refers to a mathematical concept used to model movement of entities in a graph. By simulating random walks over a graph, information about relationships between nodes can be gathered. This is why they are often used to generate training data for ML models.
            \item {\bf DL}, which is a subfield of ML that focuses on neural networks with multiple layers. DL methods can effectively encode \& represent graph data as vectors. These vectors can then be used in various tasks with remarkable performance.
        \end{enumerate}
        -- Tính linh hoạt \& tiềm năng của học đồ thị khiến nó trở thành 1 lĩnh vực nghiên cứu \& phát triển thú vị. Nghiên cứu về đồ thị đã phát triển nhanh chóng trong những năm gần đây, nhờ vào sự sẵn có của các tập dữ liệu lớn, tài nguyên điện toán mạnh mẽ, \& những tiến bộ trong ML \& AI. Do đó, có thể liệt kê 4 nhóm kỹ thuật học đồ thị nổi bật [1]:
        \begin{enumerate}
            \item {\bf Xử lý tín hiệu đồ thị}, áp dụng các phương pháp xử lý tín hiệu truyền thống vào đồ thị, e.g.: biến đổi Fourier đồ thị \& phân tích phổ. Các kỹ thuật này tiết lộ các đặc tính nội tại của đồ thị, e.g.: tính kết nối \& cấu trúc của nó.
            \item {\bf Phân tích ma trận}, tìm cách tìm ra các biểu diễn chiều thấp của các ma trận lớn. Mục tiêu của phân tích ma trận: xác định các yếu tố hoặc mô hình tiềm ẩn giải thích các mối quan hệ quan sát được trong ma trận gốc. Phương pháp này có thể cung cấp 1 biểu diễn dữ liệu \& có thể diễn giải được.
            \item {\bf Bước ngẫu nhiên}, đề cập đến 1 khái niệm toán học được sử dụng để mô hình hóa chuyển động của các thực thể trong đồ thị. Bằng cách mô phỏng các bước ngẫu nhiên trên đồ thị, thông tin về mối quan hệ giữa các nút có thể được thu thập. Đây là lý do tại sao chúng thường được sử dụng để tạo dữ liệu huấn luyện cho các mô hình ML.
            \item {\bf DL}, 1 lĩnh vực con của ML tập trung vào các mạng nơ-ron nhiều lớp. Các phương pháp DL có thể mã hóa hiệu quả \& biểu diễn dữ liệu đồ thị dưới dạng vectơ. Các vectơ này sau đó có thể được sử dụng trong nhiều tác vụ khác nhau với hiệu suất đáng kể.
        \end{enumerate}
        Important: these techniques are not mutually exclusive \& often overlap in their applications. In practice, they are often combined to form hybrid models that leverage strengths of each. E.g., matrix factorization \& DL techniques might be used in combination to learn low-dimensional representations of graph-structured data.

        -- Điều quan trọng cần lưu ý: các kỹ thuật này không loại trừ lẫn nhau \& thường chồng chéo trong ứng dụng. Trong thực tế, chúng thường được kết hợp để tạo thành các mô hình lai tận dụng thế mạnh của từng kỹ thuật. Ví dụ: kỹ thuật phân tích ma trận \& DL có thể được sử dụng kết hợp để học các biểu diễn dữ liệu có cấu trúc đồ thị ít chiều.

        As delve into world of graph learning, crucial to understand fundamental building block of any ML technique: dataset. Traditional tabular datasets, e.g. spreadsheets, represent data as rows \& columns with each row representing a single data point. However, in many real-world scenarios, relationships between data points are just as meaningful as data points themselves. This is where graph datasets come in. Graph datasets represent data points as nodes in a graph \& relationships between those data points as edges.

        {\sf Fig. 1.3: Family tree as a tabular dataset vs. a graph dataset}: this dataset represents information about 5 members of a family. Each member has 3 features (or attributes): name, age, \& gender. However, tabular version of this dataset does not show connections between these people. On contrary, graph version represents them with edges, which allows us to understand relationships in this family. In many contexts, connections between nodes are crucial in understanding data, which is why representing data in graph form is becoming increasingly popular.

        -- {\sf Hình 1.3: Cây phả hệ dưới dạng tập dữ liệu bảng so với tập dữ liệu đồ thị}: tập dữ liệu này biểu diễn thông tin về 5 thành viên trong 1 gia đình. Mỗi thành viên có 3 đặc điểm (hoặc thuộc tính): tên, tuổi, \& giới tính. Tuy nhiên, phiên bản bảng của tập dữ liệu này không thể hiện mối liên hệ giữa những người này. Ngược lại, phiên bản đồ thị biểu diễn họ bằng các cạnh, cho phép chúng ta hiểu các mối quan hệ trong gia đình này. Trong nhiều bối cảnh, mối liên hệ giữa các nút rất quan trọng để hiểu dữ liệu, đó là lý do tại sao việc biểu diễn dữ liệu dưới dạng đồ thị ngày càng trở nên phổ biến.

        Now have a basic understanding of graph ML \& different types of tasks it involves, can move on to exploring 1 of most important approaches for solving these tasks: GNNs.
        \item {\sf1.3. Why GNNs?} In this book, focus on DL family of graph learning techniques, often referred to as graph neural networks. GNNs are a new category of DL architecture \& are specifically designed for graph-structured data. Unlike traditional DL algorithms, which have been primarily developed for text \& images, GNNs are explicitly made to process \& analyze graph dataset {\sf Fig. 1.4: High-level architecture of a GNN pipeline, with a graph as input \& an output that corresponds to a given task: a. Node classification. b. Link prediction. c. Graph classification.}

        -- Trong cuốn sách này, chúng tôi tập trung vào họ kỹ thuật học đồ thị DL, thường được gọi là mạng nơ-ron đồ thị. GNN là 1 loại kiến trúc DL mới \& được thiết kế riêng cho dữ liệu có cấu trúc đồ thị. Không giống như các thuật toán DL truyền thống, vốn chủ yếu được phát triển cho văn bản \& hình ảnh, GNN được thiết kế rõ ràng để xử lý \& phân tích tập dữ liệu đồ thị {\sf Hình 1.4: Kiến trúc cấp cao của 1 đường ống GNN, với đồ thị làm đầu vào \& đầu ra tương ứng với 1 tác vụ nhất định: a. Phân loại nút. b. Dự đoán liên kết. c. Phân loại đồ thị.}

        GNNs have emerged as a powerful tool for graph learning \& have shown excellent results in various tasks \& industries. 1 of most striking examples is how a CNN model identified a new antibiotic [2]. Model was trained on 2500 molecules \& was tested on a library of 6000 compounds. It predicted that a molecule called halicin should be able to kill many antibiotic-resistant bacteria while having low toxicity to human cells. Based on this prediction, researchers used halicin to treat mice infected with antibiotic-resistant bacteria. They demonstrated its effectiveness \& believe model could be used to design new drugs.

        -- Mạng nơ-ron nhân tạo (GNN) đã nổi lên như 1 công cụ mạnh mẽ cho việc học đồ thị \& đã cho thấy kết quả xuất sắc trong nhiều nhiệm vụ \& ngành công nghiệp. 1 trong những ví dụ nổi bật nhất là cách 1 mô hình CNN xác định 1 loại kháng sinh mới [2]. Mô hình được huấn luyện trên 2500 phân tử \& đã được thử nghiệm trên thư viện gồm 6000 hợp chất. Nó dự đoán rằng 1 phân tử có tên là halicin có thể tiêu diệt nhiều loại vi khuẩn kháng kháng sinh trong khi có độc tính thấp đối với tế bào người. Dựa trên dự đoán này, các nhà nghiên cứu đã sử dụng halicin để điều trị cho chuột bị nhiễm vi khuẩn kháng kháng sinh. Họ đã chứng minh hiệu quả của nó \& tin rằng mô hình có thể được sử dụng để thiết kế các loại thuốc mới.

        How do GNNs work? Take example of a node classification task in a social network, like previous family tree. In a node classification task, GNNs take advantage of information from different sources to create a vector representation of each node in graph. This representation encompasses not only original node features (e.g. name, age, \& gender) but also information from edge features (e.g. strength of relationships between nodes) \& global features (e.g. network-wide statistics).

        -- Mạng GNN hoạt động như thế nào? Hãy lấy ví dụ về tác vụ phân loại nút trong mạng xã hội, chẳng hạn như cây phả hệ trước đây. Trong tác vụ phân loại nút, GNN tận dụng thông tin từ các nguồn khác nhau để tạo biểu diễn vectơ cho mỗi nút trong đồ thị. Biểu diễn này không chỉ bao gồm các đặc điểm nút gốc (e.g.: tên, tuổi, \& giới tính) mà còn bao gồm thông tin từ các đặc điểm biên (e.g.: cường độ mối quan hệ giữa các nút) \& các đặc điểm toàn cục (e.g.: thống kê toàn mạng).

        This is why GNNs are more efficient than traditional ML techniques on graphs. Instead of being limited to original attributes, GNNs enrich original node features with attributes from neighboring nodes, edges, \& global features, making representation much more comprehensive \& meaningful. New node representations are then used to perform a specific task, e.g. node classification, regression, or link prediction.

        -- Đây là lý do tại sao GNN hiệu quả hơn các kỹ thuật ML truyền thống trên đồ thị. Thay vì bị giới hạn ở các thuộc tính gốc, GNN làm giàu các đặc trưng nút gốc bằng các thuộc tính từ các nút lân cận, cạnh, \& đặc trưng toàn cục, giúp biểu diễn trở nên toàn diện hơn \& có ý nghĩa hơn nhiều. Các biểu diễn nút mới sau đó được sử dụng để thực hiện 1 tác vụ cụ thể, e.g.: phân loại nút, hồi quy hoặc dự đoán liên kết.

        Specifically, GNNs define a graph convolution operation that aggregates information from neighboring nodes \& edges to update node representation. This operation is performed iteratively, allowing model to learn more complex relationships between nodes as number of iterations increases. E.g., {\sf Fig. 1.5: Input graph vs. computation graph representing how a GNN computes representation of node 5 based on its neighbors.} shows how a GNN would calculate representation of node 5 using neighboring nodes.

        -- Cụ thể, GNN định nghĩa 1 phép toán tích chập đồ thị tổng hợp thông tin từ các nút \& cạnh lân cận để cập nhật biểu diễn nút. Phép toán này được thực hiện lặp đi lặp lại, cho phép mô hình học các mối quan hệ phức tạp hơn giữa các nút khi số lần lặp tăng lên. Ví dụ: {\sf Hình 1.5: Đồ thị đầu vào so với đồ thị tính toán biểu diễn cách GNN tính toán biểu diễn của nút 5 dựa trên các nút lân cận.} cho thấy cách GNN tính toán biểu diễn của nút 5 bằng cách sử dụng các nút lân cận.

        Worth noting: {\sf Fig. 1.5} provides a simplified illustration of a computation graph. In reality, there are various kinds of GNNs \& GNN layers, each of which has a unique structure \& way of aggregating information from neighboring nodes. These different variants of GNNs also have their own advantages \& limitations \& are well-suited for specific types of graph data \& tasks. When selecting appropriate GNN architecture for a particular problem, crucial to understand characteristics of graph data \& desired outcome.

        -- Lưu ý: {\sf Hình 1.5} cung cấp 1 minh họa đơn giản về đồ thị tính toán. Trên thực tế, có nhiều loại GNN \& các lớp GNN, mỗi loại có 1 cấu trúc riêng \& cách tổng hợp thông tin từ các nút lân cận. Các biến thể GNN khác nhau này cũng có những ưu điểm \& hạn chế riêng \& phù hợp với các loại dữ liệu đồ thị \& tác vụ cụ thể. Khi lựa chọn kiến trúc GNN phù hợp cho 1 bài toán cụ thể, điều quan trọng là phải hiểu các đặc điểm của dữ liệu đồ thị \& kết quả mong muốn.

        More generally, GNNs, like other DL techniques, are most effective when applied to specific problems. These problems are characterized by high complexity, i.e., learning good representations is critical to solving task at hand. E.g., a highly complex task could be recommending right products among billions of options to millions of customers. On other hand, some problems, e.g. finding youngest member of our family tree, can be solved without any ML technique.

        -- Nói chung, GNN, giống như các kỹ thuật DL khác, hiệu quả nhất khi được áp dụng cho các vấn đề cụ thể. Những vấn đề này được đặc trưng bởi độ phức tạp cao, i.e., việc học các biểu diễn tốt là rất quan trọng để giải quyết nhiệm vụ. E.g., 1 nhiệm vụ cực kỳ phức tạp có thể là đề xuất sản phẩm phù hợp giữa hàng tỷ lựa chọn cho hàng triệu khách hàng. Mặt khác, 1 số vấn đề, e.g. tìm thành viên trẻ nhất trong cây phả hệ gia đình, có thể được giải quyết mà không cần bất kỳ kỹ thuật ML nào.

        Furthermore, GNNs require a substantial amount of data to perform effectively. Traditional ML techniques might be a better fit in cases where dataset is small, as they are less reliant on large amounts of data. However, these techniques do not scale as well as GNNs. GNNs can process bigger datasets thanks to parallel \& distributed training. They can also exploit additional information more efficiently, which produces better results.

        -- Hơn nữa, GNN cần 1 lượng dữ liệu đáng kể để hoạt động hiệu quả. Các kỹ thuật ML truyền thống có thể phù hợp hơn trong trường hợp tập dữ liệu nhỏ, vì chúng ít phụ thuộc vào lượng dữ liệu lớn. Tuy nhiên, các kỹ thuật này không có khả năng mở rộng tốt như GNN. GNN có thể xử lý các tập dữ liệu lớn hơn nhờ đào tạo song song \& phân tán. Chúng cũng có thể khai thác thông tin bổ sung hiệu quả hơn, mang lại kết quả tốt hơn.
        \item {\sf Summary.} In this chap, answered 3 main questions: why graphs, why graph learning, \& why GNNs? 1st, explored versatility of graphs in representing various data types, e.g. social networks \& transportation networks, but also text \& images. Discussed different applications of graph learning, including node classification \& graph classification, \& highlighted 4 main families of graph learning techniques. Finally, emphasized significance of GNNs \& their superiority over other techniques, especially regarding large, complex datasets. By answering these 3 main questions, aimed to provide a comprehensive overview of importance of GNNs \& why they are becoming vital tools in ML.

        -- Trong chương này, chúng tôi đã trả lời 3 câu hỏi chính: tại sao lại dùng đồ thị, tại sao lại dùng học đồ thị, \& tại sao lại dùng mạng lưới thần kinh nhân tạo (GNN)? Đầu tiên, chúng tôi khám phá tính linh hoạt của đồ thị trong việc biểu diễn các loại dữ liệu khác nhau, e.g. mạng xã hội \& mạng lưới giao thông, cũng như văn bản \& hình ảnh. Chúng tôi đã thảo luận về các ứng dụng khác nhau của học đồ thị, bao gồm phân loại nút \& phân loại đồ thị, \& làm nổi bật 4 nhóm kỹ thuật học đồ thị chính. Cuối cùng, chúng tôi nhấn mạnh tầm quan trọng của mạng lưới thần kinh nhân tạo (GNN) \& sự vượt trội của chúng so với các kỹ thuật khác, đặc biệt là đối với các tập dữ liệu lớn \& phức tạp. Bằng cách trả lời 3 câu hỏi chính này, chúng tôi mong muốn cung cấp 1 cái nhìn tổng quan toàn diện về tầm quan trọng của mạng lưới thần kinh nhân tạo (GNN) \& lý do tại sao chúng đang trở thành những công cụ thiết yếu trong học máy.

        In Chap. 2: Graph Theory for GNNs, dive deeper into basics of graph theory, which provides foundation for understanding GNNs. This chap will cover fundamental concepts of graph theory, including concepts e.g. adjacency matrices \& degrees. Additionally, delve into different types of graphs \& their applications, e.g. directed \& undirected graphs, \& weighted \& unweighted graphs.

        -- Trong Chương 2: Lý thuyết Đồ thị cho Mạng Lưới Hàm Truyền Thống (GNN), tìm hiểu sâu hơn về những kiến thức cơ bản của lý thuyết đồ thị, vốn là nền tảng để hiểu về GNN. Chương này sẽ đề cập đến các khái niệm cơ bản của lý thuyết đồ thị, bao gồm các khái niệm như ma trận kề \& bậc. Ngoài ra, tìm hiểu sâu hơn về các loại đồ thị khác nhau \& ứng dụng của chúng, e.g. đồ thị có hướng \& vô hướng, \& trọng số \& không trọng số.
    \end{itemize}
    \item {\sf2. Graph Theory for Graph Neural Networks.} Graph theory is a fundamental branch of mathematics that deals with study of graphs \& networks. A graph is a visual representation of complex data structures that helps us understand relationships between different entities. Graph theory provides us with tools to model \& analyze a vast array of real-world problems, e.g. transportation systems, social networks, \& internet connectivity.

    -- Lý thuyết đồ thị là 1 nhánh cơ bản của toán học, chuyên nghiên cứu về đồ thị \& mạng. Đồ thị là 1 biểu diễn trực quan của các cấu trúc dữ liệu phức tạp, giúp chúng ta hiểu được mối quan hệ giữa các thực thể khác nhau. Lý thuyết đồ thị cung cấp cho chúng ta các công cụ để mô hình hóa \& phân tích 1 loạt các vấn đề thực tế, e.g. hệ thống giao thông, mạng xã hội, \& kết nối internet.

    In this chap, delve into essentials of graph theory, covering 3 main topics: graph properties, graph concepts, \& graph algorithms. Begin by defining graphs \& their components. Then introduce different types of graphs \& explain their properties \& applications. Next, cover fundamental graph concepts, objects, \& measures, including adjacency matrix. Finally, dive into graph algorithms, focusing on 2 fundamental algorithms, BFS \& DFS. By end of this chap, have a solid foundation in graph theory, allowing you to tackle more advanced topics \& design GNNs. In this chap, cover 3 main topics: introducing graph properties, discovering graph concepts, exploring graph algorithms.

    -- Trong chương này, chúng ta sẽ đi sâu vào những kiến thức cốt lõi của lý thuyết đồ thị, bao gồm 3 chủ đề chính: tính chất đồ thị, khái niệm đồ thị \& thuật toán đồ thị. Bắt đầu bằng việc định nghĩa đồ thị \& các thành phần của chúng. Sau đó, giới thiệu các loại đồ thị khác nhau \& giải thích tính chất cũng như ứng dụng của chúng. Tiếp theo, chúng ta sẽ tìm hiểu các khái niệm cơ bản về đồ thị, đối tượng, độ đo, bao gồm cả ma trận kề. Cuối cùng, chúng ta sẽ đi sâu vào các thuật toán đồ thị, tập trung vào 2 thuật toán cơ bản: BFS \& DFS. Kết thúc chương này, bạn sẽ có nền tảng vững chắc về lý thuyết đồ thị, cho phép bạn giải quyết các chủ đề nâng cao hơn \& thiết kế mạng nơ-ron nhân tạo (GNN). Trong chương này, chúng ta sẽ tìm hiểu 3 chủ đề chính: giới thiệu tính chất đồ thị, khám phá các khái niệm đồ thị \& tìm hiểu thuật toán đồ thị.
    \begin{itemize}
        \item {\sf2.1. Introducing graph properties.} In graph theory, a graph is a mathematical structure consisting of a set of objects, called {\it vertices} or {\it nodes}, \& a set of connections, called {\it edges}, which link pairs of vertices. Notation $G = (V,E)$ is used to represent a graph, where $G$ is graph, $V$: set of vertices, $E$: set of edges. Nodes of a graph can represent any objects, e.g. cities, people, web pages, or molecules, \& edges represent relationships or connections between them, e.g. physical roads, social relationships, hyperlinks, or chemical bonds. This sect provides an overview of fundamental graph properties used extensively in later chaps.

        -- {\sf Giới thiệu các tính chất của đồ thị.} Trong lý thuyết đồ thị, đồ thị là 1 cấu trúc toán học bao gồm 1 tập hợp các đối tượng, được gọi là {\it đỉnh} hoặc {\it nút}, \& 1 tập hợp các kết nối, được gọi là {\it cạnh}, nối các cặp đỉnh. Ký hiệu $G = (V,E)$ được sử dụng để biểu diễn 1 đồ thị, trong đó $G$ là đồ thị, $V$: tập hợp các đỉnh, $E$: tập hợp các cạnh. Các nút của đồ thị có thể biểu diễn bất kỳ đối tượng nào, e.g.: thành phố, con người, trang web hoặc phân tử, \& các cạnh biểu diễn các mối quan hệ hoặc kết nối giữa chúng, e.g.: đường xá thực tế, mối quan hệ xã hội, siêu liên kết hoặc liên kết hóa học. Phần này cung cấp tổng quan về các tính chất cơ bản của đồ thị được sử dụng rộng rãi trong các chương sau.
        \item {\sf Directed graphs.} 1 of most basic properties of a graph is whether it is directed or undirected. In a {\it directed graph}, also called a {\it digraph}, each edge has a direction or orientation, i.e., edge connects 2 nodes in a particular direction, where 1 node is source \& other is destination. In contrast, an undirected graph has undirected edges, where edges have no direction, i.e., edge between 2 vertices can be traversed in either direction, \& order in which we visit nodes does not matter. In Python, can use {\tt networkx} library to define an undirected graph as follows with {\tt nx.Graph()}:

        -- 1 trong những đặc điểm cơ bản nhất của đồ thị là nó có hướng hay vô hướng. Trong 1 đồ thị có hướng, còn được gọi là đồ thị có hướng, mỗi cạnh có 1 hướng hoặc hướng, i.e., cạnh nối 2 nút theo 1 hướng cụ thể, trong đó 1 nút là nguồn \& nút còn lại là đích. Ngược lại, 1 đồ thị vô hướng có các cạnh vô hướng, trong đó các cạnh không có hướng, i.e., cạnh giữa 2 đỉnh có thể được duyệt theo cả hai hướng, \& thứ tự chúng ta duyệt các nút không quan trọng. Trong Python, có thể sử dụng thư viện {\tt networkx} để định nghĩa 1 đồ thị vô hướng như sau với {\tt nx.Graph()}:
        \begin{verbatim}
import networkx as nx
# define an undirected graph
G = nx.Graph()
G.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'D'), ('B', 'E'), ('C', 'F'), ('C', 'G')])
        \end{verbatim}
        Code to create a directed graph is similar; simply replace {\tt nx.Graph()} with {\tt nx.DiGraph()}:
        \begin{verbatim}
# define a directed graph
DG = nx.DiGraph()
DG.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'D'), ('B', 'E'), ('C', 'F'), ('C', 'G')])
        \end{verbatim}
        In directed graphs, edges are typically represented using arrows to denote their orientation.
        \item {\sf Weighted graphs.} Another important property of graphs is whether edges are weighted or unweighted. In a {\it weighted graph}, each edge has a weight or cost associated with it. These weights can represent various factors, e.g. distance, travel time, or cost.

        -- 1 thuộc tính quan trọng khác của đồ thị là các cạnh có trọng số hay không có trọng số. Trong 1 đồ thị có trọng số, mỗi cạnh đều có trọng số hoặc chi phí liên quan. Các trọng số này có thể biểu thị nhiều yếu tố khác nhau, e.g.: khoảng cách, thời gian di chuyển hoặc chi phí.

        E.g., in a transportation network, weights of edges might represent distances between different cities or time it takes to travel between them. In contrast, unweighted graphs have no weight associated with their edges. These types of graphs are commonly used in situations where relationships between nodes are binary, \& edges simply indicate presence or absence of a connection between them.

        -- E.g., trong mạng lưới giao thông, trọng số của các cạnh có thể biểu thị khoảng cách giữa các thành phố khác nhau hoặc thời gian di chuyển giữa chúng. Ngược lại, đồ thị không trọng số không gắn trọng số với các cạnh của chúng. Các loại đồ thị này thường được sử dụng trong các trường hợp mối quan hệ giữa các nút là nhị phân, \& cạnh chỉ đơn giản biểu thị sự có hoặc không có kết nối giữa chúng.

        Can modify previous undirected graph to add weights to our edges. In {\tt networkx}, edges of graph are defined with a tuple containing start \& end nodes \& a dictionary specifying edge's weight:
        \begin{verbatim}
# define an weighted graph
WG = nx.Graph()
WG.add_edges_from([('A', 'B', {"weight": 10}), ('A', 'C', {"weight": 20}), ('B', 'D', {"weight": 30}), ('B', 'E', {"weight": 40}), ('C', 'F', {"weight": 50}), ('C', 'G', {"weight": 60})])
labels = nx.get_edge_attributes(WG, "weight")
        \end{verbatim}
        \item {\sf Connected graphs.} Graph connectivity is a fundamental concept in graph theory that is closely related to graph's structure \& function.

        -- Kết nối đồ thị là 1 khái niệm cơ bản trong lý thuyết đồ thị có liên quan chặt chẽ đến cấu trúc \& chức năng của đồ thị.

        In a connected graph, there is a path between any 2 vertices in graph. Formally, a graph $G$ is connected iff for every pair of vertices $u,v$ in $G$, there exists a path from $u$ to $v$. In contrast, a graph is disconnected if it is not connected, i.e., at least 2 vertices are not connected by a path.

        -- Trong 1 đồ thị liên thông, luôn có 1 đường đi giữa hai đỉnh bất kỳ trong đồ thị. Về mặt hình thức, 1 đồ thị $G$ được coi là liên thông nếu \& chỉ nếu (iff) với mọi cặp đỉnh $u,v$ trong $G$, tồn tại 1 đường đi từ $u$ đến $v$. Ngược lại, 1 đồ thị được coi là không liên thông nếu nó không liên thông, i.e., có ít nhất 2 đỉnh không được nối với nhau bằng 1 đường đi.

        {\tt networkx} library provides a built-in function for verifying whether a graph is connected or not. In following example, 1st graph contains isolated nodes (4 \& 5), unlike 2nd graph.

        -- Thư viện {\tt networkx} cung cấp 1 hàm tích hợp để kiểm tra xem đồ thị có kết nối hay không. Trong ví dụ sau, đồ thị thứ nhất chứa các nút riêng biệt (4 \& 5), không giống như đồ thị thứ 2.
        \begin{verbatim}
# verify whether a graph is connected or disconnected
G1 = nx.Graph()
G1.add_edges_from([(1, 2), (2, 3), (3, 1), (4, 5)])
print(f"Is graph 1 connected? {nx.is_connected(G1)}")

G2 = nx.Graph()
G2.add_edges_from([(1, 2), (2, 3), (3, 1), (1, 4)])
print(f"Is graph 2 connected? {nx.is_connected(G2)}")
        \end{verbatim}
        This property is easy to visualize with small graphs. Connected graphs have several interesting properties \& applications. E.g., in a communication network, a connected graph ensures that any 2 nodes can communicate with each other through a path. In contrast, disconnected graphs can have isolated nodes that cannot communicate with other nodes in network, making it challenging to design efficient routing algorithms.

        -- Tính chất này dễ hình dung bằng các đồ thị nhỏ. Đồ thị liên thông có 1 số tính chất \& ứng dụng thú vị. Ví dụ, trong mạng truyền thông, 1 đồ thị liên thông đảm bảo rằng bất kỳ 2 nút nào cũng có thể giao tiếp với nhau thông qua 1 đường dẫn. Ngược lại, đồ thị liên thông có thể có các nút bị cô lập không thể giao tiếp với các nút khác trong mạng, khiến việc thiết kế các thuật toán định tuyến hiệu quả trở nên khó khăn.

        There are different ways to measure connectivity of a graph. 1 of most common measures is minimum number of edges that need to be removed to disconnect graph, which is known as graph's minimum cut. Minimum cut problem has several applications in network flow optimization, clustering, \& community detection.

        -- Có nhiều cách khác nhau để đo lường khả năng kết nối của 1 đồ thị. 1 trong những phép đo phổ biến nhất là số cạnh tối thiểu cần loại bỏ để ngắt kết nối đồ thị, được gọi là đường cắt tối thiểu của đồ thị. Bài toán đường cắt tối thiểu có nhiều ứng dụng trong tối ưu hóa luồng mạng, phân cụm, \& phát hiện cộng đồng.
        \item {\sf Types of graphs.} In addition to commonly used graph types, there are some special types of graphs that have unique properties \& characteristics:
        \begin{enumerate}
            \item A tree is a connected, undirected graph with no cycles. Since there is only 1 path between any 2 nodes in a tree, a tree is a special case of a graph. Trees are often used to model hierarchical structures, e.g. family trees, organizational structures, or classification trees.
            \item A rooted tree is a tree in which 1 node is designated as root, \& all other vertices are connected to it by a unique path. Rooted trees are often used in CS to represent hierarchical data structures, e.g. filesystems or structures of XML documents.
            \item A directed acyclic graph (DAG) is a directed graph that has no cycles, i.e., edges can only be traversed in a particular direction, \& there are no loops or cycles. DAGs are often used to model dependencies between tasks or events -- e.g., in project management or in computing critical path of a job.
            \item A bipartite graph is a graph in which vertices can be divided into 2 disjoint sets, s.t. all edges connect vertices in different sets. Bipartite graphs are often used in mathematics \& CS to model relationships between 2 different types of objects, e.g. buyers \& sellers, or employees \& projects.
            \item A complete graph is a graph in which every pair of vertices is connected by an edge. Complete graphs are often used in combinatorics to model problems involving all possible pairwise connections, \& in computer networks to model fully connected networks.
        \end{enumerate}
        Now have reviewed essential types of graphs, explore some of most important graph objects. Understanding these concepts will help analyze \& manipulate graphs effectively.

        -- {\sf Các loại đồ thị.} Ngoài các loại đồ thị thường dùng, còn có 1 số loại đồ thị đặc biệt với các thuộc tính \& đặc điểm riêng:
        \begin{enumerate}
            \item Cây là đồ thị liên thông, vô hướng \& không có chu trình. Vì chỉ có 1 đường đi giữa 2 nút bất kỳ trong cây, nên cây là 1 trường hợp đặc biệt của đồ thị. Cây thường được sử dụng để mô hình hóa các cấu trúc phân cấp, e.g.: cây phả hệ, cấu trúc tổ chức hoặc cây phân loại.
            \item Cây có gốc là cây trong đó 1 nút được chỉ định là gốc, \& tất cả các đỉnh khác được kết nối với nó bằng 1 đường đi duy nhất. Cây có gốc thường được sử dụng trong CS để biểu diễn các cấu trúc dữ liệu phân cấp, e.g.: hệ thống tệp hoặc cấu trúc của tài liệu XML.
            \item Đồ thị có hướng phi chu trình (DAG) là đồ thị có hướng không có chu trình, i.e., các cạnh chỉ có thể được duyệt theo 1 hướng cụ thể, \& không có vòng lặp hoặc chu trình. DAG thường được sử dụng để mô hình hóa sự phụ thuộc giữa các tác vụ hoặc sự kiện -- e.g.: trong quản lý dự án hoặc tính toán đường dẫn tới hạn của 1 công việc.
            \item Đồ thị hai phía là đồ thị trong đó các đỉnh có thể được chia thành 2 tập rời rạc, nghĩa là tất cả các cạnh đều nối các đỉnh thuộc các tập khác nhau. Đồ thị hai phía thường được sử dụng trong toán học \& Khoa học Máy tính để mô hình hóa mối quan hệ giữa 2 loại đối tượng khác nhau, e.g.: người mua \& người bán, hoặc nhân viên \& dự án.
            \item Đồ thị đầy đủ là đồ thị trong đó mỗi cặp đỉnh được nối bởi 1 cạnh. Đồ thị đầy đủ thường được sử dụng trong tổ hợp để mô hình hóa các bài toán liên quan đến tất cả các kết nối từng cặp có thể có, \& trong mạng máy tính để mô hình hóa các mạng được kết nối đầy đủ.
        \end{enumerate}
        Bây giờ chúng ta đã xem xét các loại đồ thị thiết yếu, khám phá 1 số đối tượng đồ thị quan trọng nhất. Việc hiểu các khái niệm này sẽ giúp phân tích \& thao tác đồ thị 1 cách hiệu quả.
        \item {\sf Discovering graph concepts.} In this sect, explore some of essential concepts in graph theory, including graph objects (e.g. degree \& neighbors), graph measures (e.g., centrality \& density), \& adjacency matrix representation.

        -- {\sf Khám phá các khái niệm về đồ thị.} Trong phần này, khám phá 1 số khái niệm thiết yếu trong lý thuyết đồ thị, bao gồm các đối tượng đồ thị (e.g.: bậc \& lân cận), các phép đo đồ thị (e.g.: độ trung tâm \& mật độ), \& biểu diễn ma trận kề.
        \begin{itemize}
            \item {\sf Fundamental objects.} 1 of key concepts in graph theory is degree of a node, which is number of edges incident to this node. An edge is said to be incident on a node if that node is 1 of edge's endpoints. Degree of a node $v$ is often denoted by $\deg v$. It can be defined for both directed \& undirected graphs:
            \begin{itemize}
                \item In an undirected graph, degree of a vertex is number of edges connected to it. If node is connected to itself (called a loop, or self-loop), it adds 2 to degree.
                \item In a directed graph, degree is divided into 2 types: indegree \& outdegree. Indegree (denoted by $\deg_-(v)$) of a node represents number of edges that point towards that node, while outdegree (denoted by $\deg^+(v)$) represents number of edges that start from that node. In this case, a self-loop adds 1 to indegree \& to outdegree.
            \end{itemize}
            Indegree \& outdegree are essential for analyzing \& understanding directed graphs, as they provide insight into how information or resources are distributed within graph. E.g., nodes with high indegree are likely to be important sources of information or resources. In contrast, nodes with high outdegree are likely to be important destinations or consumers of information or resources.

            -- {\sf Các đối tượng cơ bản.} 1 trong những khái niệm chính trong lý thuyết đồ thị là bậc của 1 nút, i.e., số cạnh liên thuộc nút này. 1 cạnh được gọi là liên thuộc nút nếu nút đó là 1 trong các điểm cuối của cạnh đó. Bậc của 1 nút $v$ thường được ký hiệu là $\deg v$. Nó có thể được định nghĩa cho cả đồ thị có hướng \& vô hướng:
            \begin{itemize}
                \item Trong đồ thị vô hướng, bậc của 1 đỉnh là số cạnh được nối với nó. Nếu nút được nối với chính nó (gọi là vòng lặp, hoặc tự vòng lặp), nó cộng thêm 2 vào bậc.
                \item Trong đồ thị có hướng, bậc được chia thành 2 loại: bậc vào \& bậc ra. Bậc vào (ký hiệu là $\deg_-(v)$) của 1 nút biểu diễn số cạnh hướng đến nút đó, trong khi bậc ra (ký hiệu là $\deg^+(v)$) biểu diễn số cạnh bắt đầu từ nút đó. Trong trường hợp này, 1 vòng lặp tự thêm 1 vào bậc vào \& vào bậc ra.
            \end{itemize}
            Bậc vào \& bậc ra rất cần thiết để phân tích \& hiểu đồ thị có hướng, vì chúng cung cấp cái nhìn sâu sắc về cách thông tin hoặc tài nguyên được phân bổ trong đồ thị. Ví dụ: các nút có bậc vào cao có thể là nguồn thông tin hoặc tài nguyên quan trọng. Ngược lại, các nút có bậc ra cao có thể là đích đến hoặc người tiêu thụ thông tin hoặc tài nguyên quan trọng.

            In {\tt networkx}, can simply calculate node degree, indegree, or outdegree using built-in methods:

            -- Trong {\tt networkx}, có thể dễ dàng tính toán bậc nút, bậc vào hoặc bậc ra bằng các phương pháp tích hợp:
            \begin{verbatim}
# calculate node degree, indegree, or outdegree
G = nx.Graph()
G.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'D'), ('B', 'E'), ('C', 'F'), ('C', 'G')])
print(f"deg(A) = {G.degree['A']}")

DG = nx.DiGraph()
DG.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'D'), ('B', 'E'), ('C', 'F'), ('C', 'G')])
print(f"deg^-(A) = {DG.in_degree['A']}")
print(f"deg^+(A) = {DG.out_degree['A']}")
            \end{verbatim}
            Concept of node degree is related to that of neighbors. Neighbors refer to nodes directly connected to a particular node through an edge. Moreover, 2 nodes are said to be adjacent if they share at least 1 common neighbor. Concepts of neighbors \& adjacency are fundamental to many graph algorithms \& applications, e.g. searching for a path between 2 nodes or identifying clusters in a network.

            -- Khái niệm bậc nút liên quan đến khái niệm lân cận. Lân cận là các nút được kết nối trực tiếp với 1 nút cụ thể thông qua 1 cạnh. Hơn nữa, 2 nút được gọi là lân cận nếu chúng có chung ít nhất 1 lân cận. Khái niệm lân cận \& kề là nền tảng cho nhiều thuật toán đồ thị \& ứng dụng, e.g.: tìm kiếm đường đi giữa 2 nút hoặc xác định các cụm trong mạng.

            In graph theory, a path is a sequence of edges that connects 2 nodes (or more) in a graph. Length of a path is number of edges that are traversed along path. There are different types of paths, but 2 of them are particularly important:
            \begin{enumerate}
                \item A simple path is a path that does not visit any node more than once, except for start \& end vertices.
                \item A cycle is a path in which 1st \& last vertices are same. A graph is said to be acylic if it contains no cycles, e.g. trees \& DAGs.
            \end{enumerate}
            Degrees \& paths can be used to determine importance of a node in a network. This measure is referred to as {\it centrality}.

            -- Trong lý thuyết đồ thị, đường đi là 1 chuỗi các cạnh nối 2 nút (hoặc nhiều hơn) trong 1 đồ thị. Độ dài của đường đi là số cạnh được đi qua dọc theo đường đi. Có nhiều loại đường đi khác nhau, nhưng có 2 loại đặc biệt quan trọng:
            \begin{enumerate}
                \item Đường đi đơn là đường đi không đi qua bất kỳ nút nào quá 1 lần, ngoại trừ đỉnh đầu \& đỉnh cuối.
                \item Chu trình là đường đi mà đỉnh đầu \& đỉnh cuối giống nhau. 1 đồ thị được gọi là chu trình nếu nó không chứa chu trình nào, e.g.: cây \& DAG.
            \end{enumerate}
            Bậc \& đường đi có thể được sử dụng để xác định tầm quan trọng của 1 nút trong mạng. Thước đo này được gọi là độ trung tâm.
            \item {\sf Graph measures.} Centrality quantifies importance of a vertex or node in a network. It helps us to identify key node in a graph based on their connectivity \& influence on flow of information or interactions within network. There are several measures of centrality, each providing a different perspective on importance of a node:
            \begin{enumerate}
                \item {\bf Degree centrality} is 1 of simplest \& most commonly used measures of centrality. It is simply defined as degree of node. A high degree centrality indicates a vertex is highly connected to other vertices in graph, \& thus significantly influences network.
                \item {\bf Closeness centrality} measures how close a node is to all other nodes in graph. It corresponds to average length of shortest path between target node \& all other nodes in graph. A node with high closeness centrality can quickly reach all other vertices in network.
                \item {\bf Betweenness centrality} measures number of times a node lies on shortest path between pairs of other nodes in graph. A node with high betweenness centrality acts as a bottleneck or bridge between different parts of graph.
            \end{enumerate}
            Calculate these measure on previous graphs using built-in functions of {\tt networkx} \& analyze result:

            -- {\sf Đo lường đồ thị.} Độ tập trung định lượng tầm quan trọng của 1 đỉnh hoặc nút trong mạng. Nó giúp chúng ta xác định nút chính trong đồ thị dựa trên khả năng kết nối \& ảnh hưởng của chúng đến luồng thông tin hoặc tương tác trong mạng. Có 1 số thước đo độ tập trung, mỗi thước đo cung cấp 1 góc nhìn khác nhau về tầm quan trọng của 1 nút:
            \begin{enumerate}
                \item {\bf Độ tập trung bậc} là 1 trong những thước đo độ tập trung đơn giản nhất \& được sử dụng phổ biến nhất. Nó được định nghĩa đơn giản là bậc của nút. Độ tập trung bậc cao cho thấy 1 đỉnh có kết nối cao với các đỉnh khác trong đồ thị, \& do đó ảnh hưởng đáng kể đến mạng.
                \item {\bf Độ tập trung gần} đo lường mức độ gần của 1 nút với tất cả các nút khác trong đồ thị. Nó tương ứng với độ dài trung bình của đường đi ngắn nhất giữa nút mục tiêu \& tất cả các nút khác trong đồ thị. 1 nút có độ tập trung gần cao có thể nhanh chóng tiếp cận tất cả các đỉnh khác trong mạng.
                \item {\bf Độ tập trung giữa} đo lường số lần 1 nút nằm trên đường đi ngắn nhất giữa các cặp nút khác trong đồ thị. 1 nút có tính trung tâm cao đóng vai trò như 1 nút thắt cổ chai hoặc cầu nối giữa các phần khác nhau của đồ thị.
            \end{enumerate}
            Tính toán các phép đo này trên các đồ thị trước đó bằng cách sử dụng các hàm tích hợp của {\tt networkx} \& phân tích kết quả:
            \begin{verbatim}
# Graph measures
print(f"Degree centrality = {nx.degree_centrality(G)}")
print(f"Closeness centrality = {nx.closeness_centrality(G)}")
print(f"Betweenness centrality = {nx.betweenness_centrality(G)}")
            \end{verbatim}
            Importance of nodes A, B, C in a graph depends on type of centrality used. Degree centrality considers nodes B \& C to be more important because they have more neighbors than node A. However, in closeness centrality, node A is most important as it can reach any other node in graph in shortest possible path. On other hand, nodes A, B, \& C have equal betweenness centrality, as they all lie on a large number of shortest paths between other nodes.

            -- Tầm quan trọng của các nút A, B, C trong đồ thị phụ thuộc vào loại trung tâm được sử dụng. Trung tâm bậc coi các nút B \& C quan trọng hơn vì chúng có nhiều hàng xóm hơn nút A. Tuy nhiên, về trung tâm gần, nút A quan trọng nhất vì nó có thể đến bất kỳ nút nào khác trong đồ thị theo đường đi ngắn nhất có thể. Mặt khác, các nút A, B, \& C có trung tâm nằm giữa bằng nhau, vì tất cả chúng đều nằm trên 1 số lượng lớn các đường đi ngắn nhất giữa các nút khác.

            In addition to these measures, see how to calculate importance of a node using ML techniques in next chaps. However, it is not only measure covered. Indeed, {\it density} is another important measure, indicating how connected graph is. It is a ratio between actual number of edges \& maximum possible number of edges in graph. A graph with high density is considered more connected \& has more information flow compared to a graph with low density.

            -- Ngoài các phép đo này, xem cách tính tầm quan trọng của 1 nút bằng kỹ thuật ML trong các chương tiếp theo. Tuy nhiên, đây không chỉ là phép đo được đề cập. Thật vậy, {\it density} là 1 phép đo quan trọng khác, cho biết mức độ kết nối của đồ thị. Nó là tỷ lệ giữa số cạnh thực tế \& số cạnh tối đa có thể có trong đồ thị. 1 đồ thị có mật độ cao được coi là kết nối hơn \& có luồng thông tin lớn hơn so với đồ thị có mật độ thấp.

            Formula to calculate density depends on whether graph is directed or undirected. For an undirected graph with $n$ nodes, maximum possible number of edges is $\frac{n(n - 1)}{2}$. For a directed graph with $n$ nodes, maximum number of edges is $n(n - 1)$. Density of a graph is calculated as number of edges divided by maximum number of edges.

            -- Công thức tính mật độ phụ thuộc vào việc đồ thị có hướng hay vô hướng. Đối với đồ thị vô hướng có $n$ nút, số cạnh tối đa có thể là $\frac{n(n - 1)}{2}$. Đối với đồ thị có hướng có $n$ nút, số cạnh tối đa là $n(n - 1)$. Mật độ của đồ thị được tính bằng số cạnh chia cho số cạnh tối đa.

            A dense graph has a density closer to 1, while a sparse graph has a density closer to 0. There is no strict rule for what constitutes a dense or sparse graph, but generally, a graph is considered dense if its density is $> 0.5$ \& sparse if its density is $< 0.1$. This measure is directly connected to a fundamental problem with graphs: how to represent adjacency matrix.

            -- Đồ thị dày đặc có mật độ gần bằng 1, trong khi đồ thị thưa có mật độ gần bằng 0. Không có quy tắc nghiêm ngặt nào về việc thế nào là đồ thị dày đặc hay thưa thớt, nhưng nhìn chung, 1 đồ thị được coi là dày đặc nếu mật độ của nó > 0,5 \& thưa nếu mật độ của nó < 0,1. Phép đo này liên quan trực tiếp đến 1 vấn đề cơ bản với đồ thị: cách biểu diễn ma trận kề.
            \item {\sf Adjacency matrix representation.} An adjacency matrix is a matrix that represents edges in a graph, where each cell indicates whether there is an edge between 2 nodes. Adjacency matrix is a square matrix of size $n\times n$, where $n$: number of nodes in graph. A value of 1 in cell $(i, j)$ indicates that there is an edge between node $i$ \& node $j$, while a value of 0 indicates that there is no edge. For an undirected graph, adjacency matrix is symmetric, while for a directed graph, adjacency matrix is not necessarily symmetric. In Python, it can be implemented as a list of lists:

            -- {\sf Biểu diễn ma trận kề.} Ma trận kề là 1 ma trận biểu diễn các cạnh trong đồ thị, trong đó mỗi ô biểu thị liệu có cạnh giữa 2 nút hay không. Ma trận kề là 1 ma trận vuông có kích thước $n\times n$, trong đó $n$: số nút trong đồ thị. Giá trị 1 trong ô $(i, j)$ biểu thị rằng có 1 cạnh giữa nút $i$ \& nút $j$, trong khi giá trị 0 biểu thị rằng không có cạnh nào. Đối với đồ thị vô hướng, ma trận kề là đối xứng, trong khi đối với đồ thị có hướng, ma trận kề không nhất thiết phải đối xứng. Trong Python, ma trận kề có thể được triển khai dưới dạng danh sách các danh sách.

            Adjacency matrix is a straightforward representation that can be easily visualized as a 2D array. 1 of key advantages of using an adjacency matrix: checking whether 2 nodes are connected is a constant time operation $O(1)$. This makes it an efficient way to test existence of an edge in graph. Moreover, it is used to perform matrix operations, which are useful for certain graph algorithms, e.g. calculating shortest path between 2 nodes.

            -- Ma trận kề là 1 biểu diễn đơn giản có thể dễ dàng hình dung dưới dạng 1 mảng 2D. 1 trong những lợi thế chính của việc sử dụng ma trận kề: việc kiểm tra xem 2 nút có được kết nối hay không là 1 phép toán thời gian hằng số $O(1)$. Điều này khiến nó trở thành 1 cách hiệu quả để kiểm tra sự tồn tại của 1 cạnh trong đồ thị. Hơn nữa, nó được sử dụng để thực hiện các phép toán ma trận, hữu ích cho 1 số thuật toán đồ thị, e.g.: tính toán đường đi ngắn nhất giữa 2 nút.

            However, adding or removing nodes can be costly, as matrix needs to be resized or shifted. 1 of main drawbacks of using an adjacency matrix is its space complexity: as number of nodes in graph grows, space required to store adjacency matrix increases exponentially. Formally, say: adjacency matrix has a space complexity of $O(|V|^2)$, where $|V|$ represents number of nodes in graph.

            -- Tuy nhiên, việc thêm hoặc xóa các nút có thể tốn kém, vì ma trận cần được thay đổi kích thước hoặc dịch chuyển. 1 trong những nhược điểm chính của việc sử dụng ma trận kề là độ phức tạp về không gian: khi số lượng nút trong đồ thị tăng lên, không gian cần thiết để lưu trữ ma trận kề cũng tăng theo cấp số nhân. Ví dụ: ma trận kề có độ phức tạp về không gian là $O(|V|^2)$, trong đó $|V|$ biểu thị số lượng nút trong đồ thị.

            Overall, while adjacency matrix is a useful data structure for representing small graphs, it may not be practical for larger ones due to its space complexity. Additionally, overhead of adding or removing nodes can make it inefficient for dynamically changing graphs.

            -- Nhìn chung, mặc dù ma trận kề là 1 cấu trúc dữ liệu hữu ích để biểu diễn các đồ thị nhỏ, nhưng nó có thể không thực tế đối với các đồ thị lớn hơn do độ phức tạp về không gian. Ngoài ra, việc thêm hoặc bớt các nút có thể khiến ma trận kề không hiệu quả đối với các đồ thị thay đổi động.

            This is why other representation can be helpful. E.g., another popular way to store graphs is {\it edge list}. An edge list is a list of all edges in a graph. Each edge is represented by a tuple or a pair of vertices. Edge list can also include weight or cost of each edge. This is data structure used to create our graphs with {\tt networkx}:

            -- Đây là lý do tại sao các biểu diễn khác có thể hữu ích. Ví dụ, 1 cách phổ biến khác để lưu trữ đồ thị là {\tt edge list}. Danh sách cạnh là danh sách tất cả các cạnh trong 1 đồ thị. Mỗi cạnh được biểu diễn bằng 1 bộ hoặc 1 cặp đỉnh. Danh sách cạnh cũng có thể bao gồm trọng số hoặc chi phí của mỗi cạnh. Đây là cấu trúc dữ liệu được sử dụng để tạo đồ thị của chúng ta với {\tt networkx}:
            \begin{verbatim}
edge_list = [(0, 1), (0, 2), (1, 3), (1, 4), (2, 5), (2, 6)]
            \end{verbatim}
            When compare both data structures applied to our graph, clear: edge list is less verbose. This is case because our graph is fairly sparse. On other hand, if our graph was complete, would require 21 tuples instead of 6. This is explained by a space complexity of $O(|E|)$, where $|E|$ is number of edges. Edge lists are more efficient for storing sparse graphs, where number of edges is much smaller than number of nodes.

            -- Khi so sánh cả hai cấu trúc dữ liệu được áp dụng cho đồ thị của chúng ta, rõ ràng: danh sách cạnh ít chi tiết hơn. Điều này là do đồ thị của chúng ta khá thưa thớt. Mặt khác, nếu đồ thị của chúng ta đầy đủ, sẽ cần 21 bộ thay vì 6. Điều này được giải thích bởi độ phức tạp không gian là $O(|E|)$, trong đó $|E|$ là số cạnh. Danh sách cạnh hiệu quả hơn trong việc lưu trữ đồ thị thưa thớt, trong đó số cạnh nhỏ hơn nhiều so với số nút.

            However, checking whether 2 vertices are connected in an edge list requires iterating through entire list, which can be time-consuming for large graphs with many edges. Therefore, edge lists are commonly used in applications where space is a concern.

            -- Tuy nhiên, việc kiểm tra xem 2 đỉnh có được kết nối trong danh sách cạnh hay không đòi hỏi phải lặp lại toàn bộ danh sách, điều này có thể tốn thời gian đối với các đồ thị lớn có nhiều cạnh. Do đó, danh sách cạnh thường được sử dụng trong các ứng dụng đòi hỏi không gian.

            A 3rd \& popular representation is {\it adjacency list}. It consists of a list of pairs, where each pair represents a node in graph \& its adjacent nodes. The pairs can be stored in a linked list, dictionary, or other data structures, depending on implementation. An adjacency list has several advantages over an adjacency matrix or an edge list. 1st, space complexity is $O(|V| + |E|)$. This is more efficient than $O(|V|^2)$ space complexity of an adjacency matrix for sparse graphs. 2nd, it allows for efficient iteration through adjacent vertices of a node, which is useful in many graph algorithms. Finally, adding a node or an edge can be done in constant time.

            -- Biểu diễn phổ biến thứ 3 là {\it adjacency list}. Nó bao gồm 1 danh sách các cặp, trong đó mỗi cặp biểu diễn 1 nút trong đồ thị \& các nút liền kề của nó. Các cặp có thể được lưu trữ trong danh sách liên kết, từ điển hoặc các cấu trúc dữ liệu khác, tùy thuộc vào cách triển khai. Danh sách kề có 1 số ưu điểm so với ma trận kề hoặc danh sách cạnh. Thứ nhất, độ phức tạp không gian là $O(|V| + |E|)$. Điều này hiệu quả hơn độ phức tạp không gian $O(|V|^2)$ của ma trận kề đối với đồ thị thưa. Thứ hai, nó cho phép lặp hiệu quả qua các đỉnh liền kề của 1 nút, điều này hữu ích trong nhiều thuật toán đồ thị. Cuối cùng, việc thêm 1 nút hoặc 1 cạnh có thể được thực hiện trong thời gian không đổi.

            However, checking whether 2 vertices are connected can be slower than with an adjacency matrix. This is because it requires iterating through adjacency list of 1 of vertices, which can be time-consuming for large graphs.

            -- Tuy nhiên, việc kiểm tra xem 2 đỉnh có được kết nối hay không có thể chậm hơn so với ma trận kề. Điều này là do ma trận này yêu cầu phải lặp qua danh sách kề của 1 đỉnh, điều này có thể tốn thời gian đối với các đồ thị lớn.

            Each data structure has its own advantages \& disadvantages that depend on specific application \& requirements. In next sect, process graphs \& introduce 2 most fundamental graph algorithms.

            -- Mỗi cấu trúc dữ liệu đều có ưu điểm \& nhược điểm riêng, tùy thuộc vào yêu cầu \& ứng dụng cụ thể. Trong phần tiếp theo, đồ thị quy trình \& giới thiệu 2 thuật toán đồ thị cơ bản nhất.
        \end{itemize}
        \item {\sf Exploring graph algorithms.} Graph algorithms are critical in solving problems related to graphs, e.g. finding shortest path between 2 nodes or detecting cycles. This sect will discuss 2 graph traversal algorithms: BFS \& DFS.

        -- {\sf Khám phá thuật toán đồ thị.} Thuật toán đồ thị đóng vai trò quan trọng trong việc giải quyết các bài toán liên quan đến đồ thị, e.g.: tìm đường đi ngắn nhất giữa 2 nút hoặc phát hiện chu trình. Phần này sẽ thảo luận về 2 thuật toán duyệt đồ thị: BFS \& DFS.
        \begin{itemize}
            \item {\sf Breadth-first search.} BFS is a graph traversal algorithm that starts at root node \& explores all neighboring nodes at a particular level before moving to next level of nodes. It works by maintaining a queue of nodes to visit \& marking each visited node as it is added to queue. Algorithm then dequeues next node in queue \& explores all its neighbors, adding them to queue if they have not been visited yet. How can implement BFS in Python:
            \begin{itemize}
                \item Create an empty graph \& add edges with \verb|add_edges_from()| method.
                \item Define a function called {\tt bfs()} that implements BFs algorithm on a graph. Function takes 2 arguments: {\tt graph} object \& starting node for search.
                \item Initialize 2 lists {\tt visited, queue} \& add starting node. {\tt visited} list keeps track of nodes that have been visited during search, while {\tt queue} list stores nodes that need to be visited.
                \item Enter a {\tt while} loop that continues until {\tt queue} list is empty. Inside loop, remove 1st node in {\tt queue} list using {\tt pop(0)} method \& store result in {\tt node} variable.
                \item Iterate through neighbors of node using a {\tt for} loop. For each neighbor that has not been visited yet, add it to {\tt visited} list \& to end of {\tt queue} list using {\tt append()} method. When it is complete, return {\tt visited} list.
                \item Call {\tt bfs()} function with {\tt G} argument \& {\tt'A'} starting node: {\tt bfs(G, 'A')}.
                \item Function returns list of visited nodes in order in which they were visited.
            \end{itemize}
            BFS is particularly useful in finding shortest path between 2 nodes in an unweighted graph. This is because algorithm visits nodes in order of their distance from starting node, so 1st time target node is visited, it must be along shortest path from starting node.

            -- BFS đặc biệt hữu ích trong việc tìm đường đi ngắn nhất giữa 2 nút trong đồ thị không trọng số. Điều này là do thuật toán sẽ duyệt các nút theo thứ tự khoảng cách từ nút bắt đầu, do đó, lần đầu tiên nút đích được duyệt, nó phải nằm trên đường đi ngắn nhất từ nút bắt đầu.

            In addition to finding shortest path, BFS can also be used to check whether a graph is connected or to find all connected components of a graph. It is also used in applications e.g. web crawlers, social network analysis, \& shortest path routing in networks.

            -- Ngoài việc tìm đường đi ngắn nhất, BFS còn có thể được sử dụng để kiểm tra xem 1 đồ thị có liên thông hay không hoặc để tìm tất cả các thành phần liên thông của đồ thị. Nó cũng được sử dụng trong các ứng dụng như trình thu thập dữ liệu web, phân tích mạng xã hội \& định tuyến đường đi ngắn nhất trong mạng.

            Time complexity of BFS is $O(|V| + |E|)$. This can be a significant issue for graphs with a high degree of connectivity or for graphs that are sparse. Several variants of BFS have been developed to mitigate this issue, e.g. bidirectional BFS \& A* search, wjich use heuristics to reduce number of nodes that need to be explored.

            -- Độ phức tạp thời gian của BFS là $O(|V| + |E|)$. Điều này có thể là 1 vấn đề đáng kể đối với các đồ thị có mức độ kết nối cao hoặc các đồ thị thưa thớt. 1 số biến thể của BFS đã được phát triển để giảm thiểu vấn đề này, e.g.: tìm kiếm BFS song hướng \& A*, sử dụng các thuật toán tìm kiếm để giảm số lượng nút cần khám phá.
            \item {\sf Depth-first search.} DFS is a recursive algorithm that starts at root node \& explores as far as possible along each branch before backtracking. It chooses a node \& explores all of its unvisited neighbors, visiting 1st neighbor that has not been explored \& backtracking only when all neighbors have been visited. By doing so, it explores graph by following as deep a path from starting node as possible before backtracking to explore other branches. This continues until all nodes have been explored. Implement DFS in Python:
            \begin{enumerate}
                \item 1st initialize an empty list called {\tt visited}.
                \item Define a function called {\tt dfs()} that takes in {\tt visited, graph, node} as arguments: {\tt def dfs(visited, graph, node):}
                \item If current {\tt node} is not in {\tt visited} list, append it to list.
                \item Then iterate through each neighbor of current {\tt node}. For each neighbor, recursively call {\tt dfs()} function passing in {\tt visited, graph, neighbor} as arguments.
                \item {\sf dfs()} function continues to explore graph depth-1st, visiting all neighbors of each node until there are no more unvisited neighbors. Finally, {\tt visited} list is returned.
            \end{enumerate}
            -- {\sf Tìm kiếm theo chiều sâu.} DFS là 1 thuật toán đệ quy bắt đầu từ nút gốc \& khám phá càng xa càng tốt dọc theo mỗi nhánh trước khi quay lui. Nó chọn 1 nút \& khám phá tất cả các hàng xóm chưa được thăm của nó, thăm hàng xóm thứ nhất chưa được thăm \& chỉ quay lui khi tất cả các hàng xóm đã được thăm. Bằng cách này, nó khám phá đồ thị bằng cách đi theo đường dẫn sâu nhất có thể từ nút bắt đầu trước khi quay lui để khám phá các nhánh khác. Quá trình này tiếp tục cho đến khi tất cả các nút đã được khám phá. Triển khai DFS trong Python:
            \begin{enumerate}
                \item Đầu tiên, khởi tạo 1 danh sách rỗng có tên là {\tt visited}.
                \item Định nghĩa 1 hàm có tên là {\tt dfs()} nhận {\tt visited, graph, node} làm đối số: {\tt def dfs(visited, graph, node):}
                \item Nếu {\tt node} hiện tại không có trong danh sách {\tt visited}, thêm nó vào danh sách.
                \item Sau đó, lặp qua từng hàng xóm của {\tt nút} hiện tại. Với mỗi hàng xóm, gọi đệ quy hàm {\tt dfs()} truyền vào {\tt visited, graph, neighbor} làm đối số.
                \item Hàm {\sf dfs()} tiếp tục khám phá đồ thị theo chiều sâu 1, duyệt tất cả các hàng xóm của mỗi nút cho đến khi không còn hàng xóm nào chưa được duyệt. Cuối cùng, danh sách {\tt visited} được trả về.
            \end{enumerate}
            DFS is useful in solving various problems, e.g. finding connected components, topological sorting, \& solving maze problems. It is particularly useful in finding cycles in a graph since it traverses graph in a depth-1st order, \& a cycle exists iff a node is visited twice during traversal.

            -- DFS hữu ích trong việc giải quyết nhiều bài toán khác nhau, e.g. tìm các thành phần liên thông, sắp xếp tôpô, \& giải các bài toán mê cung. Nó đặc biệt hữu ích trong việc tìm chu trình trong đồ thị vì nó duyệt đồ thị theo thứ tự độ sâu 1, \& tồn tại 1 chu trình chỉ khi \& chỉ khi 1 nút được thăm hai lần trong quá trình duyệt.

            Like BFS, it has a time complexity of $O(|V| + |E|)$. It requires less memory but does not guarantee shallowest path solution. Finally, unlike BFS, you can be trapped in infinite loops using DFS.

            -- Giống như BFS, DFS có độ phức tạp thời gian là $O(|V| + |E|)$. Nó đòi hỏi ít bộ nhớ hơn nhưng không đảm bảo giải pháp đường đi nông nhất. Cuối cùng, không giống như BFS, bạn có thể bị mắc kẹt trong vòng lặp vô hạn khi sử dụng DFS.

            Additionally, many other algorithms in graph theory build upon BFS \& DFS, e.g. Dijkstra's shortest path algorithm, Kruskal's minimum spanning tree algorithm, \& Tarjan's strongly connected components algorithm. Therefore, a solid understanding of BFS \& DFS is essential for anyone who wants to work with graphs \& develop more advanced graph algorithms.

            -- Ngoài ra, nhiều thuật toán khác trong lý thuyết đồ thị được xây dựng dựa trên BFS \& DFS, e.g. thuật toán đường đi ngắn nhất Dijkstra, thuật toán cây bao trùm nhỏ nhất Kruskal, \& thuật toán thành phần liên thông mạnh Tarjan. Do đó, việc hiểu rõ về BFS \& DFS là điều cần thiết cho bất kỳ ai muốn làm việc với đồ thị \& phát triển các thuật toán đồ thị nâng cao hơn.
        \end{itemize}
        \item {\sf Summary.} In  this chap, covered essentials of graph theory, a branch of mathematics that studies graphs \& networks. Began by defining what a graph is \& explained different types of graphs, e.g., directed, weighted, \& connected graphs. Then introduced fundamental graph objects (including neighbors) \& measures (e.g. centrality \& density), which are used to understand \& analyze graph structures.

        -- Chương này trình bày những kiến thức cơ bản về lý thuyết đồ thị, 1 nhánh của toán học nghiên cứu đồ thị \& mạng lưới. Bắt đầu bằng việc định nghĩa đồ thị \& giải thích các loại đồ thị khác nhau, e.g.: đồ thị có hướng, có trọng số, \& liên thông. Sau đó, giới thiệu các đối tượng đồ thị cơ bản (bao gồm cả các lân cận) \& các phép đo (e.g.: độ tập trung \& mật độ), được sử dụng để hiểu \& phân tích cấu trúc đồ thị.

        Additionally, discussed adjacency matrix \& its different representations. Finally, explored 2 fundamental graph algorithms, BFS \& DFS, which form foundation for developing more complex graph algorithms.

        -- Ngoài ra, chúng tôi còn thảo luận về ma trận kề \& các biểu diễn khác nhau của nó. Cuối cùng, chúng tôi đã khám phá 2 thuật toán đồ thị cơ bản, BFS \& DFS, tạo thành nền tảng cho việc phát triển các thuật toán đồ thị phức tạp hơn.

        In Chap. 3: Creating Node Representation with DeepWalk, explore DeepWalk architecture \& its 2 components: {\tt Word2Vec} \& random walks. Start by understanding {\tt Word2Vec} architecture \& then implement it using a specialized library. Then, delve into DeepWalk algorithm \& implement random walks on a graph.

        -- Trong Chương 3: Tạo Biểu diễn Nút với DeepWalk, khám phá kiến trúc DeepWalk \& 2 thành phần của nó: {\tt Word2Vec} \& bước ngẫu nhiên. Bắt đầu bằng cách tìm hiểu kiến trúc {\tt Word2Vec} \& sau đó triển khai nó bằng 1 thư viện chuyên dụng. Sau đó, đi sâu vào thuật toán DeepWalk \& triển khai bước ngẫu nhiên trên đồ thị.
    \end{itemize}
    \item {\sf3. Creating Node Representations with DeepWalk.} DeepWalk is 1 of 1st major successful applications of ML techniques to graph data. It introduces important concepts e.g. embeddings that are at core of GNNs. Unlike traditional neural networks, goal of this architecture: produce representations that are then fed to other models, which perform downstream tasks (e.g., node classification).

    -- {\sf3. Tạo Biểu diễn Nút với DeepWalk.} DeepWalk là 1 trong những ứng dụng thành công đầu tiên của kỹ thuật ML vào đồ thị dữ liệu. Nó giới thiệu các khái niệm quan trọng, e.g. nhúng, vốn là cốt lõi của mạng nơ-ron nhân tạo (GNN). Không giống như mạng nơ-ron truyền thống, mục tiêu của kiến trúc này là tạo ra các biểu diễn sau đó được đưa vào các mô hình khác, thực hiện các tác vụ tiếp theo (e.g.: phân loại nút).

    In this chap, learn about DeepWalk architecture \& its 2 major components: Word2Vec \& random walks. Explain how Word2Vec architecture works, with a particular focus on skip-gram model. Implement this model with popular {\tt gensim} library on a NLP example to understand how it is supposed to be used.

    -- Trong chương này, chúng ta sẽ tìm hiểu về kiến trúc DeepWalk \& 2 thành phần chính của nó: Word2Vec \& random walks. Giải thích cách thức hoạt động của kiến trúc Word2Vec, đặc biệt tập trung vào mô hình skip-gram. Triển khai mô hình này với thư viện {\tt gensim} phổ biến trên 1 ví dụ NLP để hiểu cách sử dụng.

    Then focus on DeepWalk algorithm \& see how performance can be improved using hierarchical softmax (H-Softmax). This powerful optimization of softmax function can be found in many fields: it is incredibly useful when you have a lot of possible classes in your classification task. Also implement random walks on a graph before wrapping things up with an end-to-end supervised classification exercise on Zachary's Karate Club.

    -- Sau đó, tập trung vào thuật toán DeepWalk \& xem cách cải thiện hiệu suất bằng cách sử dụng softmax phân cấp (H-Softmax). Việc tối ưu hóa mạnh mẽ hàm softmax này có thể được tìm thấy trong nhiều lĩnh vực: nó cực kỳ hữu ích khi bạn có nhiều lớp khả thi trong tác vụ phân loại của mình. Hãy triển khai các bước ngẫu nhiên trên đồ thị trước khi kết thúc bằng bài tập phân loại có giám sát đầu cuối trên Zachary's Karate Club.

    By end of this chap, master Word2Vec in context of NLP \& beyond. Able to create node embeddings using topological information of graphs \& solve classification tasks on graph data. In this chap, cover main topics: introducing Word2Vec, DeepWalk \& random walks, implementing DeepWalk.

    -- Đến cuối chương này, bạn sẽ nắm vững Word2Vec trong ngữ cảnh NLP \& hơn thế nữa. Có khả năng tạo nhúng nút bằng thông tin tôpô của đồ thị \& giải quyết các bài toán phân loại trên dữ liệu đồ thị. Trong chương này, bạn sẽ tìm hiểu các chủ đề chính: giới thiệu Word2Vec, DeepWalk \& bước ngẫu nhiên, \& triển khai DeepWalk.
    \begin{itemize}
        \item {\sf Introducing Word2Vec.} 1st step to comprehending DeepWalk algorithm: understand its major component: Word2Vec. Word2Vec has been 1 of most influential DL techniques in NLP. Published in 2013 by {\sc Tomas Mikolov} et al. (Google) in 2 different papers, it proposed a new technique to translate words into vectors (also known as {\it embeddings}) using large datasets of text. These representations can then be used in downstream tasks, e.g. sentiment classification. It is also 1 of rare examples of patented \& popular ML architecture.

        -- {\sf Giới thiệu Word2Vec.} Bước đầu tiên để hiểu thuật toán DeepWalk: hiểu thành phần chính của nó: Word2Vec. Word2Vec là 1 trong những kỹ thuật DL có ảnh hưởng nhất trong NLP. Được công bố vào năm 2013 bởi {\sc Tomas Mikolov} \& cộng sự (Google) trong 2 bài báo khác nhau, kỹ thuật này đã đề xuất 1 kỹ thuật mới để dịch từ thành các vectơ (còn được gọi là {\sc nhúng}) bằng cách sử dụng các tập dữ liệu văn bản lớn. Các biểu diễn này sau đó có thể được sử dụng trong các tác vụ tiếp theo, e.g. phân loại cảm xúc. Đây cũng là 1 trong những ví dụ hiếm hoi về kiến trúc ML phổ biến \& đã được cấp bằng sáng chế.

        A few examples of how Word2Vec can transform words into vectors: $vec(king) = [-2.1,4.1,0.6],vec(queen) = [-1.9,2.6,1.5],vec(man) = [3.0,-1.1,-2],vec(woman) = [2.8,-2.6,-1.1]$. Can see in this example: in terms of Euclidean distance, word vectors for {\it king \& queen} are closer than ones for {\it king \& woman} (4.37 vs. 8.47). In general, other metrics, e.g. popular {\it cosine similarity}, are used to measure likeness of these words. Cosine similarity focuses on angle between vectors \& does not consider their magnitude (length), which is more helpful in comparing them. Here is how it is defined:
        \begin{equation*}
            \mbox{cosine similarity}(\vec{A},\vec{B}) = \cos\theta = \frac{\vec{A}\cdot\vec{B}}{\|\vec{A}\|\|\vec{B}\|}.
        \end{equation*}
        1 of most surprising results of Word2Vec is its ability to solve analogies. A popular example is how it can answer question ``man is to woman, what kind is to $\ldots$?'' It can be calculated as follows: $vec(king) - vec(man) + vec(woman)\approx vec(queen)$. This is not true with any analogy, but this property can bring interesting applications to perform arithmetic operations with embeddings.

        -- 1 vài ví dụ về cách Word2Vec có thể chuyển đổi từ thành vectơ: $vec(king) = [-2.1,4.1,0.6],vec(queen) = [-1.9,2.6,1.5],vec(man) = [3.0,-1.1,-2],vec(woman) = [2.8,-2.6,-1.1]$. Có thể thấy trong ví dụ này: về khoảng cách Euclidean, vectơ từ cho {\it king \& queen} gần hơn vectơ cho {\it king \& woman} (4,37 so với 8,47). Nhìn chung, các số liệu khác, e.g. {\it cosine similarity} phổ biến, được sử dụng để đo mức độ giống nhau của các từ này. Độ giống nhau cosine tập trung vào góc giữa các vectơ \& không xem xét độ lớn (chiều dài) của chúng, điều này hữu ích hơn khi so sánh chúng. Đây là cách định nghĩa của nó:
        \begin{equation*}
            \mbox{cosine similarity}(\vec{A},\vec{B}) = \cos\theta = \frac{\vec{A}\cdot\vec{B}}{\|\vec{A}\|\|\vec{B}\|}.
        \end{equation*}
        1 trong những kết quả đáng ngạc nhiên nhất của Word2Vec là khả năng giải quyết các phép loại suy. 1 ví dụ phổ biến là cách nó có thể trả lời câu hỏi ``đàn ông là gì với phụ nữ, vậy loại nào là $\ldots$?''. Nó có thể được tính như sau: $vec(king) - vec(man) + vec(woman)\approx vec(queen)$. Điều này không đúng với bất kỳ phép loại suy nào, nhưng tính chất này có thể mang lại những ứng dụng thú vị để thực hiện các phép toán số học với phép nhúng.
        \begin{itemize}
            \item {\sf CBOW vs. skip-gram.} A model must be trained on a pretext task to produce these vectors. Task itself does not need to be meaningful: its only goal is to produce high-quality embeddings. In practice, this task is always related to predicting words given a certain context.

            -- {\sf CBOW so với skip-gram.} 1 mô hình phải được huấn luyện trên 1 tác vụ giả định để tạo ra các vectơ này. Bản thân tác vụ không cần phải có ý nghĩa: mục tiêu duy nhất của nó là tạo ra các nhúng chất lượng cao. Trên thực tế, tác vụ này luôn liên quan đến việc dự đoán các từ trong 1 ngữ cảnh nhất định.

            Authors proposed 2 architectures with similar tasks:
            \begin{itemize}
                \item {\bf Continuous bag-of-words (CBOW) model.} This is trained to predict a word its surrounding context (words coming before \& after target word). Order of context words does not matter since their embeddings are summed in model. Authors claim to obtain better results using 4 words before \& after the one that is predicted.
                \item {\bf Continuous skip-gram model.} Here we feed a single word to model \& try to predict words around it. Increasing range of context words lead to better embeddings but also increases training time.
            \end{itemize}
            In summary, here are inputs \& outputs of both models: {\sf Fig. 3.1: CBOW \& skip-gram architectures.}

            -- Các tác giả đã đề xuất 2 kiến trúc với các nhiệm vụ tương tự:
            \begin{itemize}
                \item {\bf Mô hình túi từ liên tục (CBOW).} Mô hình này được huấn luyện để dự đoán 1 từ trong ngữ cảnh xung quanh nó (các từ đứng trước \& sau từ mục tiêu). Thứ tự của các từ ngữ cảnh không quan trọng vì các nhúng của chúng được tổng hợp trong mô hình. Các tác giả tuyên bố thu được kết quả tốt hơn khi sử dụng 4 từ đứng trước \& sau từ được dự đoán.
                \item {\bf Mô hình bỏ qua liên tục.} Ở đây, chúng tôi đưa 1 từ duy nhất vào mô hình \& cố gắng dự đoán các từ xung quanh nó. Việc tăng phạm vi từ ngữ cảnh dẫn đến các nhúng tốt hơn nhưng cũng làm tăng thời gian huấn luyện.
            \end{itemize}
            Tóm lại, đây là đầu vào \& đầu ra của cả hai mô hình: {\sf Hình 3.1: Kiến trúc CBOW \& bỏ qua.}

            In general, CBOW model is considered faster to train, but skip-gram model is more accurate thanks to its ability to learn infrequent words. This topic is still debated in NLP community: a different implementation could fix issues related to CBOW in some contexts.

            -- Nhìn chung, mô hình CBOW được đánh giá là nhanh hơn khi huấn luyện, nhưng mô hình skip-gram lại chính xác hơn nhờ khả năng học các từ ít phổ biến. Chủ đề này vẫn đang được tranh luận trong cộng đồng NLP: 1 triển khai khác có thể khắc phục các vấn đề liên quan đến CBOW trong 1 số bối cảnh.
            \item {\sf Creating skip-grams.} For now, focus on skip-gram model since it is architecture used by DeepWalk. Skip-grams are implemented as pairs of words with following structure (target word, context word), where {\it target word} is input \& {\it context word} is word to predict. Number of skip grams for same target word depends on a parameter called {\it context size}, as shown in {\sf Fig. 3.2: Text to skip-grams}. Same idea can be applied to a corpus of text instead of a single sentence.

            -- {\sf Tạo skip-gram.} Trước mắt, hãy tập trung vào mô hình skip-gram vì đây là kiến trúc được DeepWalk sử dụng. Skip-gram được triển khai theo cặp từ với cấu trúc sau (từ đích, từ ngữ cảnh), trong đó {\sf từ đích} là đầu vào \& {\sf từ ngữ cảnh} là từ cần dự đoán. Số lượng skip-gram cho cùng 1 từ đích phụ thuộc vào 1 tham số gọi là {\sf kích thước ngữ cảnh}, như thể hiện trong {\sf Hình 3.2: Văn bản thành skip-gram}. Ý tưởng tương tự có thể được áp dụng cho 1 tập hợp văn bản thay vì 1 câu đơn lẻ.

            In practice, store all context words for same target word in a list to save memory. See how it is done with an example on an entire paragraph. In following example, create skip-grams for an entire paragraph stored in {\tt text} variable. Set \verb|CONTEXT_SIZE| variable to 2, i.e., look at 2 words before \& after our target word:
            \begin{enumerate}
                \item Start by importing necessary libraries: numpy.
                \item Then need to set \verb|CONTEXT_SIZE| variable to 2 \& bring in text we want to analyze:
                \item Next create skip-grams thanks to a simple {\tt for} loop to consider every word in {\tt text}. A list comprehension generates context words, stored in {\tt skipgrams} list:
                \begin{verbatim}
skipgrams = []
for i in range(CONTEXT_SIZE, len(text) - CONTEXT_SIZE):
    array = [text[j] for j in np.arange(i - CONTEXT_SIZE, i + CONTEXT_SIZE + 1) if j != i]
    skipgrams.append((text[i], array))
                \end{verbatim}
                \item Finally, use {\tt print()} function to see skip-grams we generated:
                \begin{verbatim}
print(skipgrams[0:2])
                \end{verbatim}
            \end{enumerate}
            These 2 target words, with their corresponding context, work to show what inputs to Word2Vec look like.

            -- Trong thực tế, hãy lưu trữ tất cả các từ ngữ cảnh cho cùng 1 từ mục tiêu trong 1 danh sách để tiết kiệm bộ nhớ. Xem cách thực hiện với ví dụ trên toàn bộ 1 đoạn văn. Trong ví dụ sau, hãy tạo skip-gram cho toàn bộ 1 đoạn văn được lưu trữ trong biến {\tt text}. Đặt biến \verb|CONTEXT_SIZE| thành 2, i.e., xem xét 2 từ trước \& sau từ mục tiêu của chúng ta:
            \begin{enumerate}
                \item Bắt đầu bằng cách nhập các thư viện cần thiết: numpy.
                \item Sau đó, cần đặt biến \verb|CONTEXT_SIZE| thành 2 \& nhập văn bản chúng ta muốn phân tích:
                \item Tiếp theo, hãy tạo skip-gram nhờ vòng lặp {\tt for} đơn giản để xem xét mọi từ trong {\tt text}. 1 danh sách hiểu biết tạo ra các từ ngữ cảnh, được lưu trữ trong danh sách {\tt skipgrams}:
                \begin{verbatim}
skipgrams = []
for i in range(CONTEXT_SIZE, len(text) - CONTEXT_SIZE):
    array = [text[j] for j in np.arange(i - CONTEXT_SIZE, i + CONTEXT_SIZE + 1) if j != i]
    skipgrams.append((text[i], array))
                \end{verbatim}
                \item Cuối cùng, sử dụng hàm {\tt print()} để xem các skip-gram mà chúng ta đã tạo ra:
                \begin{verbatim}
print(skipgrams[0:2])
                \end{verbatim}
            \end{enumerate}
            Hai từ mục tiêu này, cùng với ngữ cảnh tương ứng, sẽ hiển thị các dữ liệu đầu vào của Word2Vec trông như thế nào.
            \item {\sf skip-gram model.} Goal of Word2Vec: produce high-quality word embeddings. To learn these embeddings, training task of skip-gram model consists of predicting correct context words given a target word.

            -- {\sf mô hình skip-gram.} Mục tiêu của Word2Vec: tạo ra các nhúng từ chất lượng cao. Để học các nhúng này, nhiệm vụ huấn luyện của mô hình skip-gram bao gồm dự đoán các từ ngữ cảnh chính xác dựa trên 1 từ mục tiêu.

            Imagine we have a sequence of $N$ words $w_1,w_2,\ldots,w_N$. Probability of seeing word $w_2$ given word $w_1$ is written $p(w_2|w_1)$. Goal: maximize sum of every probability of seeing a context word given a target word in an entire text:
            \begin{equation*}
                \frac{1}{N}\sum_{n=1}^N\sum_{-c\le j\le c,\ j\ne0} \log p(w_{n+j}|w_n),
            \end{equation*}
            where $c$ is size of context vector.

            -- Tưởng tượng chúng ta có 1 chuỗi $N$ từ $w_1,w_2,\ldots,w_N$. Xác suất nhìn thấy từ $w_2$ cho từ $w_1$ được viết là $p(w_2|w_1)$. Mục tiêu: tối đa hóa tổng của mọi xác suất nhìn thấy 1 từ ngữ cảnh cho 1 từ đích trong toàn bộ văn bản:
            \begin{equation*}
                \frac{1}{N}\sum_{n=1}^N\sum_{-c\le j\le c,\ j\ne0} \log p(w_{n+j}|w_n),
            \end{equation*}
            trong đó $c$ là kích thước của vectơ ngữ cảnh.

            \begin{remark}
                Why do we use a log probability in prev equation? Transforming probabilities into log probabilities is a common technique in ML (\& CS in general) for 2 main reasons. Products become additions (\& divisions become subtractions). Multiplications are more computationally expensive than additions, so it is faster to compute log probability: $\log(ab) = \log a + \log b$. Way computers store very small numbers, e.g., {\tt3.14e-128} is not perfectly accurate, unlike log of same numbers $-127.5$ in this case. These small errors can add up \& bias final results when events are extremely unlikely. On whole, this simple transformation allows us to gain speed \& accuracy without changing our initial objective.

                -- Tại sao chúng ta sử dụng xác suất logarit trong phương trình prev? Việc chuyển đổi xác suất thành xác suất logarit là 1 kỹ thuật phổ biến trong ML (\& CS nói chung) vì 2 lý do chính. Tích trở thành phép cộng (\& phép chia trở thành phép trừ). Phép nhân tốn kém về mặt tính toán hơn phép cộng, do đó, tính xác suất logarit nhanh hơn: $\log(ab) = \log a + \log b$. Cách máy tính lưu trữ những số rất nhỏ, e.g.: {\tt3.14e-128} không hoàn toàn chính xác, không giống như logarit của cùng 1 số $-127.5$ trong trường hợp này. Những sai số nhỏ này có thể cộng lại \& làm sai lệch kết quả cuối cùng khi các sự kiện cực kỳ khó xảy ra. Nhìn chung, phép biến đổi đơn giản này cho phép chúng ta tăng tốc \& độ chính xác mà không làm thay đổi mục tiêu ban đầu.
            \end{remark}
            Basic skip-gram models uses softmax function to calculate probability of a context word embedding $h_c$ given a target word embedding $h_t$:
            \begin{equation*}
                p(w_c|w_t) = \frac{\exp(h_ch_t^\top)}{\sum_{i=1}^{|V| \exp(h_ih_t^\top)}},
            \end{equation*}
            where $V$ is vocabulary of size $|V|$. This vocabulary corresponds to list of unique words the model tries to predict. Can obtain this list using {\tt set} data structure to remove duplicate words:

            -- Các mô hình skip-gram cơ bản sử dụng hàm softmax để tính xác suất nhúng từ ngữ cảnh $h_c$ cho 1 từ mục tiêu nhúng $h_t$:
            \begin{equation*}
                p(w_c|w_t) = \frac{\exp(h_ch_t^\top)}{\sum_{i=1}^{|V| \exp(h_ih_t^\top)}},
            \end{equation*}
            trong đó $V$ là từ vựng có kích thước $|V|$. Từ vựng này tương ứng với danh sách các từ duy nhất mà mô hình cố gắng dự đoán. Có thể lấy danh sách này bằng cách sử dụng cấu trúc dữ liệu {\tt set} để loại bỏ các từ trùng lặp:
            \begin{verbatim}
# skip-gram model
vocab = set(text)
VOCAB_SIZE = len(vocab)
print(f"Length of vocabulary = {VOCAB_SIZE}")
            \end{verbatim}
            Now have size of our vocabulary, there is 1 more parameter we need to define: $N$, dimensionality of word vectors. Typically, this value is set between 100 \& 1000. In this example, set it to 10 because of limited size of our dataset.

            -- Bây giờ, với kích thước của vốn từ vựng, chúng ta cần định nghĩa thêm 1 tham số nữa: $N$, số chiều của vectơ từ. Thông thường, giá trị này được đặt trong khoảng từ 100 \ đến 1000. Trong ví dụ này, hãy đặt thành 10 vì kích thước tập dữ liệu của chúng ta có hạn.

            Skip-gram model is composed of only 2 layers:
            \begin{itemize}
                \item A {\it projection layer} with a weight matrix $W_{\rm embed}$, which takes a 1-hot encoded-word vector as an input \& returns corresponding $N$-dim word embedding. It acts as a simple lookup table that stores embeddings of a predefined dimensionality.
                \item A {\it fully connected layer} with a weight matrix $W_{\rm output}$, which takes a word embedding as input \& outputs $|V|$-dim logits. A softmax function is applied to these predictions to transform logits into probabilities.
            \end{itemize}
            -- Mô hình Skip-gram chỉ bao gồm 2 lớp:
            \begin{itemize}
                \item A {\it projection layer} với ma trận trọng số $W_{\rm embed}$, lấy 1 vectơ từ được mã hóa 1-hot làm đầu vào \& trả về nhúng từ $N$-dim tương ứng. Nó hoạt động như 1 bảng tra cứu đơn giản lưu trữ các nhúng có chiều được xác định trước.
                \item A {\it fully connected layer} với ma trận trọng số $W_{\rm output}$, lấy 1 nhúng từ làm đầu vào \& trả về logit $|V|$-dim. 1 hàm softmax được áp dụng cho các dự đoán này để chuyển đổi logit thành xác suất.
            \end{itemize}

            \begin{remark}
                There is no activation function: {\tt Word2Vec} is a linear classifier that models a linear relationship between words.

                -- Không có hàm kích hoạt: {\tt Word2Vec} là 1 bộ phân loại tuyến tính mô hình hóa mối quan hệ tuyến tính giữa các từ.
            \end{remark}
            Call ${\bf x}$ 1-hot encoded-word vector {\it input}. Corresponding word embedding can be calculated as a simple projection $h = W_{\rm embed}^\top\cdot{\bf x}$. Using skip-gram model, can rewrite previous probability as follows:
            \begin{equation*}
                p(w_c|w_t) \frac{\exp(W_{\rm output}\cdot h)}{\sum_{i=1}^{|V|} \exp(W_{\rm output(i)}\cdot h)}.
            \end{equation*}
            Skip-gram model outputs a $|V|$-dim vector, which is conditional probability of every word in vocabulary:
            \begin{equation*}
                word2vec(w_t) = \begin{bmatrix}
                    p(w_1|w_t)\\p(w_2|w_t)\\\vdots\\p(w_{|V|}|w_t)
                \end{bmatrix}.
            \end{equation*}
            During training, these probabilities are compared to correct 1-hot encoded-target word vectors. Difference between these values (calculated by a loss function e.g. cross-entropy loss) is backpropagated through network to update weights \& obtain better predictions.

            -- Gọi ${\bf x}$ là vectơ từ mã hóa 1-hot {\it input}. Nhúng từ tương ứng có thể được tính toán như 1 phép chiếu đơn giản $h = W_{\rm embed}^\top\cdot{\bf x}$. Sử dụng mô hình skip-gram, có thể viết lại xác suất trước đó như sau:
            \begin{equation*}
                p(w_c|w_t) \frac{\exp(W_{\rm output}\cdot h)}{\sum_{i=1}^{|V|} \exp(W_{\rm output(i)}\cdot h)}.
            \end{equation*}
            Mô hình Skip-gram đưa ra 1 vectơ $|V|$-dim, là xác suất có điều kiện của mọi từ trong từ vựng:
            \begin{equation*}
                word2vec(w_t) = \begin{bmatrix}
                    p(w_1|w_t)\\p(w_2|w_t)\\vdots\\p(w_{|V|}|w_t)
                \end{bmatrix}.
            \end{equation*}
            Trong quá trình huấn luyện, các xác suất này được so sánh với các vectơ từ mục tiêu được mã hóa 1-hot chính xác. Sự khác biệt giữa các giá trị này (được tính bằng hàm mất mát, e.g.: mất mát entropy chéo) được truyền ngược qua mạng để cập nhật trọng số \& thu được dự đoán tốt hơn.

            Entire Word2Vec architecture is summarized in following diagram, with both matrices \& final softmax layer: {\sf Fig. 3.3: Word2Vec architecture}. Can implement this model using {\tt gensim} library, which is also used in official implementation of DeepWalk. Can then build vocabulary \& train our model based on previous text:
            \begin{enumerate}
                \item Begin by installing {\tt gensim} \& importing {\tt Word2Vec} class:
                \item Initialize a skip-gram model with a {\tt Word2Vec} object \& an {\tt sg = 1} parameter (skip-gram = 1).
                \item A good idea to check shape of our 1st weight matrix. It should correspond to vocabulary size \& word embeddings' dimensionality.
                \item Train model for 10 epochs.
                \item Can print a word embedding to see what result of this training looks like.
            \end{enumerate}
            While this approach works well with small vocabularies, computational cost of applying a full softmax function to millions of words (vocabulary size) is too costly in most cases. This has been a limiting factor in developing accurate language models for a long time. Fortunately for us, other approaches have been designed to solve this issue.

            -- Toàn bộ kiến trúc Word2Vec được tóm tắt trong sơ đồ sau, với cả ma trận \& lớp softmax cuối cùng: {\sf Hình 3.3: Kiến trúc Word2Vec}. Có thể triển khai mô hình này bằng thư viện {\tt gensim}, cũng được sử dụng trong triển khai chính thức của DeepWalk. Sau đó, có thể xây dựng từ vựng \& huấn luyện mô hình của chúng ta dựa trên văn bản trước đó:
            \begin{enumerate}
                \item Bắt đầu bằng cách cài đặt {\tt gensim} \& nhập lớp {\tt Word2Vec}:
                \item Khởi tạo mô hình skip-gram với đối tượng {\tt Word2Vec} \& tham số {\tt sg = 1} (skip-gram = 1).
                \item Nên kiểm tra hình dạng của ma trận trọng số thứ nhất. Nó phải tương ứng với kích thước từ vựng \& chiều của nhúng từ.
                \item Huấn luyện mô hình trong 10 kỷ nguyên.
                \item Có thể in 1 nhúng từ để xem kết quả của quá trình huấn luyện này trông như thế nào.

            \end{enumerate}
            Mặc dù phương pháp này hiệu quả với các từ vựng nhỏ, nhưng chi phí tính toán khi áp dụng hàm softmax đầy đủ cho hàng triệu từ (kích thước từ vựng) thường quá tốn kém. Đây là 1 yếu tố hạn chế trong việc phát triển các mô hình ngôn ngữ chính xác trong 1 thời gian dài. May mắn thay, các phương pháp khác đã được thiết kế để giải quyết vấn đề này.

            Word2Vec (\& DeepWalk) implements 1 of these techniques, called H-Softmax. Instead of a flat softmax that directly calculates probability of every word, this technique uses a binary tree structure where leaves are words. Even more interestingly, a Huffman tree can be used, where infrequent words are stored at deeper levels than common words. In most cases, this dramatically speeds up word prediction by a factor $\ge50$.

            -- Word2Vec (\& DeepWalk) triển khai 1 trong những kỹ thuật này, được gọi là H-Softmax. Thay vì 1 softmax phẳng tính toán trực tiếp xác suất của từng từ, kỹ thuật này sử dụng cấu trúc cây nhị phân với các lá là các từ. Thú vị hơn nữa, có thể sử dụng cây Huffman, trong đó các từ ít phổ biến được lưu trữ ở các cấp độ sâu hơn so với các từ phổ biến. Trong hầu hết các trường hợp, điều này giúp tăng tốc đáng kể việc dự đoán từ lên gấp $\ge50$.

            H-Softmax can be activated in {\tt gensim} using {\tt hs = 1}. This was most difficult part of DeepWalk architecture. But before can implement it, need 1 more component: how to create our training data.

            -- H-Softmax có thể được kích hoạt trong {\tt gensim} bằng cách sử dụng {\tt hs = 1}. Đây là phần khó nhất của kiến trúc DeepWalk. Nhưng trước khi có thể triển khai nó, cần thêm 1 thành phần nữa: cách tạo dữ liệu huấn luyện.
        \end{itemize}
        \item {\sf DeepWalk \& random walks.} Proposed in 2014 by {\sc Perozzi} et al., DeepWalk quickly became extremely popular among graph researchers. Inspired by recent advances in NLP, it consistently outperformed other methods on several datasets. While more performant architectures have been proposed since then, DeepWalk is a simple \& reliable baseline that can be quickly implemented to solve a lot of problems.

        -- {\sf DeepWalk \& random walks.} Được đề xuất vào năm 2014 bởi {\sc Perozzi} \& cộng sự, DeepWalk nhanh chóng trở nên cực kỳ phổ biến trong giới nghiên cứu đồ thị. Lấy cảm hứng từ những tiến bộ gần đây trong NLP, nó luôn vượt trội hơn các phương pháp khác trên 1 số tập dữ liệu. Mặc dù các kiến trúc hiệu suất cao hơn đã được đề xuất kể từ đó, DeepWalk là 1 đường cơ sở đơn giản \& đáng tin cậy, có thể được triển khai nhanh chóng để giải quyết nhiều vấn đề.

        Goal of DeepWalk: produce high-quality feature representations of nodes in an unsupervised way. This architecture is heavily inspired by Word2Vec in NLP. However, instead of words, our dataset is composed of nodes. This is why we use random walks to generate meaningful sequence of nodes that act like sentences. Following diagram illustrates connection between sentences \& graphs: {\sf Fig. 3.4: Sentences can be represented as graphs}. Random walks are sequences of nodes produced by randomly choosing a neighboring node at every step. Thus, nodes can appear several times in same sequence.

        -- Mục tiêu của DeepWalk: tạo ra các biểu diễn đặc trưng chất lượng cao của các nút theo cách không giám sát. Kiến trúc này lấy cảm hứng rất nhiều từ Word2Vec trong NLP. Tuy nhiên, thay vì các từ, tập dữ liệu của chúng tôi được tạo thành từ các nút. Đây là lý do tại sao chúng tôi sử dụng các bước ngẫu nhiên để tạo ra chuỗi các nút có ý nghĩa hoạt động như các câu. Sơ đồ sau minh họa mối liên hệ giữa các câu \& đồ thị: {\sf Hình 3.4: Các câu có thể được biểu diễn dưới dạng đồ thị}. Các bước ngẫu nhiên là các chuỗi nút được tạo ra bằng cách chọn ngẫu nhiên 1 nút lân cận ở mỗi bước. Do đó, các nút có thể xuất hiện nhiều lần trong cùng 1 chuỗi.

        Why are random walks important? Even if nodes are randomly selected, fact they often appear together in a sequence means: they are close to each other. Under {\it network homophily} hypothesis, notes that are close to each other are similar. This is particularly case in social networks, where people are connected to friends \& family.

        -- Tại sao các bước ngẫu nhiên lại quan trọng? Ngay cả khi các nút được chọn ngẫu nhiên, việc chúng thường xuất hiện cùng nhau theo 1 trình tự có nghĩa là: chúng gần nhau. Theo giả thuyết {\it network homophily}, các nốt gần nhau thì tương tự nhau. Điều này đặc biệt đúng trong các mạng xã hội, nơi mọi người kết nối với bạn bè \& gia đình.

        This idea is at core of DeepWalk algorithm: when nodes are close to each other, want to obtain high similarity scores. On contrary, want low scores when they are farther apart. Implement a random walk function using a {\tt networkx} graph:

        -- Ý tưởng này là cốt lõi của thuật toán DeepWalk: khi các nút gần nhau, chúng ta muốn đạt được điểm tương đồng cao. Ngược lại, chúng ta muốn đạt điểm tương đồng thấp khi chúng ở xa nhau. Triển khai hàm bước ngẫu nhiên bằng đồ thị {\tt networkx}:
        \begin{enumerate}
            \item Import required libraries \& initialize random number generator for reproducibility:
            \begin{verbatim}
mport networkx as nx
import matplotlib.pyplot as plt
import numpy as np
import random
random.seed(0) # initialize random number generator for reproducibility
            \end{verbatim}
            \item Generate a random graph thanks to \verb|erdos_renyi_graph| function with a fixed number of nodes 10 \& a predefined probability of creating an edge between 2 nodes 0.3:
            \begin{verbatim}
G = nx.erdos_renyi_graph(10, 0.3, seed = 1, directed = False)
            \end{verbatim}
            \item Plot this random graph to see what it looks like.
            \item Implement random walks with a simple function. This function takes 2 parameters: starting node {\tt start} \& length of walk {\tt length}. At every step, randomly select a neighboring node (using {\tt np.random.choice}) until walk is complete:
            \begin{verbatim}
# implement random walks
def random_walk(start, length):
    walk = [str(start)] # starting node
    for i in range(length):
        neighbors = [node for node in G.neighbors(start)]
        next_node = np.random.choice(neighbors, 1)[0]
        walk.append(str(next_node))
        start = next_node
    return walk
            \end{verbatim}
            \item Print result of this function with starting node as 0 \& a length of 10:
            \begin{verbatim}
print(random_walk(0, 10))
            \end{verbatim}
        \end{enumerate}
        Can see: certain nodes, e.g. 0 \& 9, are often found together. Considering that it is a homophilic graph, i.e., they are similar. It is precisely type of relationship we are trying to capture with DeepWalk. Now have implemented Word2Vec \& random walks separately, combine them to create DeepWalk.

        -- Có thể thấy: 1 số nút nhất định, e.g.: 0 \& 9, thường được tìm thấy cùng nhau. Xét đến việc đây là 1 đồ thị đồng dạng, i.e., chúng tương tự nhau. Đây chính xác là loại mối quan hệ mà chúng ta đang cố gắng nắm bắt bằng DeepWalk. Bây giờ, chúng ta đã triển khai Word2Vec \& random walks riêng biệt, hãy kết hợp chúng để tạo ra DeepWalk.
        \item {\sf Implementing DeepWalk.} Now have a good understanding of every component in this architecture, use it to solve an ML problem. Dataset we use is Zachary's Karate Club. It simply represents relationships within a karate club studied by {\sc Wayne W. Zachary} in 1970s. It is a kind of social network where every node is a member, \& members who interact outside club are connected.

        -- {\sf Triển khai DeepWalk.} Bây giờ, hãy hiểu rõ từng thành phần trong kiến trúc này, hãy sử dụng nó để giải quyết 1 bài toán ML. Tập dữ liệu chúng tôi sử dụng là Câu lạc bộ Karate của Zachary. Nó đơn giản biểu diễn các mối quan hệ trong 1 câu lạc bộ karate được {\sc Wayne W. Zachary} nghiên cứu vào những năm 1970. Đây là 1 dạng mạng xã hội, trong đó mỗi nút là 1 thành viên, \& các thành viên tương tác bên ngoài câu lạc bộ được kết nối.

        In this example, club is divided into 2 groups: could like to assign right group to every member (node classification) just by looking at their connections:
        \begin{enumerate}
            \item Import dataset using \verb|nx.karate_club_graph()|:
            \begin{verbatim}
G = nx.karate_club_graph()
            \end{verbatim}
            \item Need to convert string class labels into numerical values {\tt Mr. Hi = 0, Officer = 1}:
            \item Plot this graph using our new labels.
            \item Next step: generate our dataset, random walks. Want to be as exhaustive as possible, which is why we will create 80 random walks of a length of 10 for every node in graph.
            \item Print a walk to verify that it is correct: 1st walk that was generated.
            \item Final step consists of implementing Word2Vec. Here use skip-gram model previously seen with H-Softmax. Can play with other parameters to improve quality of embeddings.
            \item Model is then simply trained on random walks generated:
            \begin{verbatim}
model.train(walks, total_examples = model.corpus_count, epochs = 30, report_delay = 1)
            \end{verbatim}
            \item Now our model is trained, see its different applications. 1st one allows us to find most similar nodes to a given one (in terms of cosine similarity):
            \begin{verbatim}
print('Nodes that are the most similar to node 0:')
for similarity in model.wv.most_similar(positive = ['0']):
    print(f'	{similarity}')
            \end{verbatim}
            This produces following output for {\tt Nodes} most similar to node 0. Another important application is calculating similarity score between 2 nodes. It can be performed as follows:
            \begin{verbatim}
# similarity between 2 nodes
print(f"Similarity between node 0 & 4: {model.wv.similarity('0', '4')}")
            \end{verbatim}
        \end{enumerate}
        -- Trong ví dụ này, câu lạc bộ được chia thành 2 nhóm: có thể muốn gán đúng nhóm cho từng thành viên (phân loại nút) chỉ bằng cách xem xét các kết nối của họ:
        \begin{enumerate}
            \item Nhập tập dữ liệu bằng \verb|nx.karate_club_graph()|:
            \begin{verbatim}
                G = nx.karate_club_graph()
            \end{verbatim}
            \item Cần chuyển đổi nhãn lớp chuỗi thành giá trị số {\tt Mr. Hi = 0, Officer = 1}:
            \item Vẽ đồ thị này bằng các nhãn mới của chúng ta.
            \item Bước tiếp theo: tạo tập dữ liệu của chúng ta, các bước đi ngẫu nhiên. Muốn càng đầy đủ càng tốt, đó là lý do tại sao chúng ta sẽ tạo 80 bước đi ngẫu nhiên có độ dài 10 cho mỗi nút trong đồ thị.
            \item In 1 bước đi để xác minh rằng nó chính xác: Bước đi đầu tiên đã được tạo.
            \item Bước cuối cùng bao gồm việc triển khai Word2Vec. Ở đây sử dụng mô hình skip-gram đã thấy trước đó với H-Softmax. Có thể sử dụng các tham số khác để cải thiện chất lượng nhúng.
            \item Mô hình sau đó được huấn luyện đơn giản dựa trên các bước đi ngẫu nhiên được tạo ra:
            \begin{verbatim}
                model.train(walks, total_examples = model.corpus_count, epochs = 30, report_delay = 1)
            \end{verbatim}
            \item Bây giờ mô hình của chúng ta đã được huấn luyện, hãy xem các ứng dụng khác nhau của nó. Ứng dụng đầu tiên cho phép chúng ta tìm các nút giống nhau nhất với 1 nút cho trước (theo độ tương đồng cosin):
            \begin{verbatim}
                print('Các nút giống nhau nhất với nút 0:')
                để biết độ tương đồng trong model.wv.most_similar(positive = ['0']):
                print(f' {similarity}')
            \end{verbatim}
            Điều này tạo ra kết quả sau cho {\tt Nút} giống nhau nhất với nút 0. 1 ứng dụng quan trọng khác là tính điểm tương đồng giữa 2 nút. Có thể thực hiện như sau:
            \begin{verbatim}
                # độ tương đồng giữa 2 nút
                print(f"Độ tương đồng giữa nút 0 \& 4: {model.wv.similarity('0', '4')}")
            \end{verbatim}
        \end{enumerate}
        Can plot resulting embeddings using {\it t-distributed stochastic neighbor embedding} (t-SNE) to visualize these high-dimensional vectors in 2D:
        \begin{enumerate}
            \item Import TSNE class from {\tt sklearn}:
            \begin{verbatim}
from sklearn.manifold import TSNE
            \end{verbatim}
            \item Create 2 arrays: 1 to store word embeddings \& the other 1 to store labels.
            \item Next train t-SNE model with 2 dimension \verb|n_components = 2| on embeddings.
            \item Plot 2D vectors produced by trained t-SNE model with corresponding labels.
        \end{enumerate}
        This plot is quite encouraging since we can see a clear line the separates 2 classes. It should be possible for a simple ML algorithm to classify these nodes with enough examples (training data). Implement a classifier \& train it on our node embeddings:
        \begin{enumerate}
            \item Import a Random Forest model from {\tt sklearn}, which is a popular choice when it comes to classification. Accuracy score is metric we will use to evaluate this model:
            \begin{verbatim}
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
            \end{verbatim}
            \item Need to split embeddings into 2 groups: training \& test data. A simple way of doing it: create masks as follows.
            \item Next train Random Forest classifier on training data with appropriate labels.
            \item Finally evaluate trained model on test data based on its accuracy score to give final result of our classifier.
        \end{enumerate}
        -- Có thể vẽ đồ thị các phép nhúng kết quả bằng cách sử dụng {\tt nhúng lân cận ngẫu nhiên phân phối t} (t-SNE) để trực quan hóa các vectơ nhiều chiều này trong 2D:
        \begin{enumerate}
            \item Nhập lớp TSNE từ {\tt sklearn}:
            \begin{verbatim}
                from sklearn.manifold import TSNE
            \end{verbatim}
            \item Tạo 2 mảng: 1 để lưu trữ các phép nhúng từ \& 1 để lưu trữ các nhãn.
            \item Huấn luyện mô hình t-SNE tiếp theo với \verb|n_components = 2| 2 chiều trên các phép nhúng.
            \item Vẽ đồ thị các vectơ 2D được tạo ra bởi mô hình t-SNE đã huấn luyện với các nhãn tương ứng.
        \end{enumerate}
        Đồ thị này khá đáng khích lệ vì chúng ta có thể thấy 1 đường rõ ràng phân tách 2 lớp. 1 thuật toán ML đơn giản có thể phân loại các nút này với đủ ví dụ (dữ liệu huấn luyện). Triển khai bộ phân loại \& huấn luyện nó trên các nhúng nút của chúng ta:
        \begin{enumerate}
            \item Nhập 1 mô hình Rừng Ngẫu nhiên từ {\tt sklearn}, đây là 1 lựa chọn phổ biến khi phân loại. Điểm chính xác là thước đo chúng ta sẽ sử dụng để đánh giá mô hình này:
            \begin{verbatim}
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.metrics import accuracy_score
            \end{verbatim}
            \item Cần chia các nhúng thành 2 nhóm: dữ liệu huấn luyện \& dữ liệu kiểm tra. 1 cách đơn giản để thực hiện: tạo mặt nạ như sau.
            \item Tiếp theo, huấn luyện bộ phân loại Rừng Ngẫu nhiên trên dữ liệu huấn luyện với các nhãn phù hợp.
            \item Cuối cùng, đánh giá mô hình đã huấn luyện trên dữ liệu kiểm tra dựa trên điểm chính xác của nó để đưa ra kết quả cuối cùng cho bộ phân loại của chúng ta.
        \end{enumerate}
        Our model obtains an accuracy score of 95.45\%, which is pretty good considering unfavorable train{\tt/}test split we gave it. There is still room for improvement, but this example showed 2 useful applications of DeepWalk:
        \begin{itemize}
            \item {\it Discovering similarities between nodes} using embeddings \& cosine similarity (unsupervised learning)
            \item {\it Using these embeddings as a dataset} for a supervised task e.g. node classification.
        \end{itemize}
        See in following chaps, ability to learn node representations offers a lot of flexibility to design deeper \& more complex architectures.

        -- Mô hình của chúng tôi đạt điểm chính xác là 95,45\%, khá tốt khi xét đến việc phân tách bài kiểm tra train{\tt/}bất lợi mà chúng tôi đã đưa ra. Vẫn còn nhiều điểm cần cải thiện, nhưng ví dụ này đã cho thấy 2 ứng dụng hữu ích của DeepWalk:
        \begin{itemize}
            \item {\it Khám phá điểm tương đồng giữa các nút} bằng cách sử dụng nhúng \& độ tương đồng cosine (học không giám sát)
            \item {\it Sử dụng các nhúng này làm tập dữ liệu} cho 1 tác vụ có giám sát, e.g.: phân loại nút.
        \end{itemize}
        Xem trong các chương tiếp theo, khả năng học biểu diễn nút mang lại rất nhiều tính linh hoạt để thiết kế các kiến trúc sâu hơn \& phức tạp hơn.
        \item {\sf Summary.} In this chap, learned about DeepWalk architecture \& its major components. Then transformed graph data into sequences using random walks to apply powerful Word2Vec algorithm. Resulting embeddings can be used to find similarities between nodes or as input to other algorithms. In particular, solved a node classification problem using a supervised approach.

        -- Trong chương này, chúng ta đã tìm hiểu về kiến trúc DeepWalk \& các thành phần chính của nó. Sau đó, dữ liệu đồ thị được chuyển đổi thành chuỗi bằng cách sử dụng các bước ngẫu nhiên để áp dụng thuật toán Word2Vec mạnh mẽ. Kết quả nhúng có thể được sử dụng để tìm điểm tương đồng giữa các nút hoặc làm đầu vào cho các thuật toán khác. Cụ thể, chúng ta đã giải quyết được bài toán phân loại nút bằng phương pháp tiếp cận có giám sát.

        In Chap. 4: Improving Embeddings with Biased Random Walks in Node2Vec, introduce a 2nd algorithm based on Word2Vec. Difference with DeepWalk: random walks can be biased towards more or less exploration, which directly impacts embeddings that are produced. Implement this algorithm on a new example \& compare its representations with those obtained using DeepWalk.

        -- Trong Chương 4: Cải thiện nhúng với bước ngẫu nhiên có thiên vị trong Node2Vec, giới thiệu thuật toán thứ 2 dựa trên Word2Vec. Điểm khác biệt với DeepWalk: bước ngẫu nhiên có thể thiên vị theo hướng khám phá nhiều hơn hoặc ít hơn, điều này ảnh hưởng trực tiếp đến các nhúng được tạo ra. Triển khai thuật toán này trên 1 ví dụ mới \& so sánh các biểu diễn của nó với các biểu diễn thu được bằng DeepWalk.
    \end{itemize}
    PART 2: FUNDAMENTALS. In this 2nd part of book, delve into process of constructing node representations using graph learning. Start by exploring traditional graph learning techniques, drawing on advancements made in NLP. Our aim: understand how these techniques can be applied to graphs \& how they can be used to build node representations.

    -- Trong phần 2 của cuốn sách này, chúng ta sẽ đi sâu vào quá trình xây dựng biểu diễn nút bằng phương pháp học đồ thị. Bắt đầu bằng việc khám phá các kỹ thuật học đồ thị truyền thống, dựa trên những tiến bộ trong NLP. Mục tiêu của chúng ta: tìm hiểu cách áp dụng các kỹ thuật này vào đồ thị \& cách sử dụng chúng để xây dựng biểu diễn nút.

    Then move on to incorporating node features into our models \& explore how they can be used to build even more accurate representations. Finally, introduce 2 of most fundamental GNN architectures, Graph Convolutional Network (GCN) \& Graph Attention Network (GAT). These 2 architectures are building blocks of many state-of-art graph learning methods \& will provide a solid foundation for next part.

    -- Sau đó, chuyển sang việc tích hợp các đặc trưng nút vào mô hình của chúng ta \& khám phá cách sử dụng chúng để xây dựng các biểu diễn chính xác hơn nữa. Cuối cùng, hãy giới thiệu 2 kiến trúc GNN cơ bản nhất: Mạng Tích chập Đồ thị (GCN) \& Mạng Chú ý Đồ thị (GAT). Hai kiến trúc này là nền tảng của nhiều phương pháp học đồ thị tiên tiến \& sẽ cung cấp nền tảng vững chắc cho phần tiếp theo.

    By end of this part, have a deeper understanding of how traditional graph learning techniques, e.g. random walks, can be used to crate node representations \& develops graph applications. Additionally, learn how to build even more powerful representations using GNNs. Introduce 2 key GNN architectures \& learn how they can be used to tackle various graph-based tasks.

    -- Đến cuối phần này, bạn sẽ hiểu sâu hơn về cách sử dụng các kỹ thuật học đồ thị truyền thống, e.g. bước đi ngẫu nhiên, để tạo ra các biểu diễn nút \& phát triển các ứng dụng đồ thị. Ngoài ra, hãy tìm hiểu cách xây dựng các biểu diễn mạnh mẽ hơn bằng GNN. Giới thiệu 2 kiến trúc GNN chính \& tìm hiểu cách sử dụng chúng để giải quyết các tác vụ dựa trên đồ thị khác nhau.
    \item {\sf4. Improving Embeddings with Biased Random Walks in Node2Vec.} Node2Vec is an architecture largely based on DeepWalk. In prev chap,, saw 2 main components of this architecture: random walks \& Word2Vec. How can improve quality of our embeddings? Interestingly enough, not with more ML. Instead, Node2Vec brings critical modifications to way random walks themselves are generated.

    -- {\sf Cải thiện nhúng với bước ngẫu nhiên có thiên vị trong Node2Vec.} Node2Vec là 1 kiến trúc phần lớn dựa trên DeepWalk. Trong chương trước, chúng ta đã thấy hai thành phần chính của kiến trúc này: bước ngẫu nhiên \& Word2Vec. Làm thế nào để cải thiện chất lượng nhúng của chúng ta? Điều thú vị là, không phải bằng cách tăng cường ML. Thay vào đó, Node2Vec mang đến những thay đổi quan trọng đối với cách tạo ra các bước ngẫu nhiên.

    In this chap, talk about these modifications \& how to find best parameters for a given graph. Implement Node2Vec architecture \& compare it to using DeepWalk on Zachary's Karate Club. This give you a good understanding of differences between 2 architectures. Finally, use this technology to build a real application: a move {\it recommender system (RecSys)} powered by Node2Vec.

    -- Trong chương này, chúng ta sẽ thảo luận về những thay đổi này \& cách tìm tham số tốt nhất cho 1 đồ thị nhất định. Triển khai kiến trúc Node2Vec \& so sánh với việc sử dụng DeepWalk trên Zachary's Karate Club. Điều này giúp bạn hiểu rõ sự khác biệt giữa hai kiến trúc. Cuối cùng, hãy sử dụng công nghệ này để xây dựng 1 ứng dụng thực tế: 1 hệ thống đề xuất di chuyển (RecSys) được hỗ trợ bởi Node2Vec.

    By end of this chap, know how to implement Node2Vec on any graph dataset \& how to select good parameters. Understand why this architecture works better than DeepWalk in general, \& how to apply it to build creative applications.

    -- Đến cuối chương này, bạn sẽ biết cách triển khai Node2Vec trên bất kỳ tập dữ liệu đồ thị nào \& cách chọn tham số phù hợp. Hiểu lý do tại sao kiến trúc này hoạt động tốt hơn DeepWalk nói chung, \& cách áp dụng nó để xây dựng các ứng dụng sáng tạo.
    \begin{itemize}
        \item {\sf Introducing Node2Vec.} {\tt Node2Vec} was introduced in 2016 by {\sc Grover \& Leskovec} from Stanford University [1]. It keeps same 2 main components from DeepWalk: random walks \& Word2Vec. Difference: instead of obtaining sequences of nodes with a uniform distribution, random walks are carefully biased in Node2Vec. See why these {\it biased random walks} perform better \& how to implement them in 2 following sects: defining a {\it neighborhood}, introducing biases in random walks. Start by questioning our intuitive concept of neighborhoods.

        -- {\sf Giới thiệu Node2Vec.} {\tt Node2Vec} được giới thiệu vào năm 2016 bởi {\sc Grover \& Leskovec} từ Đại học Stanford [1]. Nó giữ nguyên 2 thành phần chính từ DeepWalk: các bước ngẫu nhiên \& Word2Vec. Điểm khác biệt: thay vì thu được chuỗi các nút có phân phối đều, các bước ngẫu nhiên được phân bổ cẩn thận trong Node2Vec. Xem lý do tại sao các bước ngẫu nhiên {\it biased random walks} này hoạt động tốt hơn \& cách triển khai chúng trong 2 phần sau: định nghĩa 1 {\it neighborhood}, giới thiệu các bước ngẫu nhiên trong các bước ngẫu nhiên. Bắt đầu bằng cách đặt câu hỏi về khái niệm trực quan của chúng ta về các vùng lân cận.
        \begin{itemize}
            \item {\sf Defining a neighborhood.} How do you define neighborhood of a node? Key concept introduced in Node2Vec is flexible notion of a neighborhood. Intuitively, we think of it as something close to initial node, but what does ``close'' mean in context of a graph? Want to explore 3 nodes in neighborhood of node A. This exploration process is also called a {\it sampling strategy}:
            \begin{itemize}
                \item A possible solution would be to consider 3 closest nodes in terms of connections. In this case, neighborhood of $A$, noted $N(A)$, would be $\{B,C,D\}$.
                \item Another possible sampling strategy consists of selecting nodes that are not adjacent to previous nodes 1st. In our example, $N(A) = \{D,E,F\}$.
            \end{itemize}
            I.e., want to implement a BFS in 1st case \& a DFS in 2nd one.

            -- {\sf Định nghĩa vùng lân cận.} Bạn định nghĩa vùng lân cận của 1 nút như thế nào? Khái niệm chính được giới thiệu trong Node2Vec là khái niệm linh hoạt về vùng lân cận. Theo trực giác, chúng ta nghĩ về nó như 1 thứ gì đó gần với nút ban đầu, nhưng ``đóng'' có nghĩa là gì trong ngữ cảnh của đồ thị? Bạn muốn khám phá 3 nút trong vùng lân cận của nút A. Quá trình khám phá này cũng được gọi là {\it chiến lược lấy mẫu}:
            \begin{itemize}
                \item 1 giải pháp khả thi là xem xét 3 nút gần nhất về mặt kết nối. Trong trường hợp này, vùng lân cận của $A$, ký hiệu là $N(A)$, sẽ là $\{B,C,D\}$.
                \item 1 chiến lược lấy mẫu khả thi khác bao gồm việc chọn các nút không liền kề với các nút trước đó trước. Trong ví dụ của chúng ta, $N(A) = \{D,E,F\}$.
            \end{itemize}
            Ví dụ, bạn muốn triển khai BFS trong trường hợp thứ nhất \& DFS trong trường hợp thứ 2.

            What is important to notice here: these sampling strategies have opposite behaviors: BFS focuses on local network around a node while DFS establishes a more macro view of graph. Considering our intuitive definition of a neighborhood, it is temping to simply discard DFS. However, Node2vec's authors argue: this would be a mistake: each approach captures a different but valuable representation of network.

            -- Điều quan trọng cần lưu ý ở đây: các chiến lược lấy mẫu này có hành vi trái ngược nhau: BFS tập trung vào mạng cục bộ xung quanh 1 nút trong khi DFS thiết lập 1 góc nhìn vĩ mô hơn về đồ thị. Xét theo định nghĩa trực quan của chúng ta về vùng lân cận, việc loại bỏ DFS là điều dễ hiểu. Tuy nhiên, các tác giả của Node2vec lập luận: đây sẽ là 1 sai lầm: mỗi phương pháp nắm bắt 1 biểu diễn mạng khác nhau nhưng có giá trị.

            They argue: BFS is ideal to emphasize structural equivalence since this strategy only looks at neighboring nodes. In these random walks, nodes are often repeated \& stay close to each other. DFS, on other hand, emphasizes opposite of homophily by creating sequences of distant nodes. These random walks can sample nodes that are far from source \& thus become less representative. This is why we are looking for a trade-off between these 2 properties: homophily may be more helpful for understanding certain graphs \& vice versa.

            -- Họ lập luận: BFS là lý tưởng để nhấn mạnh tính tương đương về mặt cấu trúc vì chiến lược này chỉ xem xét các nút lân cận. Trong các bước đi ngẫu nhiên này, các nút thường được lặp lại \& ở gần nhau. Mặt khác, DFS nhấn mạnh tính đối lập của tính đồng dạng bằng cách tạo ra các chuỗi nút ở xa. Các bước đi ngẫu nhiên này có thể lấy mẫu các nút ở xa nguồn \& do đó trở nên kém đại diện hơn. Đây là lý do tại sao chúng ta đang tìm kiếm sự đánh đổi giữa 2 tính chất này: tính đồng dạng có thể hữu ích hơn trong việc hiểu 1 số đồ thị \& ngược lại.

            If you are confused about this connection, you are not alone: several papers \& blogs wrongly assume: BFS emphasizes homophily \& DFS is connected to structural equivalence. In any case, consider graphs that combine homophily \& structural equivalence to be desired solution. This is why, regardless of these connections, want to use both sampling strategies to create our dataset. See how can implement them to generate random walks.

            -- Nếu bạn đang bối rối về mối liên hệ này, bạn không phải là người duy nhất: 1 số bài báo \& blog đã sai lầm khi cho rằng: BFS nhấn mạnh tính đồng dạng \& DFS có liên quan đến tính tương đương về mặt cấu trúc. Trong mọi trường hợp, hãy coi các đồ thị kết hợp tính đồng dạng \& tính tương đương về mặt cấu trúc là giải pháp mong muốn. Đây là lý do tại sao, bất kể những mối liên hệ này, chúng ta nên sử dụng cả hai chiến lược lấy mẫu để tạo tập dữ liệu. Hãy xem cách triển khai chúng để tạo các bước ngẫu nhiên.
            \item {\sf Introducing biases in random walks.} As a reminder, random walks are sequences of nodes that are randomly selected in a graph. They have a starting point, which can also be random, \& a predefined length. Nodes that often appear together in these walks are like words that appear together in sentences: under homophily hypothesis, they share a similar meaning, hence a similar representation.

            -- {\sf Giới thiệu về độ lệch trong các bước ngẫu nhiên.} Xin nhắc lại, các bước ngẫu nhiên là chuỗi các nút được chọn ngẫu nhiên trong đồ thị. Chúng có điểm khởi đầu, cũng có thể là ngẫu nhiên, \& độ dài được xác định trước. Các nút thường xuất hiện cùng nhau trong các bước này giống như các từ xuất hiện cùng nhau trong câu: theo giả thuyết đồng dạng, chúng có ý nghĩa tương tự nhau, do đó có biểu diễn tương tự nhau.

            In Node2Vec, goal: bias randomness of these walks to either 1 of following:
            \begin{enumerate}
                \item Promoting nodes that are not connected to previous one (similar to DFS)
                \item Promoting nodes that are close to prevous one (similar to BFS)/
            \end{enumerate}
            Current node is called $j$, previous node is $i$, \& future node is $k$. Note $\pi_{jk}$ unnormalized transition probability from node $j$ to node $k$. This probability can be decomposed as $\pi_{jk} = \alpha(i,k)\cdot\omega_{jk}$, where $\alpha(i,k)$ is {\it search bias} between nodes $i,k$ \& $\omega_{jk}$ is weight of edge from $j$ to $k$.

            -- Trong Node2Vec, mục tiêu: điều chỉnh độ ngẫu nhiên của các bước này thành 1 trong các bước sau:
            \begin{enumerate}
                \item Thăng hạng các nút không được kết nối với nút trước đó (tương tự như DFS)
                \item Thăng hạng các nút gần nút trước đó (tương tự như BFS)/

            \end{enumerate}
            Nút hiện tại được gọi là $j$, nút trước đó là $i$, \& nút tương lai là $k$. Lưu ý $\pi_{jk}$ xác suất chuyển tiếp không chuẩn hóa từ nút $j$ đến nút $k$. Xác suất này có thể được phân tích thành $\pi_{jk} = \alpha(i,k)\cdot\omega_{jk}$, trong đó $\alpha(i,k)$ là {\it độ lệch tìm kiếm} giữa các nút $i,k$ \& $\omega_{jk}$ là trọng số của cạnh từ $j$ đến $k$.

            In DeepWalk, have $\alpha(a,b) = 1$ for any pair of nodes $a,b$. In Node2Vec, value of $\alpha(a,b)$ is defined based on distance between nodes \& 2 additional parameters: $p$: return parameter \& $q$: in-out parameter. Their role: approximate DFS \& BFS, resp. Here is how value of $\alpha(a,b)$ is defined:
            \begin{equation*}
                \alpha(a,b) = \left\{\begin{split}
                    &\frac{1}{p}&&\mbox{if } d_{ab} = 0,\\
                    &1&&\mbox{if } d_{ab} = 1,\\
                    &\frac{1}{q}&&\mbox{if } d_{ab} = 2.
                \end{split}\right.
            \end{equation*}
            Here $d_{ab}$ is shortest path distance between nodes $a,b$. Can update unnormalized transition probability from previous graphs as follows: {\sf Fig. 4.3: Graph with transition probabilities}. Decrypt these probabilities:
            \begin{itemize}
                \item Walk starts from node $i$ \& now arrive at node $j$. Probability of going back to previous node $i$ is controlled by parameter $p$. Higher it is, more random walk will explore new nodes instead of repeating same ones \& looking like DFS.
                \item Unnormalized probability of going to $k_1$ is 1 because this node is in immediate neighborhood of our previous node $i$.
                \item Finally, probability of going to node $k_2$ is controlled by parameter $q$. Higher it is, more random walk will focus on nodes that are close to previous one \& look like BFS.

                Best way to understand this is to actually implement this architecture \& play with parameters. Do it step by step on Zachary's Karate Club.
            \end{itemize}
            -- Ở đây $d_{ab}$ là khoảng cách đường đi ngắn nhất giữa các nút $a,b$. Có thể cập nhật xác suất chuyển tiếp chưa chuẩn hóa từ các đồ thị trước đó như sau: {\sf Hình 4.3: Đồ thị với xác suất chuyển tiếp}. Giải mã các xác suất này:
            \begin{itemize}
                \item Đường đi bắt đầu từ nút $i$ \& bây giờ đến nút $j$. Xác suất quay lại nút $i$ trước đó được kiểm soát bởi tham số $p$. Giá trị p càng cao, bước đi ngẫu nhiên càng khám phá các nút mới thay vì lặp lại các nút cũ \& trông giống như DFS.
                \item Xác suất chưa chuẩn hóa để đi đến $k_1$ là 1 vì nút này nằm trong vùng lân cận trực tiếp của nút $i$ trước đó.
                \item Cuối cùng, xác suất đi đến nút $k_2$ được kiểm soát bởi tham số $q$. Giá trị p càng cao, bước đi ngẫu nhiên càng tập trung vào các nút gần nút trước đó \& trông giống như BFS.

                Cách tốt nhất để hiểu điều này là thực sự triển khai kiến trúc này \& thử nghiệm với các tham số. Thực hiện từng bước trên Câu lạc bộ Karate của Zachary.
            \end{itemize}
            Note: it is an unweighted network, which is why transition probability is only determined by search bias. 1st, want to create a function that will randomly select next node in a graph based on previous node, current node, \& 2 parameters $p,q$.
            \begin{enumerate}
                \item Start by importing required libraries: {\tt networkx, random, numpy}.
                \item Define \verb|next_node| function with list of our parameters.
                \item Retrieve list of neighboring nodes from current node \& initialize a list of alpha values.
                \item For each neighbor, want to calculate appropriate alpha value: $\frac{1}{p}$ if this neighbor is previous node, 1 if this neighbor is connected to previous node, \& $\frac{1}{q}$ otherwise.
                \item Normalize these values to create probabilities.
                \item Randomly select next node based on transition probabilities calculated in previous step using {\tt np.random.choice()} \& return it.
            \end{enumerate}
            Before this function can be tested, need code to generate entire random walk.

            -- Lưu ý: đây là 1 mạng không trọng số, đó là lý do tại sao xác suất chuyển tiếp chỉ được xác định bởi độ lệch tìm kiếm. Đầu tiên, muốn tạo 1 hàm sẽ chọn ngẫu nhiên nút tiếp theo trong đồ thị dựa trên nút trước đó, nút hiện tại, \& 2 tham số $p,q$.
            \begin{enumerate}
                \item Bắt đầu bằng cách nhập các thư viện cần thiết: {\tt networkx, random, numpy}.
                \item Định nghĩa hàm \verb|next_node| với danh sách các tham số của chúng ta.
                \item Truy xuất danh sách các nút lân cận từ nút hiện tại \& khởi tạo danh sách các giá trị alpha.
                \item Đối với mỗi nút lân cận, muốn tính giá trị alpha phù hợp: $\frac{1}{p}$ nếu nút lân cận này là nút trước đó, 1 nếu nút lân cận này được kết nối với nút trước đó, \& $\frac{1}{q}$ nếu không.
                \item Chuẩn hóa các giá trị này để tạo xác suất.

                \item Chọn ngẫu nhiên nút tiếp theo dựa trên xác suất chuyển tiếp được tính toán ở bước trước bằng cách sử dụng {\tt np.random.choice()} \& trả về kết quả.
            \end{enumerate}
            Trước khi có thể kiểm tra hàm này, cần mã để tạo toàn bộ bước ngẫu nhiên.

            Way we generate these random walks is similar to what we saw in previous chap. Difference: next node is chosen by \verb|next_node()| function, which requires additional parameters: $p,q$, but also previous \& current nodes. These nodes can easily be obtained by looking at 2 last elements added to {\tt walk} variable. Also return strings instead of integers for compatibility reasons. Here is new version of \verb|random_walk()| function.

            -- Cách chúng ta tạo ra các bước ngẫu nhiên này tương tự như những gì chúng ta đã thấy trong chương trước. Điểm khác biệt: nút tiếp theo được chọn bởi hàm \verb|next_node()|, hàm này yêu cầu các tham số bổ sung: $p,q$, nhưng cũng bao gồm các nút trước đó \& nút hiện tại. Các nút này có thể dễ dàng được lấy bằng cách xem xét 2 phần tử cuối cùng được thêm vào biến {\tt walk}. Đồng thời, trả về chuỗi thay vì số nguyên vì lý do tương thích. Dưới đây là phiên bản mới của hàm \verb|random_walk()|.

            Now have every element to generate our random walks. Try one with a length of 5, $p = q = 1$: \verb|random_walk(0, 8, p = 1, q = 1)|. This should be random since every neighboring node has same transition probability. With these parameters, reproduce exact DeepWalk algorithm. Now bias them toward going back to previous node with $q = 10$: \verb|random_walk(0, 8, p = 1, q = 10)|. This time, random walk explores more node in graph, can see that it never goes back to previous node because probability is low with $p = 10$: \verb|random_walk(0, 8, p = 10, q = 1)|. See how to use these properties in a real example \& compare it to DeepWalk.

            -- Bây giờ, hãy tạo ra mọi phần tử để tạo ra các bước đi ngẫu nhiên. Hãy thử 1 bước đi có độ dài 5, $p = q = 1$: \verb|random_walk(0, 8, p = 1, q = 1)|. Bước đi này phải ngẫu nhiên vì mọi nút lân cận đều có cùng xác suất chuyển tiếp. Với các tham số này, hãy tái tạo chính xác thuật toán DeepWalk. Bây giờ, hãy hướng chúng về phía nút trước đó với $q = 10$: \verb|random_walk(0, 8, p = 1, q = 10)|. Lần này, bước đi ngẫu nhiên khám phá nhiều nút hơn trong đồ thị, có thể thấy rằng nó không bao giờ quay lại nút trước đó vì xác suất thấp với $p = 10$: \verb|random_walk(0, 8, p = 10, q = 1)|. Xem cách sử dụng các thuộc tính này trong 1 ví dụ thực tế \& so sánh nó với DeepWalk.
        \end{itemize}
        \item {\sf Implementing Node2Vec.} Now have function to generate biased random walks, implementation of Node2Vec is very similar to implementing DeepWalk. It is so similar that we can reuse same code \& create sequences with $p = q = 1$ to implement DeepWalk as a special case of Node2Vec. Reuse Zachary's Karate Club for this task. As in prev chap, goal: correctly classify each member of club as part of 1 of 2 groups (``Mr. Hi'' \& ``Officer''). Use node embeddings provided by Node2Vec as input to a ML classifier (Random Forest in this case). See how to implement it step by step:
        \begin{enumerate}
            \item 1st, want to install {\tt gensim} library to use Word2Vec. This time, use version 3.8.0 for compatibility reasons:
            \begin{verbatim}
!pip install -qI gensim==3.8.0
            \end{verbatim}
            \item Import required libraries.
            \item Load dataset (Zachary's Karate Club).
            \item Transform nodes' labels into numerical values: 0 \& 1.
            \item Generate a list of random walks as seen previously using \verb|random_walk()| function 80 times for each node in graph. Parameters $p,q$ as specified here (2 \& 1, resp).
            \item Create an instance of Word2Vec (a skip-gram model) with a hierarchical {\tt softmax} function.
            \item Skip-gram model is trained on sequences generated for 30 epochs.
            \item Create masks to train \& test classifier.
            \item Random Forest classifier is trained on training data.
            \item Evaluate it in terms of accuracy for test data.
        \end{enumerate}
        -- {\sf Triển khai Node2Vec.} Giờ đây, chúng ta đã có chức năng tạo các bước ngẫu nhiên có thiên vị, việc triển khai Node2Vec rất giống với việc triển khai DeepWalk. Nó giống nhau đến mức chúng ta có thể sử dụng lại cùng 1 mã \& tạo các chuỗi với $p = q = 1$ để triển khai DeepWalk như 1 trường hợp đặc biệt của Node2Vec. Hãy sử dụng lại Câu lạc bộ Karate của Zachary cho nhiệm vụ này. Như trong chương trước, mục tiêu: phân loại chính xác từng thành viên của câu lạc bộ thành 1 trong 2 nhóm (``Mr. Hi'' \& ``Sĩ quan''). Sử dụng các nhúng nút do Node2Vec cung cấp làm đầu vào cho bộ phân loại ML (trong trường hợp này là Random Forest). Xem cách triển khai từng bước:
        \begin{enumerate}
            \item Trước tiên, muốn cài đặt thư viện {\tt gensim} để sử dụng Word2Vec. Lần này, hãy sử dụng phiên bản 3.8.0 vì lý do tương thích:
            \begin{verbatim}
!pip install -qI gensim==3.8.0
            \end{verbatim}
            \item Nhập các thư viện cần thiết.
            \item Tải tập dữ liệu (Câu lạc bộ Karate Zachary).
            \item Chuyển đổi nhãn của các nút thành giá trị số: 0 \& 1.
            \item Tạo danh sách các bước đi ngẫu nhiên như đã thấy trước đó bằng cách sử dụng hàm \verb|random_walk()| 80 lần cho mỗi nút trong đồ thị. Các tham số $p,q$ như được chỉ định ở đây (tương ứng là 2 \& 1).
            \item Tạo 1 thể hiện của Word2Vec (mô hình skip-gram) với hàm {\tt softmax} phân cấp.
            \item Mô hình skip-gram được huấn luyện trên các chuỗi được tạo trong 30 kỷ nguyên.
            \item Tạo mặt nạ để huấn luyện \& kiểm tra bộ phân loại.
            \item Bộ phân loại Rừng ngẫu nhiên được huấn luyện trên dữ liệu huấn luyện.
            \item Đánh giá nó về độ chính xác cho dữ liệu kiểm tra.
        \end{enumerate}
        To implement DeepWalk, can repeat exact same process with $p = q = 1$. However, to make a fair comparison, cannot use a single accuracy score. Indeed, there are a lot of stochastic processes involved -- could be unlucky \& get a better result from worst model.

        -- Để triển khai DeepWalk, có thể lặp lại chính xác quy trình đó với $p = q = 1$. Tuy nhiên, để so sánh 1 cách công bằng, không thể sử dụng 1 điểm chính xác duy nhất. Thực tế, có rất nhiều quy trình ngẫu nhiên liên quan -- có thể không may mắn \& có được kết quả tốt hơn từ mô hình tệ nhất.

        To limit randomness of our results, can repeat this process 100 times \& take mean value. This result is a lot more stable \& can even include standard deviation (using {\tt np.std()}) to measure variability in accuracy scores.

        -- Để hạn chế tính ngẫu nhiên của kết quả, chúng ta có thể lặp lại quy trình này 100 lần \& lấy giá trị trung bình. Kết quả này ổn định hơn nhiều \& thậm chí có thể bao gồm độ lệch chuẩn (sử dụng {\tt np.std()}) để đo lường độ biến thiên của điểm chính xác.

        But just before we do that, play a game. In prev chap, talked about Zachary's Karate Club as a homophilic network. This property is emphasized by DFS, which is encouraged by increasing parameter $p$. If this statement \& connection between DFS \& homophily are true, should get better results with higher values of $p$.

        -- Nhưng trước khi làm điều đó, hãy chơi 1 trò chơi. Trong chương trước, chúng ta đã thảo luận về Câu lạc bộ Karate của Zachary như 1 mạng lưới đồng dạng. Tính chất này được nhấn mạnh bởi DFS, \& được hỗ trợ bằng cách tăng tham số $p$. Nếu phát biểu này \& kết nối giữa DFS \& đồng dạng là đúng, sẽ cho kết quả tốt hơn với các giá trị $p$ cao hơn.

        Repeated same experiment for values of $p,q\in[1,7]$. In a real ML project, would use validation data to perform this parameter search. In this example, use test data because this study is already our final application. {\sf Fig. 4.5: Average accuracy score \& standard deviation for different values of $p,q$.} There are several noticeable results:
        \begin{itemize}
            \item DeepWalk $p = q = 1$4 performs worse than any other combination of $p,q$ that is covered here. This is true for this dataset \& show how useful biased random walks can be. However, it is not always case: non-biased random walks can also perform better on other datasets.
            \item High values of $p$ lead to better performance, which validates our hypothesis. Knowing that this is a social network strongly suggests that biasing our random walks toward homophily is a good strategy. This is something to keep in mind when dealing with this kind of graph.

            Feel free to play with parameters \& try to find other interesting results. Could explore results with very high values of $p > 7$, or on contrary, values of $p,q$ between 0, 1.
        \end{itemize}
        Zachary's Karate Club is a basic dataset, but see in next sect how can use this technology to build much more interesting applications.

        -- Lặp lại cùng 1 thí nghiệm với các giá trị $p,q\in[1,7]$. Trong 1 dự án ML thực tế, sẽ sử dụng dữ liệu xác thực để thực hiện tìm kiếm tham số này. Trong ví dụ này, hãy sử dụng dữ liệu thử nghiệm vì nghiên cứu này đã là ứng dụng cuối cùng của chúng tôi. {\sf Hình 4.5: Điểm chính xác trung bình \& độ lệch chuẩn cho các giá trị khác nhau của $p,q$.} Có 1 số kết quả đáng chú ý:
        \begin{itemize}
            \item DeepWalk $p = q = 1$4 hoạt động kém hơn bất kỳ tổ hợp $p,q$ nào khác được đề cập ở đây. Điều này đúng với tập dữ liệu này \& cho thấy các bước đi ngẫu nhiên có thiên vị hữu ích như thế nào. Tuy nhiên, không phải lúc nào cũng vậy: các bước đi ngẫu nhiên không thiên vị cũng có thể hoạt động tốt hơn trên các tập dữ liệu khác.
            \item Các giá trị $p$ cao dẫn đến hiệu suất tốt hơn, điều này xác nhận giả thuyết của chúng tôi. Việc biết rằng đây là 1 mạng xã hội cho thấy rõ ràng rằng việc định hướng các bước đi ngẫu nhiên của chúng tôi theo hướng đồng dạng là 1 chiến lược tốt. Đây là điều cần lưu ý khi xử lý loại đồ thị này.

            Hãy thoải mái thử nghiệm với các tham số \& cố gắng tìm ra những kết quả thú vị khác. Bạn có thể khám phá các kết quả với giá trị rất cao $p > 7$, hoặc ngược lại, các giá trị $p,q$ nằm trong khoảng từ 0 đến 1.
        \end{itemize}
        Câu lạc bộ Karate của Zachary là 1 tập dữ liệu cơ bản, nhưng hãy xem trong phần tiếp theo cách sử dụng công nghệ này để xây dựng các ứng dụng thú vị hơn nhiều.
        \item {\sf Building a move RecSys.} 1 of most popular applications of GNNs is RecSys. If think about foundation of Word2Vec (\&, thus, DeepWalk \& Node2Vec), goal: produce vectors with ability to measure their similarity. Encode movies instead of words, \& can suddenly ask for movies that are most similar to a given input file. It sounds a lot like a RecSys.

        -- {\sf Xây dựng RecSys di chuyển.} 1 trong những ứng dụng phổ biến nhất của GNN là RecSys. Nếu nghĩ về nền tảng của Word2Vec (\&, i.e., DeepWalk \& Node2Vec), mục tiêu: tạo ra các vectơ có khả năng đo lường độ tương đồng của chúng. Mã hóa phim thay vì từ ngữ, \& có thể đột nhiên yêu cầu các phim giống nhất với 1 tệp đầu vào nhất định. Nghe có vẻ rất giống RecSys.

        But how to encode movies? Want to create (biased) random walks of movies, but this requires a graph dataset where similar movies are connected to each other. This is not easy to find.

        -- Nhưng làm thế nào để mã hóa phim? Bạn muốn tạo các chuỗi phim ngẫu nhiên (có thiên vị), nhưng điều này đòi hỏi 1 tập dữ liệu đồ thị trong đó các phim tương tự được kết nối với nhau. Điều này không dễ tìm.

        Another approach is to look at user ratings. There are different techniques to build a graph based on ratings: bipartite graphs, edges based on pointwise mutual information, \& so on. In this sect, implement a simple \& intuitive approach: movies that are likely by same users are connected. Then use this graph to learn movie embeddings using Node2Vec:

        pp. 60--64+++
        \item {\sf Summary.} In this chap, learned about Node2Vec, a 2nd architecture based on popular Word2Vec. Implemented functions to generate biased random walks \& explained connection between their parameters \& 2 network properties: homophily \& structural equivalence. Showed their usefulness by comparing Node2Vec's results to DeepWalk's for Zachary's Karate Club. Finally, build our 1st RecSys using a custom graph dataset \& another implementation of Node2Vec. It gave us correct recommendations that we will improve even more in later chaps.

        -- Trong chương này, chúng ta đã tìm hiểu về Node2Vec, 1 kiến trúc thứ 2 dựa trên Word2Vec phổ biến. Chúng tôi đã triển khai các hàm để tạo các bước ngẫu nhiên có thiên vị \& giải thích mối liên hệ giữa các tham số của chúng \& 2 thuộc tính mạng: đồng dạng \& tương đương cấu trúc. Chúng tôi đã chứng minh tính hữu ích của chúng bằng cách so sánh kết quả của Node2Vec với kết quả của DeepWalk cho Câu lạc bộ Karate của Zachary. Cuối cùng, chúng tôi đã xây dựng RecSys đầu tiên của mình bằng cách sử dụng 1 bộ dữ liệu đồ thị tùy chỉnh \& 1 triển khai khác của Node2Vec. Nó đã đưa ra cho chúng tôi những khuyến nghị chính xác mà chúng tôi sẽ cải thiện hơn nữa trong các chương sau.

        In Chap. 5: Including Node Features with Vanilla Neural Networks, talk about 1 overlooked issue concerning DeepWalk \& Node2Vec: lack of proper node features. Try to address this problem using traditional neural networks, which cannot understand network topology. This dilemma is important to understand before we finally introduce answer: graph neural networks.

        -- Trong Chương 5: Bao gồm các đặc điểm nút với mạng nơ-ron Vanilla, chúng ta sẽ thảo luận về 1 vấn đề thường bị bỏ qua liên quan đến DeepWalk \& Node2Vec: thiếu các đặc điểm nút phù hợp. Hãy thử giải quyết vấn đề này bằng cách sử dụng các mạng nơ-ron truyền thống, vốn không thể hiểu được cấu trúc mạng. Vấn đề nan giải này rất quan trọng cần được hiểu rõ trước khi chúng ta giới thiệu câu trả lời cuối cùng: mạng nơ-ron đồ thị.
    \end{itemize}
    \item {\sf5. Including Node Features with Vailla Neural Networks.} So far, only type of information we have considered is graph topology. However, graph datasets tend to be richer than a mere set of connections: nodes \& edges can also have features to represent scores, colors, words, \& so on. Including this additional information in our input data is essential to produce best embeddings possible. In fact, this is something natural in ML: node \& edge featues have same structure as a tabular (non-graph) dataset. I.e., traditional techniques can be applied to this data, e.g. neural networks.

    -- Cho đến nay, loại thông tin duy nhất chúng ta xem xét là tôpô đồ thị. Tuy nhiên, tập dữ liệu đồ thị thường phong phú hơn 1 tập hợp các kết nối đơn thuần: các nút \& cạnh cũng có thể có các đặc trưng để biểu diễn điểm số, màu sắc, từ ngữ, v.v. Việc đưa thông tin bổ sung này vào dữ liệu đầu vào là điều cần thiết để tạo ra các phép nhúng tốt nhất có thể. Trên thực tế, đây là điều tự nhiên trong ML: các đặc trưng nút \& cạnh có cùng cấu trúc với tập dữ liệu dạng bảng (không phải đồ thị). Tức là, các kỹ thuật truyền thống có thể được áp dụng cho dữ liệu này, e.g., như mạng nơ-ron.

    In this chap, introduce 2 new graph datasets: {\tt Cora \& Facebook Page-Page}. See how Vanilla Neural Networks perform perform on node features only by considering them as tabular datasets. Then experiment to include topological information in our neural networks. This will give us 1st GNN architecture: a simple model that considers both node features \& edges. Finally, compare performance of 2 architectures \& obtain 1 of most important results of this book.

    -- Trong chương này, chúng ta sẽ giới thiệu 2 bộ dữ liệu đồ thị mới: {\tt Cora \& Facebook Page-Page}. Xem cách Mạng Nơ-ron Vanilla hoạt động trên các đặc điểm nút chỉ bằng cách xem xét chúng dưới dạng tập dữ liệu dạng bảng. Sau đó, hãy thử nghiệm việc đưa thông tin tôpô vào mạng nơ-ron của chúng ta. Điều này sẽ cho chúng ta kiến trúc GNN đầu tiên: 1 mô hình đơn giản xem xét cả đặc điểm nút \& cạnh. Cuối cùng, hãy so sánh hiệu suất của 2 kiến trúc \& thu được 1 trong những kết quả quan trọng nhất của cuốn sách này.

    By end of this chap, master implementation of vanilla neural networks \& vanilla GNNs in PyTorch. Able to embed topological features into node representations, which is basis of every GNN architecture. This will allow you to greatly improve performance of your models by transforming tabular datasets into graphs problems.

    -- Đến cuối chương này, bạn sẽ thành thạo việc triển khai mạng nơ-ron nhân tạo thuần túy \& mạng nơ-ron nhân tạo thuần túy (GNN) trong PyTorch. Bạn có thể nhúng các đặc điểm tôpô vào biểu diễn nút, vốn là nền tảng của mọi kiến trúc GNN. Điều này sẽ cho phép bạn cải thiện đáng kể hiệu suất mô hình bằng cách chuyển đổi các tập dữ liệu dạng bảng thành các bài toán đồ thị.
    \begin{itemize}
        \item {\sf Introducing graph datasets.} Graph dataset used in this chap are richer than Zachary's Karate Club: they have more nodes, more edges, \& include node features. In this sec, introduce them to give us a good understanding of these graphs \& how to process them with PyTorch Geometric. Here are 2 datasets we will use: {\tt Cora} dataset, {\tt Facebook Page-page} dataset. Start with smaller one: popular {\tt Cora} dataset.

        -- {\sf Giới thiệu về bộ dữ liệu đồ thị.} Bộ dữ liệu đồ thị được sử dụng trong chương này phong phú hơn Zachary's Karate Club: chúng có nhiều nút hơn, nhiều cạnh hơn, \& bao gồm các đặc trưng nút. Trong phần này, chúng tôi sẽ giới thiệu chúng để giúp chúng ta hiểu rõ hơn về các đồ thị này \& cách xử lý chúng bằng PyTorch Geometric. Dưới đây là 2 bộ dữ liệu chúng ta sẽ sử dụng: bộ dữ liệu {\tt Cora}, bộ dữ liệu {\tt Facebook Page-page}. Bắt đầu với bộ dữ liệu nhỏ hơn: bộ dữ liệu {\tt Cora} phổ biến.
        \begin{itemize}
            \item {\sf Cora dataset.} p. 68+++
        \end{itemize}

        \item {\sf Classifying nodes with vanilla neural networks.}
        \item {\sf Classifying nodes with vanilla graph neural networks.}
    \end{itemize}
    \item {\sf6. Introducing Graph Convolutional Networks.}
    \item {\sf7. Graph Attention Networks.}

    PART 3: ADVANCED TECHNIQUES.
    \item {\sf8. Scaling Up Graph Neural Networks with GraphSAGE.}
    \item {\sf9. Defining Expressiveness for Graph Classification.}
    \item {\sf10. Predicting Links with Graph Neural Networks.}
    \item {\sf11. Generating Graphs Using Graph Neural Networks.}
    \item {\sf12. Learning from Heterogeneous Graphs.}
    \item {\sf13. Temporal Graph Neural Networks.}
    \item {\sf14. Explaining Graph Neural Networks.}

    PART 4: APPLICATIONS.
    \item {\sf15. Forecasting Traffic Using A3T-GCN.}
    \item {\sf16. Detecting Anomalies Using Heterogeneous GNNs.}
    \item {\sf17. Building a Recommender System Using LightGCN.}
    \item {\sf18. Unlocking the Potential of Graph Neural Networks for Real-World Applications.}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{{\sc Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao}. Graph Neural Networks: Foundations, Frontiers, \& Applications. 2022}

\begin{itemize}
    \item {\sf Foreword.} ``1st comprehensive book covering full spectrum of a young, fast-growing research field, GNNs, written by authoritative authors!'' -- Jiawei Han (Michael Aiken Chair Professor at University of Illinois at Urbana-Champaign, ACM Fellow \& IEEE Fellow)

    -- ``Cuốn sách toàn diện đầu tiên bao quát toàn bộ lĩnh vực nghiên cứu trẻ, phát triển nhanh chóng, GNN, được viết bởi các tác giả có thẩm quyền!'' -- Jiawei Han (Giáo sư Michael Aiken tại Đại học Illinois ở Urbana-Champaign, Nghiên cứu viên ACM \& Nghiên cứu viên IEEE)

    ``This book presents a comprehensive \& timely survey on graph representation learning. Edited \& contributed by best group of experts in this area, this book is a must-read for students, researchers \& practitioners who want to learn anything about GNNs.'' -- Heung-Yeung ``Harry'' Shum (Former Executive Vice President for Technology \& Research at Microsoft Research, ACM Fellow, IEEE Fellow, FREng)

    -- ``Cuốn sách này trình bày 1 khảo sát toàn diện \& kịp thời về học biểu diễn đồ thị. Được biên tập \& đóng góp bởi nhóm chuyên gia hàng đầu trong lĩnh vực này, cuốn sách này là tài liệu không thể bỏ qua cho sinh viên, nhà nghiên cứu \& những người thực hành muốn tìm hiểu bất cứ điều gì về GNN.'' -- Heung-Yeung ``Harry'' Shum (Cựu Phó Chủ tịch Điều hành Công nghệ \& Nghiên cứu tại Microsoft Research, Học giả ACM, Học giả IEEE, FREng)

    ``As new frontier of DL, GNNs offer great potential to combine probabilistic learning \& symbolic reasoning, \& bridge knowledge-driven \& data-driven paradigms, nurturing development of 3rd-generation AI. This book provides a comprehensive \& insightful introduction to GNN, ranging from foundations to frontiers, from algorithms to applications. It is a valuable resource for any scientist, engineer \& student who wants to get into  this exciting field.'' -- Bo Zhang (Member of Chinese Academy of Science, Professor at Tsinghua University)

    -- ``Là 1 lĩnh vực mới của DL, mạng lưới thần kinh nhân tạo (GNN) mang lại tiềm năng to lớn trong việc kết hợp học xác suất \& suy luận biểu tượng, \& kết nối các mô hình dựa trên kiến thức \& dựa trên dữ liệu, nuôi dưỡng sự phát triển của AI thế hệ thứ 3. Cuốn sách này cung cấp 1 giới thiệu toàn diện \& sâu sắc về GNN, từ nền tảng đến lĩnh vực, từ thuật toán đến ứng dụng. Đây là 1 nguồn tài nguyên quý giá cho bất kỳ nhà khoa học, kỹ sư \& sinh viên nào muốn bước vào lĩnh vực thú vị này.'' -- Bo Zhang (Viện Hàn lâm Khoa học Trung Quốc, Giáo sư tại Đại học Thanh Hoa)

    ``GNNs are 1 of hottest areas of ML \& this book is a wonderful in-depth resource covering a broad range of topics \& applications of graph representation learning.'' -- Jure Leskovec (Associate Professor at Stanford University, \& investigator at Chan Zuckerberg Biohub).

    -- ``GNN là 1 trong những lĩnh vực hấp dẫn nhất của ML \& cuốn sách này là nguồn tài nguyên chuyên sâu tuyệt vời bao gồm nhiều chủ đề \& ứng dụng của việc học biểu diễn đồ thị.'' -- Jure Leskovec (Phó giáo sư tại Đại học Stanford \& nhà nghiên cứu tại Chan Zuckerberg Biohub).

    ``GNNs are an emerging ML model that is already taking scientific \& industrial world by storm. Time is perfect to get in on action -- \& this book is a great resource for newcomers \& reasoned practitioners alike! Its chaps are very carefully written by many of thought leaders at forefront of area.'' -- Petar Veličković (Senior Research Scientist, DeepMind)

    -- ``GNN là 1 mô hình ML mới nổi đang gây sốt trong giới khoa học \& công nghiệp. Đã đến lúc bắt tay vào hành động -- \& cuốn sách này là 1 nguồn tài nguyên tuyệt vời cho cả người mới bắt đầu \& những người thực hành có lý trí! Các bài viết trong đó được viết rất cẩn thận bởi nhiều nhà tư tưởng hàng đầu trong lĩnh vực này.'' -- Petar Veličković (Nhà khoa học nghiên cứu cao cấp, DeepMind)
    \item {\sf Preface.} Field of GNNs has seen rapid \& incredible strides over recent years. GNNs, also known as DL on graphs, graph representation learning, or geometric DL, have become 1 of fastest-growing research topics in ML, especially DL. This wave of research at intersection of graph theory \& DL has also influenced other fields of science, including recommendation systems, computer vision, NLP, inductive logic programming, program synthesis, software mining, automated planning, cybersecurity, \& intelligent transportation.

    -- Lĩnh vực GNN đã chứng kiến những bước tiến nhanh chóng \& đáng kinh ngạc trong những năm gần đây. GNN, còn được gọi là DL trên đồ thị, học biểu diễn đồ thị, hoặc DL hình học, đã trở thành 1 trong những chủ đề nghiên cứu phát triển nhanh nhất trong ML, đặc biệt là DL. Làn sóng nghiên cứu giao thoa giữa lý thuyết đồ thị \& DL này cũng đã ảnh hưởng đến các lĩnh vực khoa học khác, bao gồm hệ thống đề xuất, thị giác máy tính, NLP, lập trình logic quy nạp, tổng hợp chương trình, khai thác phần mềm, lập kế hoạch tự động, an ninh mạng, \& giao thông thông minh.

    Although GNNs have achieved remarkable attention, it still faces many challenges when applying them into other domains, from theoretical understanding of methods to scalability \& interprebility in a real system, \& from soundness of methodology to empirical performance in an application. However, as field rapidly grows, it has been extremely challenging to gain a global perspective of developments of GNNs. Therefore, feel urgency to bridge above gap \& have a comprehensive book on this fast-growing yet challenging topic, which an benefit a broad audience including advanced undergraduate \& graduate students, postdoctoral researchers, lecturers, \& industrial practitioners.

    -- Mặc dù GNN đã thu hút được sự chú ý đáng kể, nhưng nó vẫn gặp nhiều thách thức khi áp dụng vào các lĩnh vực khác, từ hiểu biết lý thuyết về phương pháp đến khả năng mở rộng \& khả năng diễn giải trong hệ thống thực, \& từ tính vững chắc của phương pháp luận đến hiệu suất thực nghiệm trong ứng dụng. Tuy nhiên, khi lĩnh vực này phát triển nhanh chóng, việc có được cái nhìn toàn cầu về sự phát triển của GNN là vô cùng khó khăn. Do đó, chúng ta cần khẩn trương thu hẹp khoảng cách \& có 1 cuốn sách toàn diện về chủ đề đang phát triển nhanh chóng nhưng đầy thách thức này, có thể mang lại lợi ích cho nhiều đối tượng, bao gồm sinh viên đại học cao cấp \& sau đại học, nghiên cứu sinh sau tiến sĩ, giảng viên, \& những người hành nghề trong ngành.

    This book is intended to cover a broad range of topics in GNNs, from foundations to frontiers, \& from methodologies to applications. Book is dedicated to introducing fundamental concepts \& algorithms of GNNs, new research frontiers of GNNs, \& broad \& emerging applications with GNNs.

    -- Cuốn sách này được thiết kế để bao quát 1 loạt các chủ đề về GNN, từ nền tảng đến các lĩnh vực tiên tiến, \& từ phương pháp luận đến ứng dụng. Sách dành riêng để giới thiệu các khái niệm cơ bản \& thuật toán của GNN, các lĩnh vực nghiên cứu mới của GNN, \& các ứng dụng rộng \& mới nổi của GNN.
    \begin{itemize}
        \item {\sf Book Website \& Resources.} Website \url{https://graph-neural-networks.github.io} provides online preprints \& lecture slides of all chaps, also provides pointers to useful material \& resources publicly available \& relevant to GNNs.
        \item {\sf To Instructors.} Book can be used for 1-semester graduate course for graduate students. Though mainly written for students with a background in CS, students with a basic understanding of probability, statistics, graph theory, linear algebra, \& ML techniques e.g. DL will find it easily accessible. Some chaps can be skipped or assigned as homework assignments for reviewing purposes if students have knowledge of a chap. Instructors can choose to combine Chaps. 1--3 together as background introduction course at beginning.

        -- Sách có thể được sử dụng cho các khóa học sau đại học kéo dài 1 học kỳ dành cho sinh viên cao học. Mặc dù chủ yếu được viết cho sinh viên có nền tảng về Khoa học Máy tính, nhưng sinh viên có kiến thức cơ bản về xác suất, thống kê, lý thuyết đồ thị, đại số tuyến tính, \& các kỹ thuật Học máy (ML) như DL sẽ dễ dàng tiếp cận. 1 số chương có thể được bỏ qua hoặc giao làm bài tập về nhà để ôn tập nếu sinh viên đã biết về 1 chương nào đó. Giảng viên có thể chọn kết hợp các Chương 1-3 lại với nhau như 1 khóa học nhập môn nền tảng lúc bắt đầu.

        When course focuses more on foundation \& theories of GNNs, instructor can choose to focus more on Chaps. 4--8 while using Chaps. 19--27 to showcase applications, motivations, \& limitations. When course focuses more on research frontiers, Chaps. 9--18 can be pivot to organize course. E.g., an instructor can make it an advanced graduate course where students are asked to search \& present most recent research papers in each different research frontier. They can also be asked to established their course projects based on applications described in Chaps. 19--27 as well as materials provided on website.

        -- Khi khóa học tập trung nhiều hơn vào nền tảng \& lý thuyết của GNN, giảng viên có thể chọn tập trung nhiều hơn vào Chương 4-8 trong khi sử dụng Chương 19-27 để trình bày các ứng dụng, động lực, \& hạn chế. Khi khóa học tập trung nhiều hơn vào các lĩnh vực nghiên cứu, Chương 9-18 có thể là bước đệm để tổ chức khóa học. Ví dụ: giảng viên có thể biến nó thành 1 khóa học sau đại học nâng cao, trong đó sinh viên được yêu cầu tìm kiếm \& trình bày các bài báo nghiên cứu gần đây nhất trong mỗi lĩnh vực nghiên cứu khác nhau. Họ cũng có thể được yêu cầu thiết lập các dự án khóa học dựa trên các ứng dụng được mô tả trong Chương 19-27 cũng như các tài liệu được cung cấp trên trang web.
        \item {\sf To Readers.} This book was designed to cover a wide range of topics in field of GNN field, including background, theoretical foundations, methodologies, researcher frontier, \& applications. Therefore, it can be treated as a comprehensive handbook for a wide variety of readers e.g. students, researchers, \& professionals. Should have some knowledge of concepts \& terminology associated with statistics, ML, \& graph theory. Some backgrounds of basics have been provided \& referenced in 1st 8 chaps. Should better also have knowledge of DL \& Some programming experience for easily accessing most of chaps of this book. In particular, should be able to read pseudocode \& understand graph structures.

        -- Cuốn sách này được thiết kế để bao quát 1 loạt các chủ đề trong lĩnh vực Mạng Lưới Mạng (GNN), bao gồm bối cảnh, nền tảng lý thuyết, phương pháp luận, ranh giới nghiên cứu, \& ứng dụng. Do đó, nó có thể được coi là 1 cẩm nang toàn diện cho nhiều đối tượng độc giả, chẳng hạn như sinh viên, nhà nghiên cứu, \& chuyên gia. Cần có 1 số kiến thức về các khái niệm \& thuật ngữ liên quan đến thống kê, Học máy, \& lý thuyết đồ thị. 1 số kiến thức cơ bản đã được cung cấp \& tham chiếu trong 8 chương đầu tiên. Tốt hơn hết là nên có kiến thức về Học máy \& 1 số kinh nghiệm lập trình để dễ dàng tiếp cận hầu hết các chương của cuốn sách này. Đặc biệt, cần có khả năng đọc mã giả \& hiểu các cấu trúc đồ thị.

        Book is well modularized \& each chap can be learned in a standalone manner based on individual interests \& needs. For those readers who want to have a solid understanding of various techniques \& theories of GNNs, can start from Chaps. 4--9. For those who further want to perform in-depth research \& advanced related fields, read those chaps of interest among Chaps. 9--18, which provide comprehensive knowledge in most recent research issues, open problems, \& research frontiers. For those who want to apply GNNs to benefit specific domains, or aim at finding interesting applications to validate specific GNNs techniques, refer to Chaps. 19--27.

        --Sách được phân chia thành các module rõ ràng \& mỗi chương có thể được học độc lập dựa trên sở thích \& nhu cầu cá nhân. Đối với những độc giả muốn có hiểu biết vững chắc về các kỹ thuật \& lý thuyết khác nhau về GNN, có thể bắt đầu từ Chương 4-9. Đối với những độc giả muốn nghiên cứu sâu hơn \& các lĩnh vực liên quan nâng cao, hãy đọc các chương quan tâm trong Chương 9-18, cung cấp kiến thức toàn diện về các vấn đề nghiên cứu gần đây nhất, các vấn đề còn bỏ ngỏ, \& các lĩnh vực nghiên cứu tiên tiến. Đối với những độc giả muốn áp dụng GNN để mang lại lợi ích cho các lĩnh vực cụ thể, hoặc muốn tìm kiếm các ứng dụng thú vị để xác thực các kỹ thuật GNN cụ thể, hãy tham khảo Chương 19-27.
    \end{itemize}
    \item {\sf Terminologies.} This chap describes a list of definitions of terminologies related to GNNs used throughout this book.
    \begin{itemize}
        \item {\sf1. Basic concepts of Graphs.}
        \begin{enumerate}
            \item {\bf Graph.} A graph is composed of a node set \& an edge set, where nodes represent entities \& edges represent relationship between entities. Nodes \& edges form topology structure of graph. Besides graph structure, nodes, edges, \&{\tt/}or whole graph can be associated with rich information represented as node{\tt/}edge{\tt/}graph features (also known as attributes or contents).

            -- 1 đồ thị bao gồm 1 tập nút \& 1 tập cạnh, trong đó các nút biểu diễn các thực thể \& các cạnh biểu diễn mối quan hệ giữa các thực thể. Các nút \& các cạnh tạo nên cấu trúc tôpô của đồ thị. Bên cạnh cấu trúc đồ thị, các nút, cạnh, \&{\tt/}hoặc toàn bộ đồ thị có thể được liên kết với thông tin phong phú được biểu diễn dưới dạng các đặc trưng nút{\tt/}cạnh{\tt/}đồ thị (còn được gọi là thuộc tính hoặc nội dung).
            \item {\bf Subgraph.} A subgraph is a graph whose set of nodes \& set of edges are all subsets of original graph.

            -- Đồ thị con là đồ thị có tập hợp các nút \& tập hợp các cạnh đều là tập con của đồ thị gốc.
            \item {\bf Centrality.} A centrality is a measurement of importance of nodes in graph. Basic assumption of centrality: a node is thought to be important if many other important nodes also connect to it. Common centrality measurements include degree centrality, eigenvector centrality, betweenness centrality, \& closeness centrality.

            -- Độ trung tâm là phép đo tầm quan trọng của các nút trong đồ thị. Giả định cơ bản về độ trung tâm: 1 nút được coi là quan trọng nếu nhiều nút quan trọng khác cũng kết nối với nó. Các phép đo độ trung tâm phổ biến bao gồm độ trung tâm bậc, độ trung tâm vectơ riêng, độ trung tâm giữa, độ trung tâm \& độ gần.
            \item {\bf Neighborhood.} Neighborhood of a node generally refers to other nodes that are close to it. E.g., $k$-order neighborhood of a node, also called $k$-step neighborhood, denotes a set of other nodes in which shortest path distance between these nodes \& central node is no larger than $k$.

            -- Lân cận của 1 nút thường đề cập đến các nút khác gần nó. Ví dụ, lân cận bậc $k$ của 1 nút, còn được gọi là lân cận bậc $k$, biểu thị 1 tập hợp các nút khác có khoảng cách đường dẫn ngắn nhất giữa các nút này \& nút trung tâm không lớn hơn $k$.
            \item {\bf Community structure.} A community refers to a group of nodes that are densely connected internally \& less densely connected externally.

            -- {\bf Cấu trúc cộng đồng.} Cộng đồng đề cập đến 1 nhóm các nút được kết nối dày đặc bên trong \& kết nối ít dày đặc hơn bên ngoài.
            \item {\bf Graph sampling.} Graph sampling is a technique to pick a subset of nodes \&{\tt/}or edges from original graph. Graph sampling can be applied to train ML models on large-scale graphs while preventing severe scalability issues.

            -- Lấy mẫu đồ thị là 1 kỹ thuật để chọn 1 tập hợp con các nút \&{\tt/}hoặc cạnh từ đồ thị gốc. Lấy mẫu đồ thị có thể được áp dụng để huấn luyện các mô hình ML trên đồ thị quy mô lớn, đồng thời ngăn ngừa các vấn đề nghiêm trọng về khả năng mở rộng.
            \item {\bf Heterogeneous Graphs.} Graphs are called heterogeneous if nodes \&{\tt/}or edges of graph are from different types. A typical example of heteronomous graphs is knowledge graphs where edges are composed of different types.

            -- {\bf Đồ thị không đồng nhất.} Đồ thị được gọi là không đồng nhất nếu các nút \&{\tt/}hoặc các cạnh của đồ thị thuộc các loại khác nhau. 1 ví dụ điển hình của đồ thị không đồng nhất là đồ thị tri thức, trong đó các cạnh được tạo thành từ các loại khác nhau.
            \item {\bf Hypergraphs.} Hypergraphs are generalizations of graphs in which an edge can join any number of nodes.

            -- {\bf Siêu đồ thị.} Siêu đồ thị là dạng tổng quát của đồ thị trong đó 1 cạnh có thể nối với bất kỳ số lượng nút nào.
            \item {\bf Random Graph.} Random graph generally aims to model probability distributions over graphs that observed graphs are generated from. Most basic \& well-suited random graph model, known as Erdos--Renyi model, assumes: node set is fixed \& each edge is identically \& independently generated.

            -- {\bf Đồ thị ngẫu nhiên.} Đồ thị ngẫu nhiên thường nhằm mục đích mô hình hóa phân phối xác suất trên các đồ thị mà đồ thị quan sát được tạo ra. Mô hình đồ thị ngẫu nhiên cơ bản nhất, được gọi là mô hình Erdos-Renyi, giả định: tập hợp nút là cố định \& mỗi cạnh được tạo ra độc lập \& giống hệt nhau.
            \item {\bf Dynamic Graph.} Dynamic graph refers to when at least 1 component of graph data changes over time, e.g., adding or deleting nodes, adding or deleting edges, changing edges weights or changing node attributes, etc. If graphs are not dynamic, refer to them as static graphs.

            -- {\bf Đồ thị động.} Đồ thị động đề cập đến trường hợp ít nhất 1 thành phần của dữ liệu đồ thị thay đổi theo thời gian, e.g.: thêm hoặc xóa các nút, thêm hoặc xóa các cạnh, thay đổi trọng số cạnh hoặc thay đổi các thuộc tính của nút, v.v. Nếu đồ thị không động, gọi chúng là đồ thị tĩnh.
        \end{enumerate}
        \item {\sf2. ML on Graphs.}
        \begin{enumerate}
            \item {\bf Spectral Graph Theory.} Spectral graph theory analyzes matrices associated with graph e.g. its adjacency matrix or Laplacian matrix using tools of linear algebra e.g. studying eigenvalues \& eigenvectors of matrix.

            -- {\bf Lý thuyết đồ thị phổ.} Lý thuyết đồ thị phổ phân tích các ma trận liên quan đến đồ thị, e.g. ma trận kề hoặc ma trận Laplacian bằng các công cụ của đại số tuyến tính, e.g. nghiên cứu các giá trị riêng \& vectơ riêng của ma trận.
            \item {\bf Graph Signal Processing.} Graph Signal Processing (GSP) aims to develop tools for processing signals defined on graphs. A graph signal refers to a finite collection of data samples with 1 sample at each node in graph.

            -- {\bf Xử lý Tín hiệu Đồ thị.} Xử lý Tín hiệu Đồ thị (GSP) nhằm mục đích phát triển các công cụ để xử lý các tín hiệu được xác định trên đồ thị. Tín hiệu đồ thị là 1 tập hợp hữu hạn các mẫu dữ liệu với 1 mẫu tại mỗi nút trong đồ thị.
            \item {\bf Node-level Tasks.} Node-level tasks refer to ML tasks associated with individual nodes in graph. Typical examples of node-level tasks include node classification \& node regression.

            -- {\bf Nhiệm vụ cấp nút.} Nhiệm vụ cấp nút đề cập đến các nhiệm vụ học máy (ML) được liên kết với từng nút trong đồ thị. Các ví dụ điển hình về nhiệm vụ cấp nút bao gồm phân loại nút \& hồi quy nút.
            \item {\bf Edge-level Tasks.} Edge-level tasks refer to ML tasks associated with a pair of nodes in graph. A typical example of an edge-level task in link prediction.

            -- {\bf Nhiệm vụ cấp biên.} Nhiệm vụ cấp biên đề cập đến các nhiệm vụ ML liên quan đến 1 cặp nút trong đồ thị. 1 ví dụ điển hình về nhiệm vụ cấp biên trong dự đoán liên kết.
            \item {\bf Graph-level Tasks.} Graph-level tasks refer to ML tasks associated with whole graph. Typical examples of graph-level tasks include graph classification \& graph property prediction.

            -- {\bf Nhiệm vụ cấp độ đồ thị.} Nhiệm vụ cấp độ đồ thị đề cập đến các nhiệm vụ học máy (ML) liên quan đến toàn bộ đồ thị. Các ví dụ điển hình của nhiệm vụ cấp độ đồ thị bao gồm phân loại đồ thị \& dự đoán thuộc tính đồ thị.
            \item {\bf Transductive \& Inductive Learning.} Transductive learning refers to targeted instances e.g. nodes or edges are observed at training time (though labels of targeted instances remain unknown) \& inductive learning aims to learn model which is generalizable to unobserved instances.

            -- {\bf Học chuyển tiếp \& Học quy nạp.} Học chuyển tiếp đề cập đến các trường hợp mục tiêu, e.g. các nút hoặc cạnh được quan sát tại thời điểm đào tạo (mặc dù nhãn của các trường hợp mục tiêu vẫn chưa được biết) \& học quy nạp nhằm mục đích học mô hình có thể khái quát hóa cho các trường hợp chưa được quan sát.
        \end{enumerate}
        \item {\sf3. GNNs.}
        \begin{itemize}
            \item {\bf Network embedding.} Goal of network embedding: represent each node in graph as a low-dimensional vector so that useful information e.g. graph structures \& some properties of graph is preserved in embedding vectors. Network embedding is also referred to as graph embedding \& node representation learning.

            -- {\bf Nhúng mạng.} Mục tiêu của nhúng mạng: biểu diễn mỗi nút trong đồ thị dưới dạng 1 vectơ ít chiều để thông tin hữu ích, e.g. cấu trúc đồ thị \& 1 số thuộc tính của đồ thị, được bảo toàn trong các vectơ nhúng. Nhúng mạng còn được gọi là học biểu diễn nút \& nhúng đồ thị.
            \item {\bf Graph Neural Network.} GNN refers to any neural network working on graph data.

            -- {\bf Mạng nơ-ron đồ thị.} GNN đề cập đến bất kỳ mạng nơ-ron nào hoạt động trên dữ liệu đồ thị.
            \item {\bf Graph Convolutional Network.} Graph convolutional network usually refers to a specific GNN proposed by {\sc Kipf \& Welling}. It is occasionally used as a synonym for GNN, i.e., referring to any neural network working on graph data, in some literature.

            -- {\bf Mạng Tích chập Đồ thị.} Mạng tích chập đồ thị thường đề cập đến 1 GNN cụ thể do {\sc Kipf \& Welling} đề xuất. Đôi khi, nó được dùng như 1 từ đồng nghĩa với GNN, i.e., đề cập đến bất kỳ mạng nơ-ron nào hoạt động trên dữ liệu đồ thị, trong 1 số tài liệu.
            \item {\bf Message-Passing.} Message-passing is a framework of GNNs in which key step: pass messages between different nodes based on graph structures in each neural network layer. Most widely adopted formulation, usually denoted as message-passing neural networks, is to only pass messages between nodes that are directly connected Gilmer et al (2017). Message passing functions are also called {\it graph filters \& graph convolutions} in some literature.

            -- {\bf Truyền thông điệp.} Truyền thông điệp là 1 khuôn khổ của mạng nơ-ron nhân tạo (GNN), trong đó bước quan trọng nhất là truyền thông điệp giữa các nút khác nhau dựa trên cấu trúc đồ thị trong mỗi lớp mạng nơ-ron. Công thức được áp dụng rộng rãi nhất, thường được gọi là mạng nơ-ron truyền thông điệp, là chỉ truyền thông điệp giữa các nút được kết nối trực tiếp (Gilmer et al. (2017)). Các hàm truyền thông điệp còn được gọi là bộ lọc đồ thị \& tích chập đồ thị trong 1 số tài liệu.
            \item {\bf Readout.} Readout refers to functions that summarize information of individual nodes to form more high-level information e.g. forming a subgraph{\tt/}supergraph or obtaining representations of entire graph. Readout is also called pooling \& graph coarsening in some literature.

            -- {\bf Đọc ra.} Đọc ra đề cập đến các hàm tóm tắt thông tin của từng nút riêng lẻ để tạo thành thông tin cấp cao hơn, e.g.: tạo 1 siêu đồ thị hoặc thu thập biểu diễn của toàn bộ đồ thị. Đọc ra còn được gọi là gộp \& làm thô đồ thị trong 1 số tài liệu.
            \item {\bf Graph Adversarial Attack.} Graph adversarial attacks aim to generate worst-case perturbations by manipulating graph structure \&{\tt/}or node features so that performance of some models are downgraded. Graph adversarial attacks can be categorized based on attacker's goals, capabilities, \& accessible knowledge.

            -- {\bf Tấn công đối kháng đồ thị.} Các cuộc tấn công đối kháng đồ thị nhằm mục đích tạo ra nhiễu loạn trường hợp xấu nhất bằng cách thao túng cấu trúc đồ thị \&{\tt/}hoặc các đặc điểm nút để làm giảm hiệu suất của 1 số mô hình. Các cuộc tấn công đối kháng đồ thị có thể được phân loại dựa trên mục tiêu, khả năng \& kiến thức có thể tiếp cận của kẻ tấn công.
            \item {\bf Robustness certificates.} Methods providing formal guarantees prediction of a GNN is not affected even when perturbations are performed based on a certain perturbation model.

            -- {\bf Chứng chỉ độ tin cậy.} Các phương pháp cung cấp bảo đảm chính thức cho khả năng dự đoán GNN không bị ảnh hưởng ngay cả khi nhiễu loạn được thực hiện dựa trên 1 mô hình nhiễu loạn nhất định.
        \end{itemize}
    \end{itemize}
    PART I. INTRODUCTION.
    \item {\sf1. {\sc Liang Zhao, Lingfei Wu, Peng Cui, Jian Pei}. Representation Learning.} In this chap, 1st describe what representation learning is \& why need representation learning. Among various ways of learning representations, this chap focuses on DL methods: those formed by composition of multiple nonlinear transformations, with goal of resulting in more abstract \& ultimately more useful representations. Summarize representation learning techniques in different domains, focusing on unique challenges \& models for different data types including images, natural languages, speech signals \& networks.

    -- Trong chương này, trước tiên, chúng ta sẽ mô tả học biểu diễn là gì \& tại sao cần học biểu diễn. Trong số các phương pháp học biểu diễn khác nhau, chương này tập trung vào các phương pháp DL: những phương pháp được hình thành từ việc kết hợp nhiều phép biến đổi phi tuyến tính, với mục tiêu tạo ra các biểu diễn trừu tượng hơn \& cuối cùng là hữu ích hơn. Tóm tắt các kỹ thuật học biểu diễn trong các lĩnh vực khác nhau, tập trung vào các thách thức \& mô hình độc đáo cho các loại dữ liệu khác nhau bao gồm hình ảnh, ngôn ngữ tự nhiên, tín hiệu giọng nói \& mạng.
    \begin{itemize}
        \item {\sf1.1. Representation Learning: An Introduction.} Effectiveness of ML techniques heavily relies on not only design of algorithms themselves, but also a good representation (feature set) of data. Ineffective data representations that lack some important information or contains incorrect or huge redundant information could lead to poor performance of algorithm in dealing with different tasks. Goal of representation learning: extract sufficient but minimal information from data. Traditionally, this can be achieved via human efforts based on prior knowledge \& domain expertise on data \& tasks, which is also named as feature engineering. In deploying ML \& many other AI algorithms, historically a large portion of human efforts goes into design of preprocessing pipelines \& data transformations. More specifically, feature engineering is a way to take advantage of human ingenuity \& prior knowledge in hope to extract \& organize discriminative information from data for ML tasks. E.g., political scientists may be asked to define a keyword list as features of social-media text classifiers for detecting those texts on societal events. For speech transcription recognition, one may choose to extract features from raw sound waves by operations including Fourier transformations. Although feature engineering is widely adopted over years, its drawbacks are also salient, including:
        \begin{enumerate}
            \item Intensive labors from domain experts are usually needed. This is because feature engineering may require tight \& extensive collaboration between model developers \& domain experts.
            \item Incomplete \& biased feature extraction. Specically, capacity \& discriminative power of extracted features are limited by knowledge of different domain experts. Moreover, in many domains that human beings have limited knowledge, what features to extract itself is an open question to domain experts, e.g. cancer early prediction.
        \end{enumerate}
        In order to avoid these drawbacks, ML algorithms less dependent on feature engineering has been a highly desired goal in ML \& AI domains, so that novel applications could be constructed faster \& hopefully addressed more effectively.

        -- {\sf Học Biểu Diễn: Giới thiệu.} Hiệu quả của các kỹ thuật ML không chỉ phụ thuộc vào thiết kế thuật toán mà còn vào việc biểu diễn dữ liệu (bộ đặc trưng) tốt. Việc biểu diễn dữ liệu kém hiệu quả, thiếu 1 số thông tin quan trọng hoặc chứa thông tin không chính xác hoặc quá nhiều thông tin dư thừa, có thể dẫn đến hiệu suất thuật toán kém khi xử lý các tác vụ khác nhau. Mục tiêu của học biểu diễn: trích xuất đủ nhưng tối thiểu thông tin từ dữ liệu. Theo truyền thống, điều này có thể đạt được thông qua nỗ lực của con người dựa trên kiến thức \& chuyên môn về dữ liệu \& tác vụ, còn được gọi là kỹ thuật đặc trưng. Khi triển khai ML \& nhiều thuật toán AI khác, trước đây, phần lớn nỗ lực của con người được dành cho việc thiết kế các đường ống tiền xử lý \& chuyển đổi dữ liệu. Cụ thể hơn, kỹ thuật đặc trưng là 1 cách tận dụng sự khéo léo của con người \& kiến thức sẵn có với hy vọng trích xuất \& tổ chức thông tin phân biệt từ dữ liệu cho các tác vụ ML. Ví dụ: các nhà khoa học chính trị có thể được yêu cầu định nghĩa 1 danh sách từ khóa là các đặc trưng của bộ phân loại văn bản trên mạng xã hội để phát hiện các văn bản đó về các sự kiện xã hội. Đối với nhận dạng phiên âm giọng nói, người ta có thể chọn trích xuất các đặc trưng từ sóng âm thô bằng các phép toán bao gồm biến đổi Fourier. Mặc dù kỹ thuật thiết kế đặc trưng đã được áp dụng rộng rãi trong nhiều năm, nhưng nó cũng có những nhược điểm nổi bật, bao gồm:
        \begin{enumerate}
            \item Thường cần rất nhiều công sức từ các chuyên gia trong lĩnh vực. Điều này là do kỹ thuật thiết kế đặc trưng có thể đòi hỏi sự hợp tác chặt chẽ \& sâu rộng giữa các nhà phát triển mô hình \& chuyên gia trong lĩnh vực.
            \item Trích xuất đặc trưng không đầy đủ \& thiên vị. Cụ thể, khả năng \& phân biệt của các đặc trưng được trích xuất bị hạn chế bởi kiến thức của các chuyên gia trong lĩnh vực khác nhau. Hơn nữa, trong nhiều lĩnh vực mà con người có kiến thức hạn chế, việc trích xuất đặc trưng nào là 1 câu hỏi mở đối với các chuyên gia trong lĩnh vực, e.g. dự đoán sớm ung thư.
        \end{enumerate}
        Để tránh những nhược điểm này, các thuật toán ML ít phụ thuộc vào kỹ thuật thiết kế đặc trưng đã trở thành 1 mục tiêu rất được mong đợi trong lĩnh vực ML \& AI, để các ứng dụng mới có thể được xây dựng nhanh hơn \& hy vọng được giải quyết hiệu quả hơn.

        Techniques of representation learning witness development from traditional representation learning techniques to more advanced ones. Traditional methods belong to ``shallow'' models \& aim to learn transformations of data that make it easier to extract useful information when building classifiers or other predictors, e.g. Principal Component Analysis (PCA) (Wold et al, 1987), Gaussian Markov random field (GMRF) (Rue \& Held, 2005), \& Locality Preserving Projections (LPP) (He \& Niyogi, 2004). DL-based representation learning is formed by composition of multiple nonlinear transformations, with goal of yielding more abstract \& ultimately more useful representations. In light of introducing more recent advancements \& sticking to major topic of this book, here we majorly focus on DL-based representation learning, which can be categorized into several types:
        \begin{enumerate}
            \item Supervised learning, where a large number of labeled data are needed for training of DL models. Given well-trained networks, output before last fully-connected layers is always utilized as final representation of input data.
            \item Unsupervised learning (including self-supervised learning), which facilitates analysis of input data without corresponding labels \& aims to learn underlying inherent structure or distribution of data. Pre-tasks are utilized to explore supervision information from large amounts of unlabeled data. Based on this constructed supervision information, deep neural networks DNNs are trained to extract meaningful representations for future downstream tasks.
            \item Transfer learning, which involves methods that utilize any knowledge resource (i.e., data, model, labels, etc.) to increase model learning \& generalization for target task. Transfer learning encompasses different scenarios including multi-task learning (MTL), model adaptation, knowledge transfer, co-variance shift, etc.
        \end{enumerate}
        There are also other important representation learning methods e.g. reinforcement learning, few-shot learning, \& disentangled representation learning.

        -- Các kỹ thuật học biểu diễn chứng kiến sự phát triển từ các kỹ thuật học biểu diễn truyền thống đến các kỹ thuật tiên tiến hơn. Các phương pháp truyền thống thuộc về các mô hình ``nông'' \& hướng đến việc học các phép biến đổi dữ liệu giúp trích xuất thông tin hữu ích dễ dàng hơn khi xây dựng các bộ phân loại hoặc các yếu tố dự đoán khác, e.g.: Phân tích Thành phần Chính (PCA) (Wold \& cộng sự, 1987), Trường ngẫu nhiên Gauss Markov (GMRF) (Rue \& Held, 2005), \& Phép chiếu Bảo toàn Vị trí (LPP) (He \& Niyogi, 2004). Học biểu diễn dựa trên DL được hình thành bằng cách kết hợp nhiều phép biến đổi phi tuyến tính, với mục tiêu tạo ra các biểu diễn trừu tượng hơn \& cuối cùng là hữu ích hơn. Nhằm giới thiệu những tiến bộ gần đây \& bám sát chủ đề chính của cuốn sách này, ở đây chúng tôi chủ yếu tập trung vào học biểu diễn dựa trên DL, có thể được phân loại thành 1 số loại:

        \begin{enumerate}
            \item Học có giám sát, trong đó cần 1 lượng lớn dữ liệu được gắn nhãn để huấn luyện các mô hình DL. Với các mạng được đào tạo tốt, đầu ra trước các lớp kết nối đầy đủ cuối cùng luôn được sử dụng làm biểu diễn cuối cùng của dữ liệu đầu vào.
            \item Học không giám sát (bao gồm học tự giám sát), giúp phân tích dữ liệu đầu vào mà không cần nhãn tương ứng \& nhằm mục đích tìm hiểu cấu trúc hoặc phân phối dữ liệu vốn có bên dưới. Các tác vụ tiền nhiệm được sử dụng để khám phá thông tin giám sát từ 1 lượng lớn dữ liệu chưa được gắn nhãn. Dựa trên thông tin giám sát được xây dựng này, các mạng nơ-ron sâu (DNN) được đào tạo để trích xuất các biểu diễn có ý nghĩa cho các tác vụ hạ nguồn trong tương lai.
            \item Học chuyển giao, bao gồm các phương pháp sử dụng bất kỳ nguồn kiến thức nào (i.e., dữ liệu, mô hình, nhãn, v.v.) để tăng cường học mô hình \& khái quát hóa cho tác vụ mục tiêu. Học chuyển giao bao gồm các kịch bản khác nhau, bao gồm học đa tác vụ (MTL), thích ứng mô hình, chuyển giao kiến thức, dịch chuyển hiệp phương sai, v.v.
        \end{enumerate}
        Ngoài ra còn có các phương pháp học biểu diễn quan trọng khác, e.g. học tăng cường, học ít lần, \& học biểu diễn không rối.

        Important to define what is a good representation. As def by Bengio2008, representation learning is about learning (underlying) features of data that make it easier to extract useful information when building classifiers or other predictors. Thus evaluation of a learned representation is closely related to its performance on downstream tasks. E.g., in data generation task based on a generative model, a good representation is often the one that captures posterior distribution of underlying explanatory factors for observed input. While for a prediction task, a good representation is the one that captures minimal but sufficient information of input data to correctly predict target label. Besides evaluation from perspective of downstream tasks, there are also some general properties that good representations may hold, e.g. smoothness, linearity, capturing multiple explanatory \& casual factors, holding shared factors across different tasks \& simple factor dependencies.

        -- Điều quan trọng là phải xác định thế nào là 1 biểu diễn tốt. Theo định nghĩa của Bengio2008, học biểu diễn là về việc học các đặc điểm (nền tảng) của dữ liệu giúp trích xuất thông tin hữu ích dễ dàng hơn khi xây dựng các bộ phân loại hoặc các yếu tố dự đoán khác. Do đó, việc đánh giá 1 biểu diễn đã học có liên quan chặt chẽ đến hiệu suất của nó trong các tác vụ hạ lưu. Ví dụ: trong tác vụ tạo dữ liệu dựa trên mô hình sinh, 1 biểu diễn tốt thường là biểu diễn nắm bắt được phân phối sau của các yếu tố giải thích cơ bản đối với đầu vào được quan sát. Trong khi đối với tác vụ dự đoán, 1 biểu diễn tốt là biểu diễn nắm bắt được thông tin tối thiểu nhưng đủ về dữ liệu đầu vào để dự đoán chính xác nhãn mục tiêu. Bên cạnh việc đánh giá từ góc độ của các tác vụ hạ lưu, cũng có 1 số thuộc tính chung mà các biểu diễn tốt có thể có, e.g.: độ mượt, tính tuyến tính, nắm bắt nhiều yếu tố giải thích \& ngẫu nhiên, giữ các yếu tố được chia sẻ trên các tác vụ khác nhau \& phụ thuộc yếu tố đơn giản.
        \item {\sf1.2. Representation Learning in Different Areas.} In this sect, summarize development of representation learning on 4 different representative areas: (1) image processing; (2) speech recognition; (3) NLP; \& (4) network analysis. For representation learning in each research area, consider some of fundamental questions that have been driving research in this area. Specifically, what makes 1 representation better than another, \& how should we compute its representation? Why is representation learning important in that area? Also, what are appropriate objectives for learning good representations? Also introduce relevant typical methods \& their development from perspective of 3 main categories: supervised representation learning, unsupervised learning \& transfer learning, resp.

        -- {\sf Học Biểu Diễn trong Các Lĩnh Vực Khác Nhau.} Trong phần này, tóm tắt sự phát triển của học biểu diễn trên 4 lĩnh vực đại diện khác nhau: (1) xử lý ảnh; (2) nhận dạng giọng nói; (3) NLP; \& (4) phân tích mạng. Đối với học biểu diễn trong mỗi lĩnh vực nghiên cứu, hãy xem xét 1 số câu hỏi cơ bản đã thúc đẩy nghiên cứu trong lĩnh vực này. Cụ thể, điều gì làm cho 1 biểu diễn tốt hơn biểu diễn khác, \& chúng ta nên tính toán biểu diễn của nó như thế nào? Tại sao học biểu diễn lại quan trọng trong lĩnh vực đó? Ngoài ra, mục tiêu phù hợp để học các biểu diễn tốt là gì? Đồng thời giới thiệu các phương pháp điển hình có liên quan \& sự phát triển của chúng theo góc nhìn của 3 loại chính: học biểu diễn có giám sát, học không giám sát \& học chuyển giao, tương ứng.
        \begin{itemize}
            \item {\sf1.2.1. Representation Learning for Image Processing.} Image representation learning is a fundamental problem in understanding semantics of various visual data, e.g. photographs, medical images, document scans, \& video streams. Normally, goal of image representation learning for image processing: bridge semantic gap between pixel data \& semantics of images. Successful achievements of image representation learning have empowered many real-world problems, including but not limited to image search, facial recognition, medical image analysis, photo manipulation \& target detection.

            -- Học Biểu diễn Hình ảnh là 1 vấn đề cơ bản trong việc hiểu ngữ nghĩa của nhiều loại dữ liệu hình ảnh, e.g.: ảnh chụp, ảnh y tế, ảnh quét tài liệu, \& luồng video. Thông thường, mục tiêu của học biểu diễn hình ảnh trong xử lý hình ảnh là thu hẹp khoảng cách ngữ nghĩa giữa dữ liệu pixel \& ngữ nghĩa của hình ảnh. Những thành tựu thành công của học biểu diễn hình ảnh đã giải quyết được nhiều vấn đề thực tế, bao gồm nhưng không giới hạn ở tìm kiếm hình ảnh, nhận dạng khuôn mặt, phân tích hình ảnh y tế, thao tác ảnh \& phát hiện mục tiêu.

            In recent years, have witnessed a fast advancement of image representation learning from handcrafted feature engineering to that from scratch through deep neural network models. Traditionally, patterns of images are extracted with help of hand-crafted features by human beings based on prior knowledge. E.g., Huang et al (2000) extracted character's structure feature from strokes, then use them to recognize handwritten characters. Rui (2005) adopted morphology method to improve local feature of characters, then use PCA to extract features of characters. However, all of these methods need to extract features from images manually \& thus prediction performances strongly rely on prior knowledge. In field of computer vision, manual feature extraction is very cumbersome \& impractical because of high dimensionality of feature vectors. Thus, representation learning of images which can automatically extract meaningful, hidden \& complex patterns from high-dimension visual data is necessary. DL-based representation learning for images is learned in an end-to-end fashion, which can perform much better than hand-crafted features in target applications, as long as training data is of sufficient quality \& quantity.

            -- Trong những năm gần đây, đã chứng kiến sự tiến bộ nhanh chóng của việc học biểu diễn hình ảnh từ kỹ thuật đặc trưng thủ công sang kỹ thuật từ đầu thông qua các mô hình mạng nơ-ron sâu. Theo truyền thống, các mẫu hình ảnh được trích xuất với sự trợ giúp của các đặc trưng thủ công của con người dựa trên kiến thức trước đó. Ví dụ, Huang \& cộng sự (2000) đã trích xuất đặc trưng cấu trúc ký tự từ các nét, sau đó sử dụng chúng để nhận dạng ký tự viết tay. Rui (2005) đã áp dụng phương pháp hình thái để cải thiện đặc trưng cục bộ của các ký tự, sau đó sử dụng PCA để trích xuất các đặc trưng của các ký tự. Tuy nhiên, tất cả các phương pháp này đều cần trích xuất các đặc trưng từ hình ảnh theo cách thủ công \& do đó hiệu suất dự đoán phụ thuộc rất nhiều vào kiến thức trước đó. Trong lĩnh vực thị giác máy tính, việc trích xuất đặc trưng thủ công rất cồng kềnh \& không thực tế do các vectơ đặc trưng có nhiều chiều. Do đó, việc học biểu diễn hình ảnh có thể tự động trích xuất các mẫu phức tạp, ẩn \& có ý nghĩa từ dữ liệu hình ảnh có nhiều chiều là cần thiết. Việc học biểu diễn dựa trên DL cho hình ảnh được học theo cách đầu cuối, có thể hoạt động tốt hơn nhiều so với các tính năng thủ công trong các ứng dụng mục tiêu, miễn là dữ liệu đào tạo có chất lượng \& số lượng đủ.

            {\it Supervised Representation Learning for image processing.} In domain of image processing, supervised learning algorithms, e.g. Convolutional Neural Network (CNN) \& Deep Belief Network (DBN), are commonly applied in solving various tasks. 1 of earliest deep-supervised-learning-based works was proposed in 2006 (Hinton et al, 2006), which is focused on MNIST digit image classification problem, outperforming state-of-art SVMs. Following this, deep convolutional neural networks (ConvNets) showed amazing performance which is greatly depends on their properties of shift invariance, weights sharing \& local pattern capturing. Different types of network architectures were developed to increase capacity of network models, \& larger \& larger datasets were collected these days. Various networks including AlexNet (Krizhevsky et al, 2012), VGG (Simonyan \& isserman, 2014b), GoogLeNet (Szegedy et al, 2015), ResNet (He et al, 2016a), \& DenseNet (Huang et al, 2017a) \& large scale datasets, e.g. ImageNet \& OpenImage, have been proposed to train very deep convolutional neural networks. With sophisticated architectures \& large-scale datasets, performance of convolutional neural networks CNNs keeps outperforming state-of-arts in various computer vision tasks.

            -- {\it Học Biểu diễn Có Giám sát cho xử lý hình ảnh.} Trong lĩnh vực xử lý hình ảnh, các thuật toán học có giám sát, e.g. Mạng Nơ-ron Tích chập (CNN) \& Mạng Niềm tin Sâu (DBN), thường được áp dụng để giải quyết nhiều tác vụ khác nhau. 1 trong những công trình đầu tiên dựa trên học có giám sát sâu được đề xuất vào năm 2006 (Hinton \& cộng sự, 2006), tập trung vào bài toán phân loại ảnh số MNIST, vượt trội hơn các SVM hiện đại. Tiếp theo đó, các mạng nơ-ron tích chập sâu (ConvNet) đã cho thấy hiệu suất đáng kinh ngạc, phụ thuộc rất nhiều vào các đặc tính bất biến dịch chuyển, chia sẻ trọng số \& khả năng bắt mẫu cục bộ. Nhiều loại kiến trúc mạng khác nhau đã được phát triển để tăng dung lượng của các mô hình mạng, \& các tập dữ liệu lớn hơn \& lớn hơn đã được thu thập ngày nay. Nhiều mạng lưới khác nhau, bao gồm AlexNet (Krizhevsky \& cộng sự, 2012), VGG (Simonyan \& isserman, 2014b), GoogLeNet (Szegedy \& cộng sự, 2015), ResNet (He \& cộng sự, 2016a), DenseNet (Huang \& cộng sự, 2017a) \& các tập dữ liệu quy mô lớn, e.g. ImageNet \& OpenImage, đã được đề xuất để huấn luyện các mạng nơ-ron tích chập rất sâu. Với kiến trúc phức tạp \& tập dữ liệu quy mô lớn, hiệu suất của các mạng nơ-ron tích chập (CNN) vẫn vượt trội hơn các công nghệ tiên tiến trong nhiều tác vụ thị giác máy tính.

            {\it Unsupervised Representation Learning for image processing.} Collection \& annotation of large-scale datasets are time-consuming \& expensive in both image datasets \& video datasets. E.g., ImageNet contains about 1.3 million labeled images covering 1000 classes while each image is labeled by human workers with 1 class label. To alleviate extensive human annotation labors, many unsupervised methods were proposed to learn visual features from large-scale unlabeled images or videos without using any human annotations. A popular solution: propose various pretext tasks for models to solve, while models can be trained by learning objective functions of pretext tasks \& features are learned through this process. Various pretext tasks have been proposed for unsupervised learning, including colorizing gray-scale images (Zhang et al, 2016d) \& image inpainting (Pathak et al, 2016). During unsupervised training phase, a predefined pretext tasks is designed for models to solve, \& pseudo labels for pretext task are automatically generated based on some attributes to data. Then models are trained according to objective functions of pretext tasks. When trained with pretext tasks, shallower blocks of deep neural network model focus on low-level general features e.g. corners, edges, \& textures, while deeper blocks focus on high-level task-specific features e.g. objects, scenes, \& object parts. Therefore, models trained with pretext tasks can learn kernels to capture low-level features \& high-level features that are helpful for other downstream tasks. After unsupervised training is finished, learned visual features in this pre-trained models can be further transferred to downstream tasks (especially when only relatively small data is available) to improve performance \& overcome overfitting.

            -- {\it Học biểu diễn không giám sát cho xử lý hình ảnh.} Thu thập \& chú thích các tập dữ liệu quy mô lớn tốn thời gian \& tốn kém trong cả tập dữ liệu hình ảnh \& tập dữ liệu video. Ví dụ: ImageNet chứa khoảng 1,3 triệu hình ảnh được gắn nhãn bao gồm 1000 lớp trong khi mỗi hình ảnh được dán nhãn bởi công nhân con người với 1 nhãn lớp. Để giảm bớt công việc chú thích của con người, nhiều phương pháp không giám sát đã được đề xuất để học các đặc điểm trực quan từ hình ảnh hoặc video không có nhãn quy mô lớn mà không cần sử dụng bất kỳ chú thích nào của con người. 1 giải pháp phổ biến: đề xuất nhiều tác vụ tiền đề khác nhau để các mô hình giải quyết, trong khi các mô hình có thể được đào tạo bằng cách học các hàm mục tiêu của các tác vụ tiền đề \& các đặc điểm được học thông qua quá trình này. Nhiều tác vụ tiền đề khác nhau đã được đề xuất cho học không giám sát, bao gồm tô màu cho hình ảnh thang độ xám (Zhang \& cộng sự, 2016d) \& tô màu hình ảnh (Pathak \& cộng sự, 2016). Trong giai đoạn huấn luyện không giám sát, 1 nhiệm vụ tiền đề được xác định trước được thiết kế để các mô hình giải quyết, \& nhãn giả cho nhiệm vụ tiền đề được tự động tạo ra dựa trên 1 số thuộc tính của dữ liệu. Sau đó, các mô hình được huấn luyện theo các hàm mục tiêu của nhiệm vụ tiền đề. Khi được huấn luyện với các nhiệm vụ tiền đề, các khối nông hơn của mô hình mạng nơ-ron sâu tập trung vào các đặc điểm chung cấp thấp, e.g. các góc, cạnh, \& kết cấu, trong khi các khối sâu hơn tập trung vào các đặc điểm cụ thể của nhiệm vụ cấp cao, e.g. các đối tượng, cảnh, \& các bộ phận của đối tượng. Do đó, các mô hình được huấn luyện với các nhiệm vụ tiền đề có thể học các hạt nhân để nắm bắt các đặc điểm cấp thấp \& các đặc điểm cấp cao hữu ích cho các nhiệm vụ hạ lưu khác. Sau khi quá trình huấn luyện không giám sát kết thúc, các đặc điểm trực quan đã học trong các mô hình được huấn luyện trước này có thể được chuyển tiếp sang các nhiệm vụ hạ lưu (đặc biệt là khi chỉ có dữ liệu tương đối nhỏ) để cải thiện hiệu suất \& khắc phục tình trạng quá khớp.

            {\it Transfer Learning for image processing.} In real-world applications, due to high cost of manual labeling, sufficient training data that belongs to same feature space or distribution as testing data may not always be accessible. Transfer learning mimics human vision system by making use of sufficient amounts of prior knowledge in other related domains (i.e., source domains) when executing new tasks in given domain (i.e., target domain). In transfer learning, both training set \& test set can contribute to target \& source domains. In most cases, there is only 1 target domain for a transfer learning task, while either single or multiple source domains can exist. Techniques of transfer learning in images processing can be categorized into feature representation knowledge transfer \& classifier-based knowledge transfer. Specifically, feature representation transfer methods map target domain to source domains by exploiting a set of extracted features, where data divergence between target domain \& source domains can be significantly reduced so that performance of task in target domain is improved. E.g., classifier-based knowledge-transfer methods usually share common trait that learned source domain models are utilized as prior knowledge, which are used to learn target model together with training samples. Instead of minimizing cross-domain dissimilarity by updating instances' representations, classifier-based knowledge-transfer methods aim to learn a new model that minimizes generalization error in target domain via provided training set from both domains \& learned model.

            -- {\it Học chuyển giao cho xử lý hình ảnh.} Trong các ứng dụng thực tế, do chi phí gắn nhãn thủ công cao, dữ liệu đào tạo đủ thuộc cùng không gian đặc trưng hoặc phân phối với dữ liệu kiểm tra có thể không phải lúc nào cũng có thể truy cập được. Học chuyển giao mô phỏng hệ thống thị giác của con người bằng cách sử dụng đủ lượng kiến thức trước đó trong các miền liên quan khác (i.e., miền nguồn) khi thực hiện các tác vụ mới trong miền nhất định (i.e., miền đích). Trong học chuyển giao, cả tập huấn luyện \& tập kiểm tra đều có thể đóng góp vào miền đích \& miền nguồn. Trong hầu hết các trường hợp, chỉ có 1 miền đích cho 1 tác vụ học chuyển giao, trong khi có thể tồn tại 1 hoặc nhiều miền nguồn. Các kỹ thuật học chuyển giao trong xử lý hình ảnh có thể được phân loại thành chuyển giao kiến thức biểu diễn đặc trưng \& chuyển giao kiến thức dựa trên bộ phân loại. Cụ thể, các phương pháp chuyển giao biểu diễn đặc trưng ánh xạ miền đích sang miền nguồn bằng cách khai thác 1 tập hợp các đặc trưng được trích xuất, trong đó độ phân kỳ dữ liệu giữa miền đích \& miền nguồn có thể được giảm đáng kể để cải thiện hiệu suất của tác vụ trong miền đích. Ví dụ, các phương pháp chuyển giao tri thức dựa trên bộ phân loại thường có đặc điểm chung là các mô hình miền nguồn đã học được được sử dụng làm tri thức tiên nghiệm, được dùng để học mô hình đích cùng với các mẫu huấn luyện. Thay vì giảm thiểu sự khác biệt giữa các miền bằng cách cập nhật biểu diễn của các thể hiện, các phương pháp chuyển giao tri thức dựa trên bộ phân loại hướng đến việc học 1 mô hình mới giúp giảm thiểu lỗi khái quát hóa trong miền đích thông qua tập huấn luyện được cung cấp từ cả hai miền \& mô hình đã học.

            {\it Other Representation Learning for Image Processing.} Other types of representation learning are also commonly observed for dealing with image processing, e.g. reinforcement learning, \& semi-supervised learning. E.g., reinforcement learning are commonly explored in task of image captioning Liu et al (2018a); Ren et al (2017) \& image editing Kosugi \& Yamasaki (2020), where learning process is formalized as a sequence of actions based on a policy network.

            -- {\it Học Biểu diễn Khác cho Xử lý Hình ảnh.} Các loại học biểu diễn khác cũng thường được quan sát thấy khi xử lý hình ảnh, e.g. học tăng cường, \& học bán giám sát. Ví dụ, học tăng cường thường được khám phá trong nhiệm vụ chú thích hình ảnh (Liu \& cộng sự (2018a); Ren \& cộng sự (2017) \& chỉnh sửa hình ảnh) (Kosugi \& Yamasaki (2020), trong đó quá trình học được chính thức hóa thành 1 chuỗi hành động dựa trên mạng chính sách).
            \item {\sf1.2.2. Representation Learning for Speech Recognition.} Nowadays, speech interfaces or systems have become widely developed \& integrated into various real-life applications \& devices. Services like Siri [Siri is an AI assistant software built into Apple's iOS system.], Cortana [Microsoft Cortana is an intelligent personal assistant developed by Microsoft, known as ``world's 1st cross-platform intelligent personal assistant''.], \& Google Voice Search [Google Voice Search is a product of Google that allows you to use Google to search by speaking to a mobile phone or computer, i.e., to use legendary content on device to be identified by server, \& then search for information based on results of recognition.] have become a part of our daily lief \& are used by millions of users. Exploration in speech recognition \& analysis has always been motivated by a desire to enable machines to participate in verbal human-machine interactions. Research goals of enabling machines to understand human speech, identify speakers, \& detect human emotion have attracted researchers' attention for $> 60$ years across several distinct research areas, including but not limited to Automatic Speech Recognition (ASR), Speaker Recognition (SR), \& Speaker Emotion Recognition (SER).

            -- {\sf Học biểu diễn cho Nhận dạng giọng nói.} Ngày nay, các giao diện hoặc hệ thống giọng nói đã được phát triển rộng rãi \& tích hợp vào nhiều ứng dụng \& thiết bị thực tế. Các dịch vụ như Siri [Siri là phần mềm trợ lý AI được tích hợp trong hệ thống iOS của Apple.], Cortana [Microsoft Cortana là trợ lý cá nhân thông minh do Microsoft phát triển, được gọi là ``trợ lý cá nhân thông minh đa nền tảng đầu tiên trên thế giới''.], \& Tìm kiếm bằng giọng nói của Google [Tìm kiếm bằng giọng nói của Google là sản phẩm của Google cho phép bạn sử dụng Google để tìm kiếm bằng cách nói chuyện với điện thoại di động hoặc máy tính, i.e., sử dụng nội dung huyền thoại trên thiết bị để được máy chủ nhận dạng, \& sau đó tìm kiếm thông tin dựa trên kết quả nhận dạng.] đã trở thành 1 phần trong cuộc sống hàng ngày của chúng ta \& được hàng triệu người dùng sử dụng. Khám phá trong nhận dạng giọng nói \& phân tích luôn được thúc đẩy bởi mong muốn cho phép máy móc tham gia vào các tương tác bằng lời nói giữa người \& máy. Mục tiêu nghiên cứu nhằm giúp máy móc có thể hiểu được giọng nói của con người, nhận dạng người nói \& phát hiện cảm xúc của con người đã thu hút sự chú ý của các nhà nghiên cứu trong hơn 60 năm qua trên nhiều lĩnh vực nghiên cứu khác nhau, bao gồm nhưng không giới hạn ở Nhận dạng giọng nói tự động (ASR), Nhận dạng người nói (SR) \& Nhận dạng cảm xúc của người nói (SER).

            Analyzing \& processing speech has been a key application of ML algorithms. Research on speech recognition has traditionally considered task of designing efficient models to accomplish prediction \& classification decisions. There are 2 main drawbacks of this approach: 1st, feature engineering is cumbersome \& requires human knowledge is introduced above; \& 2nd, designed features might not be best for specific speech recognition tasks at hand. This has motivated adoption of recent trends in speech community towards utilization of representation learning techniques, which can learn an intermediate representation of input signal automatically that better fits into task at hand \& hence lead to improved performance. Among all these successes, DL-based speech representations play an important role. 1 of major reasons for utilization of representation learning techniques in speech technology: speech data is fundamentally different from 2D image data. Images can be analyzed as a whole or in patches, but speech has to be formatted sequentially to capture temporal dependency \& patterns.

            -- Phân tích \& xử lý giọng nói là 1 ứng dụng quan trọng của các thuật toán ML. Nghiên cứu về nhận dạng giọng nói theo truyền thống coi nhiệm vụ thiết kế các mô hình hiệu quả để thực hiện các quyết định dự đoán \& phân loại. Có 2 nhược điểm chính của phương pháp này: Thứ nhất, kỹ thuật đặc trưng cồng kềnh \& đòi hỏi kiến thức của con người đã được giới thiệu ở trên; Thứ hai, các đặc điểm được thiết kế có thể không phù hợp nhất với các tác vụ nhận dạng giọng nói cụ thể hiện có. Điều này đã thúc đẩy việc áp dụng các xu hướng gần đây trong cộng đồng giọng nói hướng tới việc sử dụng các kỹ thuật học biểu diễn, có thể tự động học 1 biểu diễn trung gian của tín hiệu đầu vào phù hợp hơn với tác vụ hiện có \& do đó dẫn đến hiệu suất được cải thiện. Trong số tất cả những thành công này, biểu diễn giọng nói dựa trên DL đóng 1 vai trò quan trọng. 1 trong những lý do chính để sử dụng các kỹ thuật học biểu diễn trong công nghệ giọng nói: dữ liệu giọng nói về cơ bản khác với dữ liệu hình ảnh 2D. Hình ảnh có thể được phân tích toàn bộ hoặc theo từng mảng, nhưng giọng nói phải được định dạng tuần tự để nắm bắt các mẫu \& phụ thuộc theo thời gian.

            {\it Supervised representation learning for speech recognition.} In domain of speech recognition \& analyzing, supervised representation learning methods are widely employed, where feature representations are learned on datasets by leveraging label information. E.g., restricted Boltmann machines (RBMs) (Jaitly \& Hinton, 2011; Dahl et al, 2010) \& deep belief networks (DBNs) (Cairong et al, 2016; Ali et al, 2018) are commonly utilized in learning features from speech for different tasks, including ASR, speaker recognition, \& SER. E.g., in 2012, Microsoft has released a new version of their MAVIS (Microsoft Audio Video Indexing Service) speech system based on context-dependent deep neural networks Seide et al, 2011). These authors managed to reduce word error rate on 4 major benchmarks by about 30\% (e.g., from 27.4\% to 18.5\% on RT03S) compared to traditional models based on Gaussian mixtures. Convolutional neural networks are another popular supervised models that are widely utilized for feature learning from speech signals in tasks e.g. speech \& speaker recognition (Palaz et al, 2015a,b) \& SER Latif et al (2019); Tzirakis et al (2018). Moreover, found: LSTMs (or GRUs) can help CNNs in learning more useful features from speech by learning both local \& long-term dependency (Dahl et al, 2010).

            -- {\it Học biểu diễn có giám sát cho nhận dạng giọng nói.} Trong lĩnh vực nhận dạng giọng nói \& phân tích, các phương pháp học biểu diễn có giám sát được sử dụng rộng rãi, trong đó các biểu diễn đặc trưng được học trên các tập dữ liệu bằng cách tận dụng thông tin nhãn. Ví dụ: máy Boltmann hạn chế (RBM) (Jaitly \& Hinton, 2011; Dahl \& cộng sự, 2010) \& mạng niềm tin sâu (DBN) (Cairong \& cộng sự, 2016; Ali \& cộng sự, 2018) thường được sử dụng để học các đặc trưng từ giọng nói cho các tác vụ khác nhau, bao gồm ASR, nhận dạng người nói, \& SER. Ví dụ: vào năm 2012, Microsoft đã phát hành phiên bản mới của hệ thống giọng nói MAVIS (Dịch vụ Lập chỉ mục Âm thanh \& Video của Microsoft) dựa trên mạng nơ-ron sâu phụ thuộc ngữ cảnh (Seide \& cộng sự, 2011). Các tác giả này đã giảm tỷ lệ lỗi từ trên 4 phép đo chuẩn chính khoảng 30\% (e.g.: từ 27,4\% xuống 18,5\% trên RT03S) so với các mô hình truyền thống dựa trên hỗn hợp Gauss. Mạng nơ-ron tích chập là 1 mô hình giám sát phổ biến khác được sử dụng rộng rãi để học đặc trưng từ tín hiệu giọng nói trong các tác vụ như nhận dạng giọng nói \& người nói (Palaz \& cộng sự, 2015a,b) \& SER Latif \& cộng sự (2019); Tzirakis \& cộng sự (2018). Hơn nữa, họ nhận thấy: LSTM (hay GRU) có thể giúp CNN học các đặc trưng hữu ích hơn từ giọng nói bằng cách học cả phụ thuộc cục bộ \& dài hạn (Dahl \& cộng sự, 2010).

            {\it Unsupervised Representation Learning for speech recognition.} Unsupervised representation learning from large unlabeled datasets is an active area of speech recognition. In context of speech analysis, able to exploit practically available unlimited amount of unlabeled corpora to learn good intermediate feature representations, which can then be used to improve performance of a variety of downstream supervised learning speech recognition tasks or speech signal synthetic tasks. In tasks of ASR \& SR, most of works are based on Variational Auto-encoder (VAEs), where a generative model \& an inference model are jointly learned, which allows them to capture latent representations from observed speech data (Chorowski et al, 2019; Hsu et al, 2019, 2017). E.g., Hsu et al (2017) proposed a hierarchical VAE to capture interpretable \& disentangled representations from speech without any supervision. Other auto-encoding architectures like Denoised Autoencoder (DAEs) are also found very promising in finding speech representations in an unsupervised way, especially for noisy speech recognition (Feng et al, 2014; Zhao et al, 2015). Beyond aforementioned, recently, adversarial learning (AL) is emerging as a powerful tool in learning unsupervised representation for speech, e.g. generative adversarial nets (GANs). It involves at least a generator \& a discriminator, where former tries to generates as realistic as possible data to obfuscate the latter which also tries its best to deobfuscate. Hence both of generator \& discriminator can be trained \& improved iteratively in an adversarial way, which result in more discriminative \& robust features. Among these, GANs (Chang \& Scherer, 2017; Donahue et al, 2018), adversarial autoencoders (AAEs) Sahu et al (2017) are becoming mostly popular in modeling speech not only in ASR but also SR \& SER.

            -- Học biểu diễn không giám sát cho nhận dạng giọng nói. Học biểu diễn không giám sát từ các tập dữ liệu lớn không có nhãn là 1 lĩnh vực năng động của nhận dạng giọng nói. Trong bối cảnh phân tích giọng nói, có thể khai thác số lượng không giới hạn các tập dữ liệu không có nhãn có sẵn trên thực tế để học các biểu diễn đặc trưng trung gian tốt, sau đó có thể được sử dụng để cải thiện hiệu suất của nhiều tác vụ nhận dạng giọng nói học có giám sát hạ lưu hoặc các tác vụ tổng hợp tín hiệu giọng nói. Trong các tác vụ ASR \& SR, hầu hết các công trình đều dựa trên Bộ mã hóa tự động biến thiên (VAE), trong đó 1 mô hình sinh \& 1 mô hình suy luận được học chung, cho phép chúng nắm bắt các biểu diễn tiềm ẩn từ dữ liệu giọng nói quan sát (Chorowski \& cộng sự, 2019; Hsu \& cộng sự, 2019, 2017). Ví dụ, Hsu \& cộng sự (2017) đã đề xuất 1 VAE phân cấp để nắm bắt các biểu diễn có thể diễn giải \& không bị rối từ giọng nói mà không cần bất kỳ sự giám sát nào. Các kiến trúc mã hóa tự động khác như Denoised Autoencoder (DAE) cũng được đánh giá rất hứa hẹn trong việc tìm kiếm biểu diễn giọng nói theo cách không giám sát, đặc biệt là đối với nhận dạng giọng nói có nhiễu (Feng \& cộng sự, 2014; Zhao \& cộng sự, 2015). Ngoài những kiến trúc đã đề cập ở trên, gần đây, học đối kháng (AL) đang nổi lên như 1 công cụ mạnh mẽ trong việc học biểu diễn giọng nói không giám sát, e.g. mạng đối kháng sinh sinh (GAN). Nó bao gồm ít nhất 1 bộ tạo \& 1 bộ phân biệt, trong đó bộ tạo \& bộ phân biệt cố gắng tạo ra dữ liệu thực tế nhất có thể để làm tối nghĩa bộ tạo \ cũng cố gắng hết sức để giải tối nghĩa. Do đó, cả bộ tạo \& bộ phân biệt đều có thể được huấn luyện \& cải tiến lặp đi lặp lại theo cách đối kháng, dẫn đến các tính năng phân biệt \& mạnh mẽ hơn. Trong số đó, GAN (Chang \& Scherer, 2017; Donahue \& cộng sự, 2018), bộ mã hóa tự động đối nghịch (AAE) Sahu \& cộng sự (2017) đang trở nên phổ biến trong việc mô hình hóa giọng nói không chỉ trong ASR mà còn trong SR \& SER.

            {\it Transfer Learning for speech recognition.} Transfer learning (TL) encompasses different approaches, including MTL, model adaptation, knowledge transfer, covariance shift, etc. In domain of speech recognition, representation learning gained much interest in these approaches of TL including but not limited to domain adaptation, multitask learning, \& self-taught learning. In terms of Domain Adaptation, speech is a typical example of heterogeneous data \& thus, a mismatch always exists between probability distributions of source \& target domain area. To build more robust systems for speech-related applications in real-life, domain adaptation techniques are usually applied in training pipeline of deep neural networks to learn representations which are able to explicitly minimize difference between distribution of data in source \& target domains (Sun et al, 2017; Swietojanski et al, 2016). In terms of MTL, representations learned can successfully increases performance of speech recognition without requiring contextual speech data, since speech contains multidimensional information (message, speaker, gender, or emotion) that can be used as auxiliary tasks. E.g., in task of ASR, by using MTL with different auxiliary tasks including gender, speaker adaptation, speech enhancement, shown: learned shared representations for different tasks can act as complementary information about acoustics environment \& give a lower word error rate (WER) Parthasarathy \& Busso, 2017; Xia \& Liu, 2015).

            -- Học chuyển giao cho nhận dạng giọng nói. Học chuyển giao (TL) bao gồm các phương pháp tiếp cận khác nhau, bao gồm MTL, thích ứng mô hình, chuyển giao kiến thức, dịch chuyển hiệp phương sai, v.v. Trong lĩnh vực nhận dạng giọng nói, học biểu diễn đã thu hút được nhiều sự quan tâm trong các phương pháp tiếp cận TL này bao gồm nhưng không giới hạn ở thích ứng miền, học đa nhiệm, \& học tự học. Về mặt Thích ứng miền, giọng nói là 1 ví dụ điển hình về dữ liệu không đồng nhất \& do đó, luôn tồn tại sự không khớp giữa phân phối xác suất của miền nguồn \& miền đích. Để xây dựng các hệ thống mạnh mẽ hơn cho các ứng dụng liên quan đến giọng nói trong đời thực, các kỹ thuật thích ứng miền thường được áp dụng trong đường ống đào tạo của mạng nơ-ron sâu để học các biểu diễn có khả năng giảm thiểu rõ ràng sự khác biệt giữa phân phối dữ liệu trong miền nguồn \& miền đích (Sun \& cộng sự, 2017; Swietojanski \& cộng sự, 2016). Về mặt MTL, các biểu diễn đã học có thể tăng hiệu suất nhận dạng giọng nói 1 cách thành công mà không yêu cầu dữ liệu giọng nói theo ngữ cảnh, vì giọng nói chứa thông tin đa chiều (thông điệp, người nói, giới tính hoặc cảm xúc) có thể được sử dụng làm các tác vụ phụ trợ. Ví dụ, trong nhiệm vụ ASR, bằng cách sử dụng MTL với các nhiệm vụ phụ trợ khác nhau bao gồm giới tính, sự thích ứng của người nói, sự cải thiện giọng nói, đã chỉ ra: các biểu diễn chung đã học được cho các nhiệm vụ khác nhau có thể đóng vai trò là thông tin bổ sung về môi trường âm học \& mang lại tỷ lệ lỗi từ thấp hơn (WER) (Parthasarathy \& Busso, 2017; Xia \& Liu, 2015).

            {\it Other Representation Learning for speech recognition.} Other than abovementioned 3 categories of representation learning fro speech signals, there are also some other representation learning techniques commonly explored, e.g. semi-supervised learning \& reinforcement learning. E.g., in speech recognition for ASR, semi-supervised learning is mainly used to circumvent lack of sufficient training data. This can be achieved either by creating features fronts ends (Thomas et al, 2013), or by using multilingual acoustic representations (Cui et al, 2015), or by extracting an intermediate representation from large unpaired datasets (Karita et al, 2018). RL is also gaining interest in area of speech recognition, \& there have been multiple approaches to model different speech problems, including dialog modeling \& optimization (Levin et al, 2000), speech recognition (Shen et al, 2019), \& emotion recognition (Sangeetha \& Jayasankar, 2019).

            -- Học biểu diễn khác cho nhận dạng giọng nói. Ngoài 3 loại học biểu diễn cho tín hiệu giọng nói đã đề cập ở trên, còn có 1 số kỹ thuật học biểu diễn khác thường được khám phá, e.g. học bán giám sát \& học tăng cường. Ví dụ, trong nhận dạng giọng nói cho ASR, học bán giám sát chủ yếu được sử dụng để khắc phục tình trạng thiếu dữ liệu huấn luyện. Điều này có thể đạt được bằng cách tạo các đặc trưng ở đầu cuối (Thomas \& cộng sự, 2013), hoặc bằng cách sử dụng biểu diễn âm thanh đa ngôn ngữ (Cui \& cộng sự, 2015), hoặc bằng cách trích xuất biểu diễn trung gian từ các tập dữ liệu lớn không ghép đôi (Karita \& cộng sự, 2018). RL cũng đang thu hút sự quan tâm trong lĩnh vực nhận dạng giọng nói, \& đã có nhiều phương pháp để mô hình hóa các vấn đề giọng nói khác nhau, bao gồm mô hình hóa hội thoại \& tối ưu hóa (Levin \& cộng sự, 2000), nhận dạng giọng nói (Shen \& cộng sự, 2019), \& nhận dạng cảm xúc (Sangeetha \& Jayasankar, 2019).
            \item {\sf1.2.3. Representation Learning for Natural Language Processing.} p. 10+++
            \item {\sf1.2.4. Representation Learning for Networks.} p. 13+++
            \item {\sf Summary.} Representation learning is a very active \& important field currently, which heavily influences effectiveness of ML techniques. Representation learning is about learning representations of data that makes it easier to extract useful \& discriminative information when building classifiers or other predictors. Among various ways of learning representations, DL algorithms have increasingly been employed in many areas nowadays where good representation can be learned in an efficient \& automatic way based on large amount of complex \& high dimensional data. Evaluation of a representation is closely related to its performance on downstream tasks. Generally, there are also some general properties that good representations may hold, e.g. smoothness, linearity, disentanglement, as well as capturing multiple explanatory \& casual factors.

            -- Học biểu diễn là 1 lĩnh vực rất năng động \& quan trọng hiện nay, ảnh hưởng lớn đến hiệu quả của các kỹ thuật ML. Học biểu diễn liên quan đến việc học các biểu diễn dữ liệu giúp trích xuất thông tin \& phân biệt hữu ích dễ dàng hơn khi xây dựng bộ phân loại hoặc các yếu tố dự đoán khác. Trong số nhiều cách học biểu diễn, các thuật toán DL ngày càng được sử dụng rộng rãi trong nhiều lĩnh vực, nơi biểu diễn tốt có thể được học 1 cách hiệu quả \& tự động dựa trên lượng lớn dữ liệu phức tạp \& đa chiều. Đánh giá 1 biểu diễn có liên quan chặt chẽ đến hiệu suất của nó trong các tác vụ hạ nguồn. Nhìn chung, cũng có 1 số đặc tính chung mà các biểu diễn tốt có thể sở hữu, e.g. độ mượt, tính tuyến tính, độ tách rời, cũng như nắm bắt được nhiều yếu tố giải thích \& ngẫu nhiên.

            Summarized representation learning techniques in different domains, focusing on unique challenges \& models for different areas including processing of images, natural language, \& speech signals. For each area, there emerges many DL-based representation techniques from different categories, including supervised learning, unsupervised learning, transfer learning, disentangled representation learning, reinforcement learning, etc. Have also briefly mentioned about representation learning on networks \& its relations to that on images, texts, \& speech, in order for elaboration of it in following chaps.

            -- Tóm tắt các kỹ thuật học biểu diễn trong các lĩnh vực khác nhau, tập trung vào những thách thức \& mô hình độc đáo cho các lĩnh vực khác nhau, bao gồm xử lý hình ảnh, ngôn ngữ tự nhiên, \& tín hiệu giọng nói. Đối với mỗi lĩnh vực, có nhiều kỹ thuật biểu diễn dựa trên DL từ các danh mục khác nhau, bao gồm học có giám sát, học không giám sát, học chuyển giao, học biểu diễn không rối rắm, học tăng cường, v.v. Bài viết cũng đề cập ngắn gọn về học biểu diễn trên mạng \& mối quan hệ của nó với hình ảnh, văn bản, \& giọng nói, để có thể trình bày chi tiết hơn trong các chương sau.
        \end{itemize}
    \end{itemize}
    \item {\sf2. {\sc Peng Cui, Lingfei Wu, Jian Pei, Liang Zhao, Xiao Wang}. Graph Representation Learning.} Graph representation learning aims at assigning nodes in a graph to low-dimensional representations \& effectively preserving graph structures. Recently, a significant amount of progress has been made toward this emerging graph analysis paradigm. In this chap, 1st summarize motivation of graph representation learning. Afterwards \& primarily, provide a comprehensive overview of a large number of graph representation learning methods in a systematic manner, covering traditional graph representation learning, modern graph representation learning, \& GNNs.

    -- Học biểu diễn đồ thị hướng đến việc gán các nút trong đồ thị cho các biểu diễn số chiều thấp \& bảo toàn hiệu quả cấu trúc đồ thị. Gần đây, mô hình phân tích đồ thị mới nổi này đã đạt được nhiều tiến bộ đáng kể. Trong chương này, trước tiên, chúng tôi tóm tắt động lực của việc học biểu diễn đồ thị. Sau đó, chúng tôi chủ yếu cung cấp tổng quan toàn diện về 1 số lượng lớn các phương pháp học biểu diễn đồ thị 1 cách có hệ thống, bao gồm học biểu diễn đồ thị truyền thống, học biểu diễn đồ thị hiện đại, \& Mạng nơ-ron nhân tạo (GNN).
    \begin{itemize}
        \item {\sf2.1. Graph Representation Learning: An Introduction.} Many complex systems take form of graphs, e.g. social networks, biological networks, \& information networks. Well recognized: graph data is often sophisticated \& thus is challenging to deal with. To process graph data effectively, 1st critical challenge: find effective graph data representation, i..e, how to represent graphs concisely so that advanced analytic tasks, e.g. pattern discovery, analysis, \& prediction, can be conducted efficiently in both time \& space. Traditionally, usually represent a graph as $G = (V,E)$, where $V$ is a node set \& $E$ is an edge set. For large graphs, e.g. those with billions of nodes, traditional graph representation poses several challenges to graph processing \& analysis.

        -- {\sf Học Biểu diễn Đồ thị: Giới thiệu.} Nhiều hệ thống phức tạp có dạng đồ thị, e.g.: mạng xã hội, mạng sinh học, \& mạng thông tin. Nhận thức rõ ràng: dữ liệu đồ thị thường phức tạp \& do đó rất khó xử lý. Để xử lý dữ liệu đồ thị hiệu quả, thách thức quan trọng đầu tiên là: tìm cách biểu diễn dữ liệu đồ thị hiệu quả, i.e., cách biểu diễn đồ thị 1 cách ngắn gọn để các tác vụ phân tích nâng cao, e.g.: khám phá mẫu, phân tích, \& dự đoán, có thể được thực hiện hiệu quả trong cả không gian \& thời gian. Theo truyền thống, biểu diễn đồ thị thường là $G = (V,E)$, trong đó $V$ là tập nút \& $E$ là tập cạnh. Đối với các đồ thị lớn, e.g.: đồ thị có hàng tỷ nút, biểu diễn đồ thị truyền thống đặt ra 1 số thách thức cho việc xử lý \& phân tích đồ thị.
        \begin{enumerate}
            \item {\bf High computational complexity.} These relationships encoded by edge set $E$ take most of graph processing or analysis algorithms either iterative or combinatorial computation steps. E.g., a popular way: use shortest or average path length between 2 nodes to represent their distance. To compute such a distance using traditional graph representation, have to enumerate many possible paths between 2 nodes, which is in nature a combinatorial problem. Such methods result in high computational complexity that prevents them from being applicable to large-scale real-world graphs.

            -- {\bf Độ phức tạp tính toán cao.} Các mối quan hệ được mã hóa bởi tập cạnh $E$ này đòi hỏi hầu hết các thuật toán xử lý hoặc phân tích đồ thị phải thực hiện các bước tính toán lặp hoặc kết hợp. Ví dụ, 1 cách phổ biến: sử dụng độ dài đường đi ngắn nhất hoặc trung bình giữa 2 nút để biểu diễn khoảng cách của chúng. Để tính khoảng cách như vậy bằng cách sử dụng biểu diễn đồ thị truyền thống, cần phải liệt kê nhiều đường đi khả thi giữa 2 nút, về bản chất là 1 bài toán kết hợp. Các phương pháp như vậy dẫn đến độ phức tạp tính toán cao, khiến chúng không thể áp dụng cho các đồ thị thực tế quy mô lớn.
            \item {\bf Low parallelizability.} Parallel \& distributed computing is de facto to processes \& analyze large-scale data. Graph data represented in traditional way, however, casts severe difficulties to design \& implement of parallel \& distributed algorithms. Bottleneck: nodes in a graph are coupled to each other explicitly reflected by $E$. Thus, distributing different nodes in different shards or servers often causes demandingly high communication cost among servers, \& holds back speed-up ratio.

            -- {\bf Khả năng song song hóa thấp.} Tính toán song song \& phân tán thực tế là để xử lý \& phân tích dữ liệu quy mô lớn. Tuy nhiên, dữ liệu đồ thị được biểu diễn theo cách truyền thống gây ra những khó khăn nghiêm trọng trong việc thiết kế \& triển khai các thuật toán song song \& phân tán. Nút thắt cổ chai: các nút trong đồ thị được kết nối với nhau 1 cách rõ ràng, được phản ánh bởi $E$. Do đó, việc phân phối các nút khác nhau trong các phân đoạn hoặc máy chủ khác nhau thường gây ra chi phí truyền thông cao giữa các máy chủ, \& làm giảm tốc độ xử lý.
            \item {\bf Inapplicability of ML methods.} Recently, ML methods, especially DL, are very powerful in many areas. For graph data represented in traditional way, however, most of off-shelf ML methods may not be applicable. Those methods usually assume: data samples can be represented by independent vectors in a vector space, while samples in graph data (i.e., nodes) are dependent to each other to some degree determined by $E$. Although can simply represent a node by its corresponding row vector in adjacency matrix of graph, extremely high dimensionality of such a representation in a large graph with many nodes makes the in sequel graph processing \& analysis difficult.

            -- {\bf Tính không áp dụng được của các phương pháp ML.} Gần đây, các phương pháp ML, đặc biệt là DL, rất mạnh mẽ trong nhiều lĩnh vực. Tuy nhiên, đối với dữ liệu đồ thị được biểu diễn theo cách truyền thống, hầu hết các phương pháp ML có sẵn có thể không áp dụng được. Các phương pháp này thường giả định: các mẫu dữ liệu có thể được biểu diễn bằng các vectơ độc lập trong không gian vectơ, trong khi các mẫu trong dữ liệu đồ thị (i.e., các nút) phụ thuộc lẫn nhau ở 1 mức độ nào đó được xác định bởi $E$. Mặc dù có thể biểu diễn 1 nút đơn giản bằng vectơ hàng tương ứng của nó trong ma trận kề của đồ thị, nhưng tính đa chiều cực cao của biểu diễn như vậy trong 1 đồ thị lớn với nhiều nút khiến việc xử lý \& phân tích đồ thị tiếp theo trở nên khó khăn.
        \end{enumerate}
        To tackle these challenges, substantial effort has been committed to develop novel graph representation learning, i.e., learning dense \& continuous low-dimensional vector representations for nodes, so that noise or redundant information can be reduced \& intrinsic structure information can be preserved. In learned representation space, relationships among nodes, which were originally represented by edges or other high-order topological measures in graphs, are captures by distances between nodes in vector space, \& structural characteristics of a node are encoded into its representation vector.

        -- Để giải quyết những thách thức này, nhiều nỗ lực đáng kể đã được thực hiện để phát triển phương pháp học biểu diễn đồ thị mới, i.e., học các biểu diễn vectơ liên tục, chiều thấp \& dày đặc cho các nút, nhờ đó có thể giảm nhiễu hoặc thông tin dư thừa \& bảo toàn thông tin cấu trúc nội tại. Trong không gian biểu diễn đã học, các mối quan hệ giữa các nút, ban đầu được biểu diễn bằng các cạnh hoặc các phép đo tôpô bậc cao khác trong đồ thị, được nắm bắt bằng khoảng cách giữa các nút trong không gian vectơ, \& các đặc điểm cấu trúc của 1 nút được mã hóa vào vectơ biểu diễn của nó.

        Basically, in order to make representation space well supporting graph analysis tasks, there are 2 goals for graph representation learning. 1st, original graph can be reconstructed from learned representation space. It requires: if there is an edge or relationship between 2 nodes, then distance of these 2 nodes in representation space should be relatively small. 2nd, learned representation space can effectively support graph inference, e.g. predicting unseen links, identifying important nodes, \& inferring node labels. It should ne noted: a representation space with only goal of graph reconstruction is not sufficient for graph inference. After representation is obtained, downstream tasks e.g. node classification, node clustering, graph visualization \& link prediction can be dealt with based on these representations. Overall, there are 3 main categories of graph representation learning methods: traditional graph embedding, modern graph embedding, \& GNNs.

        -- Về cơ bản, để tạo không gian biểu diễn hỗ trợ tốt cho các tác vụ phân tích đồ thị, có 2 mục tiêu cho việc học biểu diễn đồ thị. Thứ nhất, đồ thị gốc có thể được tái tạo từ không gian biểu diễn đã học. Điều này yêu cầu: nếu có 1 cạnh hoặc mối quan hệ giữa 2 nút, thì khoảng cách của 2 nút này trong không gian biểu diễn phải tương đối nhỏ. Thứ hai, không gian biểu diễn đã học có thể hỗ trợ hiệu quả cho suy luận đồ thị, e.g.: dự đoán các liên kết chưa thấy, xác định các nút quan trọng, \& suy ra nhãn nút. Cần lưu ý: không gian biểu diễn chỉ có mục tiêu tái tạo đồ thị là không đủ cho suy luận đồ thị. Sau khi có được biểu diễn, các tác vụ hạ nguồn e.g.: phân loại nút, nhóm nút, trực quan hóa đồ thị \& dự đoán liên kết có thể được xử lý dựa trên các biểu diễn này. Nhìn chung, có 3 loại chính của các phương pháp học biểu diễn đồ thị: nhúng đồ thị truyền thống, nhúng đồ thị hiện đại, \& GNN.
        \item {\sf2.2. Traditional Graph Embedding.} Traditional graph embedding methods are originally studied as dimension reduction techniques. A graph is usually constructed from a feature represented data set, like image data set. Graph embedding usually has 2 goals, i.e., reconstructing original graph structures \& support graph inference. Objective functions of traditional graph embedding methods mainly target goal of graph reconstruction.

        -- {\sf Nhúng Đồ Thị Truyền Thống.} Các phương pháp nhúng đồ thị truyền thống ban đầu được nghiên cứu như các kỹ thuật giảm chiều. 1 đồ thị thường được xây dựng từ 1 tập dữ liệu được biểu diễn bằng đặc trưng, chẳng hạn như tập dữ liệu ảnh. Nhúng đồ thị thường có 2 mục tiêu: tái tạo cấu trúc đồ thị gốc \& hỗ trợ suy luận đồ thị. Hàm mục tiêu của các phương pháp nhúng đồ thị truyền thống chủ yếu hướng đến mục tiêu tái tạo đồ thị.

        Specifically, Tenenbaum et al (2000) 1st constructs a neighborhood graph $G$ using connectivity algorithms e.g. K nearest neighbors (KNN). Then based on $G$, shortest path between different data can be computed. Consequently, for all $N$ data entries in data set, have matrix of graph distances. Finally, classical multidimensional scaling (MDS) method is applied to matrix to obtain coordinate vectors. Representations learned by Isomap approximately preserve geodesic distances of entry pairs in low-dimensional space. Key problem of Isomap is its high complexity due to computing of pairwise shortest paths. Locally linear embedding (LLE) (Roweis \& Saul, 2000) is proposed to eliminate need to estimate pairwise distances between widely separated entries. LLE assumes: each entry \& its neighbors lie on or close to a locally linear patch of a manifold. To characterize local geometry, each entry can be reconstructed from its neighbors. Finally, in low-dimensional space, LLE construct a neighborhood-preserving mapping based on locally linear reconstruction Laplacian eigenmaps (LE) (Belkin \& Niyogi, 2002) also begins with constructing a graph using $\varepsilon$-neighborhoods or K nearest neighbors. Then heat kernel (Berline et al, 2003) is utilized to choose weight of 2 nodes in graph. Finally, node representations can be obtained by based on Laplacian matrix regularization. Furthermore, locality preserving projection (LPP) (Berline et al, 2003), a linear approximation of nonlinear LE, is proposed.

        -- Cụ thể, Tenenbaum \& cộng sự (2000) lần đầu tiên xây dựng 1 đồ thị lân cận $G$ bằng các thuật toán kết nối, e.g. K láng giềng gần nhất (KNN). Sau đó, dựa trên $G$, đường đi ngắn nhất giữa các dữ liệu khác nhau có thể được tính toán. Do đó, với mọi $N$ mục dữ liệu trong tập dữ liệu, có ma trận khoảng cách đồ thị. Cuối cùng, phương pháp tỷ lệ đa chiều cổ điển (MDS) được áp dụng cho ma trận để thu được các vectơ tọa độ. Các biểu diễn được học bởi Isomap gần đúng bảo toàn khoảng cách trắc địa của các cặp mục trong không gian chiều thấp. Vấn đề chính của Isomap là độ phức tạp cao do tính toán các đường đi ngắn nhất từng cặp. Nhúng tuyến tính cục bộ (LLE) (Roweis \& Saul, 2000) được đề xuất để loại bỏ nhu cầu ước tính khoảng cách từng cặp giữa các mục cách xa nhau. LLE giả định: mỗi mục \& các hàng xóm của nó nằm trên hoặc gần 1 bản vá tuyến tính cục bộ của 1 đa tạp. Để mô tả hình học cục bộ, mỗi mục có thể được tái tạo từ các hàng xóm của nó. Cuối cùng, trong không gian ít chiều, LLE xây dựng 1 ánh xạ bảo toàn lân cận dựa trên phép tái tạo tuyến tính cục bộ các phép riêng Laplacian (LE) (Belkin \& Niyogi, 2002) cũng bắt đầu bằng việc xây dựng 1 đồ thị sử dụng các lân cận $\varepsilon$ hoặc K lân cận gần nhất. Sau đó, hạt nhân nhiệt (Berline \& cộng sự, 2003) được sử dụng để chọn trọng số của 2 nút trong đồ thị. Cuối cùng, các biểu diễn nút có thể thu được bằng cách dựa trên phép chính quy hóa ma trận Laplacian. Hơn nữa, phép chiếu bảo toàn lân cận (LPP) (Berline \& cộng sự, 2003), 1 phép xấp xỉ tuyến tính của LE phi tuyến tính, đã được đề xuất.

        These methods are extended in rich literature of graph embedding by considering different characteristics of constructed graphs (Fu \& Ma, 2012). Can find: traditional graph embedding mostly works on graphs constructed from feature represented data sets, where proximity among nodes encoded by edge weights is well defined in original feature space. While, in contrast, modern graph embedding mostly works on naturally formed networks, e.g. social networks, biology networks, \& e-commerce networks. In those networks, proximities among nodes are not explicitly or directly defined. E.g., an edge between 2 nodes usually just implies there is a relationship between them, but cannot indicate specific proximity. Also, even if there is no edge between 2 nodes, cannot say proximity between these 2 nodes is 0. Def of node proximities depends on specific analytic tasks \& application scenarios. Therefore, modern graph embedding usually incorporates rich information, e.g. network structures, properties, side information \& advanced information, to facilitate different problems \& applications. Modern graph embedding needs to target both of goals mentioned before. In view of this, traditional graph embedding can be regarded as a special case of modern graph embedding, \& recent research progress on modern graph embedding pays more attention to network inference.

        -- Các phương pháp này được mở rộng trong các tài liệu phong phú về nhúng đồ thị bằng cách xem xét các đặc điểm khác nhau của đồ thị được xây dựng (Fu \& Ma, 2012). Có thể thấy: nhúng đồ thị truyền thống chủ yếu hoạt động trên các đồ thị được xây dựng từ các tập dữ liệu được biểu diễn bằng đặc trưng, trong đó độ gần giữa các nút được mã hóa bởi trọng số cạnh được xác định rõ trong không gian đặc trưng gốc. Ngược lại, nhúng đồ thị hiện đại chủ yếu hoạt động trên các mạng được hình thành tự nhiên, e.g.: mạng xã hội, mạng sinh học, \& mạng thương mại điện tử. Trong các mạng đó, độ gần giữa các nút không được xác định rõ ràng hoặc trực tiếp. Ví dụ: 1 cạnh giữa 2 nút thường chỉ ngụ ý có mối quan hệ giữa chúng, nhưng không thể chỉ ra độ gần cụ thể. Ngoài ra, ngay cả khi không có cạnh giữa 2 nút, cũng không thể nói rằng độ gần giữa 2 nút này bằng 0. Định nghĩa về độ gần của các nút phụ thuộc vào các tác vụ phân tích \& kịch bản ứng dụng cụ thể. Do đó, nhúng đồ thị hiện đại thường kết hợp thông tin phong phú, e.g.: cấu trúc mạng, thuộc tính, thông tin phụ \& thông tin nâng cao, để tạo điều kiện cho các vấn đề \& ứng dụng khác nhau. Nhúng đồ thị hiện đại cần hướng đến cả hai mục tiêu đã đề cập trước đó. Theo quan điểm này, nhúng đồ thị truyền thống có thể được coi là 1 trường hợp đặc biệt của nhúng đồ thị hiện đại, \& tiến bộ nghiên cứu gần đây về nhúng đồ thị hiện đại chú trọng hơn đến suy luận mạng.
        \item {\sf2.3. Modern Graph Embedding.} To well support network inference, modern graph embedding considers much richer information in a graph. According to types of information that are preserved in graph representation learning, existing methods can be categorized into 3 categories:
        \begin{enumerate}
            \item graph structures \& properties preserving graph embedding,
            \item graph representation learning with side information, \&
            \item advanced information preserving graph representation learning.
        \end{enumerate}
        In technique view, different models are adopted to incorporate different types of information or address different goals. Commonly used models include matrix factorization, random walk, deep neural networks \& their variations.

        -- {\sf2.3. Nhúng Đồ thị Hiện đại.} Để hỗ trợ tốt cho suy luận mạng, nhúng đồ thị hiện đại xem xét thông tin phong phú hơn nhiều trong đồ thị. Theo các loại thông tin được bảo toàn trong học biểu diễn đồ thị, các phương pháp hiện có có thể được phân loại thành 3 loại:
        \begin{enumerate}
            \item cấu trúc đồ thị \& thuộc tính bảo toàn nhúng đồ thị,
            \item học biểu diễn đồ thị với thông tin phụ, \&
            \item học biểu diễn đồ thị bảo toàn thông tin nâng cao.
        \end{enumerate}
        Về mặt kỹ thuật, các mô hình khác nhau được áp dụng để kết hợp các loại thông tin khác nhau hoặc giải quyết các mục tiêu khác nhau. Các mô hình thường được sử dụng bao gồm phân tích ma trận, bước ngẫu nhiên, mạng nơ-ron sâu \& các biến thể của chúng.
        \begin{itemize}
            \item {\sf2.3.1. Structure-Property Preserving Graph Representation Learning.}
            \begin{itemize}
                \item {\sf2.3.1.1. Structure Preserving Graph Representation Learning.}
                \item {\sf2.3.1.2. Property Preserving Graph Representation Learning.}
            \end{itemize}
            \item {\sf2.3.2. Graph Representation Learning with Side Information.}
            \item {\sf2.3.3. Advanced Information Preserving Graph Representation Learning.}
        \end{itemize}
        \item {\sf2.4. GNNs.} Over past decade, DL has become ``crown jewel'' of AI \& ML, showing superior performance in acoustics, images \& NLP, etc. Although well known: graphs are ubiquitous in real world, very challenging to utilize DL methods to analyze graph data. This problem is nontrivial because of following challenges:
        \begin{enumerate}
            \item Irregular structures of graphs. Unlike images, audio, \& text, which have a clear grid structure, graphs have irregular structures, making it hard to generalize some of basic mathematical operations to graphs. E.g., defining convolution \& pooling operations, which are fundamental operations in convolutional neural networks (CNNs), for graph data is not straightforward.
            \item Heterogeneity \& diversity of graphs. A graph itself can be complicated, containing diverse types \& properties. These diverse types, properties, \& tasks require different model architectures to tackle specific problems.
            \item Large-scale graphs. In big-data era, real graphs can easily have millions or billions of nodes \& edges. How to design scalable models, preferably models that have a linear time complexity w.r.t. graph size, is a key problem.
            \item Incorporating interdisciplinary knowledge. Graphs are often connected to other disciplines, e.g. biology, chemistry, \& social sciences. This interdisciplinary nature provides both opportunities \& challenges: domain knowledge can be leveraged to solve specific problems but integrating domain knowledge can complicate model designs.
        \end{enumerate}
        -- Trong thập kỷ qua, DL đã trở thành ``viên ngọc quý'' của AI \& ML, thể hiện hiệu suất vượt trội trong âm học, hình ảnh \& NLP, v.v. Mặc dù đã được biết đến rộng rãi: đồ thị có mặt khắp nơi trong thế giới thực, nhưng việc sử dụng các phương pháp DL để phân tích dữ liệu đồ thị lại rất khó khăn. Vấn đề này không hề đơn giản vì những thách thức sau:
        \begin{enumerate}
            \item Cấu trúc đồ thị bất quy tắc. Không giống như hình ảnh, âm thanh, \& văn bản có cấu trúc lưới rõ ràng, đồ thị có cấu trúc bất quy tắc, khiến việc khái quát hóa 1 số phép toán cơ bản thành đồ thị trở nên khó khăn. Ví dụ: việc xác định các phép toán tích chập \& gộp, vốn là các phép toán cơ bản trong mạng nơ-ron tích chập (CNN), cho dữ liệu đồ thị không hề đơn giản.
            \item Tính không đồng nhất \& đa dạng của đồ thị. Bản thân 1 đồ thị có thể phức tạp, chứa đựng nhiều loại \& thuộc tính khác nhau. Những loại, thuộc tính, \& tác vụ đa dạng này đòi hỏi các kiến trúc mô hình khác nhau để giải quyết các vấn đề cụ thể.
            \item Đồ thị quy mô lớn. Trong kỷ nguyên dữ liệu lớn, đồ thị thực tế có thể dễ dàng có hàng triệu hoặc hàng tỷ nút \& cạnh. Làm thế nào để thiết kế các mô hình có khả năng mở rộng, tốt nhất là các mô hình có độ phức tạp thời gian tuyến tính so với kích thước đồ thị, là 1 vấn đề then chốt.
            \item Kết hợp kiến thức liên ngành. Đồ thị thường được kết nối với các ngành khác, e.g.: sinh học, hóa học, \& khoa học xã hội. Bản chất liên ngành này mang lại cả cơ hội \& thách thức: kiến thức chuyên ngành có thể được tận dụng để giải quyết các vấn đề cụ thể, nhưng việc tích hợp kiến thức chuyên ngành có thể làm phức tạp thiết kế mô hình.
        \end{enumerate}
        Currently, GNNs have attracted considerable research attention over past several years. Adopted architectures \& training strategies vary greatly, ranging from supervised to unsupervised \& from convolution to recursive, including graph recurrent neural networks (Graph RNNs) , graph convolutional networks (GCNs), graph autoencoders (GAEs), graph reinforcement learning (Graph GL), \& graph adversarial methods. Specifically, Graroperty h RNNs capture recursive \& sequential patterns of graphs by modeling states at either node-level or graph-level; GCNs define convolution \& readout operations on irregular graph structures to capture common local \& global structural patterns; GAEs assume low-rank graph structures \& adopt unsupervised methods for node representation learning; Graph RL defines graph-based actions \& rewards to obtain feedbacks on graph tasks while following constraints; Graph adversarial methods adopt adversarial training techniques to enhance generalization ability of graphbased models \& test their robustness by adversarial attacks.

        -- Hiện nay, GNN đã thu hút được sự chú ý nghiên cứu đáng kể trong vài năm qua. Các kiến trúc được áp dụng \& các chiến lược đào tạo rất khác nhau, từ có giám sát đến không giám sát \& từ tích chập đến đệ quy, bao gồm mạng nơ-ron hồi quy đồ thị (Graph RNN), mạng tích chập đồ thị (GCN), bộ mã hóa tự động đồ thị (GAE), học tăng cường đồ thị (Graph GL) \& các phương pháp đối kháng đồ thị. Cụ thể, Graroperty h RNN nắm bắt các mẫu đệ quy \& tuần tự của đồ thị bằng cách mô hình hóa các trạng thái ở cấp độ nút hoặc cấp độ đồ thị; GCN định nghĩa tích chập \& các phép toán đọc trên các cấu trúc đồ thị bất thường để nắm bắt các mẫu cấu trúc cục bộ \& toàn cục chung; GAE giả định các cấu trúc đồ thị hạng thấp \& áp dụng các phương pháp không giám sát để học biểu diễn nút; Graph RL định nghĩa các hành động dựa trên đồ thị \& phần thưởng để có được phản hồi về các tác vụ đồ thị trong khi tuân theo các ràng buộc; Các phương pháp đối kháng đồ thị áp dụng các kỹ thuật đào tạo đối kháng để tăng cường khả năng khái quát hóa của các mô hình dựa trên đồ thị \& kiểm tra tính mạnh mẽ của chúng bằng các cuộc tấn công đối kháng.

        There are many ongoing or future research directions which are also worthy of further study, including new models for unstudied graph structures, compositionality of existing models, dynamic graphs, interpretability \& robustness, etc. On whole, DL on graphs is a promising \& fast-developing research field that both offers exciting opportunities \& presents many challenges. Studying DL on graphs constitutes a critical building block in modeling relational data, \& it is an important step towards a future with better ML \& AI techniques.

        -- Có rất nhiều hướng nghiên cứu đang \& sẽ được triển khai cũng đáng được nghiên cứu sâu hơn, bao gồm các mô hình mới cho cấu trúc đồ thị chưa được nghiên cứu, tính tổng hợp của các mô hình hiện có, đồ thị động, khả năng diễn giải \& tính mạnh mẽ, v.v. Nhìn chung, DL trên đồ thị là 1 lĩnh vực nghiên cứu đầy hứa hẹn \& đang phát triển nhanh chóng, vừa mang đến những cơ hội thú vị \& vừa đặt ra nhiều thách thức. Nghiên cứu DL trên đồ thị là 1 nền tảng quan trọng trong việc mô hình hóa dữ liệu quan hệ, \& đây là 1 bước tiến quan trọng hướng tới tương lai với các kỹ thuật ML \& AI tốt hơn.
        \item {\sf2.5. Summary.} In this chap, introduce motivation of graph representation learning. Then in Sect. 2, discuss traditional graph embedding methods \& modern graph embedding methods are introduced in Sect. 3. Basically, structure \& property preserving graph representation learning is foundation. If one cannot preserve well graph structures \& retain important graph properties in representation space, serious information will be lost, which hurts analytic tasks in sequel. Based on structures \& property preserving graph representation learning, one may apply off-shelf ML methods. If some side information is available, it can be incorporated into graph representation learning. Furthermore, domain knowledge of some certain applications as advanced information can be considered. As shown in Sect. 4, utilizing DL methods on graphs is a promising \& fast-developing research field that both offers exciting opportunities \& presents many challenges. Studying DL on graphs constitutes a critical building block in modeling relational data, \& it is an important step toward a future with better ML \& AI techniques.

        -- Trong chương này, giới thiệu động lực của việc học biểu diễn đồ thị. Sau đó, trong Phần 2, thảo luận về các phương pháp nhúng đồ thị truyền thống \& các phương pháp nhúng đồ thị hiện đại được giới thiệu trong Phần 3. Về cơ bản, học biểu diễn đồ thị bảo toàn cấu trúc \& tính chất là nền tảng. Nếu không thể bảo toàn tốt các cấu trúc đồ thị \& giữ lại các tính chất quan trọng của đồ thị trong không gian biểu diễn, thông tin quan trọng sẽ bị mất, gây ảnh hưởng đến các tác vụ phân tích sau này. Dựa trên học biểu diễn đồ thị bảo toàn cấu trúc \& tính chất, người ta có thể áp dụng các phương pháp ML có sẵn. Nếu có sẵn 1 số thông tin phụ, nó có thể được kết hợp vào học biểu diễn đồ thị. Hơn nữa, kiến thức chuyên môn về 1 số ứng dụng nhất định dưới dạng thông tin nâng cao có thể được coi là thông tin nâng cao. Như đã trình bày trong Phần 4, việc sử dụng các phương pháp DL trên đồ thị là 1 lĩnh vực nghiên cứu đầy hứa hẹn \& phát triển nhanh chóng, vừa mang đến những cơ hội thú vị \& đặt ra nhiều thách thức. Nghiên cứu DL trên đồ thị là 1 nền tảng quan trọng trong việc mô hình hóa dữ liệu quan hệ, \& đây là 1 bước quan trọng hướng tới tương lai với các kỹ thuật ML \& AI tốt hơn.
    \end{itemize}
    \item {\sf3. {\sc Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao, Le Song}. Graph Neural Networks.}

    PART II. FOUNDATIONS OF GRAPH NEURAL NETWORKS.
    \item {\sf4. {\sc Jian Tang, Renjie Liao}. Graph Neural Networks for Node Classification.} GNNs are neural architectures specifically designed for graph-structured data, which have been receiving increasing attention recently \& applied to different domains \& applications. In this chap, focus on a fundamental task on graphs: node classification. Give a detailed definition of node classification \& also introduce some classical approaches e.g. label propagation. Afterwards, introduce a few representative architectures of GNNs for node classification. Further point out main difficulty -- oversmoothing problem -- of training deep graph neural networks \& present some latest advancement along this direction e.g. continuous GNNs.

    -- GNN là kiến trúc nơ-ron được thiết kế riêng cho dữ liệu có cấu trúc đồ thị, gần đây đang ngày càng được quan tâm \& ứng dụng trong nhiều lĩnh vực \& ứng dụng khác nhau. Trong chương này, chúng tôi tập trung vào 1 nhiệm vụ cơ bản trên đồ thị: phân loại nút. Đưa ra định nghĩa chi tiết về phân loại nút \& đồng thời giới thiệu 1 số phương pháp tiếp cận cổ điển, e.g. lan truyền nhãn. Sau đó, chúng tôi giới thiệu 1 số kiến trúc GNN tiêu biểu cho phân loại nút. Chúng tôi cũng chỉ ra khó khăn chính -- vấn đề làm mịn quá mức -- của việc huấn luyện mạng nơ-ron đồ thị sâu \& trình bày 1 số tiến bộ mới nhất theo hướng này, e.g. GNN liên tục.
    \begin{itemize}
        \item {\sf4.1. Background \& Problem Def.} Graph-structured data (e.g., social networks, WWW, protein-protein interaction networks) are ubiquitous in real-world, covering a variety of applications. A fundamental task on graphs is node classification, which tries to classify nodes into a few predefined categories. E.g., in social networks, want to predict political bias of each user; in protein-protein interaction networks, interested in predicting function role of each protein; in WWW, may have to classify web pages into different semantic categories. To make effective prediction, a critical problem is to have very effective node representations, which largely determine performance of node classification.

        -- Dữ liệu có cấu trúc đồ thị (e.g.: mạng xã hội, WWW, mạng tương tác protein-protein) rất phổ biến trong thế giới thực, bao gồm nhiều ứng dụng khác nhau. 1 nhiệm vụ cơ bản trên đồ thị là phân loại nút, cố gắng phân loại các nút thành 1 vài danh mục được xác định trước. Ví dụ, trong mạng xã hội, cần dự đoán khuynh hướng chính trị của từng người dùng; trong mạng tương tác protein-protein, cần dự đoán vai trò chức năng của từng protein; trong WWW, cần phân loại các trang web thành các danh mục ngữ nghĩa khác nhau. Để dự đoán hiệu quả, 1 vấn đề quan trọng là phải có các biểu diễn nút thật hiệu quả, yếu tố quyết định phần lớn hiệu suất phân loại nút.

        GNNs are neural network architectures specially designed for learning representations of graph-structured data including learning node representations of big graphs (e.g., social networks \& WWW) \& learning representations of entire graphs (e.g., molecular graphs). In this chap, focus on learning node representations for large-scale graphs \& introduce learning whole-graph representations in other chaps. A variety of GNNs have been proposed (Kipf \& Welling, 2017b; Veličković et al, 2018; Gilmer et al, 2017; Xhonneux et al, 2020; Liao et al, 2019b; Kipf \& Welling, 2016; Veličković et al, 2019). In this chap, comprehensively revisit existing GNNs for node classification including supervised approaches (Sec. 4.2), unsupervised approaches (Sec. 4.3), \& a common problem of GNNs for node classification -- oversmoothing (Sect. 4.4).

        -- GNN là kiến trúc mạng nơ-ron được thiết kế đặc biệt để học các biểu diễn dữ liệu có cấu trúc đồ thị, bao gồm học các biểu diễn nút của đồ thị lớn (e.g.: mạng xã hội \& WWW) \& học các biểu diễn của toàn bộ đồ thị (e.g.: đồ thị phân tử). Trong chương này, chúng tôi tập trung vào việc học các biểu diễn nút cho đồ thị quy mô lớn \& giới thiệu về học các biểu diễn toàn bộ đồ thị trong các chương khác. Nhiều loại GNN đã được đề xuất (Kipf \& Welling, 2017b; Veličković \& cộng sự, 2018; Gilmer \& cộng sự, 2017; Xhonneux \& cộng sự, 2020; Liao \& cộng sự, 2019b; Kipf \& Welling, 2016; Veličković \& cộng sự, 2019). Trong chương này, chúng tôi sẽ xem xét lại toàn diện các GNN hiện có để phân loại nút bao gồm các phương pháp có giám sát (Phần 4.2), các phương pháp không có giám sát (Phần 4.3) \& 1 vấn đề phổ biến của GNN để phân loại nút -- làm mịn quá mức (Phần 4.4).

        {\bf Problem Def.} 1st formally define problem of learning node representations for node classification with GNNs. Let $G = (V,E)$ denotes a graph, where $V$: set of nodes, $E$: set of edges. $A\in\mathbb{R}^{N\times N}$ represents adjacency matrix, where $N$: total number of nodes, $X\in\mathbb{R}^{N\times C}$ represents node attribute matrix, where $C$: number of features for each node. Goal of GNNs: learn effective node representations (denoted as $H\in\mathbb{R}^{N\times F}$, $F$: dimension of node representations) by combining graph structure information \& node attributes, which are further used for node classification.

        -- Đầu tiên, định nghĩa chính thức bài toán học biểu diễn nút để phân loại nút bằng GNN. Giả sử $G = (V,E)$ là 1 đồ thị, trong đó $V$: tập các nút, $E$: tập các cạnh. $A\in\mathbb{R}^{N\times N}$ là ma trận kề, trong đó $N$: tổng số nút, $X\in\mathbb{R}^{N\times C}$ là ma trận thuộc tính nút, trong đó $C$: số đặc trưng cho mỗi nút. Mục tiêu của GNN: học các biểu diễn nút hiệu quả (ký hiệu là $H\in\mathbb{R}^{N\times F}$, $F$: số chiều của biểu diễn nút) bằng cách kết hợp thông tin cấu trúc đồ thị \& thuộc tính nút, sau đó được sử dụng để phân loại nút.
        \item {\sf4.2. Supervised GNNs.} In this sect, revisit several representative methods of GNNs for node classification. Focus on supervised methods \& introduce unsupervised methods in next sect. Start by introducing a general framework of GNNs \& then introduce different variants under this framework.

        -- {\sf GNN có giám sát.} Trong phần này, hãy xem lại 1 số phương pháp GNN tiêu biểu để phân loại nút. Tập trung vào các phương pháp có giám sát \& giới thiệu các phương pháp không giám sát trong phần tiếp theo. Bắt đầu bằng cách giới thiệu 1 khuôn khổ chung về GNN \& sau đó giới thiệu các biến thể khác nhau trong khuôn khổ này.
        \begin{itemize}
            \item {\sf4.2.1. General Framework of GNNs.} Essential idea of GNNs: iteratively update node representations by combining representations of their neighbors \& their own representations. In this sect, introduce a general framework of GNNs in (Xu et al, 2019d). Starting from initial node representaiton $H^0 = X$, in each layer we have 2 important functions:
            \begin{enumerate}
                \item AGGREGATE, which tries to aggregate information from neighbors of each node
                \item COMBINE, which tries to update node representations by combining aggregated information from neighbors with current node representations.
            \end{enumerate}
            -- {\sf Khung Tổng quát của GNN.} Ý tưởng cốt lõi của GNN: cập nhật lặp lại các biểu diễn nút bằng cách kết hợp các biểu diễn của các nút lân cận \& các biểu diễn của chính chúng. Trong phần này, giới thiệu 1 khung tổng quát của GNN trong (Xu \& cộng sự, 2019d). Bắt đầu từ biểu diễn nút ban đầu $H^0 = X$, trong mỗi lớp, chúng ta có 2 hàm quan trọng:
            \begin{enumerate}
                \item AGGREGATE, cố gắng tổng hợp thông tin từ các nút lân cận của mỗi nút
                \item COMBINE, cố gắng cập nhật các biểu diễn nút bằng cách kết hợp thông tin tổng hợp từ các nút lân cận với các biểu diễn nút hiện tại.
            \end{enumerate}
            Mathematically, can define general framework of GNNs as follows: Initialization $H^0 = X$. For $k\in[K]$
            \begin{equation*}
                a_v^k = AGGREGATE^k\{H_u^{k-1}:u\in N(v)\},\ H_v^k = COMBINE^k\{H_v^{k-1},a_v^k\},
            \end{equation*}
            where $N(v)$: set of neighbors for $v$th node. Node representations $H^K$ in last layer can be treated as final node representations.

            -- Về mặt toán học, có thể định nghĩa khuôn khổ chung của GNN như sau: Khởi tạo $H^0 = X$. Với $k\in[K]$
            \begin{equation*}
                a_v^k = AGGREGATE^k\{H_u^{k-1}:u\in N(v)\},\ H_v^k = COMBINE^k\{H_v^{k-1},a_v^k\},
            \end{equation*}
            trong đó $N(v)$: tập hợp các lân cận cho nút thứ $v$. Biểu diễn nút $H^K$ ở lớp cuối cùng có thể được coi là biểu diễn nút cuối cùng.

            Once have node representations, they can be used for downstream tasks. Take node classification as an example, label of node $v$, denoted as $\hat{y}_v$, can be predicted through a Softmax function, i.e.,
            \begin{equation*}
                \hat{y}_v = {\rm Softmax}(WH_v^\top),
            \end{equation*}
            where $W\in\mathbb{R}^{|L|\times F}$, $|L|$: number of labels in output space.

            -- Sau khi có biểu diễn nút, chúng có thể được sử dụng cho các tác vụ hạ nguồn. Lấy phân loại nút làm e.g., nhãn của nút $v$, được ký hiệu là $\hat{y}_v$, có thể được dự đoán thông qua hàm Softmax, tức là,
            \begin{equation*}
                \hat{y}_v = {\rm Softmax}(WH_v^\top),
            \end{equation*}
            trong đó $W\in\mathbb{R}^{|L|\times F}$, $|L|$: số nhãn trong không gian đầu ra.

            Given a set of labeled nodes, whole model can be trained by minimizing following loss function:
            \begin{equation*}
                O = \frac{1}{n_l}\sum_{i=1}^{n_l} {\rm loss}(\hat{y}_i,y_i),
            \end{equation*}
            where $y_i$: ground truth label of node $i$, $n_l$: number of labeled nodes, ${\rm loss}(\cdot,\cdot)$: a loss function e.g. cross-entropy loss function. Whole neural networks can be optimized by minimizing objective function $O$ with backpropagation.

            -- Với 1 tập hợp các nút được gắn nhãn, toàn bộ mô hình có thể được huấn luyện bằng cách tối thiểu hóa hàm mất mát sau:
            \begin{equation*}
                O = \frac{1}{n_l}\sum_{i=1}^{n_l} {\rm loss}(\hat{y}_i,y_i),
            \end{equation*}
            trong đó $y_i$: nhãn thực tế của nút $i$, $n_l$: số nút được gắn nhãn, ${\rm loss}(\cdot,\cdot)$: hàm mất mát, e.g.: hàm mất mát entropy chéo. Toàn bộ mạng nơ-ron có thể được tối ưu hóa bằng cách tối thiểu hóa hàm mục tiêu $O$ với lan truyền ngược.

            Above present a general framework of GNNs. Introduce a few most representative instantiations or variants of GNNs in literature.

            -- Trên đây trình bày 1 khuôn khổ chung về GNN. Giới thiệu 1 số ví dụ hoặc biến thể tiêu biểu nhất của GNN trong tài liệu.
            \item {\sf4.2.2. Graph Convolutional Networks.} Start from graph convolutional networks (GCN) (Kipf \& Welling, 2017b), which is now most popular GNN architecture due to its simplicity \& effectiveness in a variety of tasks \& applications. Specifically, node representations in each layer is updated according to following propagation rule: (4.5)
            \begin{equation*}
                H^{k+1} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^kW^k).
            \end{equation*}
            $\tilde{A} = A + I$: adjacency matrix of given undirected graph $G$ with self-connections, which allows to incorporate node features itself when updating node representations. $I\in\mathbb{R}^{N\times N}$: identity matrix. $\tilde{D}$: a diagonal matrix with $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}\sigma(\cdot)$: an activation function e.g. ReLU \& Tanh. ReLU active function is widely used, defined as ${\rm ReLU}(x) = \max\{0,x\} = x^+$. $W^k\in\mathbb{R}^{F\times F'}$ ($F,F'$: dimensions of node representations in $k$th, $(k + 1)$th layer resp.) is a laywise linear transformation matrix, which will be trained during optimization.

            -- {\sf Mạng Tích chập Đồ thị.} Bắt đầu từ mạng tích chập đồ thị (GCN) (Kipf \& Welling, 2017b), hiện là kiến trúc GNN phổ biến nhất do tính đơn giản \& hiệu quả của nó trong nhiều tác vụ \& ứng dụng. Cụ thể, biểu diễn nút trong mỗi lớp được cập nhật theo quy tắc lan truyền sau:
            \begin{equation*}
                H^{k+1} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^kW^k).
            \end{equation*}
            $\tilde{A} = A + I$: ma trận kề của đồ thị vô hướng $G$ cho trước với các kết nối tự thân, cho phép kết hợp chính các đặc trưng của nút khi cập nhật biểu diễn nút. $I\in\mathbb{R}^{N\times N}$: ma trận đơn vị. $\tilde{D}$: ma trận đường chéo với $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}\sigma(\cdot)$: hàm kích hoạt, e.g.: ReLU \& Tanh. Hàm hoạt động của ReLU được sử dụng rộng rãi, được định nghĩa là ${\rm ReLU}(x) = \max\{0,x\} = x^+$. $W^k\in\mathbb{R}^{F\times F'}$ ($F,F'$: chiều của biểu diễn nút trong lớp thứ $k$, tương ứng là $(k + 1)$) là ma trận biến đổi tuyến tính từng lớp, sẽ được huấn luyện trong quá trình tối ưu hóa.

            Can further dissect (4.5) \& understand AGGREGATE \& COMBINE function defined in GCN. For a node $i$, node updating equation can be reformulated as below: (4.6)--(4.7)
            \begin{align*}
                H_i^k &= \sigma\left(\sum_{j\in\{N(i)\cup i\}} \frac{\tilde{A}_{ij}}{\sqrt{\tilde{D}_{ii}\tilde{D}_{jj}}}H_j^{k-1}W^k\right),\\
                H_i^k &= \sigma\left(\sum_{j\in N(i)} \frac{A_{ij}}{\sqrt{\tilde{D}_{ii}\tilde{D}_{jj}}}H_j^{k-1}W^k + \frac{1}{\tilde{D}_i}H_i^{k-1}W^k\right).
            \end{align*}
            In (4.7), can see: AGGREGATE function is defined as weighted average of neighbor node representations. Weight of neighbor $j$ is determined by weight of edge between $i,j$ (i.e., $A_{ij}$ normalized by degrees of 2 nodes). COMBINE function is defined as summation of aggregated messages \& node representation itself, in which node representation is normalized by its own degree.

            -- Có thể phân tích sâu hơn (4.5) \& hiểu hàm AGGREGATE \& COMBINE được định nghĩa trong GCN. Đối với 1 nút $i$, phương trình cập nhật nút có thể được xây dựng lại như sau: (4.6)--(4.7)
            \begin{align*}
                H_i^k &= \sigma\left(\sum_{j\in\{N(i)\cup i\}} \frac{\tilde{A}_{ij}}{\sqrt{\tilde{D}_{ii}\tilde{D}_{jj}}}H_j^{k-1}W^k\right),\\
                H_i^k &= \sigma\left(\sum_{j\in N(i)} \frac{A_{ij}}{\sqrt{\tilde{D}_{ii}\tilde{D}_{jj}}}H_j^{k-1}W^k + \frac{1}{\tilde{D}_i}H_i^{k-1}W^k\right).
            \end{align*}
            Trong (4.7), có thể thấy: Hàm AGGREGATE được định nghĩa là trung bình có trọng số của các biểu diễn nút lân cận. Trọng số của nút lân cận $j$ được xác định bởi trọng số của cạnh giữa $i,j$ (i.e., $A_{ij}$ được chuẩn hóa theo bậc của 2 nút). Hàm COMBINE được định nghĩa là tổng của các thông điệp được tổng hợp \& chính biểu diễn nút, trong đó biểu diễn nút được chuẩn hóa theo bậc của chính nó.

            {\bf Connections with Spectral Graph Convolutions.} Discuss connections between GGNs \& traditional spectral filters defined on graphs (Defferrard et al, 2016). Spectral convolutions on graphs can be defined as a multiplication of a node-wise signal ${\bf x}\in\mathbb{R}^N$ with a convoutional filter $g_\theta = {\rm diag}\theta$ ($\theta\in\mathbb{R}^N$: parameter of filter) in Fourier domain. Mathematically, (4.8)
            \begin{equation*}
                g_\theta\star{\bf x} = Ug_\theta U^\top{\bf x}.
            \end{equation*}
            $U$ represents matrix of eigenvectors of normalized graph Laplacian matrix $L = I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$. $L = U\Lambda U^\top$, $\Lambda$: a diagonal matrix of eigenvalues, \& $U^\top{\bf x}$: graph Fourier transform of input signal ${\bf x}$. In practice, $g_\theta$ can be understood as a function of eigenvalues of normalized graph Laplacian matrix $L$, i.e., $g_\theta(\Lambda)$. [A lot of rigorous mathematical formulas] p. 45+++

            \item {\sf4.2.3. Graph Attention Networks.} In GCNs, for a target node $i$, importance of a neighbor $j$ is determined by weight of their edge $A_{ij}$ (normalized by their node degrees). However, in practice, input graph may be noisy. Edge weights may not be able to reflect true strength between 2 nodes. As a result, a more principled approach would be to automatically learn importance of each neighbor. Graph Attention Networks (a.k.a. GAT(Veličković et al, 2018)) is built on this idea \& try to learn importance of each neighbor based on Attention mechanism (Bahdanau et al, 2015; Vaswani et al, 2017). Attention mechanism has been widely used in a variety of tasks in natural language understanding (e.g., machine translation \& question answering) \& computer vision (e.g., visual question answering \& image captioning). Next, introduce how attention is used in GNNs.

            -- {\sf Graph Attention Networks.} Trong GCN, đối với 1 nút đích $i$, tầm quan trọng của 1 nút lân cận $j$ được xác định bằng trọng số của cạnh $A_{ij}$ của chúng (được chuẩn hóa theo bậc nút của chúng). Tuy nhiên, trên thực tế, đồ thị đầu vào có thể bị nhiễu. Trọng số cạnh có thể không phản ánh được độ mạnh thực sự giữa 2 nút. Do đó, 1 cách tiếp cận có nguyên tắc hơn sẽ là tự động tìm hiểu tầm quan trọng của từng nút lân cận. Graph Attention Networks (hay còn gọi là GAT(Veličković et al, 2018)) được xây dựng dựa trên ý tưởng này \& cố gắng tìm hiểu tầm quan trọng của từng nút lân cận dựa trên cơ chế Attention (Bahdanau et al, 2015; Vaswani et al, 2017). Cơ chế Attention đã được sử dụng rộng rãi trong nhiều tác vụ khác nhau trong việc hiểu ngôn ngữ tự nhiên (e.g.: dịch máy \& trả lời câu hỏi) \& thị giác máy tính (e.g.: trả lời câu hỏi trực quan \& chú thích hình ảnh). Tiếp theo, hãy giới thiệu cách sử dụng attention trong GNN.

            {\bf Graph Attention Layer.} Graph attention layer defines how to transfer widen node representations at layer $k - 1$ (denoted as $H^{k-1}\in\mathbb{R}^{N\times F}$) to new node representations $H^k\in\mathbb{R}^{N\times F'}$. In order to guarantee sufficient expressive power to transform lower-level node representations to higher-level node representations, a shared linear transformation is applied to every node, denoted as $W\in\mathbb{R}^{F\times F'}$. Afterwards, self-attention is defined on nodes, which measures attention coefficients for any pair of nodes through a shared attentional mechanism $a:\mathbb{R}^{F'}\times\mathbb{R}^{F'}\to\mathbb{R}$: (4.14)
            \begin{equation*}
                e_{ij} = a(WH_i^{k-1},WH_j^{k-1}).
            \end{equation*}
            $e_{ij}$ indicates relationship strength between node $i,j$. In this subsect, use $H_i^{k-1}$ to represent a columnwise vector instead of a rowwise vector. For each node, can theoretically allow it to attend to every other node on graph, which however will ignore graph structural information. A more reasonable solution would be only to attend to neighbors for each node. In practice, 1st-order neighbors are only used (including node itself). \& to make coefficients comparable across different nodes, attention coefficients are usually normalized with softmax function:
            \begin{equation*}
                \alpha_{ij} = {\rm Softmax}_j)\{e_{ij}\} = \frac{\exp e_{ij}}{\sum_{l\in N(i)} \exp e_{il}}.
            \end{equation*}
            For a node $i$, $\alpha_{ij}$ essentially defines a multinomial distribution over neighbors, which can also be interpreted as transition probability from node $i$ to each of its neighbors.

            -- {\bf Lớp Chú ý Đồ thị.} Lớp chú ý đồ thị xác định cách chuyển các biểu diễn nút mở rộng ở lớp $k - 1$ (ký hiệu là $H^{k-1}\in\mathbb{R}^{N\times F}$) sang các biểu diễn nút mới $H^k\in\mathbb{R}^{N\times F'}$. Để đảm bảo đủ sức mạnh biểu đạt để chuyển đổi các biểu diễn nút cấp thấp hơn sang các biểu diễn nút cấp cao hơn, 1 phép biến đổi tuyến tính chung được áp dụng cho mọi nút, ký hiệu là $W\in\mathbb{R}^{F\times F'}$. Sau đó, sự tự chú ý được định nghĩa trên các nút, đo lường hệ số chú ý cho bất kỳ cặp nút nào thông qua cơ chế chú ý chung $a:\mathbb{R}^{F'}\times\mathbb{R}^{F'}\to\mathbb{R}$: (4.14)
            \begin{equation*}
                e_{ij} = a(WH_i^{k-1},WH_j^{k-1}).
            \end{equation*}
            $e_{ij}$ biểu thị cường độ mối quan hệ giữa nút $i,j$. Trong tiểu mục này, sử dụng $H_i^{k-1}$ để biểu diễn 1 vectơ theo cột thay vì vectơ theo hàng. Đối với mỗi nút, về mặt lý thuyết, có thể cho phép nó chú ý đến mọi nút khác trên đồ thị, tuy nhiên điều này sẽ bỏ qua thông tin cấu trúc đồ thị. 1 giải pháp hợp lý hơn sẽ chỉ chú ý đến các nút lân cận cho mỗi nút. Trong thực tế, các nút lân cận bậc 1 chỉ được sử dụng (bao gồm cả chính nút đó). \& để làm cho các hệ số có thể so sánh được giữa các nút khác nhau, các hệ số chú ý thường được chuẩn hóa bằng hàm softmax:
            \begin{equation*}
                \alpha_{ij} = {\rm Softmax}_j)\{e_{ij}\} = \frac{\exp e_{ij}}{\sum_{l\in N(i)} \exp e_{il}}.
            \end{equation*}
            Đối với 1 nút $i$, $\alpha_{ij}$ về cơ bản xác định 1 phân phối đa thức trên các nút lân cận, cũng có thể được hiểu là xác suất chuyển tiếp từ nút $i$ đến từng nút lân cận của nó.



            \item {\sf4.2.4. Neural Message Passing Networks.}
            \item {\sf4.2.5. Continuous GNNs.}
        \end{itemize}
        \item {\sf4.3. Unsupervised GNNs.}
    \end{itemize}
    \item {\sf5. Expressive Power of GNNs.}
    \item {\sf6. GNNs: Scalability.}
    \item {\sf7. Interpretability in GNNs.}
    \item {\sf8. GNNs: Adversarial Robustness.}

    PART III. FRONTIERS OF GRAPH NEURAL NETWORKS.
    \item {\sf9. Graph Neural Networks: Graph Classification.}
    \item {\sf10. Graph Neural Networks: Link Prediction.}
    \item {\sf11. GNNs: Graph Generation.}
    \item {\sf12. GNNs: Graph Transformation.}
    \item {\sf13. GNNs: Graph Matching.}
    \item {\sf14. GNNs: Graph Structure Learning.}
    \item {\sf15. Dynamic GNNs.}
    \item {\sf16. Heterogeneous GNNs.}
    \item {\sf17. GNNs: AutoML.}
    \item {\sf18. GNNs: Self-supervised Learning.}

    PART IV. BROAD \& EMERGING APPLICATIONS WITH GRAPH NEURAL NETWORKS.
    \item {\sf19. GNNs in Modern Recommender Systems.}
    \item {\sf20. GNNs in Computer Vision.}
    \item {\sf21. GNNs in Natural Language Processing.}
    \item {\sf22. GNNs in Program Analysis.}
    \item {\sf23. GNNs in Software Mining.}
    \item {\sf24. GNN-based Biomedical Knowledge Graph Mining in Drug Development.}
    \item {\sf25. GNNs in Predicting Protein Function \& Interactions.}
    \item {\sf26. GNNs in Anomaly Detection.}
    \item {\sf27. GNNs in Urban Intelligence.}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{Geeks4Geeks{\tt/}GNNs with PyTorch}
\url{https://www.geeksforgeeks.org/deep-learning/graph-neural-networks-with-pytorch/}.

GNNs represent a powerful class of ML models tailored for interpreting data described by graphs. This is particularly useful because many real-world structures are networks composed on interconnected elements, e.g. social networks, molecular structures, \& communication systems. See how can use PyTorch for building GNNs.

%------------------------------------------------------------------------------%

\subsubsection{Implementation of simple GNN model using PyTorch}
Implementing Graph Neural Networks (GNNs) with CORA dataset in PyTorch, specifically using PyTorch Geometric (PyG), involves several steps. A guide through process, including code snippets for each step.

-- Việc triển khai Mạng Nơ-ron Đồ thị (GNN) với tập dữ liệu CORA trong PyTorch, cụ thể là sử dụng PyTorch Geometric (PyG), bao gồm nhiều bước. Hướng dẫn chi tiết quy trình, bao gồm các đoạn mã cho từng bước.
\begin{enumerate}
    \item {\bf Loading CORA Dataset.} CORA dataset is a citation graph where nodes represent documents, \& edges represent citation links. Each document is classified into 1 of 7 categories, making it a popular dataset for node classification tasks in GNNs. 1st, ensure have PyTorch Geometric installed. if not, install it using {\tt pip}:

    -- {\bf Đang tải Bộ dữ liệu CORA.} Bộ dữ liệu CORA là 1 biểu đồ trích dẫn, trong đó các nút đại diện cho tài liệu, \& cạnh đại diện cho liên kết trích dẫn. Mỗi tài liệu được phân loại thành 1 trong 7 loại, khiến nó trở thành 1 bộ dữ liệu phổ biến cho các tác vụ phân loại nút trong GNN. Trước tiên, hãy đảm bảo đã cài đặt PyTorch Geometric. Nếu chưa, hãy cài đặt bằng {\tt pip}:
    \begin{verbatim}
pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric
    \end{verbatim}
    Then load CORA dataset:
    \begin{Verbatim}[numbers=left,xleftmargin=5mm]
import torch
from torch_geometric.data import Planetoid
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

# load Cora dataset
dataset = Planetoid(root = 'data/Planetoid', name = 'Cora')
    \end{Verbatim}
    \item {\bf Defining a GNN Model Using PyTorch.} Define a simple GNN model using 1 of most straightforward types of GNN layers, Graph Convolutional Network (GCN) layer, provided by PyTorch Geometric.

    -- {\bf Định nghĩa mô hình GNN bằng PyTorch.} Định nghĩa mô hình GNN đơn giản bằng 1 trong những loại lớp GNN đơn giản nhất, lớp Mạng tích chập đồ thị (GCN), do PyTorch Geometric cung cấp.

    A custom Graph Neural Network (GNN) model is built using PyTorch's {\tt torch.nn.Module} class. Model consists of 2 Graph Convolutional Network (GCN) layers, each followed by a Rectified Linear Unit (ReLU) activation function \& dropout regularization. Model's `forward' method takes feature data \& edge information as input, applies defined layers sequentially, \& outputs a log-softmax activation for classification. Additionally, an Adam optimizer is initialized to train model with a specified learning rate \& weight decay.

    -- 1 mô hình Mạng Nơ-ron Đồ thị (GNN) tùy chỉnh được xây dựng bằng lớp {\tt torch.nn.Module} của PyTorch. Mô hình bao gồm 2 lớp Mạng Tích chập Đồ thị (GCN), mỗi lớp được theo sau bởi 1 hàm kích hoạt Đơn vị Tuyến tính Chỉnh lưu (ReLU) \& chính quy hóa dropout. Phương thức `forward' của mô hình lấy dữ liệu đặc trưng \& thông tin cạnh làm đầu vào, áp dụng các lớp đã xác định theo trình tự, \& xuất ra 1 phép kích hoạt log-softmax để phân loại. Ngoài ra, 1 trình tối ưu hóa Adam được khởi tạo để huấn luyện mô hình với tốc độ học \& suy giảm trọng số được chỉ định.
    \item {\bf Training GNN model on CORA dataset.} Train model. This involves initializing model, defining optimizer, \& running training loop.

    -- {\bf Huấn luyện mô hình GNN trên tập dữ liệu CORA.} Huấn luyện mô hình. Quá trình này bao gồm khởi tạo mô hình, xác định trình tối ưu hóa \& chạy vòng lặp huấn luyện.
    \item {\bf Evaluating Model's Performance.} This code defines a basic GNN model, trains it on CORA dataset, \& evaluates its accuracy. Model architecture \& training parameters are kept simple for demonstration purposes. In practice, may want to experiment with deeper models, different types of GNN layers, \& other optimization techniques to improve performance.

    -- {\bf Đánh giá Hiệu suất Mô hình.} Mã này định nghĩa 1 mô hình GNN cơ bản, huấn luyện nó trên tập dữ liệu CORA, \& đánh giá độ chính xác của nó. Kiến trúc mô hình \& các tham số huấn luyện được giữ đơn giản cho mục đích trình diễn. Trong thực tế, có thể bạn sẽ muốn thử nghiệm với các mô hình sâu hơn, các loại lớp GNN khác nhau, \& các kỹ thuật tối ưu hóa khác để cải thiện hiệu suất.
\end{enumerate}
Output shows loss value decreasing over 200 epochs of training a GNN on CORA dataset, indicating: model is learning to classify nodes more accurately over time. Final test accuracy of 0.811 demonstrates: trained GNN model can correctly predict class of over 81\% of nodes in test set, showcasing its effectiveness in node classification tasks within graph-structured data.

-- Kết quả đầu ra cho thấy giá trị mất mát giảm dần qua 200 kỷ nguyên huấn luyện GNN trên tập dữ liệu CORA, cho thấy: mô hình đang học cách phân loại các nút chính xác hơn theo thời gian. Độ chính xác kiểm tra cuối cùng là 0,811 cho thấy: mô hình GNN đã huấn luyện có thể dự đoán chính xác lớp của hơn 81\% số nút trong tập kiểm tra, thể hiện hiệu quả của nó trong các tác vụ phân loại nút trong dữ liệu có cấu trúc đồ thị.

%------------------------------------------------------------------------------%

\subsection{Geeks4Geeks{\tt/}What is PyTorch?}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{itemize}
    \item \href{https://www.geeksforgeeks.org/deep-learning/getting-started-with-pytorch/}{Geeks for Geeks{\tt/}what is PyTorch?}
\end{itemize}
PyTorch is a DL library built on Python. It provides GPU acceleration, dynamic computation graphs \& an intuitive interface for DL researchers \& developers. PyTorch follows a ``define-by-run'' approach, i.e., its computational graphs are constructed on fly allowing for better debugging \& model customization.

-- PyTorch là 1 thư viện DL được xây dựng trên Python. Nó cung cấp khả năng tăng tốc GPU, đồ thị tính toán động \& giao diện trực quan cho các nhà nghiên cứu DL \& nhà phát triển. PyTorch áp dụng phương pháp ``xác định bằng chạy'', i.e., các đồ thị tính toán của nó được xây dựng trực tiếp, cho phép gỡ lỗi tốt hơn \& tùy chỉnh mô hình.

%------------------------------------------------------------------------------%

\subsubsection{Key features of PyTorch}

\begin{itemize}
    \item Use dynamic graphs allowing flexibility in model execution \& debugging.
    \item Provide an automatic differentiation engine that simplifies gradient computation for DL.
    \item Support CUDA allowing computations to be performed efficiently on GPUs.
\end{itemize}
PyTorch can be installed on Windows, macOS \& Linux using pip for CPU (without GPU):
\begin{verbatim}
!pip install torch torchvision torchaudio
\end{verbatim}

%------------------------------------------------------------------------------%

\subsubsection{PyTorch tensors}
Tensors are fundamental data structures in PyTorch, similar to NumPy arrays but with GPU acceleration capabilities. PyTorch tensors support automatic differentiation, making them suitable for DL tasks.
\begin{enumerate}
    \item Operations on tensors: element-wise addition, matrix multiplication
    \item Reshape \& transpose tensors
    \item Autograd \& computational graphs: {\tt autograd} module automates gradient calculation for backpropagation. This is crucial in training deep neural networks.

    PyTorch dynamically creates a computational graph that tracks operations \& gradients for backpropagation.
\end{enumerate}
-- Tensor là cấu trúc dữ liệu cơ bản trong PyTorch, tương tự như mảng NumPy nhưng có khả năng tăng tốc GPU. Tensor PyTorch hỗ trợ tính vi phân tự động, phù hợp cho các tác vụ DL.
\begin{enumerate}
    \item Các phép toán trên tensor: phép cộng từng phần tử, phép nhân ma trận
    \item Định hình lại \& chuyển vị tensor
    \item Đồ thị tính toán Autograd \&: Mô-đun {\tt autograd} tự động tính toán gradient cho phép lan truyền ngược. Điều này rất quan trọng trong việc huấn luyện mạng nơ-ron sâu.

    PyTorch tạo động 1 đồ thị tính toán theo dõi các phép toán \& gradient cho phép lan truyền ngược.
\end{enumerate}

%------------------------------------------------------------------------------%

\subsubsection{Building neural networks in PyTorch}
In PyTorch, neural networks are built using {\tt torch.nn} module where:
\begin{itemize}
    \item \verb|nn.Linear(in_features, out_features)| defines a fully connected (dense) layer.
    \item Activation functions like {\tt torch.relu, torch.sigmoid, torch.softmax} are applied between layers.
    \item {\tt forward()} method defines how data moves through network.
\end{itemize}
To built a neural network in PyTorch, create a class that inherits from {\tt torch.nn.Module} \& defines its layers \& forward pass.

-- Trong PyTorch, mạng nơ-ron được xây dựng bằng mô-đun {\tt torch.nn}, trong đó:
\begin{itemize}
    \item \verb|nn.Linear(in_features, out_features)| định nghĩa 1 lớp được kết nối đầy đủ (dense).
    \item Các hàm kích hoạt như {\tt torch.relu, torch.sigmoid, torch.softmax} được áp dụng giữa các lớp.
    \item Phương thức \item {\tt forward()} định nghĩa cách dữ liệu di chuyển qua mạng.
\end{itemize}
Để xây dựng mạng nơ-ron trong PyTorch, hãy tạo 1 lớp kế thừa từ {\tt torch.nn.Module} \& định nghĩa các lớp của nó \& forward pass.

%------------------------------------------------------------------------------%

\subsubsection{Define loss function \& optimizer}
Once define our model, need to specify:
\begin{itemize}
    \item A loss function to measure error.
    \item An optimizer to update weights based on computed gradients.
\end{itemize}
Use {\tt nn.BCELoss()} for binary cross-entropy loss \& used {\tt optim.Adam()} for Adam optimizer to combine benefits of momentum \& adaptive learning rates.

-- Sau khi xác định mô hình, cần chỉ định:
\begin{itemize}
    \item 1 hàm mất mát để đo lỗi.
    \item 1 trình tối ưu hóa để cập nhật trọng số dựa trên các gradient đã tính toán.
\end{itemize}
Sử dụng {\tt nn.BCELoss()} cho mất mát entropy chéo nhị phân \& sử dụng {\tt optim.Adam()} cho trình tối ưu hóa Adam để kết hợp lợi ích của động lượng \& tốc độ học thích ứng.

%------------------------------------------------------------------------------%

\subsubsection{Train model}
Training involves:
\begin{enumerate}
    \item Generating dummy data (100 samples, each with 10 features).
    \item Running a training loop where
    \begin{itemize}
        \item \verb|optimizer.zero_grad()| clears accumulated gradients from previous step.
        \item Forward Pass {\tt model(inputs)} passes inputs through model to generate predictions.
        \item Loss Computation {\tt criterion(outputs, targets)} computes difference between predictions \& actual labels.
        \item Backpropagation {\tt loss.backward()} computes gradients for all weights.
        \item Optimizer Step {\tt optimizer.step()} updates weights based on computed gradients.
    \end{itemize}
\end{enumerate}
-- Quá trình huấn luyện bao gồm:
\begin{enumerate}
    \item Tạo dữ liệu giả (100 mẫu, mỗi mẫu có 10 đặc trưng).
    \item Chạy vòng lặp huấn luyện trong đó
    \begin{itemize}
        \item \verb|optimizer.zero_grad()| xóa các gradient tích lũy từ bước trước.
        \item Forward Pass {\tt model(inputs)} truyền các đầu vào qua mô hình để tạo dự đoán.
        \item Loss Computation {\tt criteria(outputs, targets)} tính toán sự khác biệt giữa các nhãn dự đoán \& nhãn thực tế.
        \item Backpropagation {\tt loss.backward()} tính toán gradient cho tất cả các trọng số.
        \item Optimizer Step {\tt optimizer.step()} cập nhật các trọng số dựa trên các gradient đã tính toán.
    \end{itemize}
\end{enumerate}

%------------------------------------------------------------------------------%

\subsubsection{PyTorch vs. TensorFlow}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Feature & PyTorch & TensorFlow \\
        \hline
        Computational graph & dynamic & static (TF 1.x), dynamic (TF 2.0) \\
        \hline
        ease of use & Pythonic, easy to debug & steeper learning curve \\
        \hline
        performance & fast with eager execution & optimized for large-scale deployment \\
        \hline
        Deployment & TorchScript \& ONNX & TensorFlow Serving \& TensorFlow Lite \\
        \hline
        Popularity in research & widely used & also widely used but more in production \\
        \hline
    \end{tabular}
\end{table}

%------------------------------------------------------------------------------%

\subsubsection{Applications of PyTorch}

\begin{enumerate}
    \item Computer Vision: PyTorch is widely used in image classification, object detection \& segmentation using CNNs \& Transformers (e.g., ViT).
    \item Natural Language Processing (MLP): PyTorch supports transformers, recurrent neural networks (RNNs) \& LSTMs for applications like text generation \& sentiment analysis.
    \item Reinforcement Learning: PyTorch is used in Deep Q-Networks (DQN), Policy Gradient Methods \& Actor-Critic algorithms.
\end{enumerate}
PyTorch is used in industry for computer vision, NLP \& reinforcement learning applications. With its strong community support \& easy-to-use API, it continues to be 1 of leading DL frameworks.
\begin{enumerate}
    \item Thị giác máy tính: PyTorch được sử dụng rộng rãi trong phân loại hình ảnh, phát hiện đối tượng \& phân đoạn bằng CNN \& Bộ biến đổi (ví dụ: ViT).
    \item Xử lý ngôn ngữ tự nhiên (MLP): PyTorch hỗ trợ bộ biến đổi, mạng nơ-ron hồi quy (RNN) \& LSTM cho các ứng dụng như tạo văn bản \& phân tích cảm xúc.
    \item Học tăng cường: PyTorch được sử dụng trong Mạng Q sâu (DQN), Phương pháp Gradient chính sách \& thuật toán Actor-Critic.
\end{enumerate}
PyTorch được sử dụng trong công nghiệp cho các ứng dụng thị giác máy tính, NLP \& học tăng cường. Với sự hỗ trợ cộng đồng mạnh mẽ \& API dễ sử dụng, nó tiếp tục là 1 trong những nền tảng DL hàng đầu.

%------------------------------------------------------------------------------%

\subsection{Geeks for Geeks{\tt/}Understanding {\tt torch.nn.Parameter}}
\textbf{\textsf{Resources -- Tài nguyên.}}
\begin{itemize}
    \item \href{https://www.geeksforgeeks.org/deep-learning/understanding-torchnnparameter/}{Geeks for Geeks{\tt/}understanding {\tt torch.nn.Parameter}}.
\end{itemize}
PyTorch is a widely used library for building \& training neural networks, \& understanding its components is key to effectively using it for ML tasks. 1 of essential classes in PyTorch is  {\tt torch.nn.Parameter}, which plays a crucial role in defining trainable parameters within a model. This article will explore what {\tt torch.nn.Parameter} is, its significance, \& how it is used in PyTorch models.

-- PyTorch là 1 thư viện được sử dụng rộng rãi để xây dựng \& huấn luyện mạng nơ-ron, \& việc hiểu các thành phần của nó là chìa khóa để sử dụng hiệu quả cho các tác vụ ML. 1 trong những lớp thiết yếu trong PyTorch là {\tt torch.nn.Parameter}, đóng vai trò quan trọng trong việc xác định các tham số có thể huấn luyện trong 1 mô hình. Bài viết này sẽ tìm hiểu {\tt torch.nn.Parameter} là gì, ý nghĩa của nó \& cách sử dụng nó trong các mô hình PyTorch.

%------------------------------------------------------------------------------%

\subsubsection{What is {\tt torch.nn.Parameter}?}
{\tt torch.nn.Parameter} is a subclass of {\tt torch.Tensor}, designed specifically for holding parameters in a model that should be considered during training. When a tensor is wrapped with {\tt torch.nn.Parameter}, it automatically becomes a part of model's parameters, \& thus it will be updated when backpropagation is applied during training. This is fundamental because it tells PyTorch's optimizer which tensors should be updated through learning processes.

-- {\tt torch.nn.Parameter} là 1 lớp con của {\tt torch.Tensor}, được thiết kế đặc biệt để lưu trữ các tham số trong mô hình cần được xem xét trong quá trình huấn luyện. Khi 1 tensor được bao bọc bởi {\tt torch.nn.Parameter}, nó sẽ tự động trở thành 1 phần của các tham số mô hình, \& do đó, nó sẽ được cập nhật khi áp dụng lan truyền ngược trong quá trình huấn luyện. Điều này rất quan trọng vì nó cho trình tối ưu hóa của PyTorch biết tensor nào cần được cập nhật thông qua các quy trình học.

%------------------------------------------------------------------------------%

\subsubsection{Key features of {\tt torch.nn.Parameter}}
Here are some key features of {\tt torch.nn.Parameter}:
\begin{enumerate}
    \item {\bf Trainable Parameters}: By default, parameters wrapped in {\tt torch.nn.Parameter} are considered trainable, i.e., they are part of model's learnable parameters \& are subject to updates during gradient descent.
    \item {\bf Integration with Modules}: In PyTorch, models are typically built using {\tt torch.nn.Module} class. Any {\tt torch.nn.Module} assigned as an attribute to a module is automatically registered as a parameter of module.
    \item {\bf Easy Serialization} : Parameters can be easily saved along with other model components using PyTorch's serialization tools, making it easy to save \& load trained models.
\end{enumerate}
-- Dưới đây là 1 số tính năng chính của {\tt torch.nn.Parameter}:
\begin{enumerate}
    \item {\bf Tham số có thể huấn luyện}: Theo mặc định, các tham số được gói trong {\tt torch.nn.Parameter} được coi là có thể huấn luyện, i.e., chúng là 1 phần của các tham số có thể học được của mô hình \& có thể được cập nhật trong quá trình giảm dần gradient.
    \item {\bf Tích hợp với Mô-đun}: Trong PyTorch, các mô hình thường được xây dựng bằng lớp {\tt torch.nn.Module}. Bất kỳ {\tt torch.nn.Module} nào được gán làm thuộc tính cho 1 mô-đun sẽ tự động được đăng ký làm tham số của mô-đun.
    \item {\bf Tuần tự hóa Dễ dàng}: Các tham số có thể dễ dàng được lưu cùng với các thành phần mô hình khác bằng các công cụ tuần tự hóa của PyTorch, giúp việc lưu \& tải các mô hình đã huấn luyện trở nên dễ dàng.
\end{enumerate}

%------------------------------------------------------------------------------%

\subsubsection{Usage of {\tt torch.nn.Parameter}}
To understand how {\tt torch.nn.Parameter} is used, consider a simple example where define a custom module with learnable weights \& bias.

-- Để hiểu cách sử dụng {\tt torch.nn.Parameter}, hãy xem xét 1 ví dụ đơn giản trong đó định nghĩa 1 mô-đun tùy chỉnh với trọng số có thể học được \& bias.
\begin{Verbatim}[numbers=left,xleftmargin=5mm]
# understand torch.nn.parameter
import torch.nn as nn

class my_linear(nn.Module):
    def __init__(self, in_features, out_features):
        super(my_linear, self).__init__()
        # define weight & bias parameters
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.randn(out_features))

    def forward(self, x): # implement forward pass
        return torch.matmul(x, self.weight.t()) + self.bias
\end{Verbatim}
In this example, {\tt self.weight, self.bias} are instances of {\tt torch.nn.Parameter}. During training, these parameters will be updated to minimize loss function, thanks to their registration as module parameters.

-- Trong ví dụ này, {\tt self.weight, self.bias} là các thể hiện của {\tt torch.nn.Parameter}. Trong quá trình huấn luyện, các tham số này sẽ được cập nhật để tối thiểu hóa hàm mất mát, nhờ vào việc đăng ký chúng dưới dạng tham số mô-đun.

%------------------------------------------------------------------------------%

\subsubsection{Step-by-step guide to training a model with {\tt torch.nn.Parameter} in PyTorch}
This section provides a comprehensive explanation \& demonstration of how to use {\tt torch.nn.Parameter} in PyTorch to train a simple neural network. Each step incudes a detailed description along with corresponding code snippets.
\begin{enumerate}
    \item {\bf Import Necessary Libraries.} 1st import required PyTorch modules for neural network construction \& optimization:
    \begin{verbatim}
import torch
import torch.nn as nn
import torch.optim as optim
    \end{verbatim}
    \item {\bf Define Neural Network Class.} Create a custom neural network class {\tt SimpleNet} using {\tt nn.Module}. Define a trainable parameter {\tt self.weight} using {\tt torch.nn.Parameter}.
    \item {\bf Instantiate Model.} Create an instance of {\tt SimpleNet}. Print initial model parameters to verify that weight has been initialized.
    \item {\bf Set Up Loss Function \& Optimizer.} Define loss function \& optimizer for training. Here use Mean Squared Error (MSE) loss \& Stochastic Gradient Descent (SGD) optimizer.
    \item {\bf Prepare Training Data.} Define input \& target tensors. These tensors represent data that model will learn from during training.
    \item {\bf Train Model.} Execute training loop. This loop involves forwarding pass, loss calculation, backpropagation, \& parameter updates. Monitor training process by printing loss \& current weight every 10 epochs.
    \item {\bf Display Final Model Parameters.} After training, print final learned parameters to see how well model has learned to approximate target from input.
    \begin{verbatim}
print("Final Model Parameters:", list(model.parameters()))
    \end{verbatim}
\end{enumerate}
-- Phần này cung cấp giải thích toàn diện \& minh họa cách sử dụng {\tt torch.nn.Parameter} trong PyTorch để huấn luyện 1 mạng nơ-ron đơn giản. Mỗi bước đều bao gồm mô tả chi tiết cùng với các đoạn mã tương ứng.
\begin{enumerate}
    \item {\bf Nhập các thư viện cần thiết.} Đầu tiên, nhập các mô-đun PyTorch cần thiết để xây dựng mạng nơ-ron \& tối ưu hóa:
    \begin{verbatim}
import torch
import torch.nn as nn
import torch.optim as optim
    \end{verbatim}
    \item {\bf Định nghĩa lớp mạng nơ-ron.} Tạo 1 lớp mạng nơ-ron tùy chỉnh {\tt SimpleNet} bằng {\tt nn.Module}. Định nghĩa tham số có thể huấn luyện {\tt self.weight} bằng {\tt torch.nn.Parameter}.
    \item {\bf Khởi tạo mô hình.} Tạo 1 phiên bản của {\tt SimpleNet}. In các tham số mô hình ban đầu để xác minh rằng trọng số đã được khởi tạo.

    \item {\bf Thiết lập Hàm Mất mát \& Bộ Tối ưu hóa.} Định nghĩa hàm mất mát \& bộ tối ưu hóa cho huấn luyện. Ở đây, sử dụng bộ tối ưu hóa MSE (Sai Số Bình phương Trung bình) \& Bộ Tối ưu hóa SGD (Giảm Gradient Ngẫu nhiên).
    \item {\bf Chuẩn bị Dữ liệu Huấn luyện.} Định nghĩa tenxơ đầu vào \& mục tiêu. Các tenxơ này biểu diễn dữ liệu mà mô hình sẽ học trong quá trình huấn luyện.
    \item {\bf Huấn luyện Mô hình.} Thực hiện vòng lặp huấn luyện. Vòng lặp này bao gồm chuyển tiếp, tính toán mất mát, lan truyền ngược, \& cập nhật tham số. Theo dõi quá trình huấn luyện bằng cách in ra giá trị mất mát \& trọng số hiện tại sau mỗi 10 kỷ nguyên.
    \item {\bf Hiển thị Tham số Mô hình Cuối cùng.} Sau khi huấn luyện, in ra các tham số đã học cuối cùng để xem mô hình đã học được bao nhiêu để ước tính mục tiêu từ đầu vào.
    \begin{verbatim}
print("Final Model Parameters:", list(model.parameters()))
    \end{verbatim}
\end{enumerate}

%------------------------------------------------------------------------------%

\subsubsection{Why use {\tt torch.nn.Parameter?}}
Using {\tt torch.nn.Parameter} offers several advantages:
\begin{itemize}
    \item {\bf Explicitness}: It makes it clear which t tensors are intended to be parameters that optimizer should update, improving code readability \& maintainability.
    \item {\bf Convenience}: It simplifies implementation of custom layers \& models, as PyTorch handles underlying complexity of parameter updates.
    \item {\bf Compatibility.} Ensures compatibility with various PyTorch functionalities like optimizers, saving, \& loading mechanisms.
\end{itemize}
-- Sử dụng {\tt torch.nn.Parameter} mang lại 1 số lợi thế:
\begin{itemize}
    \item {\bf Tính rõ ràng}: Nó làm rõ các tenxơ t nào được dự định là tham số mà trình tối ưu hóa nên cập nhật, cải thiện khả năng đọc mã \& khả năng bảo trì.
    \item {\bf Tính tiện lợi}: Nó đơn giản hóa việc triển khai các lớp \& mô hình tùy chỉnh, vì PyTorch xử lý được độ phức tạp tiềm ẩn của việc cập nhật tham số.
    \item {\bf Tính tương thích.} Đảm bảo tính tương thích với nhiều chức năng khác nhau của PyTorch như trình tối ưu hóa, cơ chế lưu, \& tải.
\end{itemize}

%------------------------------------------------------------------------------%

\subsubsection{Conclusion}
Understanding \& using {\tt torch.nn.Parameter} is essential for anyone working with PyTorch, especially when implementing custom model components. It provides a structured way to define what parts of a model should learn from data, facilitating development \& training of complex neural networks. With {\tt torch.nn.Parameter}, can ensure: your model's parameters are correctly managed \& updated through training process, leading to more effective learning outcomes.

-- Hiểu \& sử dụng {\tt torch.nn.Parameter} là điều cần thiết cho bất kỳ ai làm việc với PyTorch, đặc biệt là khi triển khai các thành phần mô hình tùy chỉnh. Nó cung cấp 1 phương pháp có cấu trúc để xác định những phần nào của mô hình cần học từ dữ liệu, tạo điều kiện thuận lợi cho việc phát triển \& huấn luyện các mạng nơ-ron phức tạp. Với {\tt torch.nn.Parameter}, bạn có thể đảm bảo: các tham số của mô hình được quản lý chính xác \& cập nhật trong suốt quá trình huấn luyện, mang lại kết quả học tập hiệu quả hơn.

%------------------------------------------------------------------------------%

%------------------------------------------------------------------------------%

\section{GNNs for Combinatorial Optimization}

%------------------------------------------------------------------------------%

\subsection{{\sc Quentin Cappart, Didier Ch\'etelat, Elias B. Khalil, Andrea Lodi, Christopher Morris, Petar Veličković}. Combinatorial Optimization \& Reasoning with GNNs}

\begin{itemize}
    \item {\sf Abstract.} Combinatorial optimization is a well-established area in operations research \& CS. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using ML, especially GNNs, as a key building block for combinatorial tasks, either directly as solvers or by enhancing exact solvers. Inductive bias of GNNs effectively encodes combinatorial \& relational input due to their invariance to permutations \& awareness of input sparsity. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at optimization \& ML researchers.

    -- Tối ưu hóa tổ hợp là 1 lĩnh vực đã được xác lập rõ ràng trong nghiên cứu vận hành \& Khoa học Máy tính. Cho đến gần đây, các phương pháp của nó tập trung vào việc giải quyết các trường hợp bài toán 1 cách riêng lẻ, bỏ qua việc chúng thường xuất phát từ các phân phối dữ liệu liên quan trong thực tế. Tuy nhiên, những năm gần đây đã chứng kiến sự quan tâm ngày càng tăng đối với việc sử dụng máy học (ML), đặc biệt là mạng nơ-ron nhân tạo (GNN), như 1 nền tảng cốt lõi cho các tác vụ tổ hợp, trực tiếp như các bộ giải hoặc bằng cách tăng cường các bộ giải chính xác. Độ lệch quy nạp của GNN mã hóa hiệu quả đầu vào tổ hợp \& quan hệ do tính bất biến của chúng đối với các hoán vị \& nhận thức được tính thưa thớt của đầu vào. Bài báo này trình bày 1 tổng quan khái niệm về những tiến bộ quan trọng gần đây trong lĩnh vực mới nổi này, hướng đến các nhà nghiên cứu tối ưu hóa \& ML.

    {\bf Keywords.} Combinatorial optimization, graph neural networks, reasoning.
   \item {\sf1. Introduction.} Combinatorial optimization (CO) has developed into an interdisciplinary field spanning optimization, operations research, discrete mathematics, \& CS, with many critical real-world applications e.g. vehicle routing or scheduling ; see (Korte \& Vygen, 2012) for a general overview. Intuitively, CO deals with problems that involve optimizing a cost (or objective) function by selecting a subset from a finite set, with the latter encoding constraints on solution space. Although CO problems are generally hard from a complexity theory standpoint due to their discrete, non-convex nature (Karp, 1972), many of them are routinely solved in practice. Historically, optimization \& theoretical CS communities have been focusing on finding optimal (Korte \& Vygen, 2012), heuristic (Boussaı\"id et al., 2013), or approximate (Vazirani, 2010) solutions for individual problem instances. However, in many practical situations of interest, one often must solve problem instances with specific characteristics or patterns. E.g., a trucking company may solve vehicle routing instances for same city daily, with only slight differences across instances in travel times due to varying traffic conditions. Hence, data-dependent algorithms or ML approaches, which may exploit these patterns, have recently gained traction in CO field (Bengio et al., 2021; Gasse et al., 2022). Promise: one can develop faster algorithms for practical cases by exploiting common patterns in given instances.

   -- Tối ưu hóa tổ hợp (CO) đã phát triển thành 1 lĩnh vực liên ngành bao gồm tối ưu hóa, nghiên cứu vận hành, toán học rời rạc, \& CS, với nhiều ứng dụng thực tế quan trọng, ví dụ như định tuyến hoặc lập lịch trình xe cộ; xem (Korte và Vygen, 2012) để biết tổng quan chung. Về mặt trực quan, CO xử lý các bài toán liên quan đến việc tối ưu hóa hàm chi phí (hoặc hàm mục tiêu) bằng cách chọn 1 tập con từ 1 tập hữu hạn, với các ràng buộc mã hóa sau trên không gian nghiệm. Mặc dù các bài toán CO thường khó về mặt lý thuyết độ phức tạp do tính chất rời rạc, không lồi của chúng (Karp, 1972), nhưng nhiều bài toán trong số đó đã được giải quyết thường xuyên trong thực tế. Theo truyền thống, các cộng đồng CS lý thuyết \& tối ưu hóa đã tập trung vào việc tìm kiếm các giải pháp tối ưu (Korte và Vygen, 2012), giải pháp kinh nghiệm (Boussaı\"id và cộng sự, 2013) hoặc giải pháp gần đúng (Vazirani, 2010) cho các trường hợp vấn đề riêng lẻ. Tuy nhiên, trong nhiều tình huống thực tế quan tâm, người ta thường phải giải quyết các trường hợp vấn đề với các đặc điểm hoặc mẫu cụ thể. Ví dụ: 1 công ty vận tải có thể giải quyết các trường hợp định tuyến xe cho cùng 1 thành phố hàng ngày, với chỉ 1 số khác biệt nhỏ giữa các trường hợp về thời gian di chuyển do điều kiện giao thông khác nhau. Do đó, các thuật toán phụ thuộc dữ liệu hoặc các phương pháp ML, có thể khai thác các mẫu này, gần đây đã thu hút được sự chú ý trong lĩnh vực CO (Bengio và cộng sự, 2021; Gasse và cộng sự, 2022). Hứa hẹn: người ta có thể phát triển các thuật toán nhanh hơn cho các trường hợp thực tế bằng cách khai thác các mẫu chung trong các trường hợp nhất định.

   Due to discrete nature of most CO problems \& prevalence of network data in real world, graphs are a central object of study in CO field. E.g., well-known \& relevant problems e.g. Traveling Salesperson problem (TSP) \& other vehicle routing problems naturally induce a graph structure. In fact, from 21 NP-complete problems identified by Karp (1972), 10 are decision versions of graph optimization problems. Most other ones, e.g. set covering problem, can also be modeled over graphs. Moreover, interaction between variables \& constraints in combinatorial optimization problems naturally induces a bipartite graph, i.e., a variable \& constraint share an edge if variable appears with a nonzero coefficient in constraint. These graphs commonly exhibit patterns in their structure \& features, which ML approaches should exploit.

   -- Do tính chất rời rạc của hầu hết các vấn đề CO \& sự phổ biến của dữ liệu mạng trong thế giới thực, đồ thị là đối tượng nghiên cứu trung tâm trong lĩnh vực CO. Ví dụ, các vấn đề \& liên quan nổi tiếng \& ví dụ như bài toán Người bán hàng du lịch (TSP) \& các vấn đề định tuyến phương tiện khác tự nhiên tạo ra 1 cấu trúc đồ thị. Trên thực tế, trong số 21 bài toán NP-đầy đủ được Karp (1972) xác định, 10 bài toán là phiên bản quyết định của các bài toán tối ưu hóa đồ thị. Hầu hết các bài toán khác, ví dụ như bài toán phủ tập hợp, cũng có thể được mô hình hóa trên đồ thị. Hơn nữa, tương tác giữa các biến \& ràng buộc trong các bài toán tối ưu hóa tổ hợp tự nhiên tạo ra 1 đồ thị hai phía, tức là 1 biến \& ràng buộc chia sẻ 1 cạnh nếu biến xuất hiện với hệ số khác không trong ràng buộc. Các đồ thị này thường thể hiện các mẫu trong cấu trúc \& các tính năng của chúng, mà các phương pháp ML nên khai thác.
    \begin{itemize}
        \item {\sf1.1. What Are Challenges for ML?} There are several critical challenges in successfully applying ML methods within CO, especially for problems involving graphs. Graphs exhibit symmetries, i.e., remaining or reordering nodes does not result in different graphs. Hence, for any ML method dealing with graphs, taking into account invariance to permutation is crucial. Combinatorial optimization problem instances are large \& usually sparse, especially those arising from real world. Hence, employed ML method must be scalable \& sparsity aware.  Simultaneously, employed method has to be expressive enough to detect \& exploit relevant patterns in given instance or data distribution. ML method should be capable of handling auxiliary information, e.g. objective \& user-defined constraints. Most of current ML approaches are within supervised regime. They require a large amount of training data to optimize model's parameters. In context of CO, this means solving many possibly hard problem instances, which might prohibit application of these approaches in real-world scenarios. Further, ML method has to generalize beyond its training data, e.g., transferring to instances of different sizes.

        -- {\sf Thách thức đối với ML là gì?} Có 1 số thách thức quan trọng trong việc áp dụng thành công các phương pháp ML trong CO, đặc biệt là đối với các bài toán liên quan đến đồ thị. Đồ thị thể hiện tính đối xứng, nghĩa là việc giữ lại hoặc sắp xếp lại các nút không dẫn đến các đồ thị khác nhau. Do đó, đối với bất kỳ phương pháp ML nào xử lý đồ thị, việc tính đến tính bất biến của hoán vị là rất quan trọng. Các trường hợp bài toán tối ưu hóa tổ hợp thường lớn \& thưa thớt, đặc biệt là những trường hợp phát sinh từ thế giới thực. Do đó, phương pháp ML được sử dụng phải có khả năng mở rộng \& nhận thức được tính thưa thớt. Đồng thời, phương pháp được sử dụng phải đủ biểu đạt để phát hiện \& khai thác các mẫu liên quan trong các trường hợp hoặc phân phối dữ liệu nhất định. Phương pháp ML phải có khả năng xử lý thông tin bổ trợ, ví dụ: các ràng buộc khách quan \& do người dùng xác định. Hầu hết các phương pháp ML hiện tại đều nằm trong chế độ giám sát. Chúng yêu cầu 1 lượng lớn dữ liệu đào tạo để tối ưu hóa các tham số của mô hình. Trong bối cảnh CO, điều này có nghĩa là phải giải quyết nhiều trường hợp bài toán có thể khó, điều này có thể ngăn cản việc áp dụng các phương pháp này trong các tình huống thực tế. Hơn nữa, phương pháp ML phải khái quát hóa vượt ra ngoài dữ liệu đào tạo của nó, ví dụ, chuyển sang các trường hợp có kích thước khác nhau.

        Overall, there is a trade-off between scalability, expressivity, \& generalization, which might conflict. In summary, key challenges are:
        \begin{enumerate}
            \item ML methods that operate on graph data have to be invariant to node permutations. They should also exploit graphs' sparsity.
            \item Models should distinguish critical structural patterns in provided data while still scaling to large real-world instances.
            \item Side information in form of high-dimensional vectors attached to nodes \& edges, i.e., modeling objectives \& additional information, need to be considered.
            \item Models should be data efficient, i.e., they should ideally work without requiring large amounts of labeled data, \& they should be transferable to out-of-sample or out-of-distribution instances.
        \end{enumerate}
        -- Nhìn chung, có sự đánh đổi giữa khả năng mở rộng, khả năng biểu đạt, \& khái quát hóa, và những yếu tố này có thể xung đột. Tóm lại, những thách thức chính là:
        \begin{enumerate}
            \item Các phương pháp ML hoạt động trên dữ liệu đồ thị phải bất biến với các hoán vị nút. Chúng cũng nên khai thác tính thưa thớt của đồ thị.
            \item Các mô hình nên phân biệt các mẫu cấu trúc quan trọng trong dữ liệu được cung cấp trong khi vẫn có thể mở rộng đến các trường hợp thực tế lớn.
            \item Thông tin phụ dưới dạng các vectơ nhiều chiều được gắn vào các nút \& cạnh, tức là các mục tiêu mô hình \& thông tin bổ sung, cần được xem xét.
            \item Các mô hình nên hiệu quả về dữ liệu, nghĩa là lý tưởng nhất là chúng nên hoạt động mà không cần lượng lớn dữ liệu được gắn nhãn, \& chúng nên có thể chuyển giao cho các trường hợp ngoài mẫu hoặc ngoài phân phối.
        \end{enumerate}
        \item {\sf1.2. How Do GNNs Address These Challenges?} GNNs (Gilmer et al., 2017; Scarselli et al., 2009) have recently emerged ss ML architectures that partially address challenges above.

        Key idea underlying GNNs: compute a vectorial representation, e.g., a real vector, of each node in input graph by iteratively aggregating features of neighboring nodes. GNN is trained in an end-to-end fashion against a loss function, using (stochastic) 1st-order optimization techniques to adapt to given data distribution by parameterizing this aggregation step. Promise here: learned vector representation encodes crucial graph structures that help solve a CO problem more efficiently. GNNs are invariant \& equivariant by design, i.e., they automatically exploit invariances or symmetries inherent to given instance or data distribution. Due to their local nature, by aggregating neighborhood information, GNNs naturally exploit sparsity, leading to more scalable models on sparse inputs. Moreover, although scalability is still an issue, they scale linearly with number of edges \& employed parameters while taking multidimensional node \& edge features into account (Gilmer et al., 2017), naturally exploiting cost \& objective function information. However, data-efficiency question is still large open (Morris et al., 2021).

        -- Ý tưởng chính nền tảng của GNN: tính toán biểu diễn vectơ, ví dụ: 1 vectơ thực, của mỗi nút trong đồ thị đầu vào bằng cách tổng hợp lặp lại các đặc điểm của các nút lân cận. GNN được huấn luyện theo kiểu đầu cuối-cuối dựa trên hàm mất mát, sử dụng các kỹ thuật tối ưu hóa bậc nhất (ngẫu nhiên) để thích ứng với phân phối dữ liệu cho trước bằng cách tham số hóa bước tổng hợp này. Lời hứa ở đây: biểu diễn vectơ đã học mã hóa các cấu trúc đồ thị quan trọng giúp giải quyết vấn đề CO hiệu quả hơn. GNN được thiết kế bất biến \& tương đương, tức là chúng tự động khai thác các bất biến hoặc tính đối xứng vốn có trong phân phối dữ liệu hoặc trường hợp cho trước. Do tính chất cục bộ của chúng, bằng cách tổng hợp thông tin lân cận, GNN tự nhiên khai thác tính thưa thớt, dẫn đến các mô hình có khả năng mở rộng hơn trên các đầu vào thưa thớt. Hơn nữa, mặc dù khả năng mở rộng vẫn là 1 vấn đề, nhưng chúng mở rộng tuyến tính với số lượng cạnh \& tham số được sử dụng trong khi tính đến các đặc điểm nút \& cạnh đa chiều (Gilmer và cộng sự, 2017), tự nhiên khai thác thông tin về chi phí \& hàm mục tiêu. Tuy nhiên, câu hỏi về hiệu quả dữ liệu vẫn còn bỏ ngỏ (Morris và cộng sự, 2021).

        Although GNNs have clear limitations, they have already proven useful in CO. In fact, they have already been applied in various settings, either to directly predict a solution or as an integrated component of an existing solver. Extensively investigate both of these aspects within our survey.

        -- Mặc dù GNN có những hạn chế rõ ràng, nhưng chúng đã chứng minh được tính hữu ích trong CO. Trên thực tế, chúng đã được áp dụng trong nhiều bối cảnh khác nhau, hoặc để dự đoán trực tiếp 1 giải pháp, hoặc như 1 thành phần tích hợp của 1 bộ giải hiện có. Hãy nghiên cứu sâu rộng cả hai khía cạnh này trong khảo sát của chúng tôi.

        Perhaps 1 of most widely publicized applications of GNNs in CO at time of writing is work by Mirhoseini et al. (2021), which studies chip placement. Aim: map nodes of a netlist (graph describing desired chip) onto a chip canvas (a bounded 2D space), optimizing final power, performance, \& area. Authors observe this as a combinatorial problem \& tackle it using reinforcement learning. Owning to graph structure of netlist, at core of representation learning pipeline is a GNN, which computes node features in a (permutation-)invariant way. This represents 1st chip placement approach that can quickly generalize to previously unseen netlists, generating optimized placements for Google's TPU accelerators (Jouppi et al., 2017). While this approach has received wide coverage in popular press, believe it only scratches surface of innovations that can be enabled by a careful synergy of GNNs \& CO. Have designed our survey to facilitate future research in this emerging area.

        -- Có lẽ 1 trong những ứng dụng được công bố rộng rãi nhất của GNN trong CO tại thời điểm viết bài này là công trình của Mirhoseini và cộng sự (2021), nghiên cứu về vị trí đặt chip. Mục tiêu: ánh xạ các nút của netlist (đồ thị mô tả chip mong muốn) lên 1 khung chip (không gian 2 chiều bị chặn), tối ưu hóa công suất cuối cùng, hiệu suất và diện tích. Các tác giả coi đây là 1 vấn đề tổ hợp và giải quyết nó bằng cách sử dụng học tăng cường. Nhờ cấu trúc đồ thị của netlist, cốt lõi của quy trình học biểu diễn là GNN, tính toán các đặc điểm của nút theo cách bất biến (hoán vị). Đây là phương pháp tiếp cận vị trí đặt chip đầu tiên có thể nhanh chóng khái quát hóa thành các netlist chưa từng thấy trước đây, tạo ra các vị trí tối ưu cho các bộ tăng tốc TPU của Google (Jouppi và cộng sự, 2017). Mặc dù phương pháp này đã được báo chí đưa tin rộng rãi, nhưng chúng tôi tin rằng nó chỉ mới khai thác bề mặt của những đổi mới có thể được kích hoạt bằng sự phối hợp cẩn thận giữa GNN và CO. Chúng tôi đã thiết kế khảo sát của mình để tạo điều kiện cho các nghiên cứu trong tương lai trong lĩnh vực mới nổi này.
        \item {\sf1.3. Going Beyond Classical Algorithms.} Previous discussion mainly dealt with ML approaches, especially GNNs, replacing \& imitating classical combinatorial algorithms or parts of them, potentially adapting better to specific data distribution of naturally-occurring problem instances. However, classical algorithms heavily depend on human-made pre-processing or feature engineering by abstracting raw, real-world inputs, e.g., specifying underlying graph itself. Discrete graph input, forming basis of most CO problems, is seldom directly induced by raw data, requiring costly \& error-prone feature engineering. This might lead to biases that do not align with real world \&, consequently, imprecise decisions. Such issues have been known have been known as early as 1950s in context of railways network analysis (Harris \& Ross, 1955), but remained out of spotlight of theoretical CS that assumes problems are abstractified, to begin with.

        -- {\sf Vượt ra ngoài các thuật toán cổ điển.} Các cuộc thảo luận trước đây chủ yếu đề cập đến các phương pháp ML, đặc biệt là GNN, thay thế \& mô phỏng các thuật toán tổ hợp cổ điển hoặc 1 phần của chúng, có khả năng thích ứng tốt hơn với phân phối dữ liệu cụ thể của các trường hợp vấn đề xảy ra tự nhiên. Tuy nhiên, các thuật toán cổ điển phụ thuộc rất nhiều vào quá trình tiền xử lý hoặc kỹ thuật đặc trưng do con người tạo ra bằng cách trừu tượng hóa các đầu vào thô, thực tế, ví dụ: chỉ định chính đồ thị cơ bản. Đầu vào đồ thị rời rạc, tạo thành cơ sở của hầu hết các vấn đề CO, hiếm khi được tạo trực tiếp từ dữ liệu thô, đòi hỏi kỹ thuật đặc trưng dễ xảy ra lỗi \& tốn kém. Điều này có thể dẫn đến các sai lệch không phù hợp với thế giới thực \&, do đó, đưa ra các quyết định không chính xác. Những vấn đề như vậy đã được biết đến từ những năm 1950 trong bối cảnh phân tích mạng lưới đường sắt (Harris \& Ross, 1955), nhưng vẫn nằm ngoài tầm chú ý của khoa học máy tính lý thuyết vốn giả định rằng các vấn đề đã được trừu tượng hóa ngay từ đầu.

        In long-term, ML approaches can further enhance CO pipeline, from raw input processing to aiding in solving abstracted CO problems in an end-to-end fashion. Several viable approaches in this direction have been proposed recently, \& survey them in detail, along with motivating examples, in Sect. 3.3.3.

        -- Về lâu dài, các phương pháp tiếp cận ML có thể cải thiện hơn nữa quy trình CO, từ xử lý dữ liệu đầu vào thô đến hỗ trợ giải quyết các vấn đề CO trừu tượng theo cách toàn diện. 1 số phương pháp khả thi theo hướng này đã được đề xuất gần đây, \& xem xét chi tiết chúng, cùng với các ví dụ minh họa, trong Phần 3.3.3.
        \item {\sf1.4. Present work.} In this paper, give an overview of recent advances in using GNNs in context of CO, aiming at both CO \& ML researchers. To this end, thoroughly introduce CO, ML regimes, \& GNNs. Most importantly, give a comprehensive, structured overview of recent applications of GNNs in CO context. Discuss challenges arising from use of GNNs \& future work. Our contributions can be summarized as follows:
        \begin{enumerate}
            \item Provide a comprehensive, structured overview of application of GNNs to CO setting for both heuristic \& exact algorithms.
            \item Survey recent progress in using GNN-based end-to-end algorithmic reasoners.
            \item Highlight shortcomings of GNNs in context of CO \& provide guidelines \& recommendations on how to tackle them.
            \item Provide a list of open research directions to stimulate future research.
        \end{enumerate}
        -- Trong bài báo này, chúng tôi sẽ trình bày tổng quan về những tiến bộ gần đây trong việc sử dụng GNN trong bối cảnh CO, hướng đến cả các nhà nghiên cứu CO \& ML. Để đạt được mục đích này, chúng tôi sẽ giới thiệu kỹ lưỡng về CO, các chế độ ML, \& GNN. Quan trọng nhất, chúng tôi sẽ cung cấp 1 cái nhìn tổng quan toàn diện, có cấu trúc về các ứng dụng gần đây của GNN trong bối cảnh CO. Thảo luận về những thách thức phát sinh từ việc sử dụng GNN \& các nghiên cứu trong tương lai. Những đóng góp của chúng tôi có thể được tóm tắt như sau:
        \begin{enumerate}
            \item Cung cấp 1 cái nhìn tổng quan toàn diện, có cấu trúc về ứng dụng GNN vào thiết lập CO cho cả thuật toán heuristic \& chính xác.
            \item Khảo sát những tiến bộ gần đây trong việc sử dụng các bộ suy luận thuật toán đầu cuối dựa trên GNN.
            \item Nêu bật những hạn chế của GNN trong bối cảnh CO \& cung cấp các hướng dẫn \& khuyến nghị về cách khắc phục chúng.
            \item Cung cấp danh sách các hướng nghiên cứu mở để thúc đẩy nghiên cứu trong tương lai.
        \end{enumerate}
        Believe: reaping benefits of GNNs for CO is a promising research direction. This survey is intended to provide required material for readers eager to discover this field. On other hand, highlight: this survey should not be considered as a recommendation that GNNs is best way to solve CO problems. There are clear limitations \& fundamental challenges to tackle. Besides, results obtained are currently below what can be achieved by traditional CO solvers in most situations. Such limitations are discussed throughout paper \& summarized in Sect. 4.

        -- Tin tưởng: việc tận dụng lợi ích của GNN cho bài toán CO là 1 hướng nghiên cứu đầy hứa hẹn. Khảo sát này nhằm mục đích cung cấp tài liệu cần thiết cho những độc giả mong muốn khám phá lĩnh vực này. Mặt khác, xin lưu ý: khảo sát này không nên được coi là 1 khuyến nghị rằng GNN là giải pháp tốt nhất để giải quyết các vấn đề CO. Có những hạn chế rõ ràng \& những thách thức cơ bản cần giải quyết. Hơn nữa, kết quả thu được hiện nay còn thấp hơn so với những gì các bộ giải CO truyền thống có thể đạt được trong hầu hết các trường hợp. Những hạn chế này được thảo luận trong toàn bộ bài báo \& tóm tắt trong Phần 4.
        \item {\sf1.5. Related work.} Following briefly reviews key papers \& survey efforts involving GNNs \& ML for CO.

        {\bf GNNs.} Graph neural networks (Gilmer et al., 2017; Scarselli et al., 2009) have recently (re-)emerged as leading ML method for graph-structured inputs. Notable instances of this architecture include, e.g., Duvenaud et al. (2015); Hamilton et al. (2017); Veličković et al. (2018), \& spectral approaches proposed by, e.g., Bruna et al. (2014); Defferrard et al. (2016); Kipf \&Welling (2017); Monti et al. (2017) -- all of which descend from early work of Kireev (1995); Merkwirth \&Lengauer (2005); Scarselli et al. (2009); Sperduti \&Starita (1997). Aligned with field's recent rise in popularity, many surveys exist on recent advances in GNN techniques. Some of most recent ones include Chami et al. (2020); Wu et al. (2019); Zhou et al. (2020).

        {\bf Continuous Formulations.} Discrete nature of CO problems makes standard continuous optimization tools unavailable, e.g. 1st- \& 2nd-order gradient methods. However, many problems admit alternative reformulations as nonconvex continuous optimization problems over graphs. Such problems include graph partitioning, maximum cut, minimum vertex cover, maximum independent set, \& maximum clique problems. Some early work at intersection of ML \& combinatorial optimization involves reinterpreting these continuous optimization problems as energy-based training of Hopfield neural networks or self-organizing maps, e.g. in work of Hopfield \& Tank (1985), Durbin \& Willshaw (1987), Ramanujam \&Sadayappan (1995) \& Gold et al. (1996). Although not using GNNs, these works use graphs as a central object. They can be seen as foreshadowing various GNN-based differentiable proxy loss approaches summarized in Sect. 3.1.1.

        -- {\bf Công thức liên tục.} Bản chất rời rạc của các vấn đề CO khiến các công cụ tối ưu hóa liên tục tiêu chuẩn không khả dụng, ví dụ: phương pháp gradient bậc 1 \& 2. Tuy nhiên, nhiều vấn đề cho phép các công thức thay thế như các vấn đề tối ưu hóa liên tục không lồi trên đồ thị. Các vấn đề như vậy bao gồm phân vùng đồ thị, cắt cực đại, phủ đỉnh cực tiểu, tập độc lập cực đại, \& các vấn đề clique cực đại. 1 số công trình ban đầu tại giao điểm của ML \& tối ưu hóa tổ hợp liên quan đến việc diễn giải lại các vấn đề tối ưu hóa liên tục này dưới dạng huấn luyện dựa trên năng lượng của mạng nơ-ron Hopfield hoặc bản đồ tự tổ chức, ví dụ: trong công trình của Hopfield \& Tank (1985), Durbin \& Willshaw (1987), Ramanujam \& Sadayappan (1995) \& Gold et al. (1996). Mặc dù không sử dụng GNN, các công trình này sử dụng đồ thị làm đối tượng trung tâm. Chúng có thể được coi là điềm báo trước các phương pháp tiếp cận mất mát proxy khả vi dựa trên GNN khác nhau được tóm tắt trong Phần 3.1.1.

        {\bf Surveys.} Seminal survey of Smith (1999) centers around use of popular neural network architectures of time, namely Hopfield networks \& self-organizing maps, as a basis for combinatorial heuristics, as described in prev sect. Worth noting: such architectures were mostly used for a single instance at a time rather than being trained over a set of training instances; this may explain their limited success at time. Bengio et al. (2021) give a high-level overview of ML methods for CO, with no special focus on graph-structured input, while Lodi \& Zarpellon (2017) focus on ML for branching in context of mixed-integer programming. Concurrently to our work, Kotary et al. (2021) have categorized various approaches for ML in CO, focusing primarily on end-to-end learning setups \& paradigms, making representation learning -- \& GNNs in particular -- a secondary topic. Moreover, surveys by Mazyavkina et al. (2021); Yang and Whinston (2020) focus on using reinforcement learning for CO. Survey of Vesselinova et al. (2020) deals with ML for network problems arising in telecommunications, focusing on non-exact methods \& not including recent progress. Finally, Lamb et al. (2020) give a high-level overview of application of GNNs in various reasoning tasks, missing out on most recent developments, e.g., algorithmic reasoning direction we study in detail here.

        -- {\bf Khảo sát.} Khảo sát quan trọng của Smith (1999) tập trung vào việc sử dụng các kiến trúc mạng nơ-ron phổ biến theo thời gian, cụ thể là mạng Hopfield \& bản đồ tự tổ chức, làm cơ sở cho các phương pháp tìm kiếm kết hợp, như được mô tả trong phần trước. Điều đáng chú ý: các kiến trúc như vậy chủ yếu được sử dụng cho 1 trường hợp duy nhất tại 1 thời điểm thay vì được đào tạo qua 1 tập hợp các trường hợp đào tạo; điều này có thể giải thích thành công hạn chế của chúng theo thời gian. Bengio và cộng sự (2021) đưa ra tổng quan cấp cao về các phương pháp ML cho CO, không tập trung đặc biệt vào đầu vào có cấu trúc đồ thị, trong khi Lodi \& Zarpellon (2017) tập trung vào ML để phân nhánh trong bối cảnh lập trình số nguyên hỗn hợp. Đồng thời với công trình của chúng tôi, Kotary và cộng sự (2021) đã phân loại các phương pháp tiếp cận khác nhau cho ML trong CO, chủ yếu tập trung vào các thiết lập học tập đầu cuối \& các mô hình, đưa việc học biểu diễn -- \& GNN nói riêng -- thành 1 chủ đề thứ yếu. Hơn nữa, các khảo sát của Mazyavkina và cộng sự (2021); Yang và Whinston (2020) tập trung vào việc sử dụng học tăng cường cho CO. Khảo sát của Vesselinova và cộng sự (2020) đề cập đến ML cho các vấn đề mạng phát sinh trong viễn thông, tập trung vào các phương pháp không chính xác \& không bao gồm những tiến bộ gần đây. Cuối cùng, Lamb và cộng sự (2020) đưa ra tổng quan cấp cao về ứng dụng của GNN trong các tác vụ suy luận khác nhau, bỏ qua hầu hết các phát triển gần đây, ví dụ như hướng suy luận thuật toán mà chúng tôi sẽ nghiên cứu chi tiết tại đây.
        \item {\sf1.6. Outline.} Start by giving necessary background on CO \& relevant optimization frameworks, ML, \& GNNs; see Sect. 2. In Sect. 3, review recent research using GNNs in CO context. Specifically, in Sect. 3.1, survey works aiming at finding primal solutions, i.e., high-quality feasible solutions to CO problems, while Sect. 3.2 gives an overview of works aiming at enhancing dual methods, i.e., proving optimality of solutions. Going beyond that, Sect. 3.3 reviews recent research trying to facilitate algorithmic reasoning behavior in GNNs, as well as applying GNNs as raw-input combinatorial optimizers. Finally, Sect. 4 discusses limits of current approaches \& offers a list of research directions intending to stimulate future research.

        -- Bắt đầu bằng cách cung cấp kiến thức nền tảng cần thiết về CO \& các khuôn khổ tối ưu hóa liên quan, ML, \& GNN; xem Mục 2. Trong Mục 3, hãy xem xét các nghiên cứu gần đây sử dụng GNN trong bối cảnh CO. Cụ thể, trong Mục 3.1, các công trình khảo sát nhằm tìm kiếm các giải pháp nguyên thủy, tức là các giải pháp khả thi chất lượng cao cho các bài toán CO, trong khi Mục 3.2 cung cấp tổng quan về các công trình nhằm cải thiện các phương pháp đối ngẫu, tức là chứng minh tính tối ưu của các giải pháp. Ngoài ra, Mục 3.3 xem xét các nghiên cứu gần đây cố gắng tạo điều kiện cho hành vi suy luận thuật toán trong GNN, cũng như ứng dụng GNN làm bộ tối ưu hóa tổ hợp đầu vào thô. Cuối cùng, Mục 4 thảo luận về các giới hạn của các phương pháp tiếp cận hiện tại \& đưa ra danh sách các hướng nghiên cứu nhằm kích thích các nghiên cứu trong tương lai.
    \end{itemize}
    \item {\sf2. Preliminaries.} Here introduce notation \& give necessary formal background on combinatorial optimization, different ML regimes, \& GNNs.
    \begin{itemize}
        \item {\sf2.1. Notation.} Let $[n] = \{1,\ldots,n\}\subset\mathbb{N}$ for $n\ge1$, \& let $\{\{\ldots\}\}$ denote a multiset. For a (finite) set $S$, denote its power set as $2^S$. A graph $G$ is a pair $(V,E)$ with a finite set of nodes $V$ \& a set of edges $E\subseteq V\times V$. Denote set of nodes \& set of edges of $G$ by $V(G),E(G)$, resp. A labeled graph $G$ is a triplet $(V,E,l)$ with a label function $l:V(G)\cup E(G)\to\Sigma$, where $\Sigma$ is some finite alphabet. Then $l(x)$ is label of $x$, for $x\in V(G)\cup E(G)$. Note $x$ here can be either a node or an edge. Neighborhood of $v$ in $V(G)$ is denoted by $N(v) = \{u\in V(G);(v,u)\in E(G)\}$. A tree is a connected graph without cycles.

        -- Cho $[n] = \{1,\ldots,n\}\subset\mathbb{N}$ với $n\ge1$, \& cho $\{\{\ldots\}\}$ biểu thị 1 đa tập. Với 1 tập (hữu hạn) $S$, hãy ký hiệu tập lũy thừa của nó là $2^S$. 1 đồ thị $G$ là 1 cặp $(V,E)$ với 1 tập hữu hạn các nút $V$ \& 1 tập các cạnh $E\subseteq V\times V$. Ký hiệu tập các nút \& tập các cạnh của $G$ theo $V(G),E(G)$, tương ứng. 1 đồ thị có nhãn $G$ là 1 bộ ba $(V,E,l)$ với hàm nhãn $l:V(G)\cup E(G)\to\Sigma$, trong đó $\Sigma$ là 1 bảng chữ cái hữu hạn. Khi đó $l(x)$ là nhãn của $x$, với $x\in V(G)\cup E(G)$. Lưu ý $x$ ở đây có thể là 1 nút hoặc 1 cạnh. Lân cận của $v$ trong $V(G)$ được ký hiệu là $N(v) = \{u\in V(G);(v,u)\in E(G)\}$. Cây là 1 đồ thị liên thông không có chu trình.

        Say 2 graphs $G,H$ are isomorphic if there exists an edge-preserving bijection $\varphi:V(G)\to V(H)$, i.e., $(u,v)\in E(G)\Leftrightarrow(\varphi(u),\varphi(v))\in E(H)$. For labeled graphs, further require $l(v) = l(\varphi(v))$ for $v\in V(G)$ \& $l((u,v)) = l((\varphi(u),\varphi(v)))$ for $(u,v)\in E(G)$.

        -- Giả sử 2 đồ thị $G,H$ là đẳng cấu nếu tồn tại 1 song ánh bảo toàn cạnh $\varphi:V(G)\to V(H)$, tức là $(u,v)\in E(G)\Leftrightarrow(\varphi(u),\varphi(v))\in E(H)$. Đối với đồ thị có nhãn, cần thêm $l(v) = l(\varphi(v))$ với $v\in V(G)$ \& $l((u,v)) = l((\varphi(u),\varphi(v)))$ với $(u,v)\in E(G)$.
        \item {\sf2.2. Combinatorial optimization.} CO deals with problems that involve optimizing a cost (or objective) function by selecting a subset from a finite set, with latter encoding constraints on solution space. Formally, define an instance of a combinatorial optimization problem as follows.

        -- CO xử lý các bài toán liên quan đến việc tối ưu hóa hàm chi phí (hoặc hàm mục tiêu) bằng cách chọn 1 tập con từ 1 tập hữu hạn, sau đó mã hóa các ràng buộc trên không gian nghiệm. Định nghĩa 1 cách hình thức 1 bài toán tối ưu hóa tổ hợp như sau.

        \begin{definition}[Combinatorial optimization instance]
               An instance of a {\rm combinatorial optimization problem} is a tuple $(\Omega,F,w)$, where $\Omega$ is a finite set, $F\subseteq2^\Omega$ is set of feasible solutions, $c:2^\Omega\to\mathbb{R}$ is a {\rm cost function} with $c(S) = \sum_{\omega\in S} w(\omega)$ for $S\in F$.
        \end{definition}
        Consequently, CO deals with selecting an element $S^*$ (optimal solution) in $F$ that minimizes $c$ over feasible set $F$. [W.l.o.g. choose minimization instead of maximization.] Corresponding decision problem asks if there exists an element in feasible set s.t. its cost is $\le$ a given value, i.e., whether there exists $S\in F$ s.t. $c(S)\le k$, i.e., require a Yes{\tt/}No answer.

        -- Do đó, CO xử lý việc lựa chọn 1 phần tử $S^*$ (giải pháp tối ưu) trong $F$ sao cho tối thiểu hóa $c$ trên tập hợp khả thi $F$. [W.l.o.g. chọn tối thiểu hóa thay vì tối đa hóa.] Bài toán quyết định tương ứng hỏi liệu có tồn tại 1 phần tử trong tập hợp khả thi s.t. hay không, chi phí của nó là $\le$ 1 giá trị cho trước, tức là liệu có tồn tại $S\in F$ s.t. $c(S)\le k$ hay không, tức là yêu cầu câu trả lời Có{\tt/}Không.

        TSP is a well-known CO problem aiming at finding a cycle along edges of a graph with minimal cost that visits each node exactly once; see {\sf Fig. 1: A complete graph with edge labels (blue \& red) \& its optimal solution for TSP (in gree). Blue edges have a cost of 1 \& red edges a cost of 2.} for an illustration of an instance of TSP problem \& its optimal solution. Corresponding decision problem asks whether a cycle exists along edges of a graph with cost $\le k$ that visits each node exactly once.

        -- TSP là 1 bài toán CO nổi tiếng, nhằm mục đích tìm 1 chu trình dọc theo các cạnh của 1 đồ thị có chi phí tối thiểu, đi qua mỗi nút đúng 1 lần; xem {\sf Hình 1: 1 đồ thị hoàn chỉnh với các nhãn cạnh (xanh lam \& đỏ) \& giải pháp tối ưu cho TSP (bằng màu xanh lục). Các cạnh xanh lam có chi phí là 1 \& các cạnh đỏ có chi phí là 2.} để minh họa 1 ví dụ về bài toán TSP \& giải pháp tối ưu của nó. Bài toán quyết định tương ứng hỏi liệu có tồn tại 1 chu trình dọc theo các cạnh của 1 đồ thị có chi phí $\le k$ đi qua mỗi nút đúng 1 lần hay không.

        \begin{example}[Traveling Salesperson Problem]
            \item {\sf Input.} A complete directed graph $G$, i.e., $E(G) = \{(u,v);u,v\in V(G)\}$, with edge costs $w:E(G)\to\mathbb{R}$.
            \item {\sf Output.} A permutation of nodes $\sigma:\{0,\ldots,n - 1\}\to V$ s.t. $\sum_{i=0}^{n-1} w((\sigma(i),\sigma((i + 1)\mod n)))$ is minimal over all permutations, where $n = |V|$.

            \item {\sf Đầu vào.} 1 đồ thị có hướng đầy đủ $G$, tức là $E(G) = \{(u,v);u,v\in V(G)\}$, với chi phí cạnh $w:E(G)\to\mathbb{R}$.
            \item {\sf Đầu ra.} 1 hoán vị của các nút $\sigma:\{0,\ldots,n - 1\}\to V$ s.t. $\sum_{i=0}^{n-1} w((\sigma(i),\sigma((i + 1)\mod n)))$ là tối thiểu trên tất cả các hoán vị, trong đó $n = |V|$.
        \end{example}
        Due to their discrete nature, many classes or sets of combinatorial decision problems arising in practice, e.g., TSP or other vehicle routing problems, are NP-hard (Korte and Vygen, 2012), \& hence likely intractable in worst-case sense. However, instances are routinely solved in practice by formulating them as integer linear optimization problems or integer linear programs (ILPs), constrained problems, or as satisfiability problems (SAT) \& utilizing well-engineered algorithms (\& associated solvers) for these problems, e.g., branch-\&-cut algorithms in case of ILPs.

        -- Do tính chất rời rạc của chúng, nhiều lớp hoặc tập hợp các bài toán quyết định tổ hợp phát sinh trong thực tế, ví dụ như TSP hoặc các bài toán định tuyến phương tiện khác, là NP-khó (Korte và Vygen, 2012), \& do đó có khả năng khó giải quyết trong trường hợp xấu nhất. Tuy nhiên, các trường hợp này thường được giải quyết trong thực tế bằng cách xây dựng chúng dưới dạng các bài toán tối ưu tuyến tính số nguyên hoặc chương trình tuyến tính số nguyên (ILP), các bài toán ràng buộc, hoặc các bài toán khả năng thỏa mãn (SAT) \& sử dụng các thuật toán được thiết kế tốt (\& các bộ giải liên quan) cho các bài toán này, ví dụ như thuật toán cắt nhánh trong trường hợp ILP.
        \item {\sf2.3. General Optimization Frameworks: ILPs, SAT, \& Constrained Problems.} Describe common modeling \& algorithmic frameworks for CO problems in following. More precisely, next 3 sects describe modeling approaches: integer programming, SAT, \& constraint satisfaction{\tt/}optimization. Finally, Sect. 2.3.4 partitions algorithmic frameworks into 3 categories. Note: this sect is not an exhaustive list of optimization approaches but serves as an introduction to main frameworks.

        -- {\sf. Khung Tối ưu hóa Chung: ILP, SAT, \& Bài toán Ràng buộc.} Mô tả các khung thuật toán \& mô hình hóa phổ biến cho các bài toán CO sau đây. Chính xác hơn, 3 phần tiếp theo mô tả các phương pháp mô hình hóa: lập trình số nguyên, SAT, \& tối ưu hóa thỏa mãn ràng buộc. Cuối cùng, Phần 2.3.4 phân chia các khung thuật toán thành 3 loại. Lưu ý: phần này không phải là danh sách đầy đủ các phương pháp tối ưu hóa mà chỉ là phần giới thiệu về các khung chính.
        \begin{itemize}
            \item {\sf2.3.1. Integer linear programs \& mixed-integer programs.} 1st, define a linear program or linear optimization problem. A linear program aims at optimizing a linear cost function over a feasible set described as intersection of finitely many half-spaces, i.e., a polyhedron. Formally, define an instance of a linear program as follows.

            -- {\sf Chương trình tuyến tính nguyên \& Chương trình số nguyên hỗn hợp.} Trước tiên, hãy định nghĩa 1 chương trình tuyến tính hoặc bài toán tối ưu tuyến tính. 1 chương trình tuyến tính nhằm mục đích tối ưu hóa 1 hàm chi phí tuyến tính trên 1 tập hợp khả thi được mô tả là giao của hữu hạn các nửa không gian, tức là 1 đa diện. Định nghĩa 1 cách hình thức 1 thể hiện của chương trình tuyến tính như sau.

            \begin{definition}[Linear programming instance]
                An instance of a {\rm linear program} (LP) is a tuple $(A,b,c)$, where $A$ is a matrix in $\mathbb{R}^{m\times n}$, $b,c$ are vectors in $\mathbb{R}^m,\mathbb{R}^n$, resp.
            \end{definition}

            \begin{dinhnghia}[Thể hiện lập trình tuyến tính]
                1 thể hiện của {\rm chương trình tuyến tính} (LP) là 1 bộ $(A,b,c)$, trong đó $A$ là 1 ma trận trong $\mathbb{R}^{m\times n}$, $b,c$ là các vectơ trong $\mathbb{R}^m,\mathbb{R}^n$, tương ứng.
            \end{dinhnghia}
            Associated optimization problem asks to minimize a linear objective over a polyhedron. [In above def, assumed: LP is feasible, i.e., $X\ne\emptyset$, \& a finite minimum value exists. In what follows, assume: both conditions are always fulfilled.] I.e., aim at finding a vector $x\in\mathbb{R}^n$ that minimizes $c^\top x$ over feasible set
            \begin{equation*}
                X = \{x\in\mathbb{R}^n;A_jx\le b_j\mbox{ for } j\in[m],\ x_i\ge0\mbox{ for } i\in[n]\}.
            \end{equation*}
            In practice, LPs are solved using Simplex method or polynomial-time interior-point methods (Bertsimas and Tsitsiklis, 1997). Due to their continuous nature, LPs cannot encode feasible set of a CO problem. Hence, extend LPs by adding integrality constraints, i.e., requiring: value assigned to each variable is an integer. Consequently, aim to find vector $x\in\mathbb{Z}^n$ that minimizes $c^\top x$ over feasible set
            \begin{equation*}
                X = \{x\in\mathbb{Z}^n;A_jx\le b_j\mbox{ for } j\in[m],\ x_i\ge0,\ x_i\in\mathbb{Z}\mbox{ for } i\in[n]\}.
            \end{equation*}
            Such integer linear optimization problems are solved by tree search algorithms, e.g., branch-\&-bound algorithms. By dropping integrality constraints, again obtain an instance of an LP, which recall relaxation. Solving LP relaxation of an ILP provides a valid lower bound on optimal solution of problem, i.e., an optimistic approximation, \& quality of such an approximation is largely responsible of effectiveness of search scheme.

            -- Bài toán tối ưu hóa liên quan yêu cầu tối thiểu hóa 1 mục tiêu tuyến tính trên 1 đa diện. [Trong định nghĩa trên, giả sử: LP khả thi, tức là $X\ne\emptyset$, \& tồn tại 1 giá trị cực tiểu hữu hạn. Trong phần tiếp theo, giả sử: cả hai điều kiện luôn được thỏa mãn.] Tức là, mục tiêu là tìm 1 vectơ $x\in\mathbb{R}^n$ tối thiểu hóa $c^\top x$ trên tập khả thi
            \begin{equation*}
                X = \{x\in\mathbb{R}^n;A_jx\le b_j\mbox{ với } j\in[m],\ x_i\ge0\mbox{ với } i\in[n]\}.
            \end{equation*}
            Trong thực tế, LP được giải bằng phương pháp Simplex hoặc phương pháp điểm trong thời gian đa thức (Bertsimas và Tsitsiklis, 1997). Do tính chất liên tục của chúng, LP không thể mã hóa tập khả thi của 1 bài toán CO. Do đó, hãy mở rộng LP bằng cách thêm các ràng buộc tích phân, tức là yêu cầu: giá trị được gán cho mỗi biến là 1 số nguyên. Do đó, hãy tìm vectơ $x\in\mathbb{Z}^n$ sao cho $c^\top x$ là nhỏ nhất trên tập hợp khả thi
            \begin{equation*}
                X = \{x\in\mathbb{Z}^n;A_jx\le b_j\mbox{ với } j\in[m],\ x_i\ge0,\ x_i\in\mathbb{Z}\mbox{ với } i\in[n]\}.
            \end{equation*}
            Các bài toán tối ưu tuyến tính số nguyên như vậy được giải bằng các thuật toán tìm kiếm cây, ví dụ như các thuật toán nhánh-\&-bound. Bằng cách loại bỏ các ràng buộc tích phân, 1 lần nữa thu được 1 thể hiện của LP, gợi nhớ đến sự thư giãn. Giải quyết việc nới lỏng LP của ILP cung cấp 1 giới hạn dưới hợp lệ cho giải pháp tối ưu của vấn đề, tức là 1 phép tính gần đúng lạc quan, \& chất lượng của phép tính gần đúng như vậy phần lớn chịu trách nhiệm cho hiệu quả của lược đồ tìm kiếm.

            \begin{example}
                Provide an ILP that encodes all feasible solutions of TSP \& selects optimal one due to objective function. Essentially, it encodes order of nodes or cities within its variables. Thereto, let
                \begin{equation*}
                    x_{ij} = \left\{\begin{split}
                        &1&&\mbox{if the cycle goes from city $i$ to city $j$},\\
                        &0&&\mbox{otherwise},
                    \end{split}\right.
                \end{equation*}
                \& let $w_{ij} > 0$ be cost or distance of traveling from city $i$ to city $j$, $i\ne j$. Then TSP can be written as following ILP [Technically, presented TSP model is for asymmetric version, where costs $w_{ij},w_{ji}$ might be different. Such a TSP version is represented in a directed graph. Instead, version in Fig. 1 is symmetric, i.e., $w_{ij} = w_{ji}$, \& it is represented on an undirected graph.]
                \begin{equation*}
                    \min\sum_{i=1}^n\sum_{j=1,j\in i}^n w_{ij}x_{ij}\mbox{ subject to }\sum_{i=1,i\ne j}^n x_{ij} = 1,\ \forall j\in[n],\ \sum_{j=1,\ j\ne i}^n x_{ij} = 1,\ \forall i\in[n],\ \sum_{i\in Q}\sum_{j\notin Q} x_{ij}\ge1,\ \forall Q\subsetneq[n],\ |Q|\ge2.
                \end{equation*}
                1st 2 constraints encode that each city should have exactly 1 in-going \& out-going edge, resp. Last constraint ensures: all cities are within same tour,  i.e., no sub-tours exist (thus, returned solution is not a collection of smaller tours).

                -- Hãy cung cấp 1 ILP mã hóa tất cả các giải pháp khả thi của TSP \& chọn ra giải pháp tối ưu dựa trên hàm mục tiêu. Về cơ bản, nó mã hóa thứ tự các nút hoặc thành phố trong các biến của nó. Do đó, hãy cho
                \begin{equation*}
                    x_{ij} = \left\{\begin{split}
                        &1&&\mbox{nếu chu trình đi từ thành phố $i$ đến thành phố $j$},\\
                        &0&&\mbox{nếu ngược lại},
                    \end{split}\right.
                \end{equation*}
                \& cho $w_{ij} > 0$ là chi phí hoặc khoảng cách di chuyển từ thành phố $i$ đến thành phố $j$, $i\ne j$. Khi đó, TSP có thể được viết dưới dạng ILP sau [Về mặt kỹ thuật, mô hình TSP được trình bày dành cho phiên bản bất đối xứng, trong đó chi phí $w_{ij},w_{ji}$ có thể khác nhau. Phiên bản TSP như vậy được biểu diễn dưới dạng đồ thị có hướng. Thay vào đó, phiên bản trong Hình 1 là đối xứng, tức là $w_{ij} = w_{ji}$, \& nó được biểu diễn trên 1 đồ thị vô hướng.]
                \begin{equation*}
                    \min\sum_{i=1}^n\sum_{j=1,j\in i}^n w_{ij}x_{ij}\mbox{ tùy thuộc vào }\sum_{i=1,i\ne j}^n x_{ij} = 1,\ \forall j\in[n],\ \sum_{j=1,\ j\ne i}^n x_{ij} = 1,\ \forall i\in[n],\ \sum_{i\in Q}\sum_{j\notin Q} x_{ij}\ge1,\ \forall Q\subsetneq[n],\ |Q|\ge2.
                \end{equation*}
                2 ràng buộc đầu tiên mã hóa rằng mỗi thành phố phải có đúng 1 cạnh vào \& ra, tương ứng. Ràng buộc cuối cùng đảm bảo: tất cả các thành phố đều nằm trong cùng 1 hành trình, tức là không có hành trình con nào tồn tại (do đó, nghiệm trả về không phải là tập hợp các hành trình nhỏ hơn).
            \end{example}
            In practice, one often faces problems consisting of integer \& continuous variables. These are commonly known as mixed-integer programs (MIPs). Formally, given $p\in\mathbb{N}^\star$, MIPs aim at finding a vector $x\in\mathbb{R}^n$ that minimizes $c^\top x$ over feasible set
            \begin{equation*}
                X = \{x\in\mathbb{R}^n;A_jx\le b_j\mbox{ for } j\in[m],\ x_i\ge0\mbox{ for } i\in[n],\ x\in\mathbb{Z}^p\times\mathbb{R}^{n - p}\}.
            \end{equation*}
            Here $n$ is number of variables we optimize, out of which $p$ must be integers.

            -- Trong thực tế, người ta thường gặp phải các bài toán bao gồm các biến số nguyên và biến liên tục. Chúng thường được gọi là các chương trình số nguyên hỗn hợp (MIP). Về mặt hình thức, với $p\in\mathbb{N}^\star$, MIP hướng đến việc tìm 1 vectơ $x\in\mathbb{R}^n$ sao cho tối thiểu hóa $c^\top x$ trên 1 tập hợp khả thi
            \begin{equation*}
                X = \{x\in\mathbb{R}^n;A_jx\le b_j\mbox{ với } j\in[m],\ x_i\ge0\mbox{ với } i\in[n],\ x\in\mathbb{Z}^p\times\mathbb{R}^{n - p}\}.
            \end{equation*}
            Ở đây $n$ là số biến chúng ta cần tối ưu hóa, trong đó $p$ phải là số nguyên.
            \item {\sf2.3.2. SAT.} Boolean satisfiability problem (SAT) asks, given a Boolean formula or propositional logic formula, if there exists a variable assignment (assign true or false to variables) s.t. formula evaluates to true. Hence, formally we can define it as follows.

            \begin{definition}[SAT]
                \item {\sf Input.} A propositional logic formula $\varphi$ with variable set $V$.
                \item {\sf Output.} Yes if there exists a variable assignment $A:V\to\{true, false\}$ s.t. formula $\varphi$ evaluates to true; No otherwise.

                \item {\sf Đầu vào.} 1 công thức logic mệnh đề $\varphi$ với tập biến $V$.
                \item {\sf Đầu ra.} Có nếu tồn tại phép gán biến $A:V\to\{true, false\}$ thì công thức $\varphi$ được đánh giá là đúng; Nếu không thì không.
            \end{definition}
            SAT problem was 1st one to be shown to be NP-complete (Cook, 1971). However, modern solvers routinely solve industrial-scale instances in practice (Prasad et al., 2005). Despite simplicity of its formalization, SAT has many practical applications, e.g. hardware verification (Clarke et al., 2003; Gupta et al., 2006), configuration management (Mancinelli et al., 2006; Tucker et al., 2007), or planning (Behnke et al., 2018). A realistic case study of SAT is illustrated in:

            -- Bài toán SAT là bài toán đầu tiên được chứng minh là NP-complete (Cook, 1971). Tuy nhiên, các trình giải hiện đại thường xuyên giải quyết các trường hợp quy mô công nghiệp trong thực tế (Prasad và cộng sự, 2005). Mặc dù hình thức hóa đơn giản, SAT có nhiều ứng dụng thực tế, ví dụ như xác minh phần cứng (Clarke và cộng sự, 2003; Gupta và cộng sự, 2006), quản lý cấu hình (Mancinelli và cộng sự, 2006; Tucker và cộng sự, 2007), hoặc lập kế hoạch (Behnke và cộng sự, 2018). 1 nghiên cứu điển hình thực tế về SAT được minh họa trong:

            \begin{example}
                Consider problem of installing a new (software) package $P$ on a system where installation is subject to dependency \& conflict constraints. Goal: determine which packages must be installed on system s.t. package $P$ is installed in system, dependencies of all installed packages are satisfied, \& there are no conflicts among installed packages. This problem can be conveniently modeled as an SAT problem (Tucker et al., 2007). Formally, [$\ldots$] NQBH: seem relevant to XOR bitwise operations.

                -- Xét bài toán cài đặt 1 gói (phần mềm) mới $P$ trên 1 hệ thống, trong đó quá trình cài đặt phải tuân theo các ràng buộc phụ thuộc \& xung đột. Mục tiêu: xác định những gói nào phải được cài đặt trên hệ thống. Ví dụ: gói $P$ được cài đặt trong hệ thống, các ràng buộc phụ thuộc của tất cả các gói đã cài đặt đều được đáp ứng, \& không có xung đột giữa các gói đã cài đặt. Bài toán này có thể được mô hình hóa 1 cách thuận tiện như 1 bài toán SAT (Tucker và cộng sự, 2007). Về mặt hình thức, [$\ldots$] NQBH: dường như liên quan đến các phép toán XOR bitwise.
            \end{example}
            A natural extension of SAT is maximum satisfiability system (MaxSAT), which aims to determine maximum number of clauses, of a given Boolean formula in conjunctive normal form that can be evaluated to true by assigning truth values to variables.

            -- 1 phần mở rộng tự nhiên của SAT là hệ thống thỏa mãn tối đa (MaxSAT), nhằm mục đích xác định số lượng mệnh đề tối đa của 1 công thức Boolean nhất định ở dạng chuẩn kết hợp có thể được đánh giá là đúng bằng cách gán giá trị chân lý cho các biến.
            \item {\sf2.3.3. Constraint satisfaction \& constraint optimization problems.} This sect presents both constraint satisfaction problems \& constraint optimization problems, most generic way to formalize CO problems. Formally, an instance of a constraint satisfaction problem is defined as follows.

            -- {\sf Bài toán thỏa mãn ràng buộc \& bài toán tối ưu hóa ràng buộc.} Phần này trình bày cả bài toán thỏa mãn ràng buộc \& bài toán tối ưu hóa ràng buộc, cách tổng quát nhất để hình thức hóa các bài toán CO. Về mặt hình thức, 1 ví dụ về bài toán thỏa mãn ràng buộc được định nghĩa như sau.

            \begin{definition}[Constraint satisfaction problem instance]
                An instance of a constraint satisfaction problem (CSP) is a tuple $(X,D(X),C)$, where $X$ is set of variables, $D(X)$ is set of domains of variables, \& $C$: set of constraints that restrict assignments of values to variables. A solution is an assignment of values from $D$ to $X$ that satisfies all constraints of $C$.
            \end{definition}

            \begin{dinhnghia}[Ví dụ về vấn đề thỏa mãn ràng buộc]
                Một ví dụ về bài toán thỏa mãn ràng buộc (CSP) là 1 bộ $(X,D(X),C)$, trong đó $X$ là tập hợp các biến, $D(X)$ là tập hợp các miền xác định của các biến, \& $C$: tập hợp các ràng buộc hạn chế việc gán giá trị cho các biến. Một giải pháp là việc gán các giá trị từ $D$ cho $X$ thỏa mãn mọi ràng buộc của $C$.
            \end{dinhnghia}
            A natural extension of CSPs are constrained optimization problems, i.e., CSPs that also have an objective function. Goal becomes finding a feasible assignment that minimizes objective function. Main difference with previous optimization frameworks: constrained optimization problems do not require underlying assumptions on variables, constraints, \& objective functions. Unlike MIPs, nonlinear objectives \& constraints are applied within this framework. E.g., a TSP model:

            \begin{example}
                Given a configuration with $n$ cities \& a weight matrix $w\in\mathbb{R}^{n\times n}$, TSP can be modeled using $n$ variables $x_i$ over domain $D(x_i)$: $[n]$. Variable $x_i$ indicates $i$th city to be visited. Objective function \& constraints read as
                \begin{equation*}
                    \min w_{x_n,x_1} + \sum_{i=1}^{n-1} w_{x_i,x_{i+1}}\mbox{ subject to } {\rm allDiffrent}(x_1,\ldots,x_n),
                \end{equation*}
                where $allDifferent(X)$ enforces: each variable from $X$ takes a different value (Régin, 1994), \& entries of weight matrix $w$ are indexed using variables. This model forces each city to have another city as a successor \& sums up distances between each pair of consecutive cities along cycle.

                --  Với cấu hình có $n$ thành phố \& ma trận trọng số $w\in\mathbb{R}^{n\times n}$, TSP có thể được mô hình hóa bằng $n$ biến $x_i$ trên miền $D(x_i)$: $[n]$. Biến $x_i$ biểu thị thành phố thứ $i$ cần ghé thăm. Hàm mục tiêu \& ràng buộc được đọc là
                \begin{equation*}
                    \min w_{x_n,x_1} + \sum_{i=1}^{n-1} w_{x_i,x_{i+1}}\mbox{ tuân theo } {\rm allDiffrent}(x_1,\ldots,x_n),
                \end{equation*}
                trong đó $allDifferent(X)$ áp dụng: mỗi biến từ $X$ nhận 1 giá trị khác nhau (Régin, 1994), \& các mục của ma trận trọng số $w$ được lập chỉ mục bằng các biến. Mô hình này buộc mỗi thành phố phải có 1 thành phố khác kế nhiệm \& tổng hợp khoảng cách giữa mỗi cặp thành phố liên tiếp theo chu kỳ.
            \end{example}
            Constrained problems can model arbitrary constraints \& objective functions. This generality makes it possible to use general-purpose solving methods e.g. local search or constraint programming. In addition to their convenience of modeling side, high-level constraints generally referred to as global constraints are also useful on solving side (Régin, 2004). They enable design of efficient algorithms dedicated to pruning search space. Leveraging pruning ability of global constraints is a fundamental component of a constraint programming solver as explained below.

            -- Các bài toán ràng buộc có thể mô hình hóa các ràng buộc \& hàm mục tiêu tùy ý. Tính tổng quát này cho phép sử dụng các phương pháp giải đa năng, ví dụ như tìm kiếm cục bộ hoặc lập trình ràng buộc. Bên cạnh sự tiện lợi về mặt mô hình hóa, các ràng buộc cấp cao, thường được gọi là ràng buộc toàn cục, cũng hữu ích trong việc giải quyết bài toán (Régin, 2004). Chúng cho phép thiết kế các thuật toán hiệu quả chuyên biệt cho việc cắt tỉa không gian tìm kiếm. Việc tận dụng khả năng cắt tỉa của các ràng buộc toàn cục là 1 thành phần cơ bản của 1 bộ giải lập trình ràng buộc như được giải thích dưới đây.
            \item {\sf2.3.4. Solving CO problems.} Major algorithmic frameworks -- whose components \& tasks have been recently considered through GNNs lens -- will be discussed when necessary in core of survey. However, in this sect, briefly distinguish 3 algorithmic categories.

            -- {\sf Giải quyết các vấn đề CO.} Các khuôn khổ thuật toán chính -- với các thành phần \& nhiệm vụ gần đây đã được xem xét dưới góc nhìn của GNN -- sẽ được thảo luận khi cần thiết trong phần cốt lõi của khảo sát. Tuy nhiên, trong phần này, chúng tôi xin phân biệt ngắn gọn 3 loại thuật toán.
            \begin{itemize}
                \item {\bf Exact methods.} ILP models are generally solved to proven optimality (or proof of infeasibility) by variations of branch-\&-bound algorithm (Land and Doig, 1960; Lodi, 2010). Essentially, algorithm is an iterative divide-\&-conquer method that
                \begin{enumerate}
                    \item solves LP relaxations (Sect. 2.3.1),
                    \item improves them through valid inequalities (or cutting planes), \&
                    \item guarantees to find an optimal solution through implicit enumeration performed by branching, see {\sf Fig. 3: Variable selection in branch-\&-bound integer programming algorithm as a MDP.}
                \end{enumerate}
                As anticipated in Sect. 2.3.1, quality of LP relaxation plays a fundamental role in effectiveness of above scheme. Thus, step 2 above is particularly important, especially at beginning of search. Above scheme is called {\it branch \& cut}. If integer programming techniques do not directly model CO problem, then combinatorial versions of branch-\&-bound framework are devised, i.e., featuring relaxations different from LP one, specifically associated with structure of CO problem at hand.

                -- {\bf Phương pháp chính xác.} Các mô hình ILP thường được giải quyết để chứng minh tính tối ưu (hoặc chứng minh tính không khả thi) bằng các biến thể của thuật toán nhánh-\&-bound (Land và Doig, 1960; Lodi, 2010). Về cơ bản, thuật toán là 1 phương pháp chia-\&-trị lặp

                \begin{enumerate}
                    \item giải các phép giãn LP (Phần 2.3.1),
                    \item cải thiện chúng thông qua các bất đẳng thức hợp lệ (hoặc các mặt phẳng cắt), \&
                    \item đảm bảo tìm ra 1 giải pháp tối ưu thông qua phép liệt kê ngầm được thực hiện bằng cách rẽ nhánh, xem {\sf Hình 3: Lựa chọn biến trong thuật toán lập trình số nguyên nhánh-\&-bound như 1 MDP.}
                \end{enumerate}
                Như đã dự đoán trong Phần 2.3.1, chất lượng giãn LP đóng vai trò cơ bản trong hiệu quả của lược đồ trên. Do đó, bước 2 ở trên đặc biệt quan trọng, đặc biệt là khi bắt đầu tìm kiếm. Lược đồ trên được gọi là {\it branch \& cut}. Nếu các kỹ thuật lập trình số nguyên không trực tiếp mô hình hóa vấn đề CO, thì các phiên bản kết hợp của khuôn khổ liên kết nhánh sẽ được đưa ra, tức là, có các biện pháp nới lỏng khác với LP, liên quan cụ thể đến cấu trúc của vấn đề CO đang được xử lý.

                Specifically designed as an exact method for constrained satisfaction \& optimization problems, constraint programming (CP) (Rossi et al., 2006) is a general framework proposing algorithmic solutions also within divide-\&-conquer scheme. It is a complete approach, i.e., possible to prove optimality of solutions found. Solving process consists of a complete enumeration of all possible variable assignments until best solution has been found. To cope with implied (exponentially) large search trees, one utilizes a mechanism called propagation, which reduces number of possibilities. Here, propagation of constraint $c$ removes values from domain violated by $c$. This process is repeated at each domain change \& for each constraint until no value exists any more. A CP solver's efficiency relies heavily on its propagators' quality. Example 4 introduced wellknown AllDifferent constraint. Its propagator (Régin, 1994) is based on maximum matching algorithms in a graph. Many other global constraints are available in literature, e.g. Element constraint, allowing indexation with variables, or Circuit constraint, enforcing a set of variables to create a valid circuit. At time of writing, global constraints catalog reports $> 400$ global constraints (Beldiceanu et al., 2005). CP search commonly proceeds in a depth-1st fashion, together with branch\&-bound. For each feasible solution found, solver adds a constraint, ensuring: following solution has to be better than current one. Upon finding an infeasible solution, search backtracks to previous decision. With this procedure, \& provided: whole search space has been explored, final solution is guaranteed to be optimal.

                -- Được thiết kế đặc biệt như một phương pháp chính xác cho các bài toán tối ưu hóa thỏa mãn ràng buộc, lập trình ràng buộc (CP) (Rossi và cộng sự, 2006) là một khuôn khổ chung đề xuất các giải pháp thuật toán cũng trong lược đồ chia-\&-trị. Đây là một phương pháp tiếp cận hoàn chỉnh, tức là có thể chứng minh tính tối ưu của các giải pháp tìm được. Quá trình giải quyết bao gồm việc liệt kê đầy đủ tất cả các phép gán biến khả thi cho đến khi tìm thấy giải pháp tốt nhất. Để xử lý các cây tìm kiếm lớn hàm ý (theo cấp số nhân), người ta sử dụng một cơ chế gọi là lan truyền, giúp giảm số lượng khả năng. Ở đây, lan truyền ràng buộc $c$ loại bỏ các giá trị khỏi miền bị vi phạm bởi $c$. Quá trình này được lặp lại tại mỗi lần thay đổi miền \& cho mỗi ràng buộc cho đến khi không còn giá trị nào tồn tại nữa. Hiệu quả của bộ giải CP phụ thuộc rất nhiều vào chất lượng của các bộ lan truyền của nó. Ví dụ 4 đã giới thiệu ràng buộc AllDifferent nổi tiếng. Bộ lan truyền của nó (Régin, 1994) dựa trên các thuật toán khớp tối đa trong một đồ thị. Nhiều ràng buộc toàn cục khác có sẵn trong tài liệu, ví dụ: Ràng buộc phần tử, cho phép lập chỉ mục bằng biến, hoặc ràng buộc mạch, áp đặt một tập hợp các biến để tạo ra một mạch hợp lệ. Tại thời điểm viết bài, danh mục ràng buộc toàn cục báo cáo $> 400$ ràng buộc toàn cục (Beldiceanu và cộng sự, 2005). Tìm kiếm CP thường tiến hành theo chiều sâu, kết hợp với ràng buộc nhánh. Với mỗi giải pháp khả thi được tìm thấy, trình giải thêm một ràng buộc, đảm bảo: giải pháp tiếp theo phải tốt hơn giải pháp hiện tại. Khi tìm thấy một giải pháp không khả thi, tìm kiếm sẽ quay lại quyết định trước đó. Với quy trình này, với điều kiện: toàn bộ không gian tìm kiếm đã được khám phá, giải pháp cuối cùng được đảm bảo là tối ưu.

                Finally, although initially designed for solving decision problem, SAT solvers can also be used for combinatorial optimization. 1 way to do that: specify objectives through soft constraints. Objective turns to satisfy as many soft constraints as possible in a solution, e.g. MaxSAT variant. Another option: add a repertoire of commonly used objective functions in solver \& invoke specialized optimization module when it corresponds to objective function that must be optimized. Satisfiability module theories (SMT) solvers, a generalization of SAT, which can handle more complex formulas, generally support both options; see e.g. Z3 solver (de Moura and Bjørner, 2008).

                -- Cuối cùng, mặc dù ban đầu được thiết kế để giải quyết bài toán quyết định, các bộ giải SAT cũng có thể được sử dụng cho tối ưu hóa tổ hợp. Một cách để làm điều đó: xác định mục tiêu thông qua các ràng buộc mềm. Mục tiêu xoay quanh việc thỏa mãn càng nhiều ràng buộc mềm càng tốt trong một giải pháp, ví dụ: biến thể MaxSAT. Một lựa chọn khác: thêm một danh mục các hàm mục tiêu thường dùng vào bộ giải \& gọi mô-đun tối ưu hóa chuyên biệt khi nó tương ứng với hàm mục tiêu cần được tối ưu hóa. Các bộ giải lý thuyết mô-đun thỏa mãn (SMT), một dạng tổng quát của SAT, có thể xử lý các công thức phức tạp hơn, thường hỗ trợ cả hai tùy chọn; xem ví dụ: bộ giải Z3 (de Moura và Bjørner, 2008).
                \item {\bf Local search \& metaheuristics.} Local search (Potvin and Gendreau, 2018) is another algorithmic framework commonly used to solve general, large-scale CO problems. Local search only partially explores solution space in a perturbative fashion \& is thus an incomplete approach that does not provide an optimality guarantee on solution it returns. In its simplest form, search starts from a candidate solution $s$ \& iteratively explores solution space by selecting a neighboring solution until no improvement occurs. Here, a solution's neighborhood is set of solutions obtained by modifying solution $s$. In practice, local search algorithms are improved through metaheuristics concepts (Glover and Kochenberger, 2006), leading to algorithms like simulated annealing (Van Laarhoven and Aarts, 1987; Delahaye et al., 2019), tabu search (Glover and Laguna, 1998; Laguna, 2018), genetic algorithms (Kramer, 2017), variable neighborhood search (Mladenović and Hansen, 1997; Hansen et al., 2019), all of which are designed to help escape local minima.

                -- {\bf Tìm kiếm cục bộ \& siêu thuật toán.} Tìm kiếm cục bộ (Potvin và Gendreau, 2018) là một khuôn khổ thuật toán khác thường được sử dụng để giải các bài toán CO tổng quát, quy mô lớn. Tìm kiếm cục bộ chỉ khám phá một phần không gian nghiệm theo cách nhiễu động \& do đó là một phương pháp chưa hoàn chỉnh, không đảm bảo tính tối ưu cho nghiệm mà nó trả về. Ở dạng đơn giản nhất, tìm kiếm bắt đầu từ một nghiệm ứng viên $s$ \& lặp lại khám phá không gian nghiệm bằng cách chọn một nghiệm lân cận cho đến khi không còn cải thiện nào xảy ra. Ở đây, lân cận của một nghiệm là tập hợp các nghiệm thu được bằng cách sửa đổi nghiệm $s$. Trên thực tế, các thuật toán tìm kiếm cục bộ được cải thiện thông qua các khái niệm siêu thuật toán (Glover và Kochenberger, 2006), dẫn đến các thuật toán như ủ mô phỏng (Van Laarhoven và Aarts, 1987; Delahaye và cộng sự, 2019), tìm kiếm tabu (Glover và Laguna, 1998; Laguna, 2018), thuật toán di truyền (Kramer, 2017), tìm kiếm lân cận biến đổi (Mladenović và Hansen, 1997; Hansen và cộng sự, 2019), tất cả đều được thiết kế để giúp thoát khỏi các cực tiểu cục bộ.
                \item {\bf Approximation algorithms.} Class of approximation algorithms (Vazirani, 2010) is designed to produce, typically in polynomial t ime, feasible solutions for CO problems. Unlike local search \& metaheuristics, value of those feasible solutions is guaranteed to be within a certain bound from optimal one. Notable examples of approximation algorithms are polynomial-time approximation schemes (PTAS) that provide a solution that is within a factor $1 + \epsilon$ (with $\epsilon > 0$ being an input for algorithm) of being optimal (e.g., Arora (1996) for TSP), or fully polynomial-time approximation schemes (FPTAS), where additional conditions on running time of algorithm are imposed (Ausiello et al., 2012).

                -- {\bf Thuật toán xấp xỉ.} Lớp thuật toán xấp xỉ (Vazirani, 2010) được thiết kế để tạo ra, thường trong thời gian đa thức, các giải pháp khả thi cho các bài toán CO. Không giống như tìm kiếm cục bộ \& metaheuristic, giá trị của các giải pháp khả thi này được đảm bảo nằm trong một giới hạn nhất định so với giá trị tối ưu. Các ví dụ đáng chú ý về thuật toán xấp xỉ là các lược đồ xấp xỉ thời gian đa thức (PTAS) cung cấp một giải pháp nằm trong hệ số $1 + \epsilon$ (với $\epsilon > 0$ là đầu vào của thuật toán) để đạt được tính tối ưu (ví dụ: Arora (1996) cho TSP), hoặc các lược đồ xấp xỉ thời gian đa thức đầy đủ (FPTAS), trong đó các điều kiện bổ sung về thời gian chạy của thuật toán được áp dụng (Ausiello và cộng sự, 2012).
            \end{itemize}
        \end{itemize}
        \item {\sf2.4. ML.} This sect gives a short \& concise overview of ML. Cover 3 main branches of field, i.e., supervised learning, unsupervised learning, \& reinforcement learning. For details, see Mohri et al. (2012); Shalev-Shwartz and Ben-David (2014). Moreover, introduce imitation learning highly relevant to CO.

        -- Phần này cung cấp một cái nhìn tổng quan ngắn gọn về Học máy. Bao gồm 3 nhánh chính của lĩnh vực này, tức là học có giám sát, học không giám sát và học tăng cường. Để biết thêm chi tiết, xem Mohri và cộng sự (2012); Shalev-Shwartz và Ben-David (2014). Ngoài ra, giới thiệu về học bắt chước, một phương pháp học tập có liên quan mật thiết đến Học máy.
        \begin{itemize}
            \item {\bf Supervised learning.} +++
        \end{itemize}
        \item {\sf2.5. Graph Neural networks.} +++
    \end{itemize}
    \item {\sf3. GNNs for Combinatorial Optimization: State of Art.} Given that many practically relevant CO problems are NP-hard, helpful to characterize algorithms for solving them as prioritizing 1 of 2 goals. Primal goal of finding good feasible solutions, \& dual goal of certifying optimality or proving infeasibility. In both cases, GNNs can serve as a tool for representing problem instances, states of an iterative algorithm, or both. Not uncommon to combine GNNs' variable or constraint representations with hand-crafted features, which would otherwise be challenging to extract automatically with GNN. Coupled with an appropriate ML paradigm (Sect. 2.4), GNNs have been shown to guide exact \& heuristic algorithms towards finding good feasible solutions faster (Sect. 3.1). GNNs have also been used to guide certifying optimality or infeasibility more efficiently (Sect. 3.2). In this case, GNNs are usually integrated with an existing complete algorithm because an optimality certificate has in general, exponential size concerning problem description size, \& not clear how to devise GNNs with such large outputs. Beyond using standard GNN models for CO, emerging paradigm of algorithmic reasoning provides new perspectives on designing \& training GNNs that satisfy natural invariants \& properties, possibly enabling improved generalization \& interpretability.

    -- {\sf GNN cho Tối ưu hóa Tổ hợp: Tình trạng Nghệ thuật.} Do nhiều bài toán CO có liên quan thực tế là NP-khó, nên việc mô tả các thuật toán để giải chúng theo thứ tự ưu tiên 1 trong 2 mục tiêu là rất hữu ích. Mục tiêu chính là tìm ra các giải pháp khả thi tốt, \& mục tiêu kép là chứng nhận tính tối ưu hoặc chứng minh tính không khả thi. Trong cả hai trường hợp, GNN có thể đóng vai trò là công cụ để biểu diễn các trường hợp bài toán, trạng thái của một thuật toán lặp, hoặc cả hai. Việc kết hợp các biểu diễn biến hoặc ràng buộc của GNN với các đặc trưng thủ công, nếu không sẽ rất khó để trích xuất tự động bằng GNN, là điều không hiếm gặp. Kết hợp với một mô hình ML phù hợp (Phần 2.4), GNN đã được chứng minh là hướng dẫn các thuật toán \& heuristic chính xác để tìm ra các giải pháp khả thi tốt nhanh hơn (Phần 3.1). GNN cũng đã được sử dụng để hướng dẫn việc chứng nhận tính tối ưu hoặc tính không khả thi hiệu quả hơn (Phần 3.2). Trong trường hợp này, GNN thường được tích hợp với một thuật toán hoàn chỉnh hiện có vì chứng chỉ tối ưu nhìn chung có kích thước theo cấp số nhân liên quan đến kích thước mô tả vấn đề, \& không rõ cách thiết kế GNN với đầu ra lớn như vậy. Ngoài việc sử dụng các mô hình GNN tiêu chuẩn cho CO, mô hình lập luận thuật toán mới nổi cung cấp những góc nhìn mới về thiết kế \& huấn luyện GNN thỏa mãn các thuộc tính bất biến tự nhiên, có thể cho phép cải thiện khả năng khái quát hóa \& diễn giải.
    \begin{itemize}
        \item {\sf3.1. On Primal Side: Finding Feasible Solutions.} Begin by discussing use of GNNs in improving solution-finding process in CO. Following practical scenarios motivate need to quickly obtain high-quality feasible solutions, even without optimality or approximation guarantees.

        -- {\sf Về Mặt Nguyên Thủy: Tìm kiếm Giải pháp Khả thi.} Hãy bắt đầu bằng việc thảo luận về việc sử dụng GNN để cải thiện quy trình tìm kiếm giải pháp trong CO. Các tình huống thực tế sau đây thúc đẩy nhu cầu nhanh chóng có được các giải pháp khả thi chất lượng cao, ngay cả khi không có sự đảm bảo về tính tối ưu hoặc xấp xỉ.
        \begin{enumerate}
            \item {\bf Optimality guarantees are often no needed.} A practitioner may only be interested in quality of a feasible solution in absolute terms rather than relative to optimal value of a problem instance. E.g., if objective value represents financial cost, it might only care about how much profit is gained from switching from 1 resolution method to another. In this scenario then, heuristics are all that are needed. Moreover, there are many CO problems which are both practically intractable with an exact solver, \& for which no proxy in form of a good dual bound is easily computable as well. For these problems, a practitioner has no other guide to assess suitability of a resolution method than to compare absolute objective values as well. E.g., many vehicle routing problems admit strong MIP formulations with an exponential number of variables or constraints, similar to TSP formulation in Example 2, see Toth and Vigo (2014). For such problems, simply computing linear relaxation (typically using column or constraint generation (Dror  et al., 1994))is challenging, so a heuristic that consistently finds good solutions within a short user-defined time limit might be preferable.

            -- {\bf Đảm bảo tính tối ưu thường không cần thiết.} Người thực hành có thể chỉ quan tâm đến chất lượng của một giải pháp khả thi theo nghĩa tuyệt đối thay vì giá trị tương đối của một trường hợp bài toán. Ví dụ: nếu giá trị khách quan biểu thị chi phí tài chính, họ có thể chỉ quan tâm đến việc lợi nhuận thu được từ việc chuyển đổi từ phương pháp giải quyết này sang phương pháp khác là bao nhiêu. Trong trường hợp này, chỉ cần sử dụng phương pháp tìm kiếm (heuristics). Hơn nữa, có nhiều bài toán CO vừa khó giải quyết trên thực tế với một trình giải chính xác, vừa không thể tính toán dễ dàng bằng bất kỳ phương pháp đại diện nào dưới dạng một ràng buộc kép tốt. Đối với những bài toán này, người thực hành không có hướng dẫn nào khác để đánh giá tính phù hợp của một phương pháp giải quyết ngoài việc so sánh các giá trị khách quan tuyệt đối. Ví dụ: nhiều bài toán định tuyến xe cộ cho phép các công thức MIP mạnh với số mũ các biến hoặc ràng buộc, tương tự như công thức TSP trong Ví dụ 2, xem Toth và Vigo (2014). Đối với những vấn đề như vậy, việc chỉ tính toán sự giãn nở tuyến tính (thường sử dụng cột hoặc thế hệ ràng buộc (Dror và cộng sự, 1994)) là một thách thức, do đó, một phương pháp tìm kiếm liên tục tìm ra các giải pháp tốt trong giới hạn thời gian ngắn do người dùng xác định có thể là lựa chọn tốt hơn.
        \end{enumerate}
    \end{itemize}
\end{itemize}

%------------------------------------------------------------------------------%

\subsection{{\sc Martin J. A. Schuetz, J. Kyle Brubaker, Helmut G. Katzgraber}. Combinatorial Optimization with Physics-Inspired GNNs}

\begin{itemize}
    \item {\sf Abstract.} Combinatorial optimization problems are pervasive across science \& industry. Modern DL tools ar poised to solve these problems at unprecedented scales, but a unifying framework that incorporates insights from statistical physics is still outstanding. Demonstrate how GNNs can be used to solve combinatorial optimization problems. Our approach is broadly applicable to canonical NP-hard problems in form of quadratic unconstrained binary optimization problems, e.g. maximum cut, minimum vertex cover, maximum independent set, as well Ising spin glasses \& higher-order generalizations thereof in form of polynomial unconstrained binary optimization problems. Apply a relaxation strategy to problem Hamiltonian to generate a differentiable loss function with which we train GNN \& apply a simple projection to integer variables once unsupervised training process has completed. Showcase our approach with numerical results for canonical maximum cut \& maximum independent set problems. Find: GNN optimizer performs on par or outperforms existing solvers, with ability to scale beyond state of art to problems with millions of variables.

    -- Các bài toán tối ưu hóa tổ hợp đang lan rộng trong khoa học \& công nghiệp. Các công cụ DL hiện đại sẵn sàng giải quyết những bài toán này ở quy mô chưa từng có, nhưng 1 khuôn khổ thống nhất kết hợp những hiểu biết từ vật lý thống kê vẫn còn rất mới mẻ. Trình bày cách sử dụng GNN để giải các bài toán tối ưu hóa tổ hợp. Phương pháp của chúng tôi có thể áp dụng rộng rãi cho các bài toán NP-khó chính tắc dưới dạng các bài toán tối ưu hóa nhị phân bậc hai không ràng buộc, e.g.: đường cắt cực đại, độ phủ đỉnh cực tiểu, tập độc lập cực đại, cũng như kính spin Ising \& các tổng quát hóa bậc cao hơn của chúng dưới dạng các bài toán tối ưu hóa nhị phân không ràng buộc đa thức. Áp dụng chiến lược thư giãn cho Hamiltonian bài toán để tạo ra 1 hàm mất mát khả vi mà chúng tôi dùng để huấn luyện GNN \& áp dụng 1 phép chiếu đơn giản lên các biến nguyên sau khi quá trình huấn luyện không giám sát hoàn tất. Trình bày phương pháp của chúng tôi với các kết quả số cho các bài toán đường cắt cực đại chính tắc \& tập độc lập cực đại. Tìm: Trình tối ưu hóa GNN hoạt động ngang bằng hoặc vượt trội hơn các trình giải hiện có, với khả năng mở rộng vượt xa công nghệ tiên tiến cho các bài toán với hàng triệu biến.
    \item {\sf1. Introduction.} Optimization is ubiquitous across science \& industry. Specifically, field of combinatorial optimization -- search for minimum of an objective function within a finite but often large set of candidate solutions -- is 1 of most important areas in field of optimization, with practical (yet notorious challenging) applications found in virtually every industry, including both private \& public sectors, as well as in areas e.g. transportation \& logistics, telecommunications, \& finance. While efficient specialized algorithms exist for specific use cases, most optimization problems remain intractable, especially in real-world applications where problems are more structured \& thus require additional steps to make them amenable to traditional optimization techniques. Despite remarkable advances in both algorithms \& computing power, significant yet generic improvements have remained elusive, generating an increased interest in new optimization approaches that are broadly applicable \& radically different from traditional operations research tools.

    -- Tối ưu hóa hiện diện khắp nơi trong khoa học \& công nghiệp. Cụ thể, lĩnh vực tối ưu hóa tổ hợp -- tìm kiếm giá trị nhỏ nhất của 1 hàm mục tiêu trong 1 tập hợp hữu hạn nhưng thường lớn các giải pháp ứng viên -- là 1 trong những lĩnh vực quan trọng nhất trong lĩnh vực tối ưu hóa, với các ứng dụng thực tế (nhưng đầy thách thức) được tìm thấy trong hầu hết mọi ngành, bao gồm cả khu vực tư nhân \& công cộng, cũng như trong các lĩnh vực như vận tải \& hậu cần, viễn thông \& tài chính. Mặc dù có các thuật toán chuyên biệt hiệu quả cho các trường hợp sử dụng cụ thể, hầu hết các bài toán tối ưu hóa vẫn còn khó giải, đặc biệt là trong các ứng dụng thực tế, nơi các bài toán có cấu trúc hơn \& do đó yêu cầu các bước bổ sung để làm cho chúng phù hợp với các kỹ thuật tối ưu hóa truyền thống. Bất chấp những tiến bộ đáng kể về cả thuật toán \& sức mạnh tính toán, những cải tiến đáng kể nhưng chung chung vẫn còn khó nắm bắt, tạo ra sự quan tâm ngày càng tăng đối với các phương pháp tối ưu hóa mới có thể áp dụng rộng rãi \& khác biệt hoàn toàn so với các công cụ nghiên cứu hoạt động truyền thống.

    In broader physics community, advent of quantum annealing devices e.g. D-Wave Systems Inc. quantum annealers [6--9] has spawned a renewed interest in development of heuristic approaches to solve discrete optimization problems. On 1 hand, recent advances in quantum science \& technology have inspired development of novel classical algorithms, sometimes dubbed nature-inspired or physics-inspired algorithms (e.g., \fbox{simulated quantum annealing} [10, 11] running on conventional CMOS hardware) that have raised bar for emerging quantum annealing hardware [12--15]. On other hand, in parallel to these algorithmic developments, substantial progress has been made in recent years on development of programmable special-purpose devices based on alternative technologies, e.g. coherent Ising machine based on optical parametric oscillators [16, 17], digital MemComputing machines based on self-organizing logic gates [18, 19], \& ASIC-based Fujitsu Digital Annealer [20–22]. Some of these approaches face severe scalability limitations. E.g., in coherent Ising machine there is a trade off between precision \& number of variables \& Fujitsu Digital Annealer -- baked into an ASIC -- can currently handle at most 8192 variables. Thus, of much interest to find new alternate approaches to tackle large-scale combinatorial optimization problems, going far beyond what is currently accessible with quantum \& nature-inspired approaches alike.

    -- Trong cộng đồng vật lý rộng lớn hơn, sự ra đời của các thiết bị ủ lượng tử, ví dụ như máy ủ lượng tử của D-Wave Systems Inc. [6--9] đã làm nảy sinh mối quan tâm mới trong việc phát triển các phương pháp tiếp cận theo phương pháp heuristic để giải quyết các vấn đề tối ưu hóa rời rạc. 1 mặt, những tiến bộ gần đây trong khoa học lượng tử \& công nghệ đã truyền cảm hứng cho sự phát triển của các thuật toán cổ điển mới, đôi khi được gọi là các thuật toán lấy cảm hứng từ thiên nhiên hoặc lấy cảm hứng từ vật lý (e.g.: \fbox{mô phỏng ủ lượng tử} [10, 11] chạy trên phần cứng CMOS thông thường) đã nâng cao tiêu chuẩn cho phần cứng ủ lượng tử mới nổi [12--15]. Mặt khác, song song với những phát triển về thuật toán này, những tiến bộ đáng kể đã được thực hiện trong những năm gần đây về phát triển các thiết bị chuyên dụng có thể lập trình dựa trên các công nghệ thay thế, ví dụ như máy Ising mạch lạc dựa trên bộ dao động tham số quang học [16, 17], máy MemComputing kỹ thuật số dựa trên cổng logic tự tổ chức [18, 19], \& Máy ủ kỹ thuật số Fujitsu dựa trên ASIC [20–22]. 1 số phương pháp này gặp phải những hạn chế nghiêm trọng về khả năng mở rộng. Ví dụ, trong máy Ising mạch lạc, có sự đánh đổi giữa độ chính xác \& số lượng biến \& Fujitsu Digital Annealer -- được tích hợp trong ASIC -- hiện chỉ có thể xử lý tối đa 8192 biến. Do đó, việc tìm kiếm các phương pháp thay thế mới để giải quyết các bài toán tối ưu hóa tổ hợp quy mô lớn, vượt xa những gì hiện có thể tiếp cận được bằng các phương pháp lượng tử \& lấy cảm hứng từ tự nhiên, là điều rất đáng quan tâm.

    In DL community, GNNs have been a burst in popularity over last few years [23–30]. In essence, GNNs are deep neural network architectures specifically designed for graph structure data, with ability to learn effective feature representations of nodes, edges, or even entire graphs. Prime examples of GNN applications include classification of users in social networks [31, 32], prediction of future interactions in recommender systems [33], \& prediction of certain properties of molecular graphs [34, 35]. As a convenient \& general framework to model a variety of real-world complex structural data, GNNs have successfully been applied to a broad set of problems, including recommender systems in social media \& e-commerce [36, 37], detection of misinformation (fake news) in social media [38], \& various domains of natural sciences including event classification in particle physics [39, 40], to name a few. While several specific implementations of GNNs exist [29, 41, 42], at their core typically GNNs iteratively update features of nodes of a graph by aggregating information from their neighbors (often referred to as {\it message passing} [43]) thereby iteratively making local updates to graph structure as training of network progresses. Because of their scalability \& inherent graph-based design, GNNs present an alternate platform to build large-scale combinatorial heuristics.

    -- Trong cộng đồng DL, GNN đã trở nên phổ biến trong vài năm trở lại đây [23–30]. Về bản chất, GNN là kiến trúc mạng nơ-ron sâu được thiết kế riêng cho dữ liệu cấu trúc đồ thị, với khả năng học các biểu diễn đặc trưng hiệu quả của các nút, cạnh hoặc thậm chí toàn bộ đồ thị. Các ví dụ điển hình về ứng dụng GNN bao gồm phân loại người dùng trong mạng xã hội [31, 32], dự đoán các tương tác trong tương lai trong hệ thống đề xuất [33], \& dự đoán 1 số thuộc tính nhất định của đồ thị phân tử [34, 35]. Là 1 khuôn khổ chung thuận tiện để mô hình hóa nhiều loại dữ liệu cấu trúc phức tạp trong thế giới thực, GNN đã được áp dụng thành công cho nhiều vấn đề, bao gồm hệ thống đề xuất trong phương tiện truyền thông xã hội \& thương mại điện tử [36, 37], phát hiện thông tin sai lệch (tin giả) trên phương tiện truyền thông xã hội [38], \& nhiều lĩnh vực khoa học tự nhiên khác nhau bao gồm phân loại sự kiện trong vật lý hạt [39, 40], v.v. Mặc dù có 1 số triển khai cụ thể của GNN [29, 41, 42], nhưng về cơ bản, GNN thường cập nhật lặp lại các đặc điểm của các nút trong đồ thị bằng cách tổng hợp thông tin từ các nút lân cận (thường được gọi là {\it message passing} [43]), do đó thực hiện lặp lại các cập nhật cục bộ cho cấu trúc đồ thị khi quá trình huấn luyện mạng tiến triển. Nhờ khả năng mở rộng \& thiết kế dựa trên đồ thị vốn có, GNN cung cấp 1 nền tảng thay thế để xây dựng các thuật toán tìm kiếm tổ hợp quy mô lớn.

    In this work, present a highly-scalable GNN-based solver to (approximately) solve combinatorial optimization problems with up to millions of variables. Approach is schematically depicted in {\sf Fig. 1:  Schematic illustration of GNN approach for combinatorial optimization presented in this work. Following a recursive neighborhood aggregation scheme, GNN is iteratively trained against a custom loss function that encodes specific optimization problem, e.g., maximum cut. At training completion, project final values for soft node assignments at final GNN layer back to binary variables $x_i = 0,1$, providing solution bit string ${\bf x} = (x_1,x_2,\ldots)$.} \& works as follows: 1st, identify Hamiltonian (cost function) $H$ that encodes optimization in terms of binary decision variables $x_v\in\{0,1\}$ \& associate this variable with a vertex $v\in V$ for an undirected graph $G = (V,E)$ with vertex set $V = [n]$ \& edge set $E = \{(i,j):i,j\in V\}$ capturing interactions between decision variables. Then apply a relaxation strategy to problem Hamiltonian to generate a differentiable loss function with which we perform unsupervised training on node representations of GNN. GNN follows a standard recursive neighborhood aggregation scheme [43, 44], where each node $v\in[n]$ collects information (encoded as feature vectors) of its neighbors to compute its new feature vector ${\bf h}_v^k$ at layer $k = 0,1,\ldots,K$. After $k$ iterations of aggregation, a node is represented by its transformed feature vector ${\bf h}_v^k$, which captures structural information within node's $k$-hop neighborhood [28]. For binary classification tasks typically use convolutional aggregation steps, followed by application of a nonlinear softmax activation function to shrink down final embeddings ${\bf h}_v^K$ to 1D soft (probabilistic) node assignments $p_v = {\bf h}_v^K\in[0,1]$. Finally, once unsupervised training process has completed, apply a projection heuristic to map these soft assignments $p_v$ back to integer variables $x_v\in\{0,1\}$ using, e.g., $x_v = {\tt int}(p_v)$. Numerically showcase our approach with results for canonical NP-hard optimization problems e.g. maximum cut (MaxCut) \& maximum independent set (MIS), showing: our GNN-based approach can perform on par or even better than existing well-established solvers, while being broadly applicable to a large class of optimization problems. Further, scalability of our approach opens up possibility of studying unprecedented problem sizes with hundreds of millions of nodes when leveraging distributed training in a mini-batch fashion on a cluster of machines as demonstrated recently in [45].

    -- Trong bài báo này, chúng tôi trình bày 1 bộ giải dựa trên GNN có khả năng mở rộng cao để (xấp xỉ) giải các bài toán tối ưu tổ hợp với tối đa hàng triệu biến. Phương pháp này được mô tả sơ đồ trong {\sf Hình 1: Minh họa sơ đồ phương pháp GNN cho tối ưu hóa tổ hợp được trình bày trong bài báo này. Theo 1 sơ đồ tổng hợp lân cận đệ quy, GNN được huấn luyện lặp lại theo 1 hàm mất mát tùy chỉnh mã hóa bài toán tối ưu cụ thể, e.g.: cắt cực đại. Khi hoàn tất đào tạo, chiếu các giá trị cuối cùng cho các phép gán nút mềm tại lớp GNN cuối cùng trở lại các biến nhị phân $x_i = 0,1$, cung cấp chuỗi bit giải pháp ${\bf x} = (x_1,x_2,\ldots)$.} \& hoạt động như sau: 1. Đầu tiên, xác định Hamiltonian (hàm chi phí) $H$ mã hóa tối ưu hóa theo các biến quyết định nhị phân $x_v\in\{0,1\}$ \& liên kết biến này với 1 đỉnh $v\in V$ cho đồ thị vô hướng $G = (V,E)$ với tập đỉnh $V = [n]$ \& tập cạnh $E = \{(i,j):i,j\in V\}$ nắm bắt các tương tác giữa các biến quyết định. Sau đó, áp dụng 1 chiến lược thư giãn cho Hamiltonian vấn đề để tạo ra 1 hàm mất mát khả vi mà chúng ta thực hiện đào tạo không giám sát trên các biểu diễn nút của GNN. GNN tuân theo 1 sơ đồ tổng hợp lân cận đệ quy chuẩn [43, 44], trong đó mỗi nút $v\in[n]$ thu thập thông tin (được mã hóa dưới dạng các vectơ đặc trưng) của các nút lân cận để tính toán vectơ đặc trưng mới ${\bf h}_v^k$ của nó tại lớp $k = 0,1,\ldots,K$. Sau $k$ lần lặp tổng hợp, 1 nút được biểu diễn bằng vectơ đặc trưng đã biến đổi ${\bf h}_v^k$ của nó, vectơ này nắm bắt thông tin cấu trúc trong lân cận $k$-hop của nút [28]. Đối với các tác vụ phân loại nhị phân thường sử dụng các bước tổng hợp tích chập, sau đó áp dụng hàm kích hoạt softmax phi tuyến tính để thu nhỏ các nhúng cuối cùng ${\bf h}_v^K$ thành các phép gán nút mềm (xác suất) 1D $p_v = {\bf h}_v^K\in[0,1]$. Cuối cùng, sau khi quá trình đào tạo không giám sát hoàn tất, hãy áp dụng phương pháp heuristic chiếu để ánh xạ các phép gán mềm $p_v$ này trở lại các biến số nguyên $x_v\in\{0,1\}$ bằng cách sử dụng, e.g.: $x_v = {\tt int}(p_v)$. Trình bày phương pháp của chúng tôi bằng số với các kết quả cho các bài toán tối ưu hóa NP-khó chuẩn, ví dụ như cắt cực đại (MaxCut) \& tập độc lập cực đại (MIS), cho thấy: phương pháp dựa trên GNN của chúng tôi có thể hoạt động ngang bằng hoặc thậm chí tốt hơn các bộ giải đã được thiết lập tốt hiện có, đồng thời có thể áp dụng rộng rãi cho 1 lớp lớn các bài toán tối ưu hóa. Hơn nữa, khả năng mở rộng của phương pháp của chúng tôi mở ra khả năng nghiên cứu các kích thước bài toán chưa từng có với hàng trăm triệu nút khi tận dụng đào tạo phân tán theo kiểu lô nhỏ trên 1 cụm máy như đã được chứng minh gần đây trong [45].

    Structure: In Sect. 2, provide some context for our work, discussing recent developments at cross-section between ML \& combinatorial optimization. Sect. 3 summarizes basic concepts underlying our approach, as well as information on class of problems that this approach can solve. Sect. 4 outlines implementation of proposed GNN-based optimizer, followed by numerical experiments in Sect. 4. In Sect. 4, discuss potential real-world applications in industry. In Sect. 7 draw conclusions \& give an outlook on future directions of research.

    -- Cấu trúc: Trong Phần 2, hãy cung cấp 1 số bối cảnh cho công việc của chúng tôi, thảo luận về những phát triển gần đây tại giao điểm giữa ML \& tối ưu hóa tổ hợp. Phần 3 tóm tắt các khái niệm cơ bản làm nền tảng cho phương pháp của chúng tôi, cũng như thông tin về các loại vấn đề mà phương pháp này có thể giải quyết. Phần 4 phác thảo việc triển khai bộ tối ưu hóa dựa trên GNN được đề xuất, tiếp theo là các thử nghiệm số trong Phần 4. Trong Phần 4, hãy thảo luận về các ứng dụng thực tế tiềm năng trong công nghiệp. Trong Phần 7, hãy rút ra kết luận \& đưa ra triển vọng về các hướng nghiên cứu trong tương lai.
    \item {\sf2. Related work.} Briefly review relevant existing literature, with goal to provide a detailed context for our work. Broadly speaking, our work makes a physics-inspired contribution to emerging cross-fertilization between combinatorial optimization \& ML, where development of novel DL architectures has sparked a renewed interest in heuristics for solving NP-hard combinatorial optimization problems using neural networks, as extensively reviewed in [46, 47]. Leaving alternative, non-graph-based approaches as presented e.g. in [48] aside, in following short survey we focus on graph-based optimization problems -- where modern DL architectures e.g. sequence models, attention mechanisms, \& GNNs provide a natural tool set [46] -- \& primarily distinguish between approaches based on supervised learning, reinforcement learning, or unsupervised learning. This categorization can be refined further w.r.t. typical size of a problem solved by a specific approach \& scope of solver (special-purpose vs. general-purpose).

    -- Tóm tắt lại các tài liệu hiện có có liên quan, với mục tiêu cung cấp bối cảnh chi tiết cho công việc của chúng tôi. Nhìn chung, công trình của chúng tôi đóng góp lấy cảm hứng từ vật lý vào sự giao thoa mới nổi giữa tối ưu hóa tổ hợp \& Học máy (ML), trong đó sự phát triển của các kiến trúc Học máy mới đã khơi dậy mối quan tâm mới về phương pháp tìm kiếm để giải các bài toán tối ưu hóa tổ hợp NP-khó bằng mạng nơ-ron, như đã được xem xét kỹ lưỡng trong [46, 47]. Bỏ qua các phương pháp tiếp cận thay thế, không dựa trên đồ thị như đã trình bày, ví dụ như trong [48], trong khảo sát ngắn sau đây, chúng tôi tập trung vào các bài toán tối ưu hóa dựa trên đồ thị -- trong đó các kiến trúc Học máy hiện đại, ví dụ như mô hình chuỗi, cơ chế chú ý, \& Mạng nơ-ron nhân tạo (GNN) cung cấp 1 bộ công cụ tự nhiên [46] -- \& chủ yếu phân biệt giữa các phương pháp dựa trên học có giám sát, học tăng cường hoặc học không giám sát. Phân loại này có thể được tinh chỉnh hơn nữa theo quy mô điển hình của 1 bài toán được giải quyết bằng 1 phương pháp tiếp cận cụ thể \& phạm vi của trình giải (mục đích chuyên biệt so với mục đích chung).

    {\bf Supervised Learning.} Majority of neural network-based approaches to combinatorial optimization are based on supervised learning, with goal to approximate some (typically complex, nonlinear) mapping from an input representation of problem to target solution, based on \fbox{minimization of some empirical, handcrafted loss function}. Early work was based on pointer networks which leverage sequence-to-sequence models to produce permutations over inputs of variable size, as, e.g., relevant for canonical traveling salesman problem (TSP) [49]. Since then, numerous studies have fused GNNs with various heuristics \& search procedures to solve specific combinatorial optimization problems, e.g. quadratic assignment [50], graph matching [51], graph coloring [52], \& TSP [53, 54]. As pointed out in [55], however, viability \& performance of supervised approaches critically depends on existence of large, labeled training data sets with previously optimized hard problem instances, resulting in a problematic chicken-\&-egg scenario, further amplified by fact: hard to efficiently sample unbiased \& representative labeled instances of NP-hard problems [56].

    -- {\bf Học có giám sát.} Phần lớn các phương pháp tiếp cận dựa trên mạng nơ-ron để tối ưu hóa tổ hợp đều dựa trên học có giám sát, với mục tiêu xấp xỉ 1 số ánh xạ (thường là phức tạp, phi tuyến tính) từ biểu diễn đầu vào của vấn đề đến giải pháp mục tiêu, dựa trên \fbox{tối thiểu hóa 1 số hàm mất mát thủ công, theo kinh nghiệm}. Các công trình ban đầu dựa trên mạng con trỏ tận dụng các mô hình chuỗi-sang-chuỗi để tạo ra các hoán vị trên các đầu vào có kích thước biến, ví dụ như có liên quan đến bài toán người bán hàng du lịch chính tắc (TSP) [49]. Kể từ đó, nhiều nghiên cứu đã hợp nhất GNN với nhiều phương pháp tìm kiếm \& thủ tục tìm kiếm khác nhau để giải quyết các vấn đề tối ưu hóa tổ hợp cụ thể, ví dụ như phép gán bậc hai [50], khớp đồ thị [51], tô màu đồ thị [52], \& TSP [53, 54]. Tuy nhiên, như đã chỉ ra trong [55], tính khả thi \& hiệu suất của các phương pháp có giám sát phụ thuộc rất nhiều vào sự tồn tại của các tập dữ liệu đào tạo có nhãn lớn với các trường hợp bài toán khó được tối ưu hóa trước đó, dẫn đến tình huống con gà \& quả trứng có vấn đề, được khuếch đại thêm bởi thực tế: khó lấy mẫu hiệu quả các trường hợp có nhãn không thiên vị \& đại diện của các bài toán khó NP [56].

    {\bf Reinforcement Learning.} Critical need for training labels can be circumvented with Reinforcement Learning (RL) techniques that aim to learn a policy with goal of maximizing some expected reward function. Specifically, optimization problems can typically be described with a native objective function that can then serve as a reward function in an RL approach [46]. Motivated by challenges associated with need for optimal target solutions, Bello et al. extended pointer network architecture [49] to an actor-critic RL framework to train an approximate TSP solver, using a RNN encoder scheme \& expected tour length as a reward signal [57]. Using a general RL framework based on a graph attention network architecture [42], significant improvements in accuracy on 2D Euclidean TSP have subsequently been presented in [58], getting close to optimal results for problems up to 100 nodes. Moreover, TSP variants with hard constraints have been analyzed in [59], with help of a multi-level RL framework in which each layer of a hierarchy learns a different policy, \& from which actions can then be sampled. Finally, while majority of RL-based approaches have focused on TSP or variants thereof, Dai et al. proposed a combination of RL \& graph embedding to learn efficient greedy meta-heuristics to incrementally construct a solution, \& showcased their approach with numerical results for Minimum Vertex Cover, MaxCut, \& TSP as test problems, for graphs with up to $\sim1000$--1200 nodes [60].

    -- {\bf Học tăng cường.} Nhu cầu quan trọng đối với nhãn đào tạo có thể được bỏ qua bằng các kỹ thuật Học tăng cường (RL) nhằm mục đích học 1 chính sách với mục tiêu tối đa hóa 1 số hàm thưởng mong đợi. Cụ thể, các vấn đề tối ưu hóa thường có thể được mô tả bằng 1 hàm mục tiêu gốc sau đó có thể đóng vai trò là hàm thưởng trong phương pháp RL [46]. Được thúc đẩy bởi những thách thức liên quan đến nhu cầu về các giải pháp mục tiêu tối ưu, Bello \& cộng sự đã mở rộng kiến trúc mạng con trỏ [49] thành khuôn khổ RL diễn viên-nhà phê bình để đào tạo 1 bộ giải TSP gần đúng, sử dụng lược đồ mã hóa RNN \& độ dài hành trình mong đợi làm tín hiệu thưởng [57]. Sử dụng khuôn khổ RL chung dựa trên kiến trúc mạng chú ý đồ thị [42], những cải tiến đáng kể về độ chính xác trên TSP Euclidean 2D sau đó đã được trình bày trong [58], đạt gần đến kết quả tối ưu cho các vấn đề lên đến 100 nút. Hơn nữa, các biến thể TSP với các ràng buộc cứng đã được phân tích trong [59], với sự trợ giúp của 1 khuôn khổ RL đa cấp, trong đó mỗi lớp của 1 hệ thống phân cấp học 1 chính sách khác nhau, \& từ đó các hành động có thể được lấy mẫu. Cuối cùng, trong khi phần lớn các phương pháp tiếp cận dựa trên RL tập trung vào TSP hoặc các biến thể của nó, Dai \& cộng sự đã đề xuất 1 sự kết hợp của RL \& nhúng đồ thị để học các siêu heuristic tham lam hiệu quả nhằm xây dựng dần dần 1 giải pháp, \& trình bày phương pháp của họ với các kết quả số cho Phủ đỉnh tối thiểu, Cắt tối đa, \& TSP làm bài toán kiểm tra, cho các đồ thị có tối đa $\sim1000$--1200 nút [60].

    {\bf Unsupervised Learning.} Conceptually, our work is most similar to those that aim to train neural networks in an unsupervised, end-to-end fashion, without need for labeled training sets [55]. Specifically, Toenshoff et al. have recently used a recurrent GNN architecture -- dubbed RUN-CSP -- to solve optimization problems that can be framed as maximum constraint satisfaction problems [61]. For other types of problems, e.g. maximum independent set problem, model relies on empirically-selected hand-crafted loss functions. Using language of constraint satisfaction problems, where system size is expressed in terms of both number of variables \& number of constraints, authors solve problem instances of Maximum 2-satisfiability, 3-colorability, MaxCut \& Maximum Independent Set with up to 5000 nodes, showing: RUN-CSP can compete with traditional approaches like greedy heuristics or semi-definite programming. Finally, by either optimizing a smooth relaxation of cut objective or applying a policy gradient, Yao et al. trained a GNN to specifically solve MaxCut problem, albeit at relatively small system sizes with up to 500 nodes [62] \& without any details on runtime.

    -- {\bf Học không giám sát.} Về mặt khái niệm, công trình của chúng tôi tương tự nhất với những công trình hướng đến việc huấn luyện mạng nơ-ron theo cách không giám sát, từ đầu đến cuối, không cần tập huấn luyện có nhãn [55]. Cụ thể, Toenshoff \& cộng sự gần đây đã sử dụng kiến trúc GNN hồi quy -- được gọi là RUN-CSP -- để giải quyết các bài toán tối ưu hóa có thể được đóng khung thành các bài toán thỏa mãn ràng buộc tối đa [61]. Đối với các loại bài toán khác, ví dụ như bài toán tập độc lập tối đa, mô hình dựa trên các hàm mất mát được tạo thủ công theo kinh nghiệm. Sử dụng ngôn ngữ của các bài toán thỏa mãn ràng buộc, trong đó kích thước hệ thống được biểu thị theo cả số biến \& số ràng buộc, các tác giả giải quyết các trường hợp bài toán về khả năng thỏa mãn tối đa 2, khả năng tô màu 3, MaxCut \& Tập độc lập tối đa với tối đa 5000 nút, cho thấy: RUN-CSP có thể cạnh tranh với các phương pháp tiếp cận truyền thống như thuật toán heuristic tham lam hoặc lập trình bán xác định. Cuối cùng, bằng cách tối ưu hóa việc nới lỏng trơn tru mục tiêu cắt hoặc áp dụng 1 gradient chính sách, Yao \& cộng sự đã đào tạo 1 GNN để giải quyết cụ thể vấn đề MaxCut, mặc dù ở quy mô hệ thống tương đối nhỏ với tối đa 500 nút [62] \& không có bất kỳ chi tiết nào về thời gian chạy.

    Here, present a highly-scalable, physics-inspired framework that uses DL tools in form of GNNs to approximate solutions to hard combinatorial optimization problems with up to millions of variables. Our GNN optimizer with up to millions of variables. Our GNN optimizer is based on a direct mathematical relation between prototypical Ising spin Hamiltonians [63], Quadratic Binary Unconstrained Optimization (QUBO) \& Polynomial Binary Unconstrained Optimization (PUBO) formalism \& differentiable loss function with which we train GNN, thereby providing 1 unifying framework for a broad class of combinatorial optimization problems, \& opening up powerful toolbox of statistical physics to modern DL approaches. Fusing concepts from statistical physics with modern ML tooling, propose a simple, generic, \& robust solver that does not rely on hand-crafted loss functions. Specifically, show: same GNN optimizer can solve different QUBO problems, without any need to change architecture or loss function, while scaling to problem instances orders of magnitude larger than what many traditional QUBO solvers can handle [6, 12, 64, 65].

    -- Ở đây, xin giới thiệu 1 khuôn khổ có khả năng mở rộng cao, lấy cảm hứng từ vật lý, sử dụng các công cụ DL dưới dạng GNN để xấp xỉ các giải pháp cho các bài toán tối ưu hóa tổ hợp khó với tối đa hàng triệu biến. Bộ tối ưu hóa GNN của chúng tôi với tối đa hàng triệu biến. Bộ tối ưu hóa GNN của chúng tôi dựa trên mối quan hệ toán học trực tiếp giữa các Hamiltonian spin Ising nguyên mẫu [63], công thức Tối ưu hóa nhị phân không ràng buộc bậc hai (QUBO) \& Tối ưu hóa nhị phân không ràng buộc đa thức (PUBO) \& hàm mất mát khả vi mà chúng tôi huấn luyện GNN, do đó cung cấp 1 khuôn khổ thống nhất cho 1 lớp rộng các bài toán tối ưu hóa tổ hợp, \& mở ra bộ công cụ mạnh mẽ của vật lý thống kê cho các phương pháp DL hiện đại. Kết hợp các khái niệm từ vật lý thống kê với công cụ ML hiện đại, đề xuất 1 bộ giải đơn giản, chung chung, \& mạnh mẽ không dựa vào các hàm mất mát được tạo thủ công. Cụ thể, hãy hiển thị: cùng 1 trình tối ưu hóa GNN có thể giải quyết các vấn đề QUBO khác nhau mà không cần phải thay đổi kiến trúc hoặc hàm mất mát, đồng thời mở rộng quy mô lên các trường hợp vấn đề lớn hơn nhiều lần so với những gì nhiều trình giải QUBO truyền thống có thể xử lý [6, 12, 64, 65].
    \item {\sf3. Preliminaries.} To set up our notation \& terminology start out with a brief review of both combinatorial optimization, \& GNNs.

    {\bf Combinatorial Optimization.} Field of combinatorial optimization is concerned with settings where a large number of yes/no decisions must be made \& each set of decisions yields a corresponding objective function value, like a cost or profit value, to be optimized [1]. Canonical combinatorial optimization problems include, among others, maximum cut problem (MaxCut), maximum independent set problem (MIS), minimum vertex cover problem, maximum clique problem \& set cover problem. In all cases exact solutions are not feasible for sufficiently-large systems due to exponential growth of solution space as number of variables $n$ increases. Bespole (approximate) algorithms to solve these problems can typically be identified, at cost of limited scope \& generalizability. Conversely, in recent years QUBO framework has resulted in a powerful approach that unifies a rich variety of these NP-hard combinatorial optimization problems [1--3, 66]. Cost function for a QUBO problem can be expressed in compact form with following Hamiltonina (1)
    \begin{equation*}
        H_{\rm QUBO} = {\bf x}^\top Q{\bf x} = \sum_{i,j} x_iQ_{ij}x_j,
    \end{equation*}
    where ${\bf x} = (x_1,x_2,\ldots)$: a vector of binary decision variables \& QUBO matrix $Q$ is a square matrix of constant numbers that encodes actual problem to solve. W.l.o.g., $Q$-matrix can be assumed to be symmetric or in upper triangular form [1]. Have omitted any irrelevant constant terms, as well as any linear terms as these can always be absorbed into $Q$-matrix because $x_i^2 = x_i$ for binary variables $x_i\in\{0,1\}$. Problem constraints, as relevant for many real-world optimization problems, can be accounted for with help of penalty terms entering objective function (rather than being explicitly imposed), as detailed in [1]. Significance of QUBO problems is further illustrated by close relation to famous Ising model, which is known to provide mathematical formulations for many NP-complete \& NP-hard problems, including all of Karp's 21 NP-complete problems [66]. As opposed to QUBO problems, Ising problems are described in terms of binary spin variables $z_i\in\{\pm1\}$, that can be mapped straightforwardly to their equivalent QUBO form, \& vice versa, using $z_i = 2x_i - 1$. By def, both QUBO \& Ising models are quadratic, but can be naturally generalized to higher order PUBO problems, as described by $N$-local Hamiltonian (2)
    \begin{equation*}
        H_{\rm PUBO} = \sum_{k=0}^N\sum_{\langle i_1,i_2,\ldots,i_k\rangle} Q_{i_1i_2\ldots i_k}x_{i_1}x_{i_2}\cdots x_{i_k},
    \end{equation*}
    with real coefficients $Q_{i_1i_2\ldots i_k}$, for some $N\ge3$, \& $\langle i_1,i_2,\ldots,i_k\rangle$ indicating a group of $k$ binary variables (or spins in Ising formulation). Terms containing a product of $k$ variables, of form $Q_{i_1i_2\ldots i_k}x_{i_1}x_{i_2}\cdots x_{i_k}$, are commonly referred to as $k$-local interactions with $Q_{i_1i_2\ldots i_k}$ being coupling constant. As exemplify below for some canonical problems, graph (hypergraph) problems can be naturally framed as QUBO (PUBO) problems. To this end, given an undirected graph $G = (V,E)$, simply associate a binary variable $x_i$ with every vertex $i\in V$, \& then express (node classification) objective as a QUBO problem, where specific assignment ${\bf x}$ can be visualized as a specific 2-tone (e.g., light \& dark) color of graph [With coloring of graph, refer to a specific node classification as given by assignment vector ${\bf x}$, taking e.g. $x_i = 0$ as red node coloring \& $x_i = 1$ as blue node coloring. Do not refer to well-known vertex coloring problem which seeks to color vertices of a graph s.t. no 2 adjacent vertices are of same color.], see {\sf Fig. 1.}

    -- {\bf Tối ưu hóa tổ hợp.} Lĩnh vực tối ưu hóa tổ hợp liên quan đến các thiết lập trong đó phải đưa ra 1 số lượng lớn các quyết định có/không \& mỗi tập hợp các quyết định tạo ra 1 giá trị hàm mục tiêu tương ứng, như giá trị chi phí hoặc lợi nhuận, cần được tối ưu hóa [1]. Các vấn đề tối ưu hóa tổ hợp chính tắc bao gồm, trong số những vấn đề khác, bài toán cắt cực đại (MaxCut), bài toán tập độc lập cực đại (MIS), bài toán phủ đỉnh cực tiểu, bài toán clique cực đại \& bài toán phủ tập hợp. Trong mọi trường hợp, các giải pháp chính xác đều không khả thi đối với các hệ thống đủ lớn do không gian nghiệm tăng theo cấp số nhân khi số biến $n$ tăng. Các thuật toán Bespole (xấp xỉ) để giải các vấn đề này thường có thể được xác định, với chi phí là phạm vi hạn chế \& khả năng khái quát hóa. Ngược lại, trong những năm gần đây, khuôn khổ QUBO đã dẫn đến 1 phương pháp tiếp cận mạnh mẽ thống nhất nhiều loại bài toán tối ưu hóa tổ hợp NP-khó này [1--3, 66]. Hàm chi phí cho bài toán QUBO có thể được biểu diễn dưới dạng rút gọn với Hamiltonina (1) sau đây
    \begin{equation*}
        H_{\rm QUBO} = {\bf x}^\top Q{\bf x} = \sum_{i,j} x_iQ_{ij}x_j,
    \end{equation*}
    trong đó ${\bf x} = (x_1,x_2,\ldots)$: 1 vectơ các biến quyết định nhị phân \& Ma trận QUBO $Q$ là 1 ma trận vuông các hằng số mã hóa bài toán thực tế cần giải. Ví dụ: ma trận $Q$ có thể được coi là đối xứng hoặc ở dạng tam giác trên [1]. Đã bỏ qua bất kỳ hằng số không liên quan nào, cũng như bất kỳ hằng số tuyến tính nào vì chúng luôn có thể được đưa vào ma trận $Q$ vì $x_i^2 = x_i$ đối với các biến nhị phân $x_i\in\{0,1\}$. Các ràng buộc của vấn đề, liên quan đến nhiều vấn đề tối ưu hóa trong thế giới thực, có thể được giải thích bằng sự trợ giúp của các điều khoản phạt nhập vào hàm mục tiêu (thay vì được áp đặt 1 cách rõ ràng), như được trình bày chi tiết trong [1]. Tầm quan trọng của các vấn đề QUBO được minh họa thêm bằng mối quan hệ chặt chẽ với mô hình Ising nổi tiếng, được biết là cung cấp các công thức toán học cho nhiều vấn đề NP-hoàn chỉnh \& NP-khó, bao gồm tất cả 21 vấn đề NP-hoàn chỉnh của Karp [66]. Ngược lại với các vấn đề QUBO, các vấn đề Ising được mô tả dưới dạng các biến spin nhị phân $z_i\in\{\pm1\}$, có thể được ánh xạ trực tiếp sang dạng QUBO tương đương của chúng, \& ngược lại, bằng cách sử dụng $z_i = 2x_i - 1$. Theo định nghĩa, cả hai mô hình QUBO \& Ising đều là bậc hai, nhưng có thể được tổng quát hóa 1 cách tự nhiên cho các bài toán PUBO bậc cao hơn, như được mô tả bởi Hamiltonian cục bộ $N$ (2)
    \begin{equation*}
        H_{\rm PUBO} = \sum_{k=0}^N\sum_{\langle i_1,i_2,\ldots,i_k\rangle} Q_{i_1i_2\ldots i_k}x_{i_1}x_{i_2}\cdots x_{i_k},
    \end{equation*}
    với hệ số thực $Q_{i_1i_2\ldots i_k}$, với 1 số $N\ge3$, \& $\langle i_1,i_2,\ldots,i_k\rangle$ biểu thị 1 nhóm $k$ biến nhị phân (hoặc spin trong công thức Ising). Các số hạng chứa tích của $k$ biến, có dạng $Q_{i_1i_2\ldots i_k}x_{i_1}x_{i_2}\cdots x_{i_k}$, thường được gọi là tương tác cục bộ $k$ với $Q_{i_1i_2\ldots i_k}$ là hằng số liên kết. Như minh họa dưới đây cho 1 số bài toán chính tắc, các bài toán đồ thị (siêu đồ thị) có thể được đóng khung tự nhiên thành các bài toán QUBO (PUBO). Để đạt được mục đích này, cho 1 đồ thị vô hướng $G = (V,E)$, chỉ cần liên kết 1 biến nhị phân $x_i$ với mọi đỉnh $i\in V$, \& sau đó biểu thị mục tiêu (phân loại nút) dưới dạng bài toán QUBO, trong đó phép gán cụ thể ${\bf x}$ có thể được hình dung dưới dạng màu 2 tông màu cụ thể (e.g.: sáng \& tối) của đồ thị [Với việc tô màu đồ thị, hãy tham chiếu đến phân loại nút cụ thể được chỉ định bởi vectơ gán ${\bf x}$, lấy ví dụ $x_i = 0$ là tô màu nút đỏ \& $x_i = 1$ là tô màu nút xanh. Không tham chiếu đến bài toán tô màu đỉnh nổi tiếng tìm cách tô màu các đỉnh của đồ thị nếu không có 2 đỉnh liền kề nào có cùng màu.], xem {\sf Hình 1.}

    {\bf Graph Neural Networks.} On a high level, GNNs are a family of neural networks capable of learning how to aggregate information in graphs for purpose of representation learning. Typically, a GNN layer is comprised of 3 functions [35]:
    \begin{enumerate}
        \item a message passing function that permits information exchange between nodes over edges,
        \item an aggregation function that combines collection of received messages into a single, fixed-length representation,
        \item a (typically nonlinear) update activation function that produces node-level representations given previous layer representation \& aggregated information.
    \end{enumerate}
    While a single-layer GNN encapsulates a node's features based on its immediate or 1-hop neighborhood, by stacking multiple layers, model can propagate each node's features through intermediate nodes, analogous to broadening receptive field in downstream layers of convolutional neural networks. Formally, at layer $k = 0$, each node $v\in V$ is represented by some initial representation ${\bf h}_v^0\in\mathbb{R}^{d_0}$, usually derived from node's label or given input features of dimensionality $d_0$ [68]. Following a recursive neighborhood aggregation scheme, GNN then iteratively updates each node's representation, in general described by some parametric function $f_\theta^k$, resulting in (3)
    \begin{equation*}
        {\bf h}_v^k = f_\theta^k({\bf h}_v^{k-1},\{{\bf h}_u^{k-1}|u\in{\cal N}_v\}),
    \end{equation*}
    for layers $k\in[K]$, with ${\cal N}_v = \{u\in V|(u,v)\in E\}$ referring to local neighborhood of node $v$, i.e., set of nodes that share edges with node $v$. Total number of layers $K$ is usually determined empirically as a hyperparameter, as are intermediate representation dimensionality $d_k$. Both can be optimized in an outer loop. While a growing number of possible implementations for GNN architectures [30] exists, here use a graph convolutional network (GCN) [29] for which (3) reads explicitly as
    \begin{equation*}
        {\bf h}_v^k = \sigma\left({\bf W}_k\sum_{u\in{\cal N}(v)} \frac{{\bf h}_u^{k-1}}{|{\cal N}(v)|} + {\bf B}_k{\bf h}_v^{k-1}\right),
    \end{equation*}
    with ${\bf W}_k,{\bf B}_k$ being (shared) trainable weight matrices, denominator $|{\cal N}(v)|$ serving as normalization factor (with other choices available as well) \& $\sigma(\cdot)$ being some (component-wise) nonlinear activation function e.g. sigmoid or ReLU. While GNNs can be used for various prediction tasks (including node classification, link prediction, community detection, network similarity, or graph classification), here focus on node classification, where usually last $K$-th layer's output is used to predict a label $y_v$ for every node $v\in V$. To this end, feed (parameterized) final node embeddings ${\bf z}_v = {\bf h}_v^K(\theta)$ into a problem-specific loss function \& run stochastic gradient descent to train weight parameters.

    -- {\bf Mạng nơ-ron đồ thị.} Ở cấp độ cao, GNN là 1 họ mạng nơ-ron có khả năng học cách tổng hợp thông tin trong đồ thị nhằm mục đích học biểu diễn. Thông thường, 1 lớp GNN bao gồm 3 hàm [35]:
    \begin{enumerate}
        \item 1 hàm truyền thông điệp cho phép trao đổi thông tin giữa các nút qua các cạnh,
        \item 1 hàm tổng hợp kết hợp tập hợp các thông điệp đã nhận thành 1 biểu diễn duy nhất có độ dài cố định,
        \item 1 hàm kích hoạt cập nhật (thường là phi tuyến tính) tạo ra các biểu diễn cấp nút dựa trên biểu diễn lớp trước đó \& thông tin tổng hợp.
    \end{enumerate}
    Trong khi GNN 1 lớp đóng gói các đặc trưng của 1 nút dựa trên vùng lân cận trực tiếp hoặc 1-hop của nó, bằng cách xếp chồng nhiều lớp, mô hình có thể truyền các đặc trưng của từng nút qua các nút trung gian, tương tự như việc mở rộng trường tiếp nhận trong các lớp hạ lưu của mạng nơ-ron tích chập. Về mặt hình thức, tại lớp $k = 0$, mỗi nút $v\in V$ được biểu diễn bằng 1 số biểu diễn ban đầu ${\bf h}_v^0\in\mathbb{R}^{d_0}$, thường được lấy từ nhãn của nút hoặc các tính năng đầu vào được cung cấp của chiều $d_0$ [68]. Theo sơ đồ tổng hợp lân cận đệ quy, GNN sau đó lặp lại việc cập nhật biểu diễn của từng nút, thường được mô tả bởi 1 hàm tham số $f_\theta^k$, dẫn đến (3)
    \begin{equation*}
        {\bf h}_v^k = f_\theta^k({\bf h}_v^{k-1},\{{\bf h}_u^{k-1}|u\in{\cal N}_v\}),
    \end{equation*}
    đối với các lớp $k\in[K]$, với ${\cal N}_v = \{u\in V|(u,v)\in E\}$ tham chiếu đến lân cận cục bộ của nút $v$, i.e., tập hợp các nút chia sẻ cạnh với nút $v$. Tổng số lớp $K$ thường được xác định theo kinh nghiệm dưới dạng siêu tham số, cũng như chiều biểu diễn trung gian $d_k$. Cả hai đều có thể được tối ưu hóa trong 1 vòng lặp ngoài. Mặc dù ngày càng có nhiều triển khai khả thi cho kiến trúc GNN [30], ở đây chúng ta sử dụng mạng tích chập đồ thị (GCN) [29] mà (3) được đọc rõ ràng là
    \begin{equation*}
        {\bf h}_v^k = \sigma\left({\bf W}_k\sum_{u\in{\cal N}(v)} \frac{{\bf h}_u^{k-1}}{|{\cal N}(v)|} + {\bf B}_k{\bf h}_v^{k-1}\right),
    \end{equation*}
    với ${\bf W}_k,{\bf B}_k$ là các ma trận trọng số có thể huấn luyện (chia sẻ), mẫu số $|{\cal N}(v)|$ đóng vai trò là hệ số chuẩn hóa (cũng có các lựa chọn khác) \& $\sigma(\cdot)$ là 1 hàm kích hoạt phi tuyến tính (theo từng thành phần), e.g.: sigmoid hoặc ReLU. Mặc dù GNN có thể được sử dụng cho nhiều tác vụ dự đoán khác nhau (bao gồm phân loại nút, dự đoán liên kết, phát hiện cộng đồng, độ tương đồng mạng hoặc phân loại đồ thị), nhưng ở đây tập trung vào phân loại nút, trong đó đầu ra của lớp $K$ cuối cùng thường được sử dụng để dự đoán nhãn $y_v$ cho mọi nút $v\in V$. Để đạt được mục đích này, hãy đưa các nhúng nút cuối cùng (đã tham số hóa) ${\bf z}_v = {\bf h}_v^K(\theta)$ vào 1 hàm mất mát cụ thể của bài toán \& chạy thuật toán giảm dần gradient ngẫu nhiên để huấn luyện các tham số trọng số.
    \item {\sf4. Combinatorial Optimization with GNNs.} Now detail how to use GNNs to solve combinatorial optimization problems, as schematically outlined in {\sf Fig. 2: Flow chart illustrating end-to-end workflow for proposed physics-inspired GNN optimizer. (a) problem is specified by a graph $G$ with associated adjacency matrix $A$, \& a cost function as described (e.g.) by QUBO Hamiltonian $H_{\rm QUBO}$. Within QUBO framework cost function is fully captured by QUBO matrix $Q$, as illustrated for both MaxCut \& MIS for a sample (undirected) graph with 5 vertices \& 6 edges. (b) Problem setup is complemented by a training strategy that specifies GNN Ansatz, a choice of hyperparameters \& a specific ML optimizer. (c) GNN is iteratively trained against a custom loss function ${\cal L}_{\rm QUBO}(\theta)$ that encodes a relaxed version of underlying optimization problem as specified by cost function $H_{\rm QUBO}$. Typically, a GNN layer operates by aggregating information within local 1-hop neighborhood (as illustrated by $k = 1$ circle for top node with label 0). By stacking layers one can extend receptive field of each node, thereby allowing distant propagation of information (as illustrated by $k = 2$ circle for top node with label 0). (d)--(e) GNN generates soft node assignments which can be viewed as class probabilities. Using some projection scheme, then project soft node assignments back to (hard) binary variables $x_i = 0,1$ (as indicated by binary black/white node coloring), providing final solution bit string ${\bf x}$.} To this end, frame combinatorial optimization problems as unsupervised node classification tasks, without need for any labeled data. Because nodes do not carry any inherent features, in our setup node embeddings ${\bf h}_v^0$ are initialized randomly. Warm-starting training process with pre-training (transfer learning) will be left for future research. Class of Hamiltonians described above are not differentiable \& cannot be used straightforwardly within GNN training process. Therefore, for a given problem Hamiltonian $H$ \& graph $G$, generate a differentiable loss function ${\cal L}(\theta)$, as required for standard back-propagation, by promoting binary decision variables $x_i\in\{0,1\}$ to continuous (parametrized) probability parameters $p_i(\theta)$ with following (heuristic) relaxation approach (5)
    \begin{equation*}
        x_i\to p_i(\theta)\in[0,1].
    \end{equation*}
    Soft assignments $p_i$ can be viewed as class probabilities. They are generated by our GNN Ansatz as final node embeddings $p_i = {\bf h}_i^K\in[0,1]$ at layer $K$, after application of a nonlinear softmax activation function. Then, they are used as input for loss function ${\cal L}(\theta)$. In particular, for QUBO-type problems: (6)
    \begin{equation*}
        H_{\rm QUBO}\to{\cal L}_{\rm QUBO}(\theta) = \sum_{i,j} p_i(\theta)Q_{ij}p_j(\theta),
    \end{equation*}
    which is differentiable w.r.t. parameters of GNN model $\theta$, \& similarly for PUBO problems on hypergraphs with higher-order terms of form $p_ip_jp_k$, etc. thereby establishing a straightforward, general connection between combinatorial optimization problems, Ising Hamiltonians \& GNNs. For training with gradient descent, standard ML optimizers e.g. ADAM can be used. Once (unsupervised) training process has completed, apply projection heuristics to map these soft assignments $p_i$ back to integer variables $x_i = 0,1$, using e.g. simply $x_i = {\tt int}(p_i)$. Application of other, more sophisticated projection schemes will be left for future research. Note: any projection heuristics can be applied throughout training after every epoch, thereby increasing pool of solution candidates, at no additional computational cost. With GNN guiding search through solution space, one can then book keep all solution candidates identified throughout training \& simply pick best solution found.

    -- {\sf4. Tối ưu hóa tổ hợp với GNN.} Bây giờ, hãy trình bày chi tiết cách sử dụng GNN để giải các bài toán tối ưu hóa tổ hợp, như được phác thảo sơ đồ trong {\sf Hình 2: Sơ đồ luồng minh họa quy trình làm việc đầu cuối cho bộ tối ưu hóa GNN lấy cảm hứng từ vật lý được đề xuất. (a) bài toán được chỉ định bởi đồ thị $G$ với ma trận kề $A$ liên kết, \& 1 hàm chi phí như được mô tả (ví dụ) bởi QUBO Hamiltonian $H_{\rm QUBO}$. Trong khuôn khổ QUBO, hàm chi phí được nắm bắt đầy đủ bởi ma trận QUBO $Q$, như được minh họa cho cả MaxCut \& MIS đối với 1 đồ thị mẫu (vô hướng) có 5 đỉnh \& 6 cạnh. (b) Thiết lập bài toán được bổ sung bởi 1 chiến lược đào tạo chỉ định GNN Ansatz, 1 lựa chọn các siêu tham số \& 1 bộ tối ưu hóa ML cụ thể. (c) GNN được huấn luyện lặp đi lặp lại theo hàm mất mát tùy chỉnh ${\cal L}_{\rm QUBO}(\theta)$ mã hóa phiên bản nới lỏng của bài toán tối ưu hóa cơ bản được chỉ định bởi hàm chi phí $H_{\rm QUBO}$. Thông thường, 1 lớp GNN hoạt động bằng cách tổng hợp thông tin trong vùng lân cận 1 bước nhảy cục bộ (như minh họa bằng $k = 1$ vòng tròn cho nút trên cùng có nhãn 0). Bằng cách xếp chồng các lớp, người ta có thể mở rộng trường tiếp nhận của mỗi nút, do đó cho phép truyền thông tin đi xa (như minh họa bằng $k = 2$ vòng tròn cho nút trên cùng có nhãn 0). (d)--(e) GNN tạo ra các phép gán nút mềm có thể được xem như xác suất lớp. Sử dụng 1 số lược đồ chiếu, sau đó chiếu các phép gán nút mềm trở lại các biến nhị phân (cứng) $x_i = 0,1$ (như được chỉ ra bằng cách tô màu nút đen/trắng nhị phân), cung cấp chuỗi bit giải pháp cuối cùng ${\bf x}$.} Để đạt được mục đích này, hãy đóng khung các bài toán tối ưu hóa tổ hợp như các tác vụ phân loại nút không giám sát, không cần bất kỳ dữ liệu có nhãn nào. Do các nút không mang bất kỳ đặc điểm cố hữu nào, nên trong thiết lập của chúng tôi, nhúng nút ${\bf h}_v^0$ được khởi tạo ngẫu nhiên. Quá trình huấn luyện khởi động ấm với tiền huấn luyện (học chuyển giao) sẽ được dành cho nghiên cứu trong tương lai. Lớp Hamiltonian được mô tả ở trên không khả vi \& không thể được sử dụng trực tiếp trong quá trình huấn luyện GNN. Do đó, đối với 1 bài toán Hamiltonian $H$ \& đồ thị $G$ cho trước, hãy tạo 1 hàm mất mát khả vi ${\cal L}(\theta)$, như yêu cầu cho lan truyền ngược chuẩn, bằng cách đưa các biến quyết định nhị phân $x_i\in\{0,1\}$ vào các tham số xác suất liên tục (tham số hóa) $p_i(\theta)$ với phương pháp nới lỏng (heuristic) sau (5)
    \begin{equation*}
        x_i\to p_i(\theta)\in[0,1].
    \end{equation*}
    Các phép gán mềm $p_i$ có thể được xem như các xác suất lớp. Chúng được tạo ra bởi GNN Ansatz của chúng tôi dưới dạng nhúng nút cuối cùng $p_i = {\bf h}_i^K\in[0,1]$ tại lớp $K$, sau khi áp dụng hàm kích hoạt softmax phi tuyến tính. Sau đó, chúng được sử dụng làm đầu vào cho hàm mất mát ${\cal L}(\theta)$. Cụ thể, đối với các bài toán kiểu QUBO: (6)
    \begin{equation*}
        H_{\rm QUBO}\to{\cal L}_{\rm QUBO}(\theta) = \sum_{i,j} p_i(\theta)Q_{ij}p_j(\theta),
    \end{equation*}
    có thể phân biệt được với các tham số của mô hình GNN $\theta$, \& tương tự đối với các bài toán PUBO trên siêu đồ thị với các số hạng bậc cao có dạng $p_ip_jp_k$, v.v., qua đó thiết lập 1 kết nối tổng quát, đơn giản giữa các bài toán tối ưu hóa tổ hợp, Ising Hamiltonian \& GNN. Đối với huấn luyện với phương pháp giảm dần độ dốc, có thể sử dụng các bộ tối ưu hóa ML tiêu chuẩn, ví dụ như ADAM. Sau khi quá trình huấn luyện (không giám sát) hoàn tất, hãy áp dụng các phương pháp suy luận chiếu để ánh xạ các phép gán mềm $p_i$ này trở lại các biến nguyên $x_i = 0,1$, e.g., sử dụng $x_i = {\tt int}(p_i)$. Việc áp dụng các lược đồ suy luận chiếu khác, phức tạp hơn sẽ được dành cho nghiên cứu trong tương lai. Lưu ý: bất kỳ phương pháp suy luận chiếu nào cũng có thể được áp dụng trong suốt quá trình huấn luyện sau mỗi kỷ nguyên, do đó tăng số lượng ứng viên giải pháp mà không tốn thêm chi phí tính toán. Với GNN hướng dẫn tìm kiếm trong không gian giải pháp, người ta có thể lưu trữ tất cả các ứng viên giải pháp đã xác định trong suốt quá trình huấn luyện \& chỉ cần chọn giải pháp tốt nhất được tìm thấy.

    Our general GNN approach features several hyperparameters, including number of layers $K$, dimensionality of embedding vectors ${\bf h}_i^k$, \& learning rate $\beta$, with details depending on specific architecture \& optimizer used. These can be fine-tuned \& optimized in an outer loop, using, e.g., standard techniques e.g. grid search or more advanced Bayesian optimization methods.

    -- Phương pháp GNN tổng quát của chúng tôi bao gồm 1 số siêu tham số, bao gồm số lớp K, số chiều của các vectơ nhúng ${\bf h}_i^k$, \& tốc độ học $\beta$, với các chi tiết tùy thuộc vào kiến trúc \& trình tối ưu hóa cụ thể được sử dụng. Những thông số này có thể được tinh chỉnh \& tối ưu hóa trong 1 vòng lặp ngoài, sử dụng, e.g., các kỹ thuật tiêu chuẩn như tìm kiếm lưới hoặc các phương pháp tối ưu hóa Bayesian tiên tiến hơn.

    Our GNN-based approach can be readily implemented with open-source libraries e.g. PyTorch Geometric [69] or Deep Graph Library [70]. Core of corresponding code is displayed in supplemental material for a GCN with 2 layers \& a loss function for any QUBO problem. For illustration, an example solution to archetypal MaxCut problem (as implemented with this Ansatz) for a 3-regular graph with $n = 100$ vertices is shown in {\sf Fig. 3: Example solution to MaxCut for a random 3-regular graph with $n = 100$ nodes. After training completion, GNN provides a binary bit string ${\bf x}$ that assigns 1 of 2 possible colors (e.g., black or white) to each vertex. An edge is said to be cut when it connects 2 vertices of different colors. For a given graph, optimization problem is to assign colors in a way that as many edges as possible can be cut at same time (corresponding to antiferromagnetic ground-state of system).} Here, cut size achieved with our GNN method amounts to 132.

    -- Cách tiếp cận dựa trên GNN của chúng tôi có thể dễ dàng được triển khai bằng các thư viện nguồn mở, e.g.: PyTorch Geometric [69] hoặc Thư viện Deep Graph [70]. Phần cốt lõi của mã tương ứng được hiển thị trong tài liệu bổ sung cho GCN có 2 lớp \& hàm mất mát cho bất kỳ bài toán QUBO nào. Để minh họa, 1 giải pháp ví dụ cho bài toán MaxCut nguyên mẫu (được triển khai với Ansatz này) cho đồ thị 3-chính quy với $n = 100$ đỉnh được hiển thị trong {\sf Hình 3: Giải pháp ví dụ cho MaxCut cho đồ thị 3-chính quy ngẫu nhiên với $n = 100$ nút. Sau khi hoàn tất quá trình huấn luyện, GNN cung cấp 1 chuỗi bit nhị phân ${\bf x}$ gán 1 trong 2 màu có thể (e.g.: đen hoặc trắng) cho mỗi đỉnh. 1 cạnh được gọi là bị cắt khi nó kết nối 2 đỉnh có màu khác nhau. Đối với 1 đồ thị cho trước, vấn đề tối ưu hóa là gán màu sao cho có thể cắt được nhiều cạnh nhất có thể cùng 1 lúc (tương ứng với trạng thái cơ bản phản sắt từ của hệ thống).} Ở đây, kích thước cắt đạt được bằng phương pháp GNN của chúng tôi là 132.
    \item {\sf5. Numerical experiments.} Perform numerical experiments using MaxCut \& MIS benchmark problems. Before providing details on these numerical experiments, 1st describe our GNN model architecture as it is consistent across $d$-regular MaxCut \& MIS problem instances described below. Certainly possible: better solutions can be found by fine-tuning hyper-parameters for every given problem instance. However, 1 of our goal: design a robust \& scalable solver that is able to solve a large sample of instances efficiently without need of hand-tuning parameters on an instance-by-instance base.

    -- Tiến hành các thí nghiệm số sử dụng các bài toán chuẩn MaxCut \& MIS. Trước khi cung cấp chi tiết về các thí nghiệm số này, trước tiên hãy mô tả kiến trúc mô hình GNN của chúng tôi vì nó nhất quán trên các trường hợp bài toán MaxCut \& MIS $d$-chính quy được mô tả bên dưới. Hoàn toàn có thể: các giải pháp tốt hơn có thể được tìm thấy bằng cách tinh chỉnh các siêu tham số cho mỗi trường hợp bài toán nhất định. Tuy nhiên, 1 trong những mục tiêu của chúng tôi là thiết kế 1 bộ giải mạnh mẽ \& có khả năng mở rộng, có thể giải quyết 1 lượng lớn các trường hợp 1 cách hiệu quả mà không cần phải điều chỉnh thủ công các tham số trên cơ sở từng trường hợp.

    {\bf GNN Architecture.} Use a simple 2-layer GCN architecture based on PyTorch GraphConv units. 1st convolutional layer is fed node embeddings of dimension $d_0$ \& outputs a representation of size $d_1$. Next, apply a component-wise, nonlinear ReLU transformation. 2nd convolutional layer is then fed this intermediate representation \& outputs output layer of size $d_2$, which is then fed through component-wise sigmoid transformation to provide a soft probability $p_i\in[0,1]$ for every node $i\in V$. Find: following simple heuristic for determining hyper-parameters $d_0,d_1$ works well: if number of nodes is large $n\ge10^5$, then set $d_0 = {\tt int}(\sqrt{n})$, else set $d_0 = {\tt int}(\sqrt[3]{n})$, \& take $d_1 = {\tt int}(\frac{d_0}{2})$. Because solve for binary classification tasks, set final output dimension as $d_2 = 1$. However, for multi-color problems this could be extended to $C > 2$ classes by passing output layer through a softmax transformation (instead of a sigmoid) \& taking argmax. Note: as graph size scales beyond $\sim10^5$ nodes, memory becomes a concern, \& so further reduce representations to allow GNN to be trained on a single GPU. Distributed training leveraging a whole cluster of machines will be discussed in Sect. 7. With GNN's output depending on random initialization of hidden feature vectors there is a risk of becoming stuck in a local optimum where GNN stops learning. To counter this issue, one can take multiple shots (i.e., run GNN training multiple times for different random seeds \& choose best solution), thereby boosting performance at cost of extended runtime. In our numerical experiments, limited number of shots per instance to 5, only re-running training when an obviously sub-optimal solution was detected. Finally, set learning rate to $\beta = 10^{-4}$ \& allow model to train for up to $\sim10^5$ epochs, with a simple early stopping rule set to an absolute tolerance of $10^{-4}$ \& a patience of $10^3$.

    -- {\bf Kiến trúc GNN.} Sử dụng kiến trúc GCN 2 lớp đơn giản dựa trên các đơn vị PyTorch GraphConv. Lớp tích chập thứ nhất được nạp các nhúng nút có chiều $d_0$ \& xuất ra 1 biểu diễn có kích thước $d_1$. Tiếp theo, áp dụng phép biến đổi ReLU phi tuyến tính từng thành phần. Lớp tích chập thứ hai sau đó được nạp biểu diễn trung gian này \& xuất ra lớp đầu ra có kích thước $d_2$, sau đó được đưa qua phép biến đổi sigmoid từng thành phần để cung cấp xác suất mềm $p_i\in[0,1]$ cho mọi nút $i\in V$. Tìm: phương pháp tìm kiếm đơn giản sau đây để xác định siêu tham số $d_0,d_1$ hoạt động tốt: nếu số nút lớn $n\ge10^5$, thì đặt $d_0 = {\tt int}(\sqrt{n})$, nếu không thì đặt $d_0 = {\tt int}(\sqrt[3]{n})$, \& lấy $d_1 = {\tt int}(\frac{d_0}{2})$. Vì giải quyết cho các nhiệm vụ phân loại nhị phân, hãy đặt chiều đầu ra cuối cùng là $d_2 = 1$. Tuy nhiên, đối với các bài toán nhiều màu, điều này có thể được mở rộng thành $C > 2$ lớp bằng cách truyền lớp đầu ra qua phép biến đổi softmax (thay vì sigmoid) \& lấy argmax. Lưu ý: khi kích thước đồ thị mở rộng vượt quá $\sim10^5$ nút, bộ nhớ trở thành 1 mối quan tâm, \& do đó giảm thêm các biểu diễn để cho phép GNN được huấn luyện trên 1 GPU duy nhất. Đào tạo phân tán tận dụng toàn bộ cụm máy sẽ được thảo luận trong Phần 7. Với đầu ra của GNN phụ thuộc vào việc khởi tạo ngẫu nhiên các vectơ đặc trưng ẩn, có nguy cơ bị kẹt trong 1 tối ưu cục bộ, nơi GNN dừng học. Để khắc phục vấn đề này, người ta có thể thực hiện nhiều lần (i.e., chạy đào tạo GNN nhiều lần cho các hạt giống ngẫu nhiên khác nhau \& chọn giải pháp tốt nhất), do đó tăng hiệu suất với chi phí là thời gian chạy kéo dài. Trong các thử nghiệm số của chúng tôi, giới hạn số lần chụp cho mỗi trường hợp là 5, chỉ chạy lại đào tạo khi phát hiện ra 1 giải pháp rõ ràng là không tối ưu. Cuối cùng, đặt tốc độ học thành $\beta = 10^{-4}$ \& cho phép mô hình đào tạo trong tối đa $\sim10^5$ kỷ nguyên, với 1 quy tắc dừng sớm đơn giản được đặt thành dung sai tuyệt đối là $10^{-4}$ \& độ kiên nhẫn là $10^3$.

    {\bf Maximum Cut.} MaxCut is an NP-hard combinatorial optimization problem with practical applications in machine scheduling [71], image recognition [72], \& electronic circuit layout design [73]. In current era of noisy intermediate-scale quantum devices, with advent of novel hybrid quantum-classical algorithms e.g. Quantum Approximate Optimization Algorithm (QAOA) [74], MaxCut problem has recently attracted considerable attention as a potential use case of pre-error-corrected quantum devices, see [75--80]. MaxCut is a graph partitioning problem defined as follows: given a graph with vertex set $V$ \& edge set $E$, seek a partition of $V$ into 2 subsets with maximum cut, where a cut refers to edges connecting 2 nodes from different vertex sets. Intuitively, i.e., score a point whenever an edge connects 2 nodes of different colors. To formulate MaxCut mathematically, introduce binary variables satisfying $x_i = 1$ if vertex $i$ is in 1 set \& $x_i = 0$ if it is in the other set. It is then easy to verify: quantity $x_i + x_j - 2x_ix_j = 1$ if edge $(i,j)$ has been cut, \& 0 otherwise. WIth help of adjacency matrix $A_{ij}$ with $A_{ij} = 0$ if edge $(i,j)$ does not exist \& $A_{ij} > 0$ if a (possibly weighted) edge connects node $i$ with $j$, MaxCut problem is described by following quadratic Hamiltonian: (7)
    \begin{equation*}
        H_{\rm MaxCut} = \sum_{i < j} A_{ij}(2x_ix_j - x_i - x_j)
    \end{equation*}
    that falls into broader class of QUBO problems described by (1); provide explicit $Q$-matrix for a sample MaxCut problem in {\sf Fig. 2}. Up to an irrelevant constant, MaxCut problem can equivalently by described by compact Ising Hamiltonian $H_{\rm MaxCut} = \sum_{i < j} J_{ij}z_iz_j$ with $J_{ij} = \frac{A_{ij}}{2}$, favoring antiferromagnetic ordering of spins for $J_{ij} > 0$, as expected intuitively based on problem def. As our figure of merit, denote largest cut found as ${\rm cut}^\star = -H_{\rm MaxCut}({\bf x}^\star)$ with ${\bf x}^\star$ referring to corresponding bit string.

    -- {\bf Cắt cực đại.} MaxCut là 1 bài toán tối ưu hóa tổ hợp NP-khó với các ứng dụng thực tế trong lập lịch máy [71], nhận dạng hình ảnh [72], \& thiết kế bố trí mạch điện tử [73]. Trong thời đại hiện tại của các thiết bị lượng tử quy mô trung gian có nhiễu, với sự ra đời của các thuật toán lượng tử-cổ điển lai mới ví dụ như Thuật toán tối ưu hóa xấp xỉ lượng tử (QAOA) [74], bài toán MaxCut gần đây đã thu hút được sự chú ý đáng kể như 1 trường hợp sử dụng tiềm năng của các thiết bị lượng tử được hiệu chỉnh trước lỗi, xem [75--80]. MaxCut là 1 bài toán phân vùng đồ thị được định nghĩa như sau: cho 1 đồ thị với tập đỉnh $V$ \& tập cạnh $E$, hãy tìm 1 phân vùng $V$ thành 2 tập con có cắt cực đại, trong đó 1 cắt đề cập đến các cạnh kết nối 2 nút từ các tập đỉnh khác nhau. Theo trực giác, i.e., ghi 1 điểm bất cứ khi nào 1 cạnh kết nối 2 nút có màu khác nhau. Để xây dựng MaxCut về mặt toán học, hãy đưa vào các biến nhị phân thỏa mãn $x_i = 1$ nếu đỉnh $i$ thuộc tập hợp 1 \& $x_i = 0$ nếu nó thuộc tập hợp còn lại. Sau đó, dễ dàng kiểm tra: đại lượng $x_i + x_j - 2x_ix_j = 1$ nếu cạnh $(i,j)$ đã bị cắt, \& 0 nếu ngược lại. Với sự trợ giúp của ma trận kề $A_{ij}$ với $A_{ij} = 0$ nếu cạnh $(i,j)$ không tồn tại \& $A_{ij} > 0$ nếu 1 cạnh (có thể có trọng số) kết nối nút $i$ với $j$, bài toán MaxCut được mô tả bằng Hamiltonian bậc hai sau: (7)
    \begin{equation*}
        H_{\rm MaxCut} = \sum_{i < j} A_{ij}(2x_ix_j - x_i - x_j)
    \end{equation*}
    thuộc lớp rộng hơn của các bài toán QUBO được mô tả bởi (1); cung cấp ma trận $Q$ rõ ràng cho 1 bài toán MaxCut mẫu trong {\sf Hình 2}. Với 1 hằng số không liên quan, bài toán MaxCut có thể được mô tả tương đương bằng Ising Hamiltonian compact $H_{\rm MaxCut} = \sum_{i < j} J_{ij}z_iz_j$ với $J_{ij} = \frac{A_{ij}}{2}$, ưu tiên sắp xếp spin phản sắt từ cho $J_{ij} > 0$, như dự kiến 1 cách trực quan dựa trên định nghĩa của bài toán. Theo công trạng của chúng ta, hãy ký hiệu vết cắt lớn nhất tìm được là ${\rm cut}^\star = -H_{\rm MaxCut}({\bf x}^\star)$ với ${\bf x}^\star$ tham chiếu đến chuỗi bit tương ứng.

    Complexity of MaxCut depends on regularity \& connectivity of underlying graph. Following an existing trend in community [76] , 1st consider MaxCut problem on random (unweighted) $d$-regular graphs, where every vertex is connected to exactly $d$ other vertices. Perform benchmarks as follows. For graphs with up to a few hundred nodes, compare our GNN-based solver to (approximate) polynomial-time Goemans--Williamson (GW) algorithm [81], which provides current record for an approximate answer within some fixed multiplicative factor of optimum (referred to as approximation ratio $\alpha$), using semidefinite programming \& randomized rounding. Specifically, GW algorithm achieves a guaranteed approximation ratio of $\alpha\sim0.878$ for generic graphs. This lower bound can be raised for specific graphs e.g. unweighted 3-regular graphs where $\alpha\sim0.9326$ [82]. Our implementation of GW algorithm is based on open-source CVXOPT solver, with CVXPY as modeling interface. For very large graphs with up to a million nodes, numerical benchmarks are not available, but can compare our best solution ${\rm cut}^\star$ to an analytical result derived in [83], where shown: with high probability (in limit $n\to\infty$) size of maximum cut for random $d$-regular graphs with $n$ nodes is given by ${\rm cut}^\star = (\frac{d}{4} + P_*\sqrt{\frac{d}{4}} + O(\sqrt{d}))n + O(n)$. Here $P_*\approx0.7632$ refers to an universal constant related to ground-state energy of Sherrington--Kirkpatrick model [84, 85] that can be expressed analytically via Parisi's formula [83]. Thus take ${\rm cut}_{\rm ub} = \left(\frac{d}{4} + P_*\sqrt{\frac{d}{4}}\right)n$ as an upper-bound estimate for maximum cut size in large-$n$ limit. Complement this upper bound with a lower bound as achieved by a simple, randomized 0.5-approximation algorithm that (on average) cuts half of edges, yielding a cut size of ${\rm cut}_{\rm rnd}\approx\frac{d}{4}n$ for a $d$-regular graph with $|E| = \frac{d}{2}n$. Our results for achieved cut size as a function of number of vertice $n$ are shown in {\sf Fig. 4: Numerical results for MaxCut. Left panel; Average cut size for $d$-regular graphs with $d = 3$ \& $d = 5$ as a function of number of vertices $n$, bootstrap-averaged over 20 random graph instances, for both GNN-based method \& Goemans--Williamson (GW) algorithm. On each graph instance, GNN solver is allowed up to 5 shots, \& GW algorithm takes 100 shots. Solid lines for $n\ge10^3$ represent theoretical upper bounds. Inset: Estimated relative approximation ratio defined as $\frac{{\rm cut}^\star}{{\rm cut}_{\rm ub}}$ shows: our approach consistently achieves high-quality solutions. Right panel: Algorithm runtime in secs for both GNN solver \& GW algorithm. Errors bars refer to twice bootstrapped standard deviations, sampled across 20 random graph instances for every data point.} All results are bootstrapped estimates of mean, with error bars denoting twice bootstrapped standard deviations, sampled across 20 random $d$-regular graphs for every data point. For graphs with up to a few hundred nodes, find: a simple 2-layer GCN architecture can perform on par with GW algorithm, while showing a runtime advantage compared to GW starting at around $n\approx100$ nodes. For large graphs with $n\approx10^4$ to $10^6$ nodes, find: our approach consistently achieves high-quality solutions with ${\rm cut}^\star\gtrsim0.9{\rm cut}_{\rm ub}$ for both $d = 3,d = 5$, resp. (i.e., much better than any naive randomized algorithm). As expected for $d$-regular graphs, find ${\rm cut}^\star$ to scale linearly with number of nodes $n$, i.e., ${\rm cut}^\star\approx\gamma_dn$ with $\gamma_3\approx1.28$ \& $\gamma_5\approx1.93$ for $d = 3,d = 5$, resp. Moreover, utilizing modern GPU hardware, observe a favorable runtime scaling at intermediate \& large system sizes that allows us to solve instances with $n = 10^6$ nodes in approximately 10 minutes (which includes both GNN model training \& post-processing steps). Specifically, as shown in {\sf Fig. 4}, observe an approximately linear scaling of total runtime with $\sim n$, for large $d$-regular graphs with $10^5\le n\le10^6$; contrasted with observed GW algorithm scaling as $\sim n^{3.5}$ for problem sizes in range $n\lesssim250$, thereby showing (expected) time complexity $\tilde{O}(n^{3.5})$ of interior-point method (as commonly used for solving semidefinite program underlying GW algorithm) that dominates GW algorithm runtime [86, 87].

    -- Độ phức tạp của MaxCut phụ thuộc vào tính chính quy \& kết nối của đồ thị cơ sở. Theo xu hướng hiện có trong cộng đồng [76], trước tiên hãy xem xét bài toán MaxCut trên đồ thị $d$-chính quy ngẫu nhiên (không có trọng số), trong đó mỗi đỉnh được kết nối với đúng $d$ đỉnh khác. Thực hiện các phép so sánh như sau. Đối với đồ thị có tối đa vài trăm nút, hãy so sánh bộ giải dựa trên GNN của chúng tôi với thuật toán Goemans--Williamson (GW) thời gian đa thức (xấp xỉ) [81], thuật toán này cung cấp bản ghi hiện tại cho 1 câu trả lời gần đúng trong 1 số hệ số nhân cố định của tối ưu (được gọi là tỷ lệ xấp xỉ $\alpha$), sử dụng lập trình bán xác định \& làm tròn ngẫu nhiên. Cụ thể, thuật toán GW đạt được tỷ lệ xấp xỉ được đảm bảo là $\alpha\sim0.878$ cho đồ thị chung. Giới hạn dưới này có thể được nâng lên đối với các đồ thị cụ thể, ví dụ: đồ thị 3-chính quy không có trọng số trong đó $\alpha\sim0.9326$ [82]. Việc triển khai thuật toán GW của chúng tôi dựa trên bộ giải CVXOPT nguồn mở, với CVXPY là giao diện mô hình. Đối với các đồ thị rất lớn có tới 1 triệu nút, các điểm chuẩn số không có sẵn, nhưng có thể so sánh giải pháp tốt nhất của chúng tôi ${\rm cut}^\star$ với kết quả phân tích thu được trong [83], trong đó được hiển thị: với xác suất cao (trong giới hạn $n\to\infty$) kích thước của vết cắt cực đại cho đồ thị $d$-chính quy ngẫu nhiên với $n$ nút được đưa ra bởi ${\rm cut}^\star = (\frac{d}{4} + P_*\sqrt{\frac{d}{4}} + O(\sqrt{d}))n + O(n)$. Ở đây $P_*\approx0.7632$ đề cập đến 1 hằng số phổ quát liên quan đến năng lượng trạng thái cơ bản của mô hình Sherrington--Kirkpatrick [84, 85] có thể được biểu thị 1 cách phân tích thông qua công thức của Parisi [83]. Do đó, hãy lấy ${\rm cut}_{\rm ub} = \left(\frac{d}{4} + P_*\sqrt{\frac{d}{4}}\right)n$ làm ước lượng giới hạn trên cho kích thước cắt tối đa trong giới hạn $n$ lớn. Bổ sung giới hạn trên này bằng giới hạn dưới đạt được bằng thuật toán xấp xỉ 0,5 ngẫu nhiên đơn giản (trung bình) cắt 1 nửa số cạnh, tạo ra kích thước cắt là ${\rm cut}_{\rm rnd}\approx\frac{d}{4}n$ cho đồ thị $d$-chính quy với $|E| = \frac{d}{2}n$. Kết quả của chúng tôi về kích thước cắt đạt được như 1 hàm của số đỉnh $n$ được hiển thị trong {\sf Hình 4: Kết quả số cho MaxCut. Bảng bên trái; Kích thước cắt trung bình cho đồ thị chính quy $d$ với $d = 3$ \& $d = 5$ theo hàm số của số đỉnh $n$, trung bình bootstrap trên 20 trường hợp đồ thị ngẫu nhiên, cho cả phương pháp dựa trên GNN \& thuật toán Goemans--Williamson (GW). Trên mỗi trường hợp đồ thị, bộ giải GNN được phép thực hiện tối đa 5 lần, \& thuật toán GW thực hiện 100 lần. Đường liền cho $n\ge10^3$ biểu diễn các giới hạn trên lý thuyết. Hình chèn: Tỷ lệ xấp xỉ tương đối ước tính được xác định là $\frac{{\rm cut}^\star}{{\rm cut}_{\rm ub}}$ cho thấy: phương pháp của chúng tôi luôn đạt được các giải pháp chất lượng cao. Bảng bên phải: Thời gian chạy thuật toán tính bằng giây cho cả bộ giải GNN \& thuật toán GW. Thanh lỗi đề cập đến độ lệch chuẩn khởi động hai lần, lấy mẫu trên 20 trường hợp đồ thị ngẫu nhiên cho mọi điểm dữ liệu.} Tất cả các kết quả đều là ước tính khởi động của giá trị trung bình, với thanh lỗi biểu thị độ lệch chuẩn khởi động hai lần, lấy mẫu trên 20 đồ thị $d$-chính quy ngẫu nhiên cho mọi điểm dữ liệu. Đối với đồ thị có tối đa vài trăm nút, tìm: kiến trúc GCN 2 lớp đơn giản có thể hoạt động ngang bằng với thuật toán GW, đồng thời thể hiện lợi thế về thời gian chạy so với GW bắt đầu từ khoảng $n\approx100$ nút. Đối với đồ thị lớn có $n\approx10^4$ đến $10^6$ nút, tìm: cách tiếp cận của chúng tôi luôn đạt được các giải pháp chất lượng cao với ${\rm cut}^\star\gtrsim0.9{\rm cut}_{\rm ub}$ cho cả $d = 3,d = 5$, tương ứng (i.e., tốt hơn nhiều so với bất kỳ thuật toán ngẫu nhiên ngây thơ nào). Như mong đợi đối với đồ thị $d$-chính quy, hãy tìm ${\rm cut}^\star$ để chia tỷ lệ tuyến tính với số nút $n$, i.e., ${\rm cut}^\star\approx\gamma_dn$ với $\gamma_3\approx1.28$ \& $\gamma_5\approx1.93$ đối với $d = 3,d = 5$, tương ứng. Hơn nữa, khi sử dụng phần cứng GPU hiện đại, hãy quan sát khả năng mở rộng thời gian chạy thuận lợi ở kích thước hệ thống trung gian \& lớn cho phép chúng ta giải quyết các trường hợp có $n = 10^6$ nút trong khoảng 10 phút (bao gồm cả các bước huấn luyện mô hình GNN \& hậu xử lý). Cụ thể, như thể hiện trong {\sf Hình 4}, hãy quan sát khả năng mở rộng thời gian chạy gần như tuyến tính với $\sim n$, đối với đồ thị $d$-chính quy lớn với $10^5\le n\le10^6$; trái ngược với việc quan sát tỷ lệ thuật toán GW là $\sim n^{3.5}$ đối với kích thước vấn đề trong phạm vi $n\lesssim250$, do đó cho thấy độ phức tạp thời gian (dự kiến) $\tilde{O}(n^{3.5})$ của phương pháp điểm bên trong (thường được sử dụng để giải chương trình bán xác định cơ bản của thuật toán GW) chi phối thời gian chạy thuật toán GW [86, 87].

    . . .

    {\bf Maximum Independent Set.} MIS problem is a prominent combinatorial optimization problem with practical applications in network design [93] \& finance [94], \& is closely related to maximum clique, minimum vertex cover, \& set packing problems. In quantum community, MIS problem has recently attracted significant interest [95] as a potential target use case for novel experimental platforms based on neutral atom arrays [96]. MIS problem reads as follows. Given an undirected graph $G = (V,E)$, an independent set is a subset of vertices that are not connected with each other. MIS problem is then task to find largest independent set, with its (maximum) cardinality typically denoted as independence number $\alpha$. To formulate MIS problem . . .

    \item {\sf6. Applications in industry.} While our previous analysis has focused on canonical graph optimization problems e.g. maximum cut \& maximum independent set, in this section discuss real-world applications in industry for which our solver could provide solutions, in particular at potentially unprecedented problem scales. Focus on applications of QUBO formalism, even though our methodology is not limited to this modeling framework. 1st review existing literature, providing relevant references across a wide stack of problem domains. Thereafter, explicitly show how to distill combinatorial QUBO problems for a few select real-world use cases, from risk diversification in finance to sensor placement problems in water distribution networks. Once in QUBO format, problem can be plugged into our general-purpose physics-inspired GNN solver, as outlined above.

    -- Trong khi phân tích trước đây của chúng tôi tập trung vào các bài toán tối ưu hóa đồ thị chuẩn, ví dụ như cắt cực đại \& tập độc lập cực đại, phần này thảo luận về các ứng dụng thực tế trong công nghiệp mà bộ giải của chúng tôi có thể cung cấp giải pháp, đặc biệt là ở các quy mô bài toán chưa từng có tiền lệ. Tập trung vào các ứng dụng của hình thức QUBO, mặc dù phương pháp luận của chúng tôi không bị giới hạn trong khuôn khổ mô hình này. Trước tiên, hãy xem xét các tài liệu hiện có, cung cấp các tài liệu tham khảo liên quan trên nhiều lĩnh vực bài toán. Sau đó, hãy trình bày rõ ràng cách chắt lọc các bài toán QUBO tổ hợp cho 1 số trường hợp sử dụng thực tế được chọn lọc, từ phân tán rủi ro trong tài chính đến các bài toán đặt cảm biến trong mạng lưới phân phối nước. Khi đã ở định dạng QUBO, bài toán có thể được đưa vào bộ giải GNN lấy cảm hứng từ vật lý đa năng của chúng tôi, như đã nêu ở trên.

    As extensively reviewed in [1,2,66], QUBO (or, equivalently, Ising) formalism provides a comprehensive modeling framework encompassing a vast array of optimization problems, including knapsack problems, task (resource) allocation problems, \& capital budgeting problems, among others. Specifically, applicability of QUBO representation has been reported for problem settings involving circuit board layouts [100], capital budgeting in financial analysis [101], computer aided design (CAD) [102], electronic traffic management [103, 104], cellular radio channel allocation [105], molecular conformation [106], \& prediction of epileptic seizures [107], among others. Practical applications of MaxCut problem can be found in machine scheduling [71], image recognition [72], \& electronic circuit layout design [73]. Similarly, in what follows discuss in detail 3 select use cases \& how they can be cast in QUBO form \& thus made amenable to our solver.

    -- Như đã được xem xét rộng rãi trong [1,2,66], hình thức QUBO (hoặc tương đương là Ising) cung cấp 1 khuôn khổ mô hình hóa toàn diện bao gồm 1 loạt lớn các vấn đề tối ưu hóa, bao gồm các vấn đề ba lô, các vấn đề phân bổ nhiệm vụ (nguồn lực), \& các vấn đề ngân sách vốn, trong số những vấn đề khác. Cụ thể, khả năng áp dụng biểu diễn QUBO đã được báo cáo cho các thiết lập vấn đề liên quan đến bố trí bảng mạch [100], ngân sách vốn trong phân tích tài chính [101], thiết kế hỗ trợ máy tính (CAD) [102], quản lý lưu lượng điện tử [103, 104], phân bổ kênh vô tuyến di động [105], cấu hình phân tử [106], \& dự đoán các cơn động kinh [107], trong số những vấn đề khác. Các ứng dụng thực tế của vấn đề MaxCut có thể được tìm thấy trong lập lịch máy [71], nhận dạng hình ảnh [72], \& thiết kế bố trí mạch điện tử [73]. Tương tự như vậy, trong phần sau sẽ thảo luận chi tiết về 3 trường hợp sử dụng được chọn \& cách chúng có thể được đúc ở dạng QUBO \& do đó phù hợp với trình giải của chúng tôi.

    . . .
    \item {\sf7. Conclusion \& outlook.}
\end{itemize}

%------------------------------------------------------------------------------%

\section{GNNs for Computer Music}

%------------------------------------------------------------------------------%

\subsection{{\sc Matej Bevec, Marko Tkalčič, Matevž Pesek}. Hybrid Music Recommendation with Graph Neural Networks}

\begin{itemize}
    \item {\sf Abstract.} Modern music streaming services rely on recommender systems to help users navigate within their large collections. Collaborative filtering (CF) methods, that leverage past user-item interactions, have been most successful, but have various limitations, like performing poorly among sparsely connected items. Conversely, content-based models circumvent data-sparsity issue by recommending based on item content alone, but have seen limited success. Recently, graph-based ML approaches have shown, in other domains, to be able to address aforementioned issues. GNN in particular promise to learn from both complex relationships within a user interaction graph, as well as content to generate hybrid recommendations. Here propose a music recommender system using a state-of-art GNN, PinSage, \& evaluate it on a novel Spotify dataset against traditional CF, graph-based CF \& content-based methods on a related song prediction task, venturing beyond accuracy in our evaluation. Our experiments show: (i) our approach is among top performers \& stands out as most well rounded compared to baselines, (ii) graph-based CF methods outperform matrix-based CF approaches, suggesting: user interaction data may be better represented as a graph \& (iii) in our evaluation, CF methods do not exhibit a performance drop in long tail, where hybrid approach does not offer an advantage.

    -- {\sf Tóm tắt.} Các dịch vụ phát nhạc trực tuyến hiện đại dựa vào hệ thống đề xuất để giúp người dùng điều hướng trong các bộ sưu tập lớn của họ. Các phương pháp lọc cộng tác (CF), tận dụng các tương tác giữa người dùng \& mục trong quá khứ, đã thành công nhất, nhưng có nhiều hạn chế, chẳng hạn như hoạt động kém giữa các mục được kết nối thưa thớt. Ngược lại, các mô hình dựa trên nội dung tránh được vấn đề thưa thớt dữ liệu bằng cách đề xuất chỉ dựa trên nội dung mục, nhưng đạt được thành công hạn chế. Gần đây, các phương pháp học máy dựa trên đồ thị đã cho thấy, trong các lĩnh vực khác, có thể giải quyết các vấn đề đã đề cập ở trên. Đặc biệt, GNN hứa hẹn sẽ học hỏi từ cả các mối quan hệ phức tạp trong biểu đồ tương tác của người dùng, cũng như nội dung để tạo ra các đề xuất kết hợp. Ở đây, đề xuất 1 hệ thống đề xuất âm nhạc sử dụng GNN hiện đại, PinSage, \& đánh giá nó trên 1 tập dữ liệu Spotify mới so với CF truyền thống, CF dựa trên đồ thị \& các phương pháp dựa trên nội dung trên 1 tác vụ dự đoán bài hát có liên quan, vượt ra ngoài độ chính xác trong đánh giá của chúng tôi. Các thí nghiệm của chúng tôi cho thấy: (i) phương pháp của chúng tôi nằm trong số những phương pháp có hiệu suất cao nhất \& nổi bật là toàn diện nhất so với các phương pháp cơ sở, (ii) các phương pháp CF dựa trên đồ thị vượt trội hơn các phương pháp CF dựa trên ma trận, cho thấy: dữ liệu tương tác của người dùng có thể được biểu diễn tốt hơn dưới dạng đồ thị \& (iii) trong đánh giá của chúng tôi, các phương pháp CF không cho thấy sự sụt giảm hiệu suất trong phần đuôi dài, trong khi phương pháp kết hợp không mang lại lợi thế.

    {\bf Keywords.} embeddings, music recommendation systems, graph neural networks, PinSage, beyond-accuracy evaluation.
    \item {\sf1. Introduction.} In recent years, a large share of music consumption has moved to subscription-based streaming platforms e.g. Spotify, Apple Music, \& YouTube Music. With increasingly large catalogs of songs provided by these services, recommender systems (RS) play a crucial role of matchmaker between content \& users.

    -- Trong những năm gần đây, 1 phần lớn nhu cầu tiêu thụ âm nhạc đã chuyển sang các nền tảng phát trực tuyến trả phí như Spotify, Apple Music \& YouTube Music. Với danh mục bài hát ngày càng phong phú do các dịch vụ này cung cấp, hệ thống đề xuất (RS) đóng vai trò quan trọng trong việc kết nối nội dung \& người dùng.

    Regardless of domain, recommender systems have traditionally been formulated as models that aim to predict which items different users might prefer. In this case, recommendation refers to predicting unknown entries in a user-item rating matrix $R$ where each entry $r_{ij}$ denotes how user $i$ rates item $j$. Modern recommender systems, however, are wide in scope \& deal with many more related tasks. In music domain, these tasks include feed recommendation, playlist generation, related song recommendation, etc.

    -- Bất kể lĩnh vực nào, các hệ thống đề xuất theo truyền thống được xây dựng dưới dạng các mô hình nhằm mục đích dự đoán những mục nào mà người dùng khác nhau có thể thích. Trong trường hợp này, đề xuất đề cập đến việc dự đoán các mục chưa biết trong ma trận đánh giá người dùng-mục $R$, trong đó mỗi mục $r_{ij}$ biểu thị cách người dùng $i$ đánh giá mục $j$. Tuy nhiên, các hệ thống đề xuất hiện đại có phạm vi rộng \& xử lý nhiều tác vụ liên quan hơn. Trong lĩnh vực âm nhạc, các tác vụ này bao gồm đề xuất nguồn cấp dữ liệu, tạo danh sách phát, đề xuất bài hát liên quan, v.v.

    Most widely adopted \& successful category of methods used in recommender systems is collaborative filtering (CF). CF algorithms operate exclusively on user behavior data in form of interaction matrix $R$, drawing from assumption that similar users prefer similar items (Xiaoyuan \& Khoshgoftaar 2009). These methods are still among top performers in terms of recommendation accuracy. They also carry many favorable characteristics, e.g. domain-independence, since no knowledge about content, only user data, is needed. They do, however, suffer from some crucial limitations. One is data sparsity problem, which refers to fact: CF algorithms cannot provide reliable recommendations songs with insufficient (sparse) interaction data -- a situation that is particularly common in music catalogs.

    -- Loại phương pháp được áp dụng rộng rãi \& thành công nhất trong các hệ thống đề xuất là lọc cộng tác (CF). Các thuật toán CF hoạt động hoàn toàn trên dữ liệu hành vi người dùng dưới dạng ma trận tương tác $R$, dựa trên giả định rằng những người dùng tương tự sẽ thích các mục tương tự (Xiaoyuan \& Khoshgoftaar 2009). Các phương pháp này vẫn nằm trong số những phương pháp có hiệu suất cao nhất về độ chính xác đề xuất. Chúng cũng mang nhiều đặc điểm thuận lợi, ví dụ như độc lập với miền, vì không cần kiến thức về nội dung, chỉ cần dữ liệu người dùng. Tuy nhiên, chúng cũng gặp phải 1 số hạn chế quan trọng. 1 trong số đó là vấn đề thưa thớt dữ liệu, ám chỉ thực tế: các thuật toán CF không thể cung cấp các đề xuất đáng tin cậy cho các bài hát khi dữ liệu tương tác không đủ (thưa thớt) -- 1 tình huống đặc biệt phổ biến trong các danh mục âm nhạc.

    A different approach to recommendations is content-based filtering (CBF). CBF methods compute similarities between items or users based solely on their content, ignoring user-item interactions. In field of music recommendations systems (MRS), ``content'' in CBF usually refers to metadata e.g. genre, artist information or lyrics (Lops et al. 2011; Schedl 2019), while music information retrieval (MIR) uses ``content'' to refer to raw music audio \& has produced many models that extract features from audio to effectively solve tasks, e.g. genre classification, emotion detection or instrument recognition (Kim et al. 2010; Cramer et al. 2019; Choi et al. 2018). Such approaches do not suffer from cold-start \& data-sparsity issues but in general give less accurate recommendations. Metadata tends to be too broad to sufficiently model user taste (e.g., genre) \& gives subpar recommendations. Conversely, music audio itself is a rich source of information with potential to directly recommend music that is perceptually in line with user's taste. Research in this domain, however, faces semantic gap -- a substantial chasm between raw audio signal \& perceptual music characteristics that affect human preference Celma Herrada et al. 2006). As such, direct application of CBF to music recommender systems has been limited.

    -- 1 cách tiếp cận khác đối với các đề xuất là lọc dựa trên nội dung (CBF). Các phương pháp CBF tính toán điểm tương đồng giữa các mục hoặc người dùng chỉ dựa trên nội dung của chúng, bỏ qua tương tác giữa người dùng \& mục. Trong lĩnh vực hệ thống đề xuất âm nhạc (MRS), ``nội dung'' trong CBF thường đề cập đến siêu dữ liệu, ví dụ như thể loại, thông tin nghệ sĩ hoặc lời bài hát (Lops \& cộng sự, 2011; Schedl, 2019), trong khi truy xuất thông tin âm nhạc (MIR) sử dụng ``nội dung'' để chỉ âm thanh nhạc thô \& đã tạo ra nhiều mô hình trích xuất các tính năng từ âm thanh để giải quyết hiệu quả các tác vụ, ví dụ như phân loại thể loại, phát hiện cảm xúc hoặc nhận dạng nhạc cụ (Kim \& cộng sự, 2010; Cramer \& cộng sự, 2019; Choi \& cộng sự, 2018). Các cách tiếp cận như vậy không gặp phải các vấn đề về khởi động lạnh \& dữ liệu thưa thớt nhưng nhìn chung đưa ra các đề xuất kém chính xác hơn. Siêu dữ liệu có xu hướng quá rộng để mô hình hóa đầy đủ sở thích của người dùng (ví dụ: thể loại) \& đưa ra các đề xuất kém chất lượng. Ngược lại, bản thân âm nhạc là 1 nguồn thông tin phong phú với tiềm năng đề xuất trực tiếp âm nhạc phù hợp với thị hiếu của người dùng. Tuy nhiên, nghiên cứu trong lĩnh vực này đang gặp phải khoảng cách ngữ nghĩa - 1 khoảng cách đáng kể giữa tín hiệu âm thanh thô \ \& các đặc điểm âm nhạc cảm nhận được, ảnh hưởng đến sở thích của con người (Celma Herrada \& cộng sự, 2006). Do đó, việc áp dụng trực tiếp CBF vào các hệ thống đề xuất âm nhạc còn hạn chế.

    Limitations of pure CF \& CBF have lead researchers in direction of hybrid recommender systems, which leverage both interaction data \& content in an attempt to mitigate shortcomings of exclusively using one or the other. This can be done in various ways, e.g. combining features learned from CF with content-based features (Vall et al. 2019) or by using content to guide CF process (Liang et al. 2015).

    -- Những hạn chế của CF thuần túy \& CBF đã dẫn dắt các nhà nghiên cứu đến các hệ thống đề xuất lai, tận dụng cả dữ liệu tương tác \& nội dung để giảm thiểu những hạn chế của việc chỉ sử dụng 1 trong hai. Điều này có thể được thực hiện theo nhiều cách, ví dụ: kết hợp các đặc điểm học được từ CF với các đặc điểm dựa trên nội dung (Vall \& cộng sự, 2019) hoặc bằng cách sử dụng nội dung để hướng dẫn quy trình CF (Liang \& cộng sự, 2015).

    Another rapidly growing branch of ML is ML with graphs. Graphs have always been a highly expressive way to represent relational data, but only recent advances in field have managed to truly utilize these structures in a ML context. Spearheaded by GNN -- DL architectures that operate directly on graphs -- graph-based methods now provide better solutions to relational tasks e.g. predicting protein--protein interactions, physical systems modeling or suggesting friends in social networks (Hamilton et al. 2017b; Wu et al. 2021).

    -- 1 nhánh phát triển nhanh chóng khác của ML là ML với đồ thị. Đồ thị luôn là 1 cách biểu diễn dữ liệu quan hệ rất biểu cảm, nhưng chỉ những tiến bộ gần đây trong lĩnh vực này mới thực sự tận dụng được các cấu trúc này trong bối cảnh ML. Dẫn đầu bởi kiến trúc GNN -- DL hoạt động trực tiếp trên đồ thị -- các phương pháp dựa trên đồ thị hiện cung cấp các giải pháp tốt hơn cho các tác vụ quan hệ, ví dụ như dự đoán tương tác protein-protein, mô hình hóa hệ thống vật lý hoặc gợi ý bạn bè trên mạng xã hội (Hamilton \& cộng sự, 2017b; Wu \& cộng sự, 2021).

    In present study, aim to inspect utility of this new family of methods in context of music recommendation, by applying them to a fundamental MRS task of {\it related song recommendation} (i.e., prediction of ground-truth similar pairs of songs). Consider this is a natural 1st task since a good model of similarity can be, \& often is, basis for various more complex tasks.

    -- Trong nghiên cứu hiện tại, mục tiêu là kiểm tra tính hữu dụng của nhóm phương pháp mới này trong bối cảnh đề xuất âm nhạc, bằng cách áp dụng chúng vào nhiệm vụ MRS cơ bản là {\it đề xuất bài hát liên quan} (i.e., dự đoán các cặp bài hát tương tự trên thực tế). Hãy coi đây là nhiệm vụ đầu tiên tự nhiên vì 1 mô hình tương đồng tốt có thể, \& thường là, cơ sở cho nhiều nhiệm vụ phức tạp hơn.

    Considering success of graph-based approaches on many tasks which deal with relational data, our research question: {\it Can ML with graphs, \& GNN in particular, address shortcomings of previous CF \& CBF approaches in context of music recommendation?}

    -- Xem xét thành công của các phương pháp tiếp cận dựa trên đồ thị trên nhiều nhiệm vụ liên quan đến dữ liệu quan hệ, câu hỏi nghiên cứu của chúng tôi: {\it liệu ML với đồ thị, \& GNN nói riêng, có thể giải quyết những thiếu sót của các phương pháp CF \& CBF trước đây trong bối cảnh đề xuất âm nhạc không?}

    Music recommendation can be translated to graph domain by representing a user--song matrix or a set of user-created playlists as a bipartite graph of songs \& users or playlists, where links denote interaction or membership. By translating user interaction information to graph topology, graph-based methods can model complex relations between items \&{\tt/}or users that matrix-based CF approaches might struggle with. {\it Therefore conjecture: both hybrid methods (GNN) \& graph-based CF methods outperform traditional matrix-based approaches}.

    -- Đề xuất âm nhạc có thể được chuyển đổi sang miền đồ thị bằng cách biểu diễn ma trận bài hát - người dùng hoặc 1 tập hợp danh sách phát do người dùng tạo dưới dạng đồ thị hai phần gồm bài hát \& người dùng hoặc danh sách phát, trong đó các liên kết biểu thị tương tác hoặc thành viên. Bằng cách chuyển đổi thông tin tương tác của người dùng sang cấu trúc đồ thị, các phương pháp dựa trên đồ thị có thể mô hình hóa các mối quan hệ phức tạp giữa các mục \&{\tt/}hoặc người dùng mà các phương pháp CF dựa trên ma trận có thể gặp khó khăn. {\it Do đó, có thể suy đoán: cả phương pháp lai (GNN) \& phương pháp CF dựa trên đồ thị đều vượt trội hơn các phương pháp dựa trên ma trận truyền thống}.

    Additionally, \& unlike other hybrid approaches, GNNs are also intrinsically hybrid -- they form node embeddings by aggregating neighboring nodes' features based on graph topology. I.e., nodes representing songs can be associated with audio embeddings to produce hybrid song representations, e.g. {\it Therefore conjecture: hybrid method, GNN, provides more accurate recommendation than CBF, while being more successful than CF in handling sparse data}.

    -- Ngoài ra, \& không giống như các phương pháp lai khác, GNN cũng mang tính lai về bản chất -- chúng tạo ra các nhúng nút bằng cách tổng hợp các đặc điểm của các nút lân cận dựa trên cấu trúc đồ thị. Ví dụ, các nút biểu diễn bài hát có thể được liên kết với các nhúng âm thanh để tạo ra các biểu diễn bài hát lai, ví dụ: {\it Do đó, có thể suy đoán: phương pháp lai, GNN, cung cấp khuyến nghị chính xác hơn CBF, đồng thời thành công hơn CF trong việc xử lý dữ liệu thưa thớt}.

    While accuracy is how recommenders are usually accessed, it has been pointed out by many: these metrics do not reflect all characteristics of recommendations that may be relevant in a real-world setting. Following this rationale, also study our results from perspective of beyond-accuracy objectives \& {\it conjecture: looking beyond accuracy will provide an alternative picture in terms of ranking methods' performance}.

    -- Mặc dù độ chính xác thường là cách tiếp cận người đề xuất, nhưng nhiều người đã chỉ ra rằng: các số liệu này không phản ánh tất cả các đặc điểm của đề xuất có thể liên quan trong bối cảnh thực tế. Dựa trên cơ sở lý luận này, hãy nghiên cứu kết quả của chúng tôi từ góc độ các mục tiêu vượt ra ngoài độ chính xác \& {\it giả thuyết: nhìn xa hơn độ chính xác sẽ cung cấp 1 bức tranh khác về hiệu suất của các phương pháp xếp hạng}.

    In present study, apply a state-of-art GNN method, PinSage (Ying et al. 2018) \& other graph-based approaches to public Spotify data to explore utility of this paradigm in space of hybrid music recommendation. Our contributions can be summarized as follows:
    \begin{itemize}
        \item Implementation of a state-of-art GNN algorithm (PinSage) in role of a hybrid music recommender, including an ablation study, \& as such, an application of ML with graphs to field of MRS.
        \item An evaluation, in terms of recommendation accuracy, of various graph-based methods on real \& current music consumption data against content-based methods (CBF) \& user-data-based methods, including traditional CF, as well as graph-based approaches.
        \item An additional analysis of considered methods in terms of {\it beyond-accuracy objectives}, with an aim to quantify characteristics of recommender systems in a more holistic way.
    \end{itemize}
    -- Trong nghiên cứu này, chúng tôi áp dụng phương pháp GNN tiên tiến, PinSage (Ying \& cộng sự, 2018) \& các phương pháp tiếp cận dựa trên đồ thị khác vào dữ liệu Spotify công khai để khám phá tính hữu dụng của mô hình này trong lĩnh vực đề xuất âm nhạc kết hợp. Đóng góp của chúng tôi có thể được tóm tắt như sau:
    \begin{itemize}
        \item Triển khai thuật toán GNN tiên tiến (PinSage) trong vai trò là công cụ đề xuất âm nhạc kết hợp, bao gồm nghiên cứu cắt bỏ, \& do đó, ứng dụng ML với đồ thị vào lĩnh vực MRS.
        \item Đánh giá, về độ chính xác của đề xuất, các phương pháp dựa trên đồ thị khác nhau trên dữ liệu tiêu thụ âm nhạc thực tế \& hiện tại so với các phương pháp dựa trên nội dung (CBF) \& các phương pháp dựa trên dữ liệu người dùng, bao gồm CF truyền thống, cũng như các phương pháp dựa trên đồ thị.
        \item Phân tích bổ sung về các phương pháp được xem xét theo {\it các mục tiêu vượt ra ngoài độ chính xác}, nhằm mục đích định lượng các đặc điểm của hệ thống đề xuất 1 cách toàn diện hơn.
    \end{itemize}
    To aid further research on intersection between ML with graphs \& MRS, publish full experimental code \url{https://github.com/MatejBevec/gcn-song-embeddings}, including PinSage implementation. Also release our novel dataset, a large-scale bipartite playlist-song membership graph, which name {\it Spotify Graph}.

    -- Để hỗ trợ nghiên cứu sâu hơn về giao điểm giữa ML với đồ thị \& MRS, hãy công bố toàn bộ mã thử nghiệm \url{https://github.com/MatejBevec/gcn-song-embeddings}, bao gồm cả triển khai PinSage. Đồng thời, chúng tôi cũng phát hành bộ dữ liệu mới của mình, 1 đồ thị thành viên bài hát-danh sách phát hai phần quy mô lớn, có tên là {\it Spotify Graph}.
    \item {\bf2. Related work.} As our study builds on previous work in both recommender systems \& graph-based ML, survey both \& position our work in this landscape.

    -- Vì nghiên cứu của chúng tôi dựa trên công trình trước đây về cả hệ thống đề xuất \& ML dựa trên đồ thị, hãy khảo sát cả \& định vị công trình của chúng tôi trong bối cảnh này.
    \begin{itemize}
        \item {\sf2.1. Music recommendation: tasks \& challenges.} Music recommender systems (MRS) deal with similar tasks as all recommender systems (RS). However, due to particularities of music as a medium, e.g. short duration of items, sequential consumption, scale of collections, or a highly subjective \& variable listening intent, they face some unique challenges.

        -- {\sf Đề xuất âm nhạc: nhiệm vụ \& thách thức.} Hệ thống đề xuất âm nhạc (MRS) xử lý các nhiệm vụ tương tự như tất cả các hệ thống đề xuất (RS). Tuy nhiên, do đặc thù của âm nhạc như 1 phương tiện, ví dụ như thời lượng ngắn của các mục, mức độ tiêu thụ tuần tự, quy mô bộ sưu tập hoặc ý định nghe nhạc rất chủ quan \& biến động, chúng phải đối mặt với 1 số thách thức riêng.
    \end{itemize}
    \item {\sf3. Theoretical framework.}
    \begin{itemize}
        \item {\sf3.1. Related song recommendation task.} Since our study explores utility of graph-based methods in MRS from a broad perspective, devise our experiment around 1 of simplest yet most fundamental tasks in music recommendation -- {\it similar song prediction}. I.e., finding similar music to a single seed (query) song, without any input about user or context. This has advantage of both being able to be framed in language of vastly different algorithms we test, \& of being widely applicable to more complex tasks. E.g., it can be used for playlist completion by fetching similar songs to existing set with some randomness.

        -- {\sf Nhiệm vụ đề xuất bài hát liên quan.} Vì nghiên cứu của chúng tôi khám phá tính hữu dụng của các phương pháp dựa trên đồ thị trong MRS từ góc nhìn rộng, hãy thiết kế thử nghiệm của chúng tôi xoay quanh 1 trong những nhiệm vụ đơn giản nhưng cơ bản nhất trong đề xuất âm nhạc -- {\it dự đoán bài hát tương tự}. Cụ thể là tìm kiếm những bài hát tương tự với 1 bài hát gốc (truy vấn) duy nhất, mà không cần bất kỳ thông tin đầu vào nào về người dùng hoặc ngữ cảnh. Điều này có lợi thế là có thể được diễn đạt bằng ngôn ngữ của các thuật toán rất khác nhau mà chúng tôi thử nghiệm, \& có thể áp dụng rộng rãi cho các nhiệm vụ phức tạp hơn. Ví dụ: nó có thể được sử dụng để hoàn thành danh sách phát bằng cách tìm nạp các bài hát tương tự vào bộ bài hát hiện có với 1 số tính ngẫu nhiên.

        Opt to use {\it consecutive song plays in listening sessions} ax proxy for similarity. More specifically, assume: songs $q,i$ should be considered similar, if they often appear together in listening sessions. While this may not be an ideal analogy, it is data we find available \& is inspired by PinSage paper (Ying et al. 2018).

        -- Chọn sử dụng {\it bài hát được phát liên tiếp trong các buổi nghe} ax proxy cho tính tương đồng. Cụ thể hơn, giả sử: các bài hát $q,i$ nên được coi là tương đồng, nếu chúng thường xuất hiện cùng nhau trong các buổi nghe. Mặc dù đây có thể không phải là phép so sánh lý tưởng, nhưng đây là dữ liệu chúng tôi tìm thấy \& được lấy cảm hứng từ bài báo của PinSage (Ying \& cộng sự, 2018).

        Algorithms do have access to another source of information, playlist membership graph, but only in training. This is not part of evaluation \& can be seen as background information. This choice, as well, as partially inspired by experiment design in PinSage study. Below is a theoretical definition of proposed experiment. Consider following data:
        \begin{enumerate}
            \item A playlist-song graph $G$, represented as adjacency matrix $G\in\mathbb{R}^{n\times m}$, where each entry $g_{ip}$ is a Boolean value, indicating whether song $i$ is present in playlist $p$.
            \item Audio-content features $F$ for each song, represented as matrix $F\in\mathbb{R}^{n\times d}$, where row $F_i$ is a $d$-dimensional dense embedding, representing 30s audio excerpt of song $i$.
            \item A set of song co-occurrences $P$, consecutive song plays from users' listening sessions. Name these ``positive pairs'' \& represent them as matrix $P\in\mathbb{R}^{n\times n}$ where $p_{ij}$ counts number of times song $i,j$ appear in such a pair.
        \end{enumerate}
        Our related song recommendation task is then defined as follows: {\bf Predict song co-occurrences in $P$, i.e., given a pair $(q,i):P_{q,i} > 0$, an algorithm should include $i$ among its recommendations $r_q$ to query $q$.}

        -- Các thuật toán có thể truy cập vào 1 nguồn thông tin khác, biểu đồ thành viên danh sách phát, nhưng chỉ trong quá trình đào tạo. Đây không phải là 1 phần của quá trình đánh giá \& có thể được xem là thông tin cơ bản. Lựa chọn này cũng được lấy cảm hứng 1 phần từ thiết kế thử nghiệm trong nghiên cứu của PinSage. Dưới đây là định nghĩa lý thuyết về thử nghiệm được đề xuất. Xem xét dữ liệu sau:
        \begin{enumerate}
            \item Đồ thị danh sách phát-bài hát $G$, được biểu diễn dưới dạng ma trận kề $G\in\mathbb{R}^{n\times m}$, trong đó mỗi mục $g_{ip}$ là 1 giá trị Boolean, cho biết bài hát $i$ có trong danh sách phát $p$ hay không.
            \item Đặc trưng nội dung âm thanh $F$ cho mỗi bài hát, được biểu diễn dưới dạng ma trận $F\in\mathbb{R}^{n\times d}$, trong đó hàng $F_i$ là 1 nhúng dày đặc $d$ chiều, biểu diễn đoạn trích âm thanh 30 giây của bài hát $i$.
            \item 1 tập hợp các bài hát đồng xuất hiện $P$, được phát liên tiếp từ các phiên nghe nhạc của người dùng. Đặt tên cho các ``cặp dương'' này \& biểu diễn chúng dưới dạng ma trận $P\in\mathbb{R}^{n\times n}$, trong đó $p_{ij}$ đếm số lần bài hát $i,j$ xuất hiện trong 1 cặp như vậy.
        \end{enumerate}
        Nhiệm vụ đề xuất bài hát liên quan của chúng ta sau đó được định nghĩa như sau: {\bf Dự đoán các bài hát đồng xuất hiện trong $P$, tức là, với 1 cặp $(q,i):P_{q,i} > 0$, thuật toán nên bao gồm $i$ trong số các đề xuất $r_q$ của nó để truy vấn $q$.}

        To this end, all tested methods utilize 1 or more of above data sources to produce a similarity function $sim(i,j)$ between songs $i,j$. Embedding methods 1st produce dense embedding $V\in\mathbb{R}^{n\times d}$ \& compute $sim(i,j)$ as cosine similarity between 2 embedding vectors $cosine(V_i,V_j)$. Recommendations to $q$ are then obtained by querying $k$ nearest neighbors to $q$ w.r.t. $sim$. Sect. 4 for information about particular data \& methods used.

        -- Để đạt được mục đích này, tất cả các phương pháp đã thử nghiệm đều sử dụng 1 hoặc nhiều nguồn dữ liệu trên để tạo ra hàm tương tự $sim(i,j)$ giữa các bài hát $i,j$. Các phương pháp nhúng đầu tiên tạo ra nhúng dày đặc $V\in\mathbb{R}^{n\times d}$ \& tính $sim(i,j)$ dưới dạng độ tương tự cosin giữa 2 vectơ nhúng $cosine(V_i,V_j)$. Sau đó, các khuyến nghị cho $q$ được thu thập bằng cách truy vấn $k$ lân cận gần nhất của $q$ so với $sim$. Xem Mục 4 để biết thông tin về dữ liệu cụ thể \& các phương pháp được sử dụng.
        \item {\sf3.2. PinSage.} At its core, PinSage is a GCN-based hybrid node embedding algorithm, which, compared to previous GCN approaches, vastly improves scalability of system, as well as quality of learned embeddings. This is mainly due to a new way of computing convolutions \& an improved training procedure. In our work, follow (Ying et al. 2018) to implement a simplified version of the algorithm. See PinSage as a state-of-art GCN methods \& a reasonable choice to explore potential of hybrid graph-based methods in music recommendation. As such, primarily interested in its promises of improved recommendation quality, rather than scalability needed at production scale. Following sects briefly describe model architecture \& training procedure. If this is of no interest, advise reader to skip Sect. 4.
    \end{itemize}
\end{itemize}


%------------------------------------------------------------------------------%

\section{Miscellaneous}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]

\end{document}